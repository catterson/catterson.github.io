[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Arman Daniel Catterson [website version]",
    "section": "",
    "text": "Psych 101 [Introduction to Psychology]\nPsych 214 [Statistics in Psychology]\nPsych 215 [Research Methods in Psychology]\nPersian Club Faculty Advisor\n\n\n\n\n\nPsych 101 [Undergraduate Statistics and Research Methods]\nPsych 102 [Undergraduate Statistics and Research Methods]\nPsych 205 [Graduate Statistics]"
  },
  {
    "objectID": "index.html#uc-berkeley",
    "href": "index.html#uc-berkeley",
    "title": "Arman Daniel Catterson [website version]",
    "section": "",
    "text": "Psych 101 [Undergraduate Statistics and Research Methods]\nPsych 205 [Graduate Statistics]"
  },
  {
    "objectID": "index.html#dvc-on-sabbatical-fa-24---sp25",
    "href": "index.html#dvc-on-sabbatical-fa-24---sp25",
    "title": "Arman Daniel Catterson [website version]",
    "section": "",
    "text": "Psych 101 [Introduction to Psychology]\nPsych 214 [Statistics in Psychology]"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Arman Daniel Catterson [website version]",
    "section": "",
    "text": "I teach statistics and psychology at Diablo Valley College (as a professor with my own office and health insurance and tenure) and UC Berkeley (as a continuing lecturer who is not required to attend meetings or serve on committees). Thank you for visiting my webpage."
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "Arman Daniel Catterson [website version]",
    "section": "",
    "text": "Wave hi if you see me on campus\nTalk to me after class\nE-mail me using the address in our course syllabus."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "gradstats.html",
    "href": "gradstats.html",
    "title": "Psych 205 (Graduate Statistcs)",
    "section": "",
    "text": "Psych 205 (Graduate Statistcs)\nHello class! Here‚Äôs a link to our course materials.\n\n\n\nWeek\nLecture Notes\nLab\n\n\n\n\n1\nWelcome\n\n\n\n2\nData\n\n\n\n3"
  },
  {
    "objectID": "index.html#uc-berkeley-continuing-lecturer",
    "href": "index.html#uc-berkeley-continuing-lecturer",
    "title": "Arman Daniel Catterson [website version]",
    "section": "",
    "text": "Psych 101 [Undergraduate Statistics and Research Methods]\nPsych 102 [Undergraduate Statistics and Research Methods]\nPsych 205 [Graduate Statistics]"
  },
  {
    "objectID": "index.html#dvc-full-professor-on-sabbatical-fa-24---sp25",
    "href": "index.html#dvc-full-professor-on-sabbatical-fa-24---sp25",
    "title": "Arman Daniel Catterson [website version]",
    "section": "",
    "text": "Psych 101 [Introduction to Psychology]\nPsych 214 [Statistics in Psychology]\nPsych 215 [Research Methods in Psychology]\nPersian Club Faculty Advisor"
  },
  {
    "objectID": "gradstats/gradstats.html",
    "href": "gradstats/gradstats.html",
    "title": "Psych 205 (Graduate Statistics - SP25)",
    "section": "",
    "text": "Hi class! Welcome to our class.\n\n\n\nProfessor | Arman Daniel Catterson, PhD [he / his / ÿßŸà ] üê±(catterson@berkeley.edu)\n\nLectures : Fridays 9:00 - 12:00 PM in 1213 Berkeley Way West\nOffice Hours : Mondays 10:30 - 11:30 AM on Zoom (e-mail / chat after class to find another time)\n\nTeaching Assistant | Sierra Semko Krouse [she / her] | sierra_semko@berkeley.edu\n\nSections : Fridays, 1:00 - 2:00 PM in 1213 Berkeley Way West\nOffice Hours : Wednesdays 2:00 - 3:00 PM via Zoom (click here to sign-up for a 15-minute slot)\n\n\n\n\n\n\n\n\nWeek\nLecture Topic\nAssignment\nSupport Readings\n\n\n\n\n1/24\nIntroductions [RScript]\nLab 1 [.qmd] + Key [.pdf]\nWS 1-2; LSR 3-4\n\n\n1/31\nDescription\nLab 2\nWS 3-4; LSR 5-6\n\n\n2/7\nVisualizing Data in ggplot2\nLab 3\nData Viz Chapter\n\n\n2/14\nProbability and Sampling Error\nLab 4\nWS 8; LSR 9-10\n\n\n2/21\nMini Exam + Linear Models Pt. 1 (Lines!)\n\nWS 5-7; LSR 15.1 - 15.2\n\n\n2/28\nLinear Models Pt. 2 (Tests & Assumptions)\nLab 5\nLSR 15.5 - 15.11\n\n\n3/7\nLinear Models Pt. 3 (Multiple Regression)\nLab 6\nWS 9; LSR 15.3\n\n\n3/14\nLinear Models Pt. 4 (Logistic / GLM)\nLab 7\nGeneralized Linear Models\n\n\n3/21\nReview / Catch-Up :)\nLab 8\n\n\n\n3/28\nSpring Break!!!\n\n\n\n\n4/4\nMega Exam\nMilestone 1\n\n\n\n4/11\nLinear Models Pt. 5a (Mixed Models)\nLab 9\nMixed Models with R\n\n\n4/18\nLinear Models Pt. 5b (Mixed Models)\nMilestone 2\nMixed Models with R\n\n\n4/25\nPCA / FA\nLab 10\nCore SEM\n\n\n5/2\nConclusion / Project Workshop\nProject Due\n\n\n\n\nNote : Support Readings are assigned from my undergraduate text Why Statistics? (WS) and https://learningstatisticswithr.com/book/ (LSR) for students looking for more support.\n\n\n\nYour grade in the class will be based on the following components.\n\n\n\n\n\n\n\n\n10% Attendance\n30% R Exams [10 + 20]\n10% Article Presentations\n\n\n20% Lab Assignments\n20% Final Project\n10% Article Discussions\n\n\n\nLetter grades will be based on the following : A+ is &gt; 96.5; A is &gt; 92.5; A- is &gt; 89.5; etc. Note that an 89.49999 is a B+.)\n\n\n\n\nAttendance (Lecture and Discussion Section): Because we all learn from each other; attendance is required. You may miss up to two lectures and two discussion sections with no penalty.\nArticle Discussion. Each week, students will be responsible for leading class discussion of one article. All students should plan to read the article, and post comments to a discussion forum on bCourses. See the [Student Presentation | Description and Rubric] for a list of articles, a link to sign up, and a description of what is required for these student-led presentations. Let me know if you have other ideas for ways to engage the class in discussion about an article (or another article you think you would be good for the class to read.)\nLab Assignments : Most weeks, you will work through a ‚Äúlab‚Äù assignment that is designed to help practice the skills we review in lecture and readings. You can expect to work on these in lecture, in your discussion section, and at home as needed. Each lab document will have a set of questions to answer - you should submit answers to these questions as a separate document to bCourses. Lab assignments are due the week after they are assigned; late lab assignments will not be accepted. You can drop your lowest submitted lab assignment.\nFinal Project (Research Paper) : You‚Äôll apply the skills you have learned this semester to analyze, interpret, and share results from a research question relevant to your research interests. There will be several milestones to support this assignment. See the [Final Project | Description and Rubric] for more information.\nR Exam : For each exam, I‚Äôll give you a novel dataset and you‚Äôll use R to answer questions based on the dataset. The exam will be open-book and open note - you must work alone, but may use any resource available to you (including the Internet). A practice exam will be posted at least one week before the exam. The mini exam should take no more than 50 minutes to complete (we will have class afterward), and you will have the full 3-hours to complete the mega exam.\nExtra Credit : There will be no extra credit offered in this course, even for students who ask kindly.¬†\n\n\n\n\n\nComputer Access. Access to a personal computer is strongly recommended. We will spend time each lecture working in R - a free and powerful statistics program that works with all operating systems and computers. Laptops are available for rent from the library (see Berkeley‚Äôs Technology Loan Program), or you can purchase one that will work for this class for as little as $150 (less than the price of most textbooks, which is not required for this class).\nLate Work Policy. Late assignments will not be accepted. You may drop your lowest scoring lab assignment. Let me know if you cannot attend the exam and we can schedule a make-up exam.\nDiscussion Section Policy : In-person attendance is expected and required for discussion sections. Students may miss up to two discussion sections with no penalty. Please talk to your GSI / the professor if there is a longer-term illness or issue that will prevent you from regularly attending your discussion section and we will work out another arrangement.\nRegrades. Students may ask for a re-grade on exams and papers if they believe they lost points for something they should have earned. To request a regrade, talk to your GSI in office hours or set up a meeting. In the case that you and your GSI cannot resolve the regrade issue, the professor will step in and regrade the exam. For final project regrades just e-mail the professor. When requesting a regrade, you consent to have your score increased or decreased if we find that you earned points you should not have earned.\nStudent Contact. It is your responsibility to regularly check e-mail for announcements. If you have specific questions to e-mail me, please make sure to (a) search the syllabus and / or Google for the answer to your question and (b) contact your GSI about your question. I am always happy to answer any questions in office hours (either during the scheduled time or online by appointment), or before, during, or after lecture. I should respond to e-mail within 24 hours on weekdays (I do not check e-mail on weekends); please send a gentle reminder if I do not respond in this timeframe. Thanks!\nAcademic Integrity. Do NOT cheat. Do NOT plagiarize. To copy text or ideas from another source (including your own submitted coursework) without appropriate reference is plagiarism. You may work on lab assignments with other students, but should be able to understand what you are doing. You may not work with other students on the R exam. Anyone caught cheating or plagiarizing will receive a zero on the assignment or test, and will be reported to the Office of Student Conduct. It is your responsibility to read the official Student Conduct Policy for more information about campus standards and policies regarding Academic Integrity.\nChat-GPT / AI Policy. If you use ChatGPT or other generative-AI software for any part of this class (writing, coding, troubleshooting, etc.), you must make this clear at the top of your assignment by writing : ‚ÄúI USED CHAT GPT TO SUPPORT THIS ASSIGNMENT‚Äù (in all-caps and red text), and include an appendix where you include screenshots of your specific queries and answers. Failure to do this will result in a zero of the assignment, and the professor submitting a report to the Academic Integrity Office. Ask your GSI / professor if you have questions about this policy.\nRespect for Others. We all belong in our classroom, and I hope that everyone feels free and supported to express our identities when relevant. To achieve this goal, make sure that you are treating others with respect in this course. This means considering others‚Äô opinions and perspectives, making sure not to generalize to groups (or ask students to be representatives of their group), and being mindful of the words that you use. (Also remember that nothing is ever really deleted from the internet, so please be extra mindful of the words you use.) While I will be monitoring class posts and communication, feel free to reach out to me if you see another student engaging in disrespectful or threatening behavior. Of course, this extends to me as well - if you feel like there are parts of the course instruction, subject matter, or class environment that create barriers to your success or inclusion, please contact me via e-mail and trust that your voice will be heard and I will not punish you for offering constructive criticisms of my teaching. I‚Äôm very open to learning how to be a better teacher, and always learn a lot from my students.\nSensitive Subjects. Our class will touch on important and potentially sensitive topics that are related to psychology and psychological processes, such as racism, depression, sex, and poverty. These topics can generate strong emotional responses in students for a variety of reasons - such as their own past histories with these topics or their reactions to the ways the information is presented or described. I will try to make sure to give you a heads up when we start to discuss these topics so you can be mindful of your reactions.\nStrategies for success. Do your readings, complete assignments on time, understand how the topics and information relate to what you‚Äôve learned (and are learning), and if you don‚Äôt understand something, please ask the question!\nMost important. Please let me know as soon as there is anything going on in your life that you think may affect your ability to do as well as you would like in this class (e.g.¬†sickness, work, small children at home, threats to immigration status, etc.), and I will do my best to work with you on a plan to succeed in the class. If you‚Äôre not comfortable talking to me directly, you can talk to your GSI who can talk to me about your issue.\n\n\n\n\n\nStudents with Disabilities : Students who require support or adaptive equipment because of a specific disability ‚Äì or would like to be tested to see if they qualify for a learning, auditory, or visual disability ‚Äì can request these services through the Disabled Students Program (DSP) office. Please note that I can only provide accommodations authorized by the DSP office. If you have DSP status, make sure you submit your letter and feel free to talk to me or a GSI about any specific accommodations you need, or other ways we might be able to make the class more accessible. [260 C√©sar E. Ch√°vez Student Center| 510-642-0518 | https://dsp.berkeley.edu/]\nStudents with Mental Health Issues : If you feel a mental health issue (e.g., depression, anxiety) is negatively impacting your ability to succeed, I‚Äôd highly encourage you to seek out experts who can help. Berkeley has several free, confidential, and science-based programs designed to help students. [https://uhs.berkeley.edu/caps | Tang Center 3rd Floor | 510-642-9494 | After hours support line : 855-817-5667]\nUndocumented Students : I believe that all people deserve equal access to education. The UC Berkeley Undocumented Student Program has organized resources to support undocumented students, protect their rights, and offer potential financial aid | https://undocu.berkeley.edu/\nStudents from Low-Income Backgrounds : The Extended Opportunity Program (EOP) provides college support services for low-income students. Services include additional counseling, financial assistance (study-time parent grants, work-study assignments, and book vouchers), child-care opportunities, and assistance transferring to four-year colleges. [119 C√©sar E. Chavez Student Center | 510-642-7224 |¬† https://eop.berkeley.edu/]\n\n\n\n\nThe goals of this final project are to 1) demonstrate the research and data analytic skills that we have learned in this class, and 2) work on something that will be useful to your research career.¬†\nNote : this is my first semester assigning this assignment for a graduate course, and open to other ideas about how this assignment might better serve as a relevant reflection of what you have learned in the class, and your interests as a researcher.\n\n\n\n\n\n\n\nCriterion\nPoints [Out of 4]\n\n\nStudent specified a specific research question to ask, articulated how this research question is relevant to their interests, background, and past psychological research, and identified a dataset that will allow them to answer this research question.\n\n\n\nStudent submitted a pre-registration plan that details the data cleaning and analyses they expect to do in order to answer their research question, a list of a post-hoc analyses that deviated from the pre-registered plan, and a link to the data and RCode.\n\n\n\nStudent conducted analyses appropriate to the research question, organized these analyses in a table, and explained the results of these analyses (descriptive, predictive, and inferential statistics).\n\n\n\nStudent included clear, presentation-ready figures to illustrate their key analyses (descriptive, predictive, and inferential statistics).\n\n\n\nStudent shared the project in a way that clearly communicates their findings and why they matter (to psychological research and / or society), articulates limitations of the project, and advances next steps / future research directions they might pursue.\n\n\n\n\n\n\n\nSign-Ups + List of Readings¬†\nEach week, I‚Äôve assigned an article for the class to read. (Let me know if you have ideas for other articles to read in this / future semesters!)\n\nAll students will submit discussion responses by Wednesday at noon; these discussion posts should demonstrate you have done the reading, and highlight a question or discussion topic you think would be good to focus on more in class.\nA small group of students will be responsible for organizing a 15-20 minute presentation and discussion on the assigned article, based on the reading and submitted student discussion posts.\n\nThis presentation will be graded on the following criteria. Though I‚Äôm very open to ideas and suggestions about alternative ways you think we could effectively use this time!\n\n\n\n\n\n\n\nCriterion\nPoints [Out of 4]\n\n\nStudents gave a quick (5 min) summary of the key points from the assigned article.\n\n\n\nStudents drew from submitted discussion posts to lead a class discussion that helped answer common questions, debate ideas, and connect readings to our work as researchers.\n\n\n\nStudents organized a presentation and / or handout to help guide audience understanding.\n\n\n\nStudent presentation demonstrated preparation, organization, thought, and coordination among group members.\n\n\n\nStudents completed the self- and peer-evaluation form."
  },
  {
    "objectID": "gradstats/gradstats.html#semester-agenda",
    "href": "gradstats/gradstats.html#semester-agenda",
    "title": "Psych 205 (Graduate Statistics - SP25)",
    "section": "",
    "text": "Week\nLecture Topic\nAssignment\nSupport Readings\n\n\n\n\n1/24\nIntroductions [RScript]\nLab 1 [.qmd] + Key [.pdf]\nWS 1-2; LSR 3-4\n\n\n1/31\nDescription\nLab 2\nWS 3-4; LSR 5-6\n\n\n2/7\nVisualizing Data in ggplot2\nLab 3\nData Viz Chapter\n\n\n2/14\nProbability and Sampling Error\nLab 4\nWS 8; LSR 9-10\n\n\n2/21\nMini Exam + Linear Models Pt. 1 (Lines!)\n\nWS 5-7; LSR 15.1 - 15.2\n\n\n2/28\nLinear Models Pt. 2 (Tests & Assumptions)\nLab 5\nLSR 15.5 - 15.11\n\n\n3/7\nLinear Models Pt. 3 (Multiple Regression)\nLab 6\nWS 9; LSR 15.3\n\n\n3/14\nLinear Models Pt. 4 (Logistic / GLM)\nLab 7\nGeneralized Linear Models\n\n\n3/21\nReview / Catch-Up :)\nLab 8\n\n\n\n3/28\nSpring Break!!!\n\n\n\n\n4/4\nMega Exam\nMilestone 1\n\n\n\n4/11\nLinear Models Pt. 5a (Mixed Models)\nLab 9\nMixed Models with R\n\n\n4/18\nLinear Models Pt. 5b (Mixed Models)\nMilestone 2\nMixed Models with R\n\n\n4/25\nPCA / FA\nLab 10\nCore SEM\n\n\n5/2\nConclusion / Project Workshop\nProject Due\n\n\n\n\nNote : Support Readings are assigned from my undergraduate text Why Statistics? (WS) and https://learningstatisticswithr.com/book/ (LSR) for students looking for more support."
  },
  {
    "objectID": "gradstats/gradstats.html#course-information",
    "href": "gradstats/gradstats.html#course-information",
    "title": "Psych 205 (Graduate Statistics - SP25)",
    "section": "",
    "text": "Professor | Arman Daniel Catterson, PhD [he / his / ÿßŸà ] üê±(catterson@berkeley.edu)\n\nLectures : Fridays 9:00 - 12:00 PM in 1213 Berkeley Way West\nOffice Hours : Mondays 10:30 - 11:30 AM on Zoom (e-mail / chat after class to find another time)\n\nTeaching Assistant | Sierra Semko Krouse [she / her] | sierra_semko@berkeley.edu\n\nSections : Fridays, 1:00 - 2:00 PM in 1213 Berkeley Way West\nOffice Hours : Wednesdays 2:00 - 3:00 PM via Zoom (click here to sign-up for a 15-minute slot)"
  },
  {
    "objectID": "gradstats/gradlabs/Gradlab1.html",
    "href": "gradstats/gradlabs/Gradlab1.html",
    "title": "Psych 205 - Lab 1",
    "section": "",
    "text": "The goal of this lab is to get more practice working in R and R Markdown, and practice loading (and evaluating) datasets.\n\n\nWe will work on these questions together in lecture.\n\nDefine a variable\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLook over the supplemental readings for instructions on how to load a dataset from a .csv file. Load each of the four datasets posted to our ‚ÄúDatasets‚Äù folder."
  },
  {
    "objectID": "gradstats/gradlabs/Gradlab1.html#in-lecture.",
    "href": "gradstats/gradlabs/Gradlab1.html#in-lecture.",
    "title": "Psych 205 - Lab 1",
    "section": "",
    "text": "We will work on these questions together in lecture.\n\nDefine a variable"
  },
  {
    "objectID": "gradstats/gradlabs/Gradlab1.html#on-your-own.",
    "href": "gradstats/gradlabs/Gradlab1.html#on-your-own.",
    "title": "Psych 205 - Lab 1",
    "section": "",
    "text": "Look over the supplemental readings for instructions on how to load a dataset from a .csv file. Load each of the four datasets posted to our ‚ÄúDatasets‚Äù folder."
  },
  {
    "objectID": "gradstats/gradlabs/1_gradlab.html",
    "href": "gradstats/gradlabs/1_gradlab.html",
    "title": "Psych 205 - Lab 1",
    "section": "",
    "text": "Psych 205 - Lab 1\nThe goal of this lab is to get more practice working in R, and practice loading (and evaluating) datasets.\n\nEnter in a math equation that used to give you difficulty as a kid below, and run the code to see the result.\nWork with your classmates in section to create another dataframe with three variables - at least one numeric and one ‚Äústring‚Äù. Create that dataframe here, and use R code to answer the following questions.\n\nthe number of individuals (rows) in the dataset.\nthe number of variables (columns) in the dataset.\nthe names of the variables in the dataset.\nthe value of the 5th row, 2nd column in the dataset.\nprint a numeric variable from the dataset\nprint a categorical variable from the dataset\n\n\n\nR also has some built in datasets. The chickwts dataset contains data on the weights of chickens, and the types of food that they ate. You can access this datasets by just typing chickwts (the name of the data object) into R. Use this dataset to answer the following questions:\n\nthe number of individuals (rows) in the dataset.\nthe number of variables (columns) in the dataset.\nthe names of the variables in the dataset.\nthe value of the 5th row, 2nd column in the dataset.\nprint a numeric variable from the dataset\nprint a categorical variable from the dataset\n\n\n\nIn lecture, we wrote a for-loop to simulate the ‚ÄúMonty Hall‚Äù problem. Adapt this for-loop such that there are 100 doors (where the contestant chooses one, then Monty opens 98 other doors.) Under these conditions, what is the probability of winning if you switch? If you don‚Äôt switch?\n\nPlease try to complete these problems on your own after section. If you get stuck, post to the class ‚Äúdiscord‚Äù (and feel free to help others get unstuck.) If you really get stuck, just explain what you tried. Thanks!\n\nLook over the supplemental readings for instructions on how to load and navigate a dataset into R from a .csv file. Load one dataset posted to our ‚ÄúDatasets‚Äù folder; make sure to give the dataset a name; check the headers, and set stringsAsFactors = T. Note that some datasets are saved as .xlsx files, and will need to be loaded using a package - feel free to skip these for now if this feels confusing. For each dataset, use R to report the following statistics. Add comments to your code that describe what you learn from the output.\n\nthe number of individuals (rows) in the dataset.\nthe number of variables (columns) in the dataset.\nthe names of the variables in the dataset.\nthe value of the 5th row, 2nd column in the dataset.\nprint a numeric variable from the dataset\nprint a categorical variable from the dataset\nPrint two variables from the dataset - what do you learn from just looking at these data? No need for summary statistics yet :)\nWhat is a question you might ask about a variable from this dataset? Why might this question matter?\n\nRead the article ‚ÄúData Organization in Spreadsheets‚Äù. Choose one of the datasets uploaded to the Datasets folder on bCourses, and look over the dataset (and corresponding Codebook). What are some ways that this dataset adhered to these ‚Äúbest practices‚Äù? What are some ways that the dataset did not? What is something from this article that you learned? What did you have a question about?"
  },
  {
    "objectID": "gradstats/gradlabs/1_gradlab.html#in-lecture.",
    "href": "gradstats/gradlabs/1_gradlab.html#in-lecture.",
    "title": "Psych 205 - Lab 1",
    "section": "",
    "text": "We will work on these questions together in lecture!\n\nOne advantage of R Markdown is that you can run code in a document. You do this using a ‚Äúcode block‚Äù. Enter in a math equation that used to give you difficulty as a kid to the code block below, and run the code to see the result.\n\n\nIn the space below, insert an R code block, and define a numeric variable in this code block.\nIn class, we will build a dataframe. Create that dataframe here, and use R code to answer the following questions.\n\nthe number of individuals (rows) in the dataset.\nthe number of variables (columns) in the dataset.\nthe names of the variables in the dataset.\nthe value of the 5th row, 2nd column in the dataset.\nprint a numeric variable from the dataset\nprint a categorical variable from the dataset\n\n\n\nR also has some built in datasets. The chickwts dataset contains data on the weights of chickens, and the types of food that they ate. You can access this datasets by just typing chickwts (the name of the data object) into R. Use this dataset to answer the following questions:\n\nthe number of individuals (rows) in the dataset.\nthe number of variables (columns) in the dataset.\nthe names of the variables in the dataset.\nthe value of the 5th row, 2nd column in the dataset.\nprint a numeric variable from the dataset\nprint a categorical variable from the dataset"
  },
  {
    "objectID": "gradstats/gradlabs/1_gradlab.html#in-discussion-section.",
    "href": "gradstats/gradlabs/1_gradlab.html#in-discussion-section.",
    "title": "Psych 205 - Lab 1",
    "section": "",
    "text": "You will work with your TA on these questions.\n\nIn lecture, we created a dataframe in R with two variables. Work with your classmates to create another dataframe with three variables - at least one numeric and one ‚Äústring‚Äù. Then, use R to report the following from this dataset:\n\nthe number of individuals (rows) in the dataset.\nthe number of variables (columns) in the dataset.\nthe names of the variables in the dataset.\nthe value of the 5th row, 2nd column in the dataset.\nprint a numeric variable from the dataset\nprint a categorical variable from the dataset\n\nIn lecture, we wrote a for-loop to simulate the ‚ÄúMonty Hall‚Äù problem. Adapt this for-loop such that there are 100 doors (where the contestant chooses one, then Monty opens 98 other doors.) Under these conditions, what is the probability of winning if you switch? If you don‚Äôt switch?"
  },
  {
    "objectID": "gradstats/gradlabs/1_gradlab.html#on-your-own.",
    "href": "gradstats/gradlabs/1_gradlab.html#on-your-own.",
    "title": "Psych 205 - Lab 1",
    "section": "",
    "text": "Please try to complete these problems on your own after section. If you get stuck, post to the class ‚Äúdiscord‚Äù (and feel free to help others get unstuck.) If you really get stuck, just explain what you tried. Thanks!\n\nLook over the supplemental readings for instructions on how to load and navigate a dataset into R from a .csv file. Load one dataset posted to our ‚ÄúDatasets‚Äù folder; make sure to give the dataset a name; check the headers, and set stringsAsFactors = T. Note that some datasets are saved as .xlsx files, and will need to be loaded using a package - feel free to skip these for now if this feels confusing. For each dataset, use R to report the following statistics. Add comments to your code that describe what you learn from the output.\n\nthe number of individuals (rows) in the dataset.\nthe number of variables (columns) in the dataset.\nthe names of the variables in the dataset.\nthe value of the 5th row, 2nd column in the dataset.\nprint two variables from the dataset - what do you learn from just looking at these data? No need for summary statistics yet :)\nWhat is a question you might ask about a variable from this dataset? Why might this question matter?\n\nRead the article ‚ÄúData Organization in Spreadsheets‚Äù. Choose one of the datasets uploaded to the Datasets folder on bCourses, and look over the dataset (and corresponding Codebook). What are some ways that this dataset adhered to these ‚Äúbest practices‚Äù? What are some ways that the dataset did not? What is something from this article that you learned? What did you have a question about?\nChallenge Problem1. Quarto (and Markdown) can also be used to make a website. Make yourself a personal website, and use one of the methods to host this website for the world to see [I use github.] Yeah!"
  },
  {
    "objectID": "gradstats/gradlabs/1_gradintro.html",
    "href": "gradstats/gradlabs/1_gradintro.html",
    "title": "Welcome to Psych 205",
    "section": "",
    "text": "You can access this document by visiting our course webpage and clicking on Lecture 1 : https://catterson.github.io/gradstats/gradstats.html\nPlease take this short survey (which will serve as your attendance for today.)\nHelp yourself to coffee & donuts if you‚Äôd like. We will get started soon!\n\n\n\n\n\n\n\n\nIntroductions to the class.\nIntroductions to statistics.\nIntroductions to the program R.\n\n\n\n\n\n\n\nWe learn from each other!1\n\n\n\n\n\n\n\nSome brief introductions [ ~ 30 seconds!]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat‚Äôs your name and where are you from?\nWhy are you at UC Berkeley?\nWhat‚Äôs your research interest in one sentence?\nFavorite place to eat in the Bay Area?\n\n\n\n\nSierra (TA; Ohio)\nI got in! Love the bay.\nMindset interventions (bias, social class)\nPomet [Oakland]\n\n\nRayna (Shenzhen)\nPost-bac!\nSleep\nJollibee\n\n\nRosaline (Palo Alto)\nPost-bac\nKids and their welfare\nSushi Tomi (worth the drive)\n\n\nEmily (Oakland)\nDev program (and got in)\nCultural transmission\nForma Bakery (Oakland)\n\n\nSabana (FL ‚Äì&gt; NY ‚Äì&gt; )\nDev psych\nsocial cog in primates!\nPie Punks\n\n\nYoung Joo Jun\nHaas & the weather\nsocial cognition & judgments of others\nI-House.\n\n\nYoko (Toyko ‚Äì&gt; EC)\nSP\nsocial & cog aspects of aesthetics experiences\nRotha French Bakery on San Pablo\n\n\nKaitlin (Reno; SP)\nLivin that Cali Dream!!!\nCulture perspectives on emotion, happiness\nDumpling Kitchen.\n\n\nSahana (Bay via Seattle)\nDev\nSTEM success; language aquisition\nCaffeinated [sp] (on Vine)\n\n\nDarby (the other CA!)\nIt‚Äôs Berkeley, California TM\ncraving regulation; substance use disorders\nNick‚Äôs Pizza\n\n\nRishika (Mumbai -&gt; DC)\n‚Äú‚Äù\naddiction science & social influence.\nIppudo\n\n\nSebastian (LA -&gt; Conn)\nwarmer & family\nsocioeconomic & bio & cognitve factors ‚Äì&gt; psychobiology\nlongdromat pizza (SF) & t‚Äôchaka\n\n\nKimy (PNW)\nother option was Kansas and my work is illegal there :(\nanti-native prejudice; increasing intergroup collective action to save the world\nCasa Latina (off San Pablo)\n\n\nRhea (NY)\n\nhiring discrimination of transgender candidates\ndumpling home (SF)\n\n\nShuyi (visiting from China)\n\nclinical / developmental psychopathology\nKiraku\n\n\nMichelle (visiting from China)\n\nhow to combine psychology with UX design\nmy kitchen (roommate labor)\n\n\nCristina (LA ‚Äì&gt; Boston)\ncloser to family\nhow kids learn to communicate\nPeacock Pansy (SF)\n\n\nAlexandra (Mex ‚Äì&gt; US)\ngot in; like lab.\ncircadian rhythms & psychiatric disorders\nFlores in SF (approved!)\n\n\nAle (NM)\nbio-anthro (knew advisor)\naging female reproductive health & social justice\nEggys\n\n\nXinyi (SF)\nclose to family and advisor\nimmigrant families & socialization / dev.\nHarborview (SF)\n\n\nKyra (NY)\nhaaaaas family\ndisability as a social identity\nocean view diner\n\n\nCarter (Idaho)\nhaas; EA cult member\njudgment & decisions about risk and probability\nSaigon Sandwich (SF)\n\n\nGarrett (redding)\nditched davis dream\npersonality & psychometrics\nHonor Bar (Emeryville)\n\n\nPeter (Michigan)\nhere for the post-bac;\ncomplicated methodology of studying substance use\nHuangchen Noodle House / Too Good to Go (Last-chance takeout)\n\n\nJunchi\nthe weather & lifestyle\nempathy\nnot yet; La Note\n\n\nJeremy (Beijing)\n\nAI aesthetics\ntasty pot (Berkeley)\n\n\nYasha (India!)\npost-bac to change career\nclose relationships and emotion regulation\nMason Alyzee (pastries approved!)\n\n\nLino\narchitecture who loves regression models; from the bay\nindoor thermal comfort\nZachary‚Äôs Pizza\n\n\nAnne (Norway)\nWhy not?\nADHD treatments & broader clinical issues\nTrader Joe‚Äôs Avocados\n\n\nYiYang (Shanghai)\nvisiting!\npersonality psych & social psych\njust arrived; blue bottle coffee\n\n\nAnn\npost-bac!\nADHD & epilepsy\nBerkeley Bowl West\n\n\n\n\n\n\n\n\n\n\n\nOnline :\n\nwebsite: syllabus and all course content.\nbCourses: datasets; submitting assignments (labs and discussions); grades.\nClass Discord: place to ask for help / build community.\n\nIRL :\n\nFriday Class 9:00 AM - 12:00 PM\n\nprofessor lecture\nstudent work time\narticle presentations & discussions\n\nDiscussion Section 1:00 - 2:00 PM\n\nreview\nwork on assignments\n\nWednesdays : reading discussions due by NOON.\n\nSyllabus overview / questions / concerns?\n\n\n\n\n\n\n\n\n\nDarby : stats is the foundation for all the research?\nKimy : proper use of statistics as evidence in support of our arguments; need to get it right so avoid errors.\nSabana : stats is an objective way to look at our data\nYoko : statistics can help you be transparent about the data; what criteria you used.\nRishika : stats is reliable; reproducable\nSebastian : involves collection of numbers, data‚Ä¶and numbers don‚Äôt lie (if used correctly)???\nJeremy : set some hypotheses and test whether they are true or not.\nYoung Joo : use statistics to test theories, evalaute others data; interpret the world.\n\n\n\n\n\n\nScience is‚Ä¶.\n\nDescription : The topics of your research interests!\nPrediction : An educated guess you have about the future.\nPower : How you use this knowledge (and predictions) to exert influence over the world / your future.\n\nRosaline : predicting how adverse childhood experiences lead to negative outcomes for kids‚Ä¶and then identify those triggers and try to prevent them from happening; developing effective interventions.\nAnn : children and adolescents with ADHD & epilepsy have lower psychological adjustments; prediction about what can improve psychological adjustment ‚Äì&gt; improved outcomes.\n\n\nIs Psychology a Science?\nShould Psychology be a Science?\n\n\n\n\nWhat happens in a good language class?\n\n\n\n\n\n\n\nHuman Language\nStatistics / Computer Language\n\n\n\n\nMistakes happen (it‚Äôs okay! normal!! part of the process); but also you are misunderstood\nYou will make mistakes and the computer will misunderstand / break (but that‚Äôs okay)\n\n\nGrammar / Syntax (the rules) and exceptions\nthere are rules we will have to follow!\n\n\nCultural immersion\nCoding all the time; Immersed in the data in some sort of flow state; understand the context of your dataset / the methods; listening to stats podcasts; reading research articles; learning about who invented the mean / regression???\n\n\nVocabulary\ncertain words have meaning in R\n\n\nFluency and proficiency allow you to think about higher level issues (prose, tone)\nwith fluency will come ability to look at ‚Äúclean‚Äù data\n\n\nit takes practice and time and patience\n\n\n\n\n\n\n\n\n\n\nThe console is where R does its work.\n\nACTIVITY : Look at the image below. What do you see? What makes sense / what seems confusing?\n\n\n\n\n\nACTIVTY : open up R and type some MATH into the console. Yeah! You are programming!!!\n\n\n\n\nIn this class, we‚Äôll be using RStudio. RStudio is an IDE (Integrated Development Environment) that includes the console along with other useful windows and tools.\n\nThe Console is at the bottom left of the IDE. Hi console!\nThe R script is at the top left of the IDE, and is a document that you use to write (and organize) code. You will want to do most of your work in the R script, and feel an appropriate level of anxiety when you notice that your Rscript is unsaved (as indicated by the red text and *).\nThe Environment is at the top right of the IDE, and shows you all of the ‚Äúobjects‚Äù that you have defined in R.\nThe File Window is at the bottom right of the IDE, and shows you the files. Note that there are tabs here for Plots (where graphs will pop up), Packages (things you can download to give R extra features), a Help viewer (sometimes very useful!).\n\n\n\n\n\nHere‚Äôs a link to an RScript!You can download this to your computer, and open it in RStudio.\nTopics We Will Review\n\nmath : common operators and functions\nvariables : string and numeric vectors\ndatasets : data.frame(); datasets::\nfunctions : descriptive statistics, base graphics, generating random numbers\ngood code : troubleshooting errors; naming datasets; white space; keeping code and files organized\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou‚Äôll need to use the following bits of code :\nvariable &lt;- c(\"\") # create a string variable (for the options)\nsample() # to \"randomly\" sample from this variable\nsetdiff() # to differentiate \nYou‚Äôre welcome to try this on your own, but below are steps to help guide your work if you‚Äôd like (and some extra hints if you‚Äôd like more specific instructions.)\nTotally okay if you struggle with this! The point is to try and do something fun (and potentially challenging) in R, and see how computers can be used to address research questions.\n\nDefine the options to choose from (three doors)\n\n\n\n\n\n\n\nHINT\n\n\n\n\n\nCreate a string variable with three options : door1, door2, and door3\n\n\n\n\nDefine player choice and treasure location.\n\n\n\n\n\n\n\nHINT\n\n\n\n\n\nuse the sample() function to define two variables named choice and treasure; each a random door.\n\n\n\n\nDefine what doors Monty can open.\n\n\n\n\n\n\n\nHINT\n\n\n\n\n\nUse the setdiff() function to identify all the doors that are not what the player chose or where the treasure is.\n\n\n\n\nDefine what Monty does open.\n\n\n\n\n\n\n\nHINT\n\n\n\n\n\nUse the sample function to select one random door from the variable that defines what doors Monty can open.\n\n\n\n\nWhat happens if we switch?\n\n\n\n\n\n\n\nHINT\n\n\n\n\n\nDefine a new variable that identifies the choice if we switch; this should selects one random option from the list of doors that are NOT what Monty opened or what the player initially chose. NOTE : You will need to use the sample and setdiff functions together.\n\n\n\n\nTest whether the player won or not.\n\n\n\n\n\n\n\nHINT\n\n\n\n\n\nUse the if-else function to define a ‚ÄúWIN‚Äù as whether the choice variable matches the treasure variable. Then repeat this step for whether the switch choice variable matches the treasure variable.\n\n\n\n\nRepeat these steps 1000 times.\n\n\n\n\n\n\n\nHINT\n\n\n\n\n\nDefine an empty array to save the result of each test. Then define a for-loop to simulate the Monty Hall 1000 times.\n\n\n\n\n\n\n\n\nExit Survey\nComplete Lab 1\nSign up for an Article Presentation\nRead (& Discussion Post) Article 1"
  },
  {
    "objectID": "gradstats/gradlabs/1_gradintro.html#goals-of-this-lecture",
    "href": "gradstats/gradlabs/1_gradintro.html#goals-of-this-lecture",
    "title": "Welcome to Psych 205",
    "section": "",
    "text": "Introductions to the class.\nIntroductions to statistics.\nIntroductions to the program R."
  },
  {
    "objectID": "gradstats/gradlabs/1_gradintro.html#why-statistics-in-psychology",
    "href": "gradstats/gradlabs/1_gradintro.html#why-statistics-in-psychology",
    "title": "Welcome to Psych 205",
    "section": "",
    "text": "student ideas go here!\n\n\n\n\n\nScience is‚Ä¶.\n\nDescription :\nPrediction :\nPower :\n\nIs Psychology a Science?\nShould Psychology be a Science?\n\n\n\n\nWhat happens in a good language class?\n\n\n\nHuman Language\nComputer Language\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe console is where R does its work.\n\nACTIVITY : Look at the image below. What do you see? What makes sense / what seems confusing?\n\n\n\n\n\nACTIVTY : open up R and type some MATH into the console. Yeah! You are programming!!!\n\n\n\n\nIn this class, we‚Äôll be using RStudio. RStudio is an IDE (Integrated Development Environment) that includes the console along with other useful windows and tools.\n\nThe Console is at the bottom left of the IDE. Hi console!\nThe R script is at the top left of the IDE, and is a document that you use to write (and organize) code. You will want to do most of your work in the R script, and feel an appropriate level of anxiety when you notice that your Rscript is unsaved (as indicated by the red text and *).\nThe Environment is at the top right of the IDE, and shows you all of the ‚Äúobjects‚Äù that you have defined in R.\nThe File Window is at the bottom right of the IDE, and shows you the files. Note that there are tabs here for Plots (where graphs will pop up), Packages (things you can download to give R extra features), a Help viewer (sometimes very useful!)."
  },
  {
    "objectID": "gradstats/gradlabs/1_gradlab.html#footnotes",
    "href": "gradstats/gradlabs/1_gradlab.html#footnotes",
    "title": "Psych 205 - Lab 1",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOptional, but potentially fun and useful if you have the time and interest.‚Ü©Ô∏é"
  },
  {
    "objectID": "gradstats/gradstats.html#grade-details",
    "href": "gradstats/gradstats.html#grade-details",
    "title": "Psych 205 (Graduate Statistics - SP25)",
    "section": "",
    "text": "Your grade in the class will be based on the following components.\n\n\n\n\n\n\n\n\n10% Attendance\n30% R Exams [10 + 20]\n10% Article Presentations\n\n\n20% Lab Assignments\n20% Final Project\n10% Article Discussions\n\n\n\nLetter grades will be based on the following : A+ is &gt; 96.5; A is &gt; 92.5; A- is &gt; 89.5; etc. Note that an 89.49999 is a B+.)"
  },
  {
    "objectID": "gradstats/gradstats.html#summary-of-course-components",
    "href": "gradstats/gradstats.html#summary-of-course-components",
    "title": "Psych 205 (Graduate Statistics - SP25)",
    "section": "",
    "text": "Attendance (Lecture and Discussion Section): Because we all learn from each other; attendance is required. You may miss up to two lectures and two discussion sections with no penalty.\nArticle Discussion. Each week, students will be responsible for leading class discussion of one article. All students should plan to read the article, and post comments to a discussion forum on bCourses. See the [Student Presentation | Description and Rubric] for a list of articles, a link to sign up, and a description of what is required for these student-led presentations. Let me know if you have other ideas for ways to engage the class in discussion about an article (or another article you think you would be good for the class to read.)\nLab Assignments : Most weeks, you will work through a ‚Äúlab‚Äù assignment that is designed to help practice the skills we review in lecture and readings. You can expect to work on these in lecture, in your discussion section, and at home as needed. Each lab document will have a set of questions to answer - you should submit answers to these questions as a separate document to bCourses. Lab assignments are due the week after they are assigned; late lab assignments will not be accepted. You can drop your lowest submitted lab assignment.\nFinal Project (Research Paper) : You‚Äôll apply the skills you have learned this semester to analyze, interpret, and share results from a research question relevant to your research interests. There will be several milestones to support this assignment. See the [Final Project | Description and Rubric] for more information.\nR Exam : For each exam, I‚Äôll give you a novel dataset and you‚Äôll use R to answer questions based on the dataset. The exam will be open-book and open note - you must work alone, but may use any resource available to you (including the Internet). A practice exam will be posted at least one week before the exam. The mini exam should take no more than 50 minutes to complete (we will have class afterward), and you will have the full 3-hours to complete the mega exam.\nExtra Credit : There will be no extra credit offered in this course, even for students who ask kindly."
  },
  {
    "objectID": "gradstats/gradstats.html#course-policies",
    "href": "gradstats/gradstats.html#course-policies",
    "title": "Psych 205 (Graduate Statistics - SP25)",
    "section": "",
    "text": "Computer Access. Access to a personal computer is strongly recommended. We will spend time each lecture working in R - a free and powerful statistics program that works with all operating systems and computers. Laptops are available for rent from the library (see Berkeley‚Äôs Technology Loan Program), or you can purchase one that will work for this class for as little as $150 (less than the price of most textbooks, which is not required for this class).\nLate Work Policy. Late assignments will not be accepted. You may drop your lowest scoring lab assignment. Let me know if you cannot attend the exam and we can schedule a make-up exam.\nDiscussion Section Policy : In-person attendance is expected and required for discussion sections. Students may miss up to two discussion sections with no penalty. Please talk to your GSI / the professor if there is a longer-term illness or issue that will prevent you from regularly attending your discussion section and we will work out another arrangement.\nRegrades. Students may ask for a re-grade on exams and papers if they believe they lost points for something they should have earned. To request a regrade, talk to your GSI in office hours or set up a meeting. In the case that you and your GSI cannot resolve the regrade issue, the professor will step in and regrade the exam. For final project regrades just e-mail the professor. When requesting a regrade, you consent to have your score increased or decreased if we find that you earned points you should not have earned.\nStudent Contact. It is your responsibility to regularly check e-mail for announcements. If you have specific questions to e-mail me, please make sure to (a) search the syllabus and / or Google for the answer to your question and (b) contact your GSI about your question. I am always happy to answer any questions in office hours (either during the scheduled time or online by appointment), or before, during, or after lecture. I should respond to e-mail within 24 hours on weekdays (I do not check e-mail on weekends); please send a gentle reminder if I do not respond in this timeframe. Thanks!\nAcademic Integrity. Do NOT cheat. Do NOT plagiarize. To copy text or ideas from another source (including your own submitted coursework) without appropriate reference is plagiarism. You may work on lab assignments with other students, but should be able to understand what you are doing. You may not work with other students on the R exam. Anyone caught cheating or plagiarizing will receive a zero on the assignment or test, and will be reported to the Office of Student Conduct. It is your responsibility to read the official Student Conduct Policy for more information about campus standards and policies regarding Academic Integrity.\nChat-GPT / AI Policy. If you use ChatGPT or other generative-AI software for any part of this class (writing, coding, troubleshooting, etc.), you must make this clear at the top of your assignment by writing : ‚ÄúI USED CHAT GPT TO SUPPORT THIS ASSIGNMENT‚Äù (in all-caps and red text), and include an appendix where you include screenshots of your specific queries and answers. Failure to do this will result in a zero of the assignment, and the professor submitting a report to the Academic Integrity Office. Ask your GSI / professor if you have questions about this policy.\nRespect for Others. We all belong in our classroom, and I hope that everyone feels free and supported to express our identities when relevant. To achieve this goal, make sure that you are treating others with respect in this course. This means considering others‚Äô opinions and perspectives, making sure not to generalize to groups (or ask students to be representatives of their group), and being mindful of the words that you use. (Also remember that nothing is ever really deleted from the internet, so please be extra mindful of the words you use.) While I will be monitoring class posts and communication, feel free to reach out to me if you see another student engaging in disrespectful or threatening behavior. Of course, this extends to me as well - if you feel like there are parts of the course instruction, subject matter, or class environment that create barriers to your success or inclusion, please contact me via e-mail and trust that your voice will be heard and I will not punish you for offering constructive criticisms of my teaching. I‚Äôm very open to learning how to be a better teacher, and always learn a lot from my students.\nSensitive Subjects. Our class will touch on important and potentially sensitive topics that are related to psychology and psychological processes, such as racism, depression, sex, and poverty. These topics can generate strong emotional responses in students for a variety of reasons - such as their own past histories with these topics or their reactions to the ways the information is presented or described. I will try to make sure to give you a heads up when we start to discuss these topics so you can be mindful of your reactions.\nStrategies for success. Do your readings, complete assignments on time, understand how the topics and information relate to what you‚Äôve learned (and are learning), and if you don‚Äôt understand something, please ask the question!\nMost important. Please let me know as soon as there is anything going on in your life that you think may affect your ability to do as well as you would like in this class (e.g.¬†sickness, work, small children at home, threats to immigration status, etc.), and I will do my best to work with you on a plan to succeed in the class. If you‚Äôre not comfortable talking to me directly, you can talk to your GSI who can talk to me about your issue."
  },
  {
    "objectID": "gradstats/gradstats.html#student-support-services",
    "href": "gradstats/gradstats.html#student-support-services",
    "title": "Psych 205 (Graduate Statistics - SP25)",
    "section": "",
    "text": "Students with Disabilities : Students who require support or adaptive equipment because of a specific disability ‚Äì or would like to be tested to see if they qualify for a learning, auditory, or visual disability ‚Äì can request these services through the Disabled Students Program (DSP) office. Please note that I can only provide accommodations authorized by the DSP office. If you have DSP status, make sure you submit your letter and feel free to talk to me or a GSI about any specific accommodations you need, or other ways we might be able to make the class more accessible. [260 C√©sar E. Ch√°vez Student Center| 510-642-0518 | https://dsp.berkeley.edu/]\nStudents with Mental Health Issues : If you feel a mental health issue (e.g., depression, anxiety) is negatively impacting your ability to succeed, I‚Äôd highly encourage you to seek out experts who can help. Berkeley has several free, confidential, and science-based programs designed to help students. [https://uhs.berkeley.edu/caps | Tang Center 3rd Floor | 510-642-9494 | After hours support line : 855-817-5667]\nUndocumented Students : I believe that all people deserve equal access to education. The UC Berkeley Undocumented Student Program has organized resources to support undocumented students, protect their rights, and offer potential financial aid | https://undocu.berkeley.edu/\nStudents from Low-Income Backgrounds : The Extended Opportunity Program (EOP) provides college support services for low-income students. Services include additional counseling, financial assistance (study-time parent grants, work-study assignments, and book vouchers), child-care opportunities, and assistance transferring to four-year colleges. [119 C√©sar E. Chavez Student Center | 510-642-7224 |¬† https://eop.berkeley.edu/]"
  },
  {
    "objectID": "gradstats/gradstats.html#final-project-description-and-rubric",
    "href": "gradstats/gradstats.html#final-project-description-and-rubric",
    "title": "Psych 205 (Graduate Statistics - SP25)",
    "section": "",
    "text": "The goals of this final project are to 1) demonstrate the research and data analytic skills that we have learned in this class, and 2) work on something that will be useful to your research career.¬†\nNote : this is my first semester assigning this assignment for a graduate course, and open to other ideas about how this assignment might better serve as a relevant reflection of what you have learned in the class, and your interests as a researcher.\n\n\n\n\n\n\n\nCriterion\nPoints [Out of 4]\n\n\nStudent specified a specific research question to ask, articulated how this research question is relevant to their interests, background, and past psychological research, and identified a dataset that will allow them to answer this research question.\n\n\n\nStudent submitted a pre-registration plan that details the data cleaning and analyses they expect to do in order to answer their research question, a list of a post-hoc analyses that deviated from the pre-registered plan, and a link to the data and RCode.\n\n\n\nStudent conducted analyses appropriate to the research question, organized these analyses in a table, and explained the results of these analyses (descriptive, predictive, and inferential statistics).\n\n\n\nStudent included clear, presentation-ready figures to illustrate their key analyses (descriptive, predictive, and inferential statistics).\n\n\n\nStudent shared the project in a way that clearly communicates their findings and why they matter (to psychological research and / or society), articulates limitations of the project, and advances next steps / future research directions they might pursue."
  },
  {
    "objectID": "gradstats/gradstats.html#student-presentation-description-and-rubric",
    "href": "gradstats/gradstats.html#student-presentation-description-and-rubric",
    "title": "Psych 205 (Graduate Statistics - SP25)",
    "section": "",
    "text": "Sign-Ups + List of Readings¬†\nEach week, I‚Äôve assigned an article for the class to read. (Let me know if you have ideas for other articles to read in this / future semesters!)\n\nAll students will submit discussion responses by Wednesday at noon; these discussion posts should demonstrate you have done the reading, and highlight a question or discussion topic you think would be good to focus on more in class.\nA small group of students will be responsible for organizing a 15-20 minute presentation and discussion on the assigned article, based on the reading and submitted student discussion posts.\n\nThis presentation will be graded on the following criteria. Though I‚Äôm very open to ideas and suggestions about alternative ways you think we could effectively use this time!\n\n\n\n\n\n\n\nCriterion\nPoints [Out of 4]\n\n\nStudents gave a quick (5 min) summary of the key points from the assigned article.\n\n\n\nStudents drew from submitted discussion posts to lead a class discussion that helped answer common questions, debate ideas, and connect readings to our work as researchers.\n\n\n\nStudents organized a presentation and / or handout to help guide audience understanding.\n\n\n\nStudent presentation demonstrated preparation, organization, thought, and coordination among group members.\n\n\n\nStudents completed the self- and peer-evaluation form."
  },
  {
    "objectID": "gradstats/gradlabs/1_gradintro.html#introductions-to-the-class",
    "href": "gradstats/gradlabs/1_gradintro.html#introductions-to-the-class",
    "title": "Welcome to Psych 205",
    "section": "",
    "text": "We learn from each other!1\n\n\n\n\n\n\n\nSome brief introductions [ ~ 30 seconds!]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat‚Äôs your name and where are you from?\nWhy are you at UC Berkeley?\nWhat‚Äôs your research interest in one sentence?\nFavorite place to eat in the Bay Area?\n\n\n\n\nSierra (TA; Ohio)\nI got in! Love the bay.\nMindset interventions (bias, social class)\nPomet [Oakland]\n\n\nRayna (Shenzhen)\nPost-bac!\nSleep\nJollibee\n\n\nRosaline (Palo Alto)\nPost-bac\nKids and their welfare\nSushi Tomi (worth the drive)\n\n\nEmily (Oakland)\nDev program (and got in)\nCultural transmission\nForma Bakery (Oakland)\n\n\nSabana (FL ‚Äì&gt; NY ‚Äì&gt; )\nDev psych\nsocial cog in primates!\nPie Punks\n\n\nYoung Joo Jun\nHaas & the weather\nsocial cognition & judgments of others\nI-House.\n\n\nYoko (Toyko ‚Äì&gt; EC)\nSP\nsocial & cog aspects of aesthetics experiences\nRotha French Bakery on San Pablo\n\n\nKaitlin (Reno; SP)\nLivin that Cali Dream!!!\nCulture perspectives on emotion, happiness\nDumpling Kitchen.\n\n\nSahana (Bay via Seattle)\nDev\nSTEM success; language aquisition\nCaffeinated [sp] (on Vine)\n\n\nDarby (the other CA!)\nIt‚Äôs Berkeley, California TM\ncraving regulation; substance use disorders\nNick‚Äôs Pizza\n\n\nRishika (Mumbai -&gt; DC)\n‚Äú‚Äù\naddiction science & social influence.\nIppudo\n\n\nSebastian (LA -&gt; Conn)\nwarmer & family\nsocioeconomic & bio & cognitve factors ‚Äì&gt; psychobiology\nlongdromat pizza (SF) & t‚Äôchaka\n\n\nKimy (PNW)\nother option was Kansas and my work is illegal there :(\nanti-native prejudice; increasing intergroup collective action to save the world\nCasa Latina (off San Pablo)\n\n\nRhea (NY)\n\nhiring discrimination of transgender candidates\ndumpling home (SF)\n\n\nShuyi (visiting from China)\n\nclinical / developmental psychopathology\nKiraku\n\n\nMichelle (visiting from China)\n\nhow to combine psychology with UX design\nmy kitchen (roommate labor)\n\n\nCristina (LA ‚Äì&gt; Boston)\ncloser to family\nhow kids learn to communicate\nPeacock Pansy (SF)\n\n\nAlexandra (Mex ‚Äì&gt; US)\ngot in; like lab.\ncircadian rhythms & psychiatric disorders\nFlores in SF (approved!)\n\n\nAle (NM)\nbio-anthro (knew advisor)\naging female reproductive health & social justice\nEggys\n\n\nXinyi (SF)\nclose to family and advisor\nimmigrant families & socialization / dev.\nHarborview (SF)\n\n\nKyra (NY)\nhaaaaas family\ndisability as a social identity\nocean view diner\n\n\nCarter (Idaho)\nhaas; EA cult member\njudgment & decisions about risk and probability\nSaigon Sandwich (SF)\n\n\nGarrett (redding)\nditched davis dream\npersonality & psychometrics\nHonor Bar (Emeryville)\n\n\nPeter (Michigan)\nhere for the post-bac;\ncomplicated methodology of studying substance use\nHuangchen Noodle House / Too Good to Go (Last-chance takeout)\n\n\nJunchi\nthe weather & lifestyle\nempathy\nnot yet; La Note\n\n\nJeremy (Beijing)\n\nAI aesthetics\ntasty pot (Berkeley)\n\n\nYasha (India!)\npost-bac to change career\nclose relationships and emotion regulation\nMason Alyzee (pastries approved!)\n\n\nLino\narchitecture who loves regression models; from the bay\nindoor thermal comfort\nZachary‚Äôs Pizza\n\n\nAnne (Norway)\nWhy not?\nADHD treatments & broader clinical issues\nTrader Joe‚Äôs Avocados\n\n\nYiYang (Shanghai)\nvisiting!\npersonality psych & social psych\njust arrived; blue bottle coffee\n\n\nAnn\npost-bac!\nADHD & epilepsy\nBerkeley Bowl West\n\n\n\n\n\n\n\n\n\n\n\nOnline :\n\nwebsite: syllabus and all course content.\nbCourses: datasets; submitting assignments (labs and discussions); grades.\nClass Discord: place to ask for help / build community.\n\nIRL :\n\nFriday Class 9:00 AM - 12:00 PM\n\nprofessor lecture\nstudent work time\narticle presentations & discussions\n\nDiscussion Section 1:00 - 2:00 PM\n\nreview\nwork on assignments\n\nWednesdays : reading discussions due by NOON.\n\nSyllabus overview / questions / concerns?"
  },
  {
    "objectID": "gradstats/gradlabs/1_gradintro.html#learn-about-our-class.",
    "href": "gradstats/gradlabs/1_gradintro.html#learn-about-our-class.",
    "title": "Welcome to Psych 205",
    "section": "",
    "text": "Hello does this work? Maybe not.\n:::\n\nIntroductions to the class.\nIntroductions to statistics.\nIntroductions to the program R."
  },
  {
    "objectID": "gradstats/gradlabs/1_gradintro.html#working-in-an-r-script",
    "href": "gradstats/gradlabs/1_gradintro.html#working-in-an-r-script",
    "title": "Welcome to Psych 205",
    "section": "",
    "text": "Topics We Will Review\n\nmath : common operators and functions\nvariables : string and numeric vectors\ndatasets : data.frame(); datasets::\nfunctions : descriptive statistics, base graphics, generating random numbers\ngood code : troubleshooting errors; naming datasets; white space; keeping code and files organized"
  },
  {
    "objectID": "gradstats/gradlabs/1_gradintro.html#the-monty-hall-problem.",
    "href": "gradstats/gradlabs/1_gradintro.html#the-monty-hall-problem.",
    "title": "Welcome to Psych 205",
    "section": "",
    "text": "The Monty Hall problem is a famous"
  },
  {
    "objectID": "gradstats/gradlabs/1_gradintro.html#working-in-quarto",
    "href": "gradstats/gradlabs/1_gradintro.html#working-in-quarto",
    "title": "Welcome to Psych 205",
    "section": "",
    "text": "Quarto is a version of R Markdown, which is a version of Markdown, which is a powerful way to author code that is meant for both humans and computers to read.\n\nAdvantages :\n\ncan create a document that works for R code, can create a presentation, or a website\nmuch faster to get your code from R to something that humans can read\n\nno more copy-paste graphs or output.\ncan update graphs and output as your needs / datset changes\n\nlots of features - open-source heritage and culture, but supported financially my Micro$oft.\n\nability to format your code; render it as a website, pdf, book, etc.\ninteractive documents (Shiny; html-live; etc.)\n\n\nDisadvantages :\n\ncode must be ‚Äúperfect‚Äù in order to correctly render.\ncan go down formatting and feature rabbit holes that are not necessarily condusive to good science.\nanother dialect of the language you are trying to learn\n\nin R : code is the default; human comments added with #s\nin Quarto : human text is the default; you insert a code block when you want R to do something (and can then comment in that code)\n\n\n\n1+1 # like this\n\n[1] 2\n\n\n\nIn this class, we will work with both .R scripts and .qmd Quarto Markdown Files\n\n.R Scripts for tinkering with data (in-class tutorials; initial analyses)\n.qmd files for ‚Äúfinal‚Äù products (Lecture notes, lab documents, your project)"
  },
  {
    "objectID": "gradstats/gradlabs/1_gradintro.html#is-there-time-a-fun-r-demonstration",
    "href": "gradstats/gradlabs/1_gradintro.html#is-there-time-a-fun-r-demonstration",
    "title": "Welcome to Psych 205",
    "section": "",
    "text": "You‚Äôll need to use the following bits of code :\nvariable &lt;- c(\"\") # create a string variable (for the options)\nsample() # to \"randomly\" sample from this variable\nsetdiff() # to differentiate \nYou‚Äôre welcome to try this on your own, but below are steps to help guide your work if you‚Äôd like (and some extra hints if you‚Äôd like more specific instructions.)\nTotally okay if you struggle with this! The point is to try and do something fun (and potentially challenging) in R, and see how computers can be used to address research questions.\n\nDefine the options to choose from (three doors)\n\n\n\n\n\n\n\nHINT\n\n\n\n\n\nCreate a string variable with three options : door1, door2, and door3\n\n\n\n\nDefine player choice and treasure location.\n\n\n\n\n\n\n\nHINT\n\n\n\n\n\nuse the sample() function to define two variables named choice and treasure; each a random door.\n\n\n\n\nDefine what doors Monty can open.\n\n\n\n\n\n\n\nHINT\n\n\n\n\n\nUse the setdiff() function to identify all the doors that are not what the player chose or where the treasure is.\n\n\n\n\nDefine what Monty does open.\n\n\n\n\n\n\n\nHINT\n\n\n\n\n\nUse the sample function to select one random door from the variable that defines what doors Monty can open.\n\n\n\n\nWhat happens if we switch?\n\n\n\n\n\n\n\nHINT\n\n\n\n\n\nDefine a new variable that identifies the choice if we switch; this should selects one random option from the list of doors that are NOT what Monty opened or what the player initially chose. NOTE : You will need to use the sample and setdiff functions together.\n\n\n\n\nTest whether the player won or not.\n\n\n\n\n\n\n\nHINT\n\n\n\n\n\nUse the if-else function to define a ‚ÄúWIN‚Äù as whether the choice variable matches the treasure variable. Then repeat this step for whether the switch choice variable matches the treasure variable.\n\n\n\n\nRepeat these steps 1000 times.\n\n\n\n\n\n\n\nHINT\n\n\n\n\n\nDefine an empty array to save the result of each test. Then define a for-loop to simulate the Monty Hall 1000 times."
  },
  {
    "objectID": "gradstats/gradlabs/1_gradintro.html#footnotes",
    "href": "gradstats/gradlabs/1_gradintro.html#footnotes",
    "title": "Welcome to Psych 205",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThanks to Frederic Theunnisen & Steve Pantiadosi for sharing their knowledge and materials with me.‚Ü©Ô∏é"
  },
  {
    "objectID": "gradstats/gradlabs/1_gradintro.html#introductions-to-statistics",
    "href": "gradstats/gradlabs/1_gradintro.html#introductions-to-statistics",
    "title": "Welcome to Psych 205",
    "section": "",
    "text": "Darby : stats is the foundation for all the research?\nKimy : proper use of statistics as evidence in support of our arguments; need to get it right so avoid errors.\nSabana : stats is an objective way to look at our data\nYoko : statistics can help you be transparent about the data; what criteria you used.\nRishika : stats is reliable; reproducable\nSebastian : involves collection of numbers, data‚Ä¶and numbers don‚Äôt lie (if used correctly)???\nJeremy : set some hypotheses and test whether they are true or not.\nYoung Joo : use statistics to test theories, evalaute others data; interpret the world.\n\n\n\n\n\n\nScience is‚Ä¶.\n\nDescription : The topics of your research interests!\nPrediction : An educated guess you have about the future.\nPower : How you use this knowledge (and predictions) to exert influence over the world / your future.\n\nRosaline : predicting how adverse childhood experiences lead to negative outcomes for kids‚Ä¶and then identify those triggers and try to prevent them from happening; developing effective interventions.\nAnn : children and adolescents with ADHD & epilepsy have lower psychological adjustments; prediction about what can improve psychological adjustment ‚Äì&gt; improved outcomes.\n\n\nIs Psychology a Science?\nShould Psychology be a Science?\n\n\n\n\nWhat happens in a good language class?\n\n\n\n\n\n\n\nHuman Language\nStatistics / Computer Language\n\n\n\n\nMistakes happen (it‚Äôs okay! normal!! part of the process); but also you are misunderstood\nYou will make mistakes and the computer will misunderstand / break (but that‚Äôs okay)\n\n\nGrammar / Syntax (the rules) and exceptions\nthere are rules we will have to follow!\n\n\nCultural immersion\nCoding all the time; Immersed in the data in some sort of flow state; understand the context of your dataset / the methods; listening to stats podcasts; reading research articles; learning about who invented the mean / regression???\n\n\nVocabulary\ncertain words have meaning in R\n\n\nFluency and proficiency allow you to think about higher level issues (prose, tone)\nwith fluency will come ability to look at ‚Äúclean‚Äù data\n\n\nit takes practice and time and patience"
  },
  {
    "objectID": "gradstats/gradlabs/1_gradintro.html#introductions-to-r-a-programming-language",
    "href": "gradstats/gradlabs/1_gradintro.html#introductions-to-r-a-programming-language",
    "title": "Welcome to Psych 205",
    "section": "",
    "text": "The console is where R does its work.\n\nACTIVITY : Look at the image below. What do you see? What makes sense / what seems confusing?\n\n\n\n\n\nACTIVTY : open up R and type some MATH into the console. Yeah! You are programming!!!\n\n\n\n\nIn this class, we‚Äôll be using RStudio. RStudio is an IDE (Integrated Development Environment) that includes the console along with other useful windows and tools.\n\nThe Console is at the bottom left of the IDE. Hi console!\nThe R script is at the top left of the IDE, and is a document that you use to write (and organize) code. You will want to do most of your work in the R script, and feel an appropriate level of anxiety when you notice that your Rscript is unsaved (as indicated by the red text and *).\nThe Environment is at the top right of the IDE, and shows you all of the ‚Äúobjects‚Äù that you have defined in R.\nThe File Window is at the bottom right of the IDE, and shows you the files. Note that there are tabs here for Plots (where graphs will pop up), Packages (things you can download to give R extra features), a Help viewer (sometimes very useful!).\n\n\n\n\n\nHere‚Äôs a link to an RScript!You can download this to your computer, and open it in RStudio.\nTopics We Will Review\n\nmath : common operators and functions\nvariables : string and numeric vectors\ndatasets : data.frame(); datasets::\nfunctions : descriptive statistics, base graphics, generating random numbers\ngood code : troubleshooting errors; naming datasets; white space; keeping code and files organized"
  },
  {
    "objectID": "gradstats/gradlabs/1_gradintro.html#is-there-time-left-a-fun-r-demonstration",
    "href": "gradstats/gradlabs/1_gradintro.html#is-there-time-left-a-fun-r-demonstration",
    "title": "Welcome to Psych 205",
    "section": "",
    "text": "You‚Äôll need to use the following bits of code :\nvariable &lt;- c(\"\") # create a string variable (for the options)\nsample() # to \"randomly\" sample from this variable\nsetdiff() # to differentiate \nYou‚Äôre welcome to try this on your own, but below are steps to help guide your work if you‚Äôd like (and some extra hints if you‚Äôd like more specific instructions.)\nTotally okay if you struggle with this! The point is to try and do something fun (and potentially challenging) in R, and see how computers can be used to address research questions.\n\nDefine the options to choose from (three doors)\n\n\n\n\n\n\n\nHINT\n\n\n\n\n\nCreate a string variable with three options : door1, door2, and door3\n\n\n\n\nDefine player choice and treasure location.\n\n\n\n\n\n\n\nHINT\n\n\n\n\n\nuse the sample() function to define two variables named choice and treasure; each a random door.\n\n\n\n\nDefine what doors Monty can open.\n\n\n\n\n\n\n\nHINT\n\n\n\n\n\nUse the setdiff() function to identify all the doors that are not what the player chose or where the treasure is.\n\n\n\n\nDefine what Monty does open.\n\n\n\n\n\n\n\nHINT\n\n\n\n\n\nUse the sample function to select one random door from the variable that defines what doors Monty can open.\n\n\n\n\nWhat happens if we switch?\n\n\n\n\n\n\n\nHINT\n\n\n\n\n\nDefine a new variable that identifies the choice if we switch; this should selects one random option from the list of doors that are NOT what Monty opened or what the player initially chose. NOTE : You will need to use the sample and setdiff functions together.\n\n\n\n\nTest whether the player won or not.\n\n\n\n\n\n\n\nHINT\n\n\n\n\n\nUse the if-else function to define a ‚ÄúWIN‚Äù as whether the choice variable matches the treasure variable. Then repeat this step for whether the switch choice variable matches the treasure variable.\n\n\n\n\nRepeat these steps 1000 times.\n\n\n\n\n\n\n\nHINT\n\n\n\n\n\nDefine an empty array to save the result of each test. Then define a for-loop to simulate the Monty Hall 1000 times."
  },
  {
    "objectID": "gradstats/gradlabs/1_gradintro.html#conclusion-and-next-time",
    "href": "gradstats/gradlabs/1_gradintro.html#conclusion-and-next-time",
    "title": "Welcome to Psych 205",
    "section": "",
    "text": "Exit Survey\nComplete Lab 1\nSign up for an Article Presentation\nRead (& Discussion Post) Article 1"
  },
  {
    "objectID": "gradstats/gradstatsSP25.html",
    "href": "gradstats/gradstatsSP25.html",
    "title": "Psych 205 (Graduate Statistics - SP25)",
    "section": "",
    "text": "Hi class! Welcome to our class.\n\n\n\nProfessor | Arman Daniel Catterson, PhD [he / his / ÿßŸà ] üê±(catterson@berkeley.edu)\n\nLectures : Fridays 9:00 - 12:00 PM in 1213 Berkeley Way West\nOffice Hours : Mondays 4:00 - 5:00 PM on Zoom (e-mail / chat after class to find another time)\n\nTeaching Assistant | Sierra Semko Krouse [she / her] | sierra_semko@berkeley.edu\n\nSections : Fridays, 1:00 - 2:00 PM in 1213 Berkeley Way West\nOffice Hours : Wednesdays 11:00 - 12:00 PM via Zoom (click here to sign-up for a 15-minute slot)\n\n\n\n\n\n\n\n\nWeek\nLecture Topic\nAssignment\nSupport Readings\n\n\n\n\n1/24\nIntroductions (RScript)\nLab 1 (.R) + Key\nWS 1-2; LSR 3-4\n\n\n1/31\nDescription (.R script)\nLab 2 (.qmd) + Key\nWS 3-4; LSR 5-6\n\n\n2/7\nNormal and Sampling Distributions\nLab 3 (.qmd) + Key\nWS 8; LSR 9-10\n\n\n2/14\nLinear Models Pt. 1 (Lines!)\nLab 4 (.qmd) + Key\nWS 5; LSR 15.1 - 15.2\n\n\n2/21\nLinear Models Pt. 2 (assumptions and ggplot2)\nLab 5 (.qmd) + Key\nLSR 15.5 - 15.11\nData Viz Chapter\n\n\n2/28\nMini Exam\n\n\n\n\n3/7\nLinear Models Pt. 3 (t-tests)\nLab 6 (.qmd) + Key\nT-Tests : WS6 & WS7; LSR 13\nNHST : WS8\n\n\n3/14\nLinear Models Pt. 4 (Power and Multiple Regression)\nLab 7 (.qmd) + Key\nWS9 & WS10; LSR 15.3\n\n\n3/21\nLinear Models Pt. 5 (Interaction Effects)\nLab 8 (Self-Care Lab)\n\n\n\n3/28\nSpring Break!!!\n\n\n\n\n4/4\nLinear Models Pt. 6 (GLM)\nLab 9 (.qmd)\nGeneralized Linear Models\n\n\n4/11\nLinear Models Pt 7 : Model Comparison\nLab 10 [due 5/9]\n\n\n\n4/18\nLinear Models Pt. 8 (Mixed Models)\nThis is Not an Exam [Assigned]\nMixed Models with R\n\n\n4/25\nLinear Models Pt. 9 (More Mixed Models)\nProject Draft [Due 5/2]\n\n\n\n5/2\nConclusion / Project Workshop\nProject Due [5/9]\nCore SEM\n\n\n\nNote : Support Readings are assigned from my undergraduate text Why Statistics? (WS) and https://learningstatisticswithr.com/book/ (LSR) for students looking for more support.\n\n\n\nYour grade in the class will be based on the following components.\n\n\n\n\n\n\n\n\n10% Attendance\n20% R Exams [10 + 10]\n10% Article Presentations\n\n\n20% Lab Assignments\n30% Final Project\n10% Article Discussions\n\n\n\nLetter grades will be based on the following : A+ is &gt; 96.5; A is &gt; 92.5; A- is &gt; 89.5; etc. Note that an 89.49999 is a B+.)\n\n\n\n\nAttendance (Lecture and Discussion Section): Because we all learn from each other; attendance is required. You may miss up to two lectures and two discussion sections with no penalty.\nArticle Discussion. Each week, students will be responsible for leading class discussion of one article. All students should plan to read the article, and post comments to a discussion forum on bCourses. See the Student Presentation - Description and Rubric for a list of articles, a link to sign up, and a description of what is required for these student-led presentations. Let me know if you have other ideas for ways to engage the class in discussion about an article (or another article you think you would be good for the class to read.)\nLab Assignments : Most weeks, you will work through a ‚Äúlab‚Äù assignment that is designed to help practice the skills we review in lecture and readings. You can expect to work on these in lecture, in your discussion section, and at home as needed. Each lab document will have a set of questions to answer - you should submit answers to these questions as a separate document to bCourses. Lab assignments are due the week after they are assigned; late lab assignments will not be accepted. You can drop your lowest submitted lab assignment.\nFinal Project (Research Paper) : You‚Äôll apply the skills you have learned this semester to analyze, interpret, and share results from a research question relevant to your research interests. There will be several milestones to support this assignment. See the Final Project - Description and Rubric for more information.\nR Exam : For each exam, I‚Äôll give you a novel dataset and you‚Äôll use R to answer questions based on the dataset. The exam will be open-book and open note - you must work alone, but may use any resource available to you (including the Internet). A practice exam will be posted at least one week before the exam. The mini exam should take no more than 50 minutes to complete (we will have class afterward), and you will have the full 3-hours to complete the mega exam.\nExtra Credit : There will be no extra credit offered in this course, even for students who ask kindly.¬†\n\n\n\n\n\nComputer Access. Access to a personal computer is strongly recommended. We will spend time each lecture working in R - a free and powerful statistics program that works with all operating systems and computers. Laptops are available for rent from the library (see Berkeley‚Äôs Technology Loan Program), or you can purchase one that will work for this class for as little as $150 (less than the price of most textbooks, which is not required for this class).\nLate Work Policy. Late assignments will not be accepted. You may drop your lowest scoring lab assignment. Let me know if you cannot attend the exam and we can schedule a make-up exam.\nDiscussion Section Policy : In-person attendance is expected and required for discussion sections. Students may miss up to two discussion sections with no penalty. Please talk to your GSI / the professor if there is a longer-term illness or issue that will prevent you from regularly attending your discussion section and we will work out another arrangement.\nRegrades. Students may ask for a re-grade on exams and papers if they believe they lost points for something they should have earned. To request a regrade, talk to your GSI in office hours or set up a meeting. In the case that you and your GSI cannot resolve the regrade issue, the professor will step in and regrade the exam. For final project regrades just e-mail the professor. When requesting a regrade, you consent to have your score increased or decreased if we find that you earned points you should not have earned.\nStudent Contact. It is your responsibility to regularly check e-mail for announcements. If you have specific questions to e-mail me, please make sure to (a) search the syllabus and / or Google for the answer to your question and (b) contact your GSI about your question. I am always happy to answer any questions in office hours (either during the scheduled time or online by appointment), or before, during, or after lecture. I should respond to e-mail within 24 hours on weekdays (I do not check e-mail on weekends); please send a gentle reminder if I do not respond in this timeframe. Thanks!\nAcademic Integrity. Do NOT cheat. Do NOT plagiarize. To copy text or ideas from another source (including your own submitted coursework) without appropriate reference is plagiarism. You may work on lab assignments with other students, but should be able to understand what you are doing. You may not work with other students on the R exam. Anyone caught cheating or plagiarizing will receive a zero on the assignment or test, and will be reported to the Office of Student Conduct. It is your responsibility to read the official Student Conduct Policy for more information about campus standards and policies regarding Academic Integrity.\nChat-GPT / AI Policy. If you use ChatGPT or other generative-AI software for any part of this class (writing, coding, troubleshooting, etc.), you must make this clear at the top of your assignment by writing : ‚ÄúI USED CHAT GPT TO SUPPORT THIS ASSIGNMENT‚Äù (in all-caps and red text), and include an appendix where you include screenshots of your specific queries and answers. Failure to do this will result in a zero of the assignment, and the professor submitting a report to the Academic Integrity Office. Ask your GSI / professor if you have questions about this policy.\nRespect for Others. We all belong in our classroom, and I hope that everyone feels free and supported to express our identities when relevant. To achieve this goal, make sure that you are treating others with respect in this course. This means considering others‚Äô opinions and perspectives, making sure not to generalize to groups (or ask students to be representatives of their group), and being mindful of the words that you use. (Also remember that nothing is ever really deleted from the internet, so please be extra mindful of the words you use.) While I will be monitoring class posts and communication, feel free to reach out to me if you see another student engaging in disrespectful or threatening behavior. Of course, this extends to me as well - if you feel like there are parts of the course instruction, subject matter, or class environment that create barriers to your success or inclusion, please contact me via e-mail and trust that your voice will be heard and I will not punish you for offering constructive criticisms of my teaching. I‚Äôm very open to learning how to be a better teacher, and always learn a lot from my students.\nSensitive Subjects. Our class will touch on important and potentially sensitive topics that are related to psychology and psychological processes, such as racism, depression, sex, and poverty. These topics can generate strong emotional responses in students for a variety of reasons - such as their own past histories with these topics or their reactions to the ways the information is presented or described. I will try to make sure to give you a heads up when we start to discuss these topics so you can be mindful of your reactions.\nStrategies for success. Do your readings, complete assignments on time, understand how the topics and information relate to what you‚Äôve learned (and are learning), and if you don‚Äôt understand something, please ask the question!\nMost important. Please let me know as soon as there is anything going on in your life that you think may affect your ability to do as well as you would like in this class (e.g.¬†sickness, work, small children at home, threats to immigration status, etc.), and I will do my best to work with you on a plan to succeed in the class. If you‚Äôre not comfortable talking to me directly, you can talk to your GSI who can talk to me about your issue.\n\n\n\n\n\nStudents with Disabilities : Students who require support or adaptive equipment because of a specific disability ‚Äì or would like to be tested to see if they qualify for a learning, auditory, or visual disability ‚Äì can request these services through the Disabled Students Program (DSP) office. Please note that I can only provide accommodations authorized by the DSP office. If you have DSP status, make sure you submit your letter and feel free to talk to me or a GSI about any specific accommodations you need, or other ways we might be able to make the class more accessible. [260 C√©sar E. Ch√°vez Student Center| 510-642-0518 | https://dsp.berkeley.edu/]\nStudents with Mental Health Issues : If you feel a mental health issue (e.g., depression, anxiety) is negatively impacting your ability to succeed, I‚Äôd highly encourage you to seek out experts who can help. Berkeley has several free, confidential, and science-based programs designed to help students. [https://uhs.berkeley.edu/caps | Tang Center 3rd Floor | 510-642-9494 | After hours support line : 855-817-5667]\nUndocumented Students : I believe that all people deserve equal access to education. The UC Berkeley Undocumented Student Program has organized resources to support undocumented students, protect their rights, and offer potential financial aid | https://undocu.berkeley.edu/\nStudents from Low-Income Backgrounds : The Extended Opportunity Program (EOP) provides college support services for low-income students. Services include additional counseling, financial assistance (study-time parent grants, work-study assignments, and book vouchers), child-care opportunities, and assistance transferring to four-year colleges. [119 C√©sar E. Chavez Student Center | 510-642-7224 |¬† https://eop.berkeley.edu/]\n\n\n\n\nThe goals of this final project are to 1) demonstrate the research and data analytic skills that we have learned in this class, and 2) work on something that will be useful to your research career.¬†\nNote : this is my first semester assigning this assignment for a graduate course, and open to other ideas about how this assignment might better serve as a relevant reflection of what you have learned in the class, and your interests as a researcher.\n\n\n\n\n\n\n\nCriterion\nPoints [Out of 4]\n\n\nStudent specified a specific research question to ask, articulated how this research question is relevant to their interests, background, and past psychological research, and identified a dataset that will allow them to answer this research question.\n\n\n\nStudent submitted a pre-registration plan that details the data cleaning and analyses they expect to do in order to answer their research question, a list of a post-hoc analyses that deviated from the pre-registered plan, and a link to the data and RCode.\n\n\n\nStudent conducted analyses appropriate to the research question, organized these analyses in a table, and explained the results of these analyses (descriptive, predictive, and inferential statistics).\n\n\n\nStudent included clear, presentation-ready figures to illustrate their key analyses (descriptive, predictive, and inferential statistics).\n\n\n\nStudent shared the project in a way that clearly communicates their findings and why they matter (to psychological research and / or society), articulates limitations of the project, and advances next steps / future research directions they might pursue.\n\n\n\n\n\n\n\nSign-Ups + List of Readings¬†\nEach week, I‚Äôve assigned an article for the class to read. (Let me know if you have ideas for other articles to read in this / future semesters!)\n\nAll students will submit discussion responses by Wednesday at noon; these discussion posts should demonstrate you have done the reading, and highlight a question or discussion topic you think would be good to focus on more in class.\nA small group of students will be responsible for organizing a 15-20 minute presentation and discussion on the assigned article, based on the reading and submitted student discussion posts.\n\nThis presentation will be graded on the following criteria. Though I‚Äôm very open to ideas and suggestions about alternative ways you think we could effectively use this time!\n\n\n\n\n\n\n\nCriterion\nPoints [Out of 4]\n\n\nStudents gave a quick (5 min) summary of the key points from the assigned article.\n\n\n\nStudents drew from submitted discussion posts to lead a class discussion that helped answer common questions, debate ideas, and connect readings to our work as researchers.\n\n\n\nStudents organized a presentation and / or handout to help guide audience understanding.\n\n\n\nStudents took notes from the presentation (and added these to the presentation document) and updated the reading list with any student-submitted readings / references that would support the article.\n\n\n\nStudent presentation demonstrated preparation, organization, thought, and coordination among group members."
  },
  {
    "objectID": "gradstats/gradstatsSP25.html#course-information",
    "href": "gradstats/gradstatsSP25.html#course-information",
    "title": "Psych 205 (Graduate Statistics - SP25)",
    "section": "",
    "text": "Professor | Arman Daniel Catterson, PhD [he / his / ÿßŸà ] üê±(catterson@berkeley.edu)\n\nLectures : Fridays 9:00 - 12:00 PM in 1213 Berkeley Way West\nOffice Hours : Mondays 4:00 - 5:00 PM on Zoom (e-mail / chat after class to find another time)\n\nTeaching Assistant | Sierra Semko Krouse [she / her] | sierra_semko@berkeley.edu\n\nSections : Fridays, 1:00 - 2:00 PM in 1213 Berkeley Way West\nOffice Hours : Wednesdays 11:00 - 12:00 PM via Zoom (click here to sign-up for a 15-minute slot)"
  },
  {
    "objectID": "gradstats/gradstatsSP25.html#semester-agenda",
    "href": "gradstats/gradstatsSP25.html#semester-agenda",
    "title": "Psych 205 (Graduate Statistics - SP25)",
    "section": "",
    "text": "Week\nLecture Topic\nAssignment\nSupport Readings\n\n\n\n\n1/24\nIntroductions (RScript)\nLab 1 (.R) + Key\nWS 1-2; LSR 3-4\n\n\n1/31\nDescription (.R script)\nLab 2 (.qmd) + Key\nWS 3-4; LSR 5-6\n\n\n2/7\nNormal and Sampling Distributions\nLab 3 (.qmd) + Key\nWS 8; LSR 9-10\n\n\n2/14\nLinear Models Pt. 1 (Lines!)\nLab 4 (.qmd) + Key\nWS 5; LSR 15.1 - 15.2\n\n\n2/21\nLinear Models Pt. 2 (assumptions and ggplot2)\nLab 5 (.qmd) + Key\nLSR 15.5 - 15.11\nData Viz Chapter\n\n\n2/28\nMini Exam\n\n\n\n\n3/7\nLinear Models Pt. 3 (t-tests)\nLab 6 (.qmd) + Key\nT-Tests : WS6 & WS7; LSR 13\nNHST : WS8\n\n\n3/14\nLinear Models Pt. 4 (Power and Multiple Regression)\nLab 7 (.qmd) + Key\nWS9 & WS10; LSR 15.3\n\n\n3/21\nLinear Models Pt. 5 (Interaction Effects)\nLab 8 (Self-Care Lab)\n\n\n\n3/28\nSpring Break!!!\n\n\n\n\n4/4\nLinear Models Pt. 6 (GLM)\nLab 9 (.qmd)\nGeneralized Linear Models\n\n\n4/11\nLinear Models Pt 7 : Model Comparison\nLab 10 [due 5/9]\n\n\n\n4/18\nLinear Models Pt. 8 (Mixed Models)\nThis is Not an Exam [Assigned]\nMixed Models with R\n\n\n4/25\nLinear Models Pt. 9 (More Mixed Models)\nProject Draft [Due 5/2]\n\n\n\n5/2\nConclusion / Project Workshop\nProject Due [5/9]\nCore SEM\n\n\n\nNote : Support Readings are assigned from my undergraduate text Why Statistics? (WS) and https://learningstatisticswithr.com/book/ (LSR) for students looking for more support."
  },
  {
    "objectID": "gradstats/gradstatsSP25.html#grade-details",
    "href": "gradstats/gradstatsSP25.html#grade-details",
    "title": "Psych 205 (Graduate Statistics - SP25)",
    "section": "",
    "text": "Your grade in the class will be based on the following components.\n\n\n\n\n\n\n\n\n10% Attendance\n20% R Exams [10 + 10]\n10% Article Presentations\n\n\n20% Lab Assignments\n30% Final Project\n10% Article Discussions\n\n\n\nLetter grades will be based on the following : A+ is &gt; 96.5; A is &gt; 92.5; A- is &gt; 89.5; etc. Note that an 89.49999 is a B+.)"
  },
  {
    "objectID": "gradstats/gradstatsSP25.html#summary-of-course-components",
    "href": "gradstats/gradstatsSP25.html#summary-of-course-components",
    "title": "Psych 205 (Graduate Statistics - SP25)",
    "section": "",
    "text": "Attendance (Lecture and Discussion Section): Because we all learn from each other; attendance is required. You may miss up to two lectures and two discussion sections with no penalty.\nArticle Discussion. Each week, students will be responsible for leading class discussion of one article. All students should plan to read the article, and post comments to a discussion forum on bCourses. See the Student Presentation - Description and Rubric for a list of articles, a link to sign up, and a description of what is required for these student-led presentations. Let me know if you have other ideas for ways to engage the class in discussion about an article (or another article you think you would be good for the class to read.)\nLab Assignments : Most weeks, you will work through a ‚Äúlab‚Äù assignment that is designed to help practice the skills we review in lecture and readings. You can expect to work on these in lecture, in your discussion section, and at home as needed. Each lab document will have a set of questions to answer - you should submit answers to these questions as a separate document to bCourses. Lab assignments are due the week after they are assigned; late lab assignments will not be accepted. You can drop your lowest submitted lab assignment.\nFinal Project (Research Paper) : You‚Äôll apply the skills you have learned this semester to analyze, interpret, and share results from a research question relevant to your research interests. There will be several milestones to support this assignment. See the Final Project - Description and Rubric for more information.\nR Exam : For each exam, I‚Äôll give you a novel dataset and you‚Äôll use R to answer questions based on the dataset. The exam will be open-book and open note - you must work alone, but may use any resource available to you (including the Internet). A practice exam will be posted at least one week before the exam. The mini exam should take no more than 50 minutes to complete (we will have class afterward), and you will have the full 3-hours to complete the mega exam.\nExtra Credit : There will be no extra credit offered in this course, even for students who ask kindly."
  },
  {
    "objectID": "gradstats/gradstatsSP25.html#course-policies",
    "href": "gradstats/gradstatsSP25.html#course-policies",
    "title": "Psych 205 (Graduate Statistics - SP25)",
    "section": "",
    "text": "Computer Access. Access to a personal computer is strongly recommended. We will spend time each lecture working in R - a free and powerful statistics program that works with all operating systems and computers. Laptops are available for rent from the library (see Berkeley‚Äôs Technology Loan Program), or you can purchase one that will work for this class for as little as $150 (less than the price of most textbooks, which is not required for this class).\nLate Work Policy. Late assignments will not be accepted. You may drop your lowest scoring lab assignment. Let me know if you cannot attend the exam and we can schedule a make-up exam.\nDiscussion Section Policy : In-person attendance is expected and required for discussion sections. Students may miss up to two discussion sections with no penalty. Please talk to your GSI / the professor if there is a longer-term illness or issue that will prevent you from regularly attending your discussion section and we will work out another arrangement.\nRegrades. Students may ask for a re-grade on exams and papers if they believe they lost points for something they should have earned. To request a regrade, talk to your GSI in office hours or set up a meeting. In the case that you and your GSI cannot resolve the regrade issue, the professor will step in and regrade the exam. For final project regrades just e-mail the professor. When requesting a regrade, you consent to have your score increased or decreased if we find that you earned points you should not have earned.\nStudent Contact. It is your responsibility to regularly check e-mail for announcements. If you have specific questions to e-mail me, please make sure to (a) search the syllabus and / or Google for the answer to your question and (b) contact your GSI about your question. I am always happy to answer any questions in office hours (either during the scheduled time or online by appointment), or before, during, or after lecture. I should respond to e-mail within 24 hours on weekdays (I do not check e-mail on weekends); please send a gentle reminder if I do not respond in this timeframe. Thanks!\nAcademic Integrity. Do NOT cheat. Do NOT plagiarize. To copy text or ideas from another source (including your own submitted coursework) without appropriate reference is plagiarism. You may work on lab assignments with other students, but should be able to understand what you are doing. You may not work with other students on the R exam. Anyone caught cheating or plagiarizing will receive a zero on the assignment or test, and will be reported to the Office of Student Conduct. It is your responsibility to read the official Student Conduct Policy for more information about campus standards and policies regarding Academic Integrity.\nChat-GPT / AI Policy. If you use ChatGPT or other generative-AI software for any part of this class (writing, coding, troubleshooting, etc.), you must make this clear at the top of your assignment by writing : ‚ÄúI USED CHAT GPT TO SUPPORT THIS ASSIGNMENT‚Äù (in all-caps and red text), and include an appendix where you include screenshots of your specific queries and answers. Failure to do this will result in a zero of the assignment, and the professor submitting a report to the Academic Integrity Office. Ask your GSI / professor if you have questions about this policy.\nRespect for Others. We all belong in our classroom, and I hope that everyone feels free and supported to express our identities when relevant. To achieve this goal, make sure that you are treating others with respect in this course. This means considering others‚Äô opinions and perspectives, making sure not to generalize to groups (or ask students to be representatives of their group), and being mindful of the words that you use. (Also remember that nothing is ever really deleted from the internet, so please be extra mindful of the words you use.) While I will be monitoring class posts and communication, feel free to reach out to me if you see another student engaging in disrespectful or threatening behavior. Of course, this extends to me as well - if you feel like there are parts of the course instruction, subject matter, or class environment that create barriers to your success or inclusion, please contact me via e-mail and trust that your voice will be heard and I will not punish you for offering constructive criticisms of my teaching. I‚Äôm very open to learning how to be a better teacher, and always learn a lot from my students.\nSensitive Subjects. Our class will touch on important and potentially sensitive topics that are related to psychology and psychological processes, such as racism, depression, sex, and poverty. These topics can generate strong emotional responses in students for a variety of reasons - such as their own past histories with these topics or their reactions to the ways the information is presented or described. I will try to make sure to give you a heads up when we start to discuss these topics so you can be mindful of your reactions.\nStrategies for success. Do your readings, complete assignments on time, understand how the topics and information relate to what you‚Äôve learned (and are learning), and if you don‚Äôt understand something, please ask the question!\nMost important. Please let me know as soon as there is anything going on in your life that you think may affect your ability to do as well as you would like in this class (e.g.¬†sickness, work, small children at home, threats to immigration status, etc.), and I will do my best to work with you on a plan to succeed in the class. If you‚Äôre not comfortable talking to me directly, you can talk to your GSI who can talk to me about your issue."
  },
  {
    "objectID": "gradstats/gradstatsSP25.html#student-support-services",
    "href": "gradstats/gradstatsSP25.html#student-support-services",
    "title": "Psych 205 (Graduate Statistics - SP25)",
    "section": "",
    "text": "Students with Disabilities : Students who require support or adaptive equipment because of a specific disability ‚Äì or would like to be tested to see if they qualify for a learning, auditory, or visual disability ‚Äì can request these services through the Disabled Students Program (DSP) office. Please note that I can only provide accommodations authorized by the DSP office. If you have DSP status, make sure you submit your letter and feel free to talk to me or a GSI about any specific accommodations you need, or other ways we might be able to make the class more accessible. [260 C√©sar E. Ch√°vez Student Center| 510-642-0518 | https://dsp.berkeley.edu/]\nStudents with Mental Health Issues : If you feel a mental health issue (e.g., depression, anxiety) is negatively impacting your ability to succeed, I‚Äôd highly encourage you to seek out experts who can help. Berkeley has several free, confidential, and science-based programs designed to help students. [https://uhs.berkeley.edu/caps | Tang Center 3rd Floor | 510-642-9494 | After hours support line : 855-817-5667]\nUndocumented Students : I believe that all people deserve equal access to education. The UC Berkeley Undocumented Student Program has organized resources to support undocumented students, protect their rights, and offer potential financial aid | https://undocu.berkeley.edu/\nStudents from Low-Income Backgrounds : The Extended Opportunity Program (EOP) provides college support services for low-income students. Services include additional counseling, financial assistance (study-time parent grants, work-study assignments, and book vouchers), child-care opportunities, and assistance transferring to four-year colleges. [119 C√©sar E. Chavez Student Center | 510-642-7224 |¬† https://eop.berkeley.edu/]"
  },
  {
    "objectID": "gradstats/gradstatsSP25.html#final-project-description-and-rubric",
    "href": "gradstats/gradstatsSP25.html#final-project-description-and-rubric",
    "title": "Psych 205 (Graduate Statistics - SP25)",
    "section": "",
    "text": "The goals of this final project are to 1) demonstrate the research and data analytic skills that we have learned in this class, and 2) work on something that will be useful to your research career.¬†\nNote : this is my first semester assigning this assignment for a graduate course, and open to other ideas about how this assignment might better serve as a relevant reflection of what you have learned in the class, and your interests as a researcher.\n\n\n\n\n\n\n\nCriterion\nPoints [Out of 4]\n\n\nStudent specified a specific research question to ask, articulated how this research question is relevant to their interests, background, and past psychological research, and identified a dataset that will allow them to answer this research question.\n\n\n\nStudent submitted a pre-registration plan that details the data cleaning and analyses they expect to do in order to answer their research question, a list of a post-hoc analyses that deviated from the pre-registered plan, and a link to the data and RCode.\n\n\n\nStudent conducted analyses appropriate to the research question, organized these analyses in a table, and explained the results of these analyses (descriptive, predictive, and inferential statistics).\n\n\n\nStudent included clear, presentation-ready figures to illustrate their key analyses (descriptive, predictive, and inferential statistics).\n\n\n\nStudent shared the project in a way that clearly communicates their findings and why they matter (to psychological research and / or society), articulates limitations of the project, and advances next steps / future research directions they might pursue."
  },
  {
    "objectID": "gradstats/gradstatsSP25.html#student-presentation-description-and-rubric",
    "href": "gradstats/gradstatsSP25.html#student-presentation-description-and-rubric",
    "title": "Psych 205 (Graduate Statistics - SP25)",
    "section": "",
    "text": "Sign-Ups + List of Readings¬†\nEach week, I‚Äôve assigned an article for the class to read. (Let me know if you have ideas for other articles to read in this / future semesters!)\n\nAll students will submit discussion responses by Wednesday at noon; these discussion posts should demonstrate you have done the reading, and highlight a question or discussion topic you think would be good to focus on more in class.\nA small group of students will be responsible for organizing a 15-20 minute presentation and discussion on the assigned article, based on the reading and submitted student discussion posts.\n\nThis presentation will be graded on the following criteria. Though I‚Äôm very open to ideas and suggestions about alternative ways you think we could effectively use this time!\n\n\n\n\n\n\n\nCriterion\nPoints [Out of 4]\n\n\nStudents gave a quick (5 min) summary of the key points from the assigned article.\n\n\n\nStudents drew from submitted discussion posts to lead a class discussion that helped answer common questions, debate ideas, and connect readings to our work as researchers.\n\n\n\nStudents organized a presentation and / or handout to help guide audience understanding.\n\n\n\nStudent presentation demonstrated preparation, organization, thought, and coordination among group members.\n\n\n\nStudents completed the self- and peer-evaluation form."
  },
  {
    "objectID": "gradstats/gradstatsSP25.html#final-project---description-and-rubric",
    "href": "gradstats/gradstatsSP25.html#final-project---description-and-rubric",
    "title": "Psych 205 (Graduate Statistics - SP25)",
    "section": "",
    "text": "The goals of this final project are to 1) demonstrate the research and data analytic skills that we have learned in this class, and 2) work on something that will be useful to your research career.¬†\nNote : this is my first semester assigning this assignment for a graduate course, and open to other ideas about how this assignment might better serve as a relevant reflection of what you have learned in the class, and your interests as a researcher.\n\n\n\n\n\n\n\nCriterion\nPoints [Out of 4]\n\n\nStudent specified a specific research question to ask, articulated how this research question is relevant to their interests, background, and past psychological research, and identified a dataset that will allow them to answer this research question.\n\n\n\nStudent submitted a pre-registration plan that details the data cleaning and analyses they expect to do in order to answer their research question, a list of a post-hoc analyses that deviated from the pre-registered plan, and a link to the data and RCode.\n\n\n\nStudent conducted analyses appropriate to the research question, organized these analyses in a table, and explained the results of these analyses (descriptive, predictive, and inferential statistics).\n\n\n\nStudent included clear, presentation-ready figures to illustrate their key analyses (descriptive, predictive, and inferential statistics).\n\n\n\nStudent shared the project in a way that clearly communicates their findings and why they matter (to psychological research and / or society), articulates limitations of the project, and advances next steps / future research directions they might pursue."
  },
  {
    "objectID": "gradstats/gradstatsSP25.html#student-presentation---description-and-rubric",
    "href": "gradstats/gradstatsSP25.html#student-presentation---description-and-rubric",
    "title": "Psych 205 (Graduate Statistics - SP25)",
    "section": "",
    "text": "Sign-Ups + List of Readings¬†\nEach week, I‚Äôve assigned an article for the class to read. (Let me know if you have ideas for other articles to read in this / future semesters!)\n\nAll students will submit discussion responses by Wednesday at noon; these discussion posts should demonstrate you have done the reading, and highlight a question or discussion topic you think would be good to focus on more in class.\nA small group of students will be responsible for organizing a 15-20 minute presentation and discussion on the assigned article, based on the reading and submitted student discussion posts.\n\nThis presentation will be graded on the following criteria. Though I‚Äôm very open to ideas and suggestions about alternative ways you think we could effectively use this time!\n\n\n\n\n\n\n\nCriterion\nPoints [Out of 4]\n\n\nStudents gave a quick (5 min) summary of the key points from the assigned article.\n\n\n\nStudents drew from submitted discussion posts to lead a class discussion that helped answer common questions, debate ideas, and connect readings to our work as researchers.\n\n\n\nStudents organized a presentation and / or handout to help guide audience understanding.\n\n\n\nStudents took notes from the presentation (and added these to the presentation document) and updated the reading list with any student-submitted readings / references that would support the article.\n\n\n\nStudent presentation demonstrated preparation, organization, thought, and coordination among group members."
  },
  {
    "objectID": "gradstats/gradlabs/2_descriptionvisual.html",
    "href": "gradstats/gradlabs/2_descriptionvisual.html",
    "title": "Psych 205 - Lab 2",
    "section": "",
    "text": "The goal of this lab is to get practice describing data and working in Quarto.\n\n\nWe will work on these questions together in lecture!\n\nOne advantage of R Markdown is that you can run code in a document. You do this using a ‚Äúcode block‚Äù. In the space below, insert an R code block, type out a math equation that used to give you difficulty as a kid into the code block below, and run the code to see the result. Below the codeblock, add text for humans to determine whether R got the math problem correct or not. Then render the document as a .pdf and .html file. Did this work?\n\n\nIn Class. We practiced how to load, navigate, and describing data from a dataset. Use R to complete the following.\n\nLoad the\n\n\n\nR also has some built in datasets. The chickwts dataset contains data on the weights of chickens, and the types of food that they ate. You can access this datasets by just typing chickwts (the name of the data object) into R. Use this dataset to answer the following questions:\n\nthe number of individuals (rows) in the dataset.\nthe number of variables (columns) in the dataset.\nthe names of the variables in the dataset.\nthe value of the 5th row, 2nd column in the dataset.\nprint a numeric variable from the dataset\nprint a categorical variable from the dataset\n\n\n\n\n\nYou will work with your TA on these questions.\n\nIn lecture, we created a dataframe in R with two variables. Work with your classmates to create another dataframe with three variables - at least one numeric and one ‚Äústring‚Äù. Then, use R to report the following from this dataset:\n\nthe number of individuals (rows) in the dataset.\nthe number of variables (columns) in the dataset.\nthe names of the variables in the dataset.\nthe value of the 5th row, 2nd column in the dataset.\nprint a numeric variable from the dataset\nprint a categorical variable from the dataset\n\nIn lecture, we wrote a for-loop to simulate the ‚ÄúMonty Hall‚Äù problem. Adapt this for-loop such that there are 100 doors (where the contestant chooses one, then Monty opens 98 other doors.) Under these conditions, what is the probability of winning if you switch? If you don‚Äôt switch?\n\n\n\n\nPlease try to complete these problems on your own after section. If you get stuck, post to the class ‚Äúdiscord‚Äù (and feel free to help others get unstuck.) If you really get stuck, just explain what you tried. Thanks!\n\nLook over the supplemental readings for instructions on how to load and navigate a dataset into R from a .csv file. Load one dataset posted to our ‚ÄúDatasets‚Äù folder; make sure to give the dataset a name; check the headers, and set stringsAsFactors = T. Note that some datasets are saved as .xlsx files, and will need to be loaded using a package - feel free to skip these for now if this feels confusing. For each dataset, use R to report the following statistics. Add comments to your code that describe what you learn from the output.\n\nthe number of individuals (rows) in the dataset.\nthe number of variables (columns) in the dataset.\nthe names of the variables in the dataset.\nthe value of the 5th row, 2nd column in the dataset.\nprint two variables from the dataset - what do you learn from just looking at these data? No need for summary statistics yet :)\nWhat is a question you might ask about a variable from this dataset? Why might this question matter?\n\nRead the article ‚ÄúData Organization in Spreadsheets‚Äù. Choose one of the datasets uploaded to the Datasets folder on bCourses, and look over the dataset (and corresponding Codebook). What are some ways that this dataset adhered to these ‚Äúbest practices‚Äù? What are some ways that the dataset did not? What is something from this article that you learned? What did you have a question about?\nChallenge Problem1. Quarto (and Markdown) can also be used to make a website. Make yourself a personal website, and use one of the methods to host this website for the world to see [I use github.] Yeah!"
  },
  {
    "objectID": "gradstats/gradlabs/2_descriptionvisual.html#in-lecture.",
    "href": "gradstats/gradlabs/2_descriptionvisual.html#in-lecture.",
    "title": "Psych 205 - Lab 2",
    "section": "",
    "text": "We will work on these questions together in lecture!\n\nOne advantage of R Markdown is that you can run code in a document. You do this using a ‚Äúcode block‚Äù. In the space below, insert an R code block, type out a math equation that used to give you difficulty as a kid into the code block below, and run the code to see the result. Below the codeblock, add text for humans to determine whether R got the math problem correct or not. Then render the document as a .pdf and .html file. Did this work?\n\n\nIn Class. We practiced how to load, navigate, and describing data from a dataset. Use R to complete the following.\n\nLoad the\n\n\n\nR also has some built in datasets. The chickwts dataset contains data on the weights of chickens, and the types of food that they ate. You can access this datasets by just typing chickwts (the name of the data object) into R. Use this dataset to answer the following questions:\n\nthe number of individuals (rows) in the dataset.\nthe number of variables (columns) in the dataset.\nthe names of the variables in the dataset.\nthe value of the 5th row, 2nd column in the dataset.\nprint a numeric variable from the dataset\nprint a categorical variable from the dataset"
  },
  {
    "objectID": "gradstats/gradlabs/2_descriptionvisual.html#in-discussion-section.",
    "href": "gradstats/gradlabs/2_descriptionvisual.html#in-discussion-section.",
    "title": "Psych 205 - Lab 2",
    "section": "",
    "text": "You will work with your TA on these questions.\n\nIn lecture, we created a dataframe in R with two variables. Work with your classmates to create another dataframe with three variables - at least one numeric and one ‚Äústring‚Äù. Then, use R to report the following from this dataset:\n\nthe number of individuals (rows) in the dataset.\nthe number of variables (columns) in the dataset.\nthe names of the variables in the dataset.\nthe value of the 5th row, 2nd column in the dataset.\nprint a numeric variable from the dataset\nprint a categorical variable from the dataset\n\nIn lecture, we wrote a for-loop to simulate the ‚ÄúMonty Hall‚Äù problem. Adapt this for-loop such that there are 100 doors (where the contestant chooses one, then Monty opens 98 other doors.) Under these conditions, what is the probability of winning if you switch? If you don‚Äôt switch?"
  },
  {
    "objectID": "gradstats/gradlabs/2_descriptionvisual.html#on-your-own.",
    "href": "gradstats/gradlabs/2_descriptionvisual.html#on-your-own.",
    "title": "Psych 205 - Lab 2",
    "section": "",
    "text": "Please try to complete these problems on your own after section. If you get stuck, post to the class ‚Äúdiscord‚Äù (and feel free to help others get unstuck.) If you really get stuck, just explain what you tried. Thanks!\n\nLook over the supplemental readings for instructions on how to load and navigate a dataset into R from a .csv file. Load one dataset posted to our ‚ÄúDatasets‚Äù folder; make sure to give the dataset a name; check the headers, and set stringsAsFactors = T. Note that some datasets are saved as .xlsx files, and will need to be loaded using a package - feel free to skip these for now if this feels confusing. For each dataset, use R to report the following statistics. Add comments to your code that describe what you learn from the output.\n\nthe number of individuals (rows) in the dataset.\nthe number of variables (columns) in the dataset.\nthe names of the variables in the dataset.\nthe value of the 5th row, 2nd column in the dataset.\nprint two variables from the dataset - what do you learn from just looking at these data? No need for summary statistics yet :)\nWhat is a question you might ask about a variable from this dataset? Why might this question matter?\n\nRead the article ‚ÄúData Organization in Spreadsheets‚Äù. Choose one of the datasets uploaded to the Datasets folder on bCourses, and look over the dataset (and corresponding Codebook). What are some ways that this dataset adhered to these ‚Äúbest practices‚Äù? What are some ways that the dataset did not? What is something from this article that you learned? What did you have a question about?\nChallenge Problem1. Quarto (and Markdown) can also be used to make a website. Make yourself a personal website, and use one of the methods to host this website for the world to see [I use github.] Yeah!"
  },
  {
    "objectID": "gradstats/gradlabs/2_descriptionvisual.html#footnotes",
    "href": "gradstats/gradlabs/2_descriptionvisual.html#footnotes",
    "title": "Psych 205 - Lab 2",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOptional, but potentially fun and useful if you have the time and interest.‚Ü©Ô∏é"
  },
  {
    "objectID": "gradstats/gradlabs/2Lab_DataDescription.html",
    "href": "gradstats/gradlabs/2Lab_DataDescription.html",
    "title": "Psych 205 - Lab 2 - Prof.¬†Key",
    "section": "",
    "text": "The goal of this lab is to get practice describing data and working in Quarto.\nDownload this file as a .qmd from the course page, and open it up in RStudio. Note : you may need to install Quarto.\n\n\n\nOne advantage of Quarto (and R Markdown) is that you can run code in a document. You do this using a ‚Äúcode block‚Äù. In the space below, insert an R code block, type out a math equation that used to give you difficulty as a kid into the code block below, and run the code to see the result. Below the code block, add text for humans to determine whether R got the math problem correct or not. Then render the document as a .pdf and .html file. Did this work?\n\n\n\nLoad the ‚Äúgrad onboarding‚Äù survey into the code block below, and answer the following questions. Note : to successfully render code in the document, you must a) explicitly load the dataset into your Quarto document and b) make sure you have no errors in your code. :)\n\nGraph the variables self.skills and class.skills side by side using the par() function. Change the formatting of the graph to make it look ready for presentation. Add vertical lines to each graph to illustrate the mean (solid line) and standard deviation (dashed lines).\nBelow each graph, report the mean and standard deviation of both variables, and interpret what these statistics tell you about the individuals in our class. (Who cares? What do these statistics tell us?)\n\n\n\nSplit your graphics window into a 2x5 grid, and graph each of the 10 ‚Äúcan.*‚Äù variables in the dataset (e.g., ‚Äúcan.import‚Äù, ‚Äúcan.clean‚Äù, etc.). Make sure each graph contains the name of the variable and that the graph looks good / is intelligible. Note : you can and should use a for-loop to do this! Then, report the frequencies of these variables - what are some things you observe about the data? Does this make sense given what you know about the participants?\n\n\n\n\n\nChoose one of the datasets from the class folder (avoid the one(s) labeled [repeated measures] as we will get to these later. I‚Äôll be focusing on the ‚ÄúPerceptions of the Wealthy‚Äù dataset if you want to follow along with my key when it‚Äôs posted). Look over the accompanying article for a guide to the variables, and identify two numeric variables from the dataset that seem interesting to you. Graph these variables, report the relevant descriptive statistics for each variable. Then explain what these statistics and graphs tell you about the individuals in the dataset, and what other questions you might ask about these individuals (e.g., what do the data NOT tell you?)\n\n\n\n\nUse these notes to answer the following questions. Let me know if anything is unclear!\nPsychologists often want to combine information from multiple questions (that measure the same construct) into one variable. There are lots of different ways to do this, but the first that we will talk about is just the humble average. For example, the Rosenberg (1965) self-esteem scale has 10 questions all related to self-esteem; rather than work with 10 different variables, it might be nice to just average these 10 and report one average number. These are called ‚Äúlikert‚Äù scales. It‚Äôs technically pronounced ‚Äòlick-ert‚Äô, but no one says that because it sounds kind of gross.\nHere‚Äôs an overview (from my 101 class notes) of how to work with such likert scales conceptually and computationally (in R). You‚Äôll need to load the psych library into R; to do this, run the following code in your console.\ninstall.packages(\"psych\") # installs the \"psych\" package. you only need to do this once.\nlibrary(psych) # loads the library. you need to do this every R session.\n\nSelf-Esteem Problems. Use the self-esteem dataset (from the class datasets folder). You should find a codebook that describes these data in the same folder.\n\nCheck to make sure the data loaded correctly. (Note that 0s in this dataset mean the person was missing data.) Report the sample size of the dataset.\nCreate a self-esteem scale from the 10-items. Make sure to reverse-score the negatively-keyed items so that for every question, higher numbers measure higher self-esteem. Graph this variable as a histogram, and make the graph look nice (ready for publication). Report the alpha reliability, mean, and standard deviation of this variable. Below your graph, describe what these statistics tell you about the self-esteem of the participants. What other questions do you have about this variable, or the output?\nThen, graph the variable gender as a categorical factor, and report the number of people who identified as ‚Äúfemale‚Äù, ‚Äúmale‚Äù, and ‚Äúother‚Äù. You will need to do some data cleaning here.\n\nSelf-Esteem is Conditional. Report the mean and standard deviation of self-esteem for people who are identified as female, male, and ‚Äúother‚Äù in the dataset. What differences do you observe? How might we use this knowledge / what other questions do you have? Note : there are MANY ways to do this in R; you can use the subset function or indexing to divide the dataset into three groups - ‚Äúfemales‚Äù, ‚Äúmales‚Äù, and people who reported ‚Äúother‚Äù. Or other fancier methods we will talk about later. See how many different ways you can do it."
  },
  {
    "objectID": "gradstats/gradlabs/2Lab_DataDescription.html#in-lecture.",
    "href": "gradstats/gradlabs/2Lab_DataDescription.html#in-lecture.",
    "title": "Psych 205 - Lab 2 - Prof.¬†Key",
    "section": "",
    "text": "One advantage of Quarto (and R Markdown) is that you can run code in a document. You do this using a ‚Äúcode block‚Äù. In the space below, insert an R code block, type out a math equation that used to give you difficulty as a kid into the code block below, and run the code to see the result. Below the code block, add text for humans to determine whether R got the math problem correct or not. Then render the document as a .pdf and .html file. Did this work?\n\n\n\nLoad the ‚Äúgrad onboarding‚Äù survey into the code block below, and answer the following questions. Note : to successfully render code in the document, you must a) explicitly load the dataset into your Quarto document and b) make sure you have no errors in your code. :)\n\nGraph the variables self.skills and class.skills side by side using the par() function. Change the formatting of the graph to make it look ready for presentation. Add vertical lines to each graph to illustrate the mean (solid line) and standard deviation (dashed lines).\nBelow each graph, report the mean and standard deviation of both variables, and interpret what these statistics tell you about the individuals in our class. (Who cares? What do these statistics tell us?)\n\n\n\nSplit your graphics window into a 2x5 grid, and graph each of the 10 ‚Äúcan.*‚Äù variables in the dataset (e.g., ‚Äúcan.import‚Äù, ‚Äúcan.clean‚Äù, etc.). Make sure each graph contains the name of the variable and that the graph looks good / is intelligible. Note : you can and should use a for-loop to do this! Then, report the frequencies of these variables - what are some things you observe about the data? Does this make sense given what you know about the participants?"
  },
  {
    "objectID": "gradstats/gradlabs/2Lab_DataDescription.html#footnotes",
    "href": "gradstats/gradlabs/2Lab_DataDescription.html#footnotes",
    "title": "Document",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOptional, but potentially fun and useful if you have the time and interest.‚Ü©Ô∏é"
  },
  {
    "objectID": "gradstats/gradlabs/2_Description.html",
    "href": "gradstats/gradlabs/2_Description.html",
    "title": "Lecture 2 : Describing Data with Pictures and Numbers",
    "section": "",
    "text": "Hi class, please work on the check-in. We will get started soon. Ask for help if you are stuck!\n\n\n\n\n\n\n\n\n\nPresentations:\n\nBegin today (professor)\nSign-Up Sheet Updated w/ another presentation :)\n\nUpdates / Questions from Week 1.\n\nShifting Mini Exam to 2/28\n\nMore time to practice\nCan cover (and review) Linear Models [foundational!]\nNo class ‚Äúafter‚Äù the exam. Just the exam. That‚Äôs enough? Yeah.\n2/28 presentation moved to 2/21\nLet me know if this causes issues / stress!\n\nDo we want a class discord (community / troubleshooting / another notification?)\n\n\n\n\n\nThis week, we‚Äôll learn how to describe data using R and statistics and human language.\n\nREVIEW : Announcements, Check-In, and Week 1 (30 Minutes)\nPART 1 : Describing Data with R, Statistics, and Human Language (50 Minutes)\n\nDescribing Categorical Data\nDescribing Numeric Data\nUsing Quarto to Make and Share Nice Reports\n\nBREAK TIME (10 Minutes) + Article Presentation (30 Minutes)\nPART 2 : Mean Thoughts (Remaining Time & Energy)\n\nThe Mean as Prediction\nThe Mean Minimizes Residual Error\nThe Mean is Sensitive to Outliers\nThe Mean is a Social Construction\n\n\n\n\n\n\n\n\n## CREATING 100 DOORS\ndoor100 &lt;- array() # this defines an empty array; a place to \"store\" my values\nfor(i in c(1:100)){ # this starts the for-loop, and tells R I want to repeat some process 100 times, and keep track with the variable i\n  door100[i] &lt;- paste(\"door\", i, sep = \"\") # the paste() function sticks the string \"door\" and the variable i together, separated by no value\n                                           # since i updates every time we iterate through the loop, each door will increase from 1 to 100.\n                                           # the assign &lt;- sticks this value to the variable door100 in position i\n} # this ends the loop\n\ndoor100 # testing to see if the loop works. it did.\n\n  [1] \"door1\"   \"door2\"   \"door3\"   \"door4\"   \"door5\"   \"door6\"   \"door7\"  \n  [8] \"door8\"   \"door9\"   \"door10\"  \"door11\"  \"door12\"  \"door13\"  \"door14\" \n [15] \"door15\"  \"door16\"  \"door17\"  \"door18\"  \"door19\"  \"door20\"  \"door21\" \n [22] \"door22\"  \"door23\"  \"door24\"  \"door25\"  \"door26\"  \"door27\"  \"door28\" \n [29] \"door29\"  \"door30\"  \"door31\"  \"door32\"  \"door33\"  \"door34\"  \"door35\" \n [36] \"door36\"  \"door37\"  \"door38\"  \"door39\"  \"door40\"  \"door41\"  \"door42\" \n [43] \"door43\"  \"door44\"  \"door45\"  \"door46\"  \"door47\"  \"door48\"  \"door49\" \n [50] \"door50\"  \"door51\"  \"door52\"  \"door53\"  \"door54\"  \"door55\"  \"door56\" \n [57] \"door57\"  \"door58\"  \"door59\"  \"door60\"  \"door61\"  \"door62\"  \"door63\" \n [64] \"door64\"  \"door65\"  \"door66\"  \"door67\"  \"door68\"  \"door69\"  \"door70\" \n [71] \"door71\"  \"door72\"  \"door73\"  \"door74\"  \"door75\"  \"door76\"  \"door77\" \n [78] \"door78\"  \"door79\"  \"door80\"  \"door81\"  \"door82\"  \"door83\"  \"door84\" \n [85] \"door85\"  \"door86\"  \"door87\"  \"door88\"  \"door89\"  \"door90\"  \"door91\" \n [92] \"door92\"  \"door93\"  \"door94\"  \"door95\"  \"door96\"  \"door97\"  \"door98\" \n [99] \"door99\"  \"door100\"\n\n## WHAT HAPPENS IF THERE ARE 100 DOORS?\nwin.stay100 &lt;- array() # defining a new place to save values when we have 100 doors. I could call this anything.\nwin.switch100 &lt;- array() # defining a new place to save values when we have 100 doors. I could call this anything.\nfor(i in c(1:10000)){ # running 10000 simulations.\n  choice &lt;- sample(door100, 1) # the new set of 100 doors\n  treasure &lt;- sample(door100, 1) \n  can.open &lt;- setdiff(door100, c(choice, treasure))\n  monty.open &lt;- sample(can.open, 98) # monty will open 98 other doors, leaving my choice + the treasure.\n  choice.switch &lt;- sample(setdiff(door100, c(monty.open, choice)), 1) # if switch, I can still only choose from whatever is left.\n  win.stay100[i] &lt;- ifelse(choice == treasure, \"WIN\", \"LOSE\") # this is the result if we stay with our initial choice\n  win.switch100[i] &lt;- ifelse(choice.switch == treasure, \"WIN\", \"LOSE\") # this is the result if we switch\n}\n\nsum(win.stay100 == \"WIN\")/length(win.stay100) # probability if I stay. Note that rather than divide by 1000, I'm dividing by the value of length(). If I change the # of simulations to run above, I don't have to update my code here. This is GOOD PRACTICE.\n\n[1] 0.0105\n\nsum(win.switch100 == \"WIN\")/length(win.switch100) # YEAH!!!!\n\n[1] 0.9895\n\n\n\n\n\nBelow are the twelve (12) criteria Broman & Woo (2018) articulated for thinking about data. Review the list. What practices below have you encountered or struggled with in working with data so far? What terms or ideas do you have questions about? Is there anything else that should be added to this list?\n\nConsistency\nGood Names\nWrite Dates as YYYY-MMM-DD\nNo Empty Cells\nPut Just One Thing in a Cell\nMake it a rectangle.\nData Dictionary\nNo calculations in the raw data file.\nNo font color or highlighting as data.\nMake backups\nData validation to avoid errors.\nSaving data in plaintext.\n\n\n\n\n\n\n\n\n\n\n\nsummary() or table(): to count frequencies of categorical data\nas.factor() and as.numeric() : to translate data\nlevels() or factor() : to relevel or change factor names\nplot : graphing categorical data\n\n\n\n\n\nsummary() or psych::describe() # this comes from the psych package which you must install : to summarize data\n\nmean()\nmedian()\nsd()\nrange()\nnote : if there are missing data (often!) you must manually tell R to remove the missing data : mean(d$variable, na.rm = T)\n\nhist() or boxplot() : graphing numeric data\n\n\n\n\nQuarto is a version of R Markdown, which is a version of Markdown, which is a powerful way to author code that is meant for both humans and computers to read.\n\nAdvantages :\n\ncan create a document that works for R code, can create a presentation, or a website\nmuch faster to get your code from R to something that humans can read\n\nno more copy-paste graphs or output.\ncan update graphs and output as your needs / datset changes\n\nlots of features - open-source heritage and culture, but supported financially my Micro$oft.\n\nability to format your code; render it as a website, pdf, book, etc.\ninteractive documents (Shiny; html-live; etc.)\n\n\nDisadvantages :\n\ncode must be ‚Äúperfect‚Äù in order to correctly render.\ncan go down formatting and feature rabbit holes that are not necessarily condusive to good science.\nanother dialect of the language you are trying to learn\n\nin R : code is the default; human comments added with #s\nin Quarto : human text is the default; you insert a code block when you want R to do something (and can then comment in that code)\n\n\n\n1+1 # like this\n\n[1] 2\n\n\nIn this class, we will work with both .R scripts and .qmd Quarto Markdown Files\n\n.R Scripts for tinkering with data (in-class tutorials; initial analyses)\n.qmd files for ‚Äúfinal‚Äù products (Lecture notes, lab documents, your project)\n\nThere are many thorough guides on how to use Quarto, but honestly the official Quarto reference book is almost constantly open on my computer (in multiple tabs‚Ä¶.sigh.) But let me know if you find another cool resource!\nThings to do in Quarto :\n\nwrite in human text\ninsert a code block\ninsert inline code\nrender everything with no pain and drama as a .pdf or .html file and share this with others.\n\n\n\n\n\n\n\nSee the article presentation slideshow [warning : everyone has edit access. be careful!]\n\n\n\n\nWe will see how far we get in exploring these ideas!\n\n\n\nIllustrating the mean as a ‚Äúline of best fit‚Äù [our first linear model!]\nIllustrating the standard deviation as the ‚Äúaverage‚Äù amount of residual error.\n\n\n\n\n\nIllustrating that the mean is the value that reduces the sum of our residual error."
  },
  {
    "objectID": "gradstats/gradlabs/2_Description.html#check-in-loading-data",
    "href": "gradstats/gradlabs/2_Description.html#check-in-loading-data",
    "title": "Lecture 2 : Describing Data with Pictures and Numbers",
    "section": "",
    "text": "Hi class, please work on the check-in. We will get started soon. Ask for help if you are stuck!"
  },
  {
    "objectID": "gradstats/gradlabs/2_Description.html#agenda-and-goals",
    "href": "gradstats/gradlabs/2_Description.html#agenda-and-goals",
    "title": "Lecture 2 : Describing Data with Pictures and Numbers",
    "section": "",
    "text": "This week, we‚Äôll learn how to describe data using R and statistics and human language.\n\nREVIEW : Announcements, Check-In, and Week 1 (30 Minutes)\nPART 1 : Describing Data with R, Statistics, and Human Language (50 Minutes)\n\nDescribing Categorical Data\nDescribing Numeric Data\nUsing Quarto to Make and Share Nice Reports\n\nBREAK TIME (10 Minutes) + Article Presentation (30 Minutes)\nPART 2 : Mean Thoughts (Remaining Time & Energy)\n\nThe Mean as Prediction\nThe Mean Minimizes Residual Error\nThe Mean is Sensitive to Outliers\nThe Mean is a Social Construction"
  },
  {
    "objectID": "gradstats/gradlabs/2_Description.html#week-1-recap-and-review",
    "href": "gradstats/gradlabs/2_Description.html#week-1-recap-and-review",
    "title": "Lecture 2 : Describing Data with Pictures and Numbers",
    "section": "",
    "text": "## CREATING 100 DOORS\ndoor100 &lt;- array() # this defines an empty array; a place to \"store\" my values\nfor(i in c(1:100)){ # this starts the for-loop, and tells R I want to repeat some process 100 times, and keep track with the variable i\n  door100[i] &lt;- paste(\"door\", i, sep = \"\") # the paste() function sticks the string \"door\" and the variable i together, separated by no value\n                                           # since i updates every time we iterate through the loop, each door will increase from 1 to 100.\n                                           # the assign &lt;- sticks this value to the variable door100 in position i\n} # this ends the loop\n\ndoor100 # testing to see if the loop works. it did.\n\n  [1] \"door1\"   \"door2\"   \"door3\"   \"door4\"   \"door5\"   \"door6\"   \"door7\"  \n  [8] \"door8\"   \"door9\"   \"door10\"  \"door11\"  \"door12\"  \"door13\"  \"door14\" \n [15] \"door15\"  \"door16\"  \"door17\"  \"door18\"  \"door19\"  \"door20\"  \"door21\" \n [22] \"door22\"  \"door23\"  \"door24\"  \"door25\"  \"door26\"  \"door27\"  \"door28\" \n [29] \"door29\"  \"door30\"  \"door31\"  \"door32\"  \"door33\"  \"door34\"  \"door35\" \n [36] \"door36\"  \"door37\"  \"door38\"  \"door39\"  \"door40\"  \"door41\"  \"door42\" \n [43] \"door43\"  \"door44\"  \"door45\"  \"door46\"  \"door47\"  \"door48\"  \"door49\" \n [50] \"door50\"  \"door51\"  \"door52\"  \"door53\"  \"door54\"  \"door55\"  \"door56\" \n [57] \"door57\"  \"door58\"  \"door59\"  \"door60\"  \"door61\"  \"door62\"  \"door63\" \n [64] \"door64\"  \"door65\"  \"door66\"  \"door67\"  \"door68\"  \"door69\"  \"door70\" \n [71] \"door71\"  \"door72\"  \"door73\"  \"door74\"  \"door75\"  \"door76\"  \"door77\" \n [78] \"door78\"  \"door79\"  \"door80\"  \"door81\"  \"door82\"  \"door83\"  \"door84\" \n [85] \"door85\"  \"door86\"  \"door87\"  \"door88\"  \"door89\"  \"door90\"  \"door91\" \n [92] \"door92\"  \"door93\"  \"door94\"  \"door95\"  \"door96\"  \"door97\"  \"door98\" \n [99] \"door99\"  \"door100\"\n\n## WHAT HAPPENS IF THERE ARE 100 DOORS?\nwin.stay100 &lt;- array() # defining a new place to save values when we have 100 doors. I could call this anything.\nwin.switch100 &lt;- array() # defining a new place to save values when we have 100 doors. I could call this anything.\nfor(i in c(1:10000)){ # running 10000 simulations.\n  choice &lt;- sample(door100, 1) # the new set of 100 doors\n  treasure &lt;- sample(door100, 1) \n  can.open &lt;- setdiff(door100, c(choice, treasure))\n  monty.open &lt;- sample(can.open, 98) # monty will open 98 other doors, leaving my choice + the treasure.\n  choice.switch &lt;- sample(setdiff(door100, c(monty.open, choice)), 1) # if switch, I can still only choose from whatever is left.\n  win.stay100[i] &lt;- ifelse(choice == treasure, \"WIN\", \"LOSE\") # this is the result if we stay with our initial choice\n  win.switch100[i] &lt;- ifelse(choice.switch == treasure, \"WIN\", \"LOSE\") # this is the result if we switch\n}\n\nsum(win.stay100 == \"WIN\")/length(win.stay100) # probability if I stay. Note that rather than divide by 1000, I'm dividing by the value of length(). If I change the # of simulations to run above, I don't have to update my code here. This is GOOD PRACTICE.\n\n[1] 0.0105\n\nsum(win.switch100 == \"WIN\")/length(win.switch100) # YEAH!!!!\n\n[1] 0.9895\n\n\n\n\n\nBelow are the twelve (12) criteria Broman & Woo (2018) articulated for thinking about data. Review the list. What practices below have you encountered or struggled with in working with data so far? What terms or ideas do you have questions about? Is there anything else that should be added to this list?\n\nConsistency\nGood Names\nWrite Dates as YYYY-MMM-DD\nNo Empty Cells\nPut Just One Thing in a Cell\nMake it a rectangle.\nData Dictionary\nNo calculations in the raw data file.\nNo font color or highlighting as data.\nMake backups\nData validation to avoid errors.\nSaving data in plaintext."
  },
  {
    "objectID": "gradstats/gradlabs/2_Description.html#lab-2",
    "href": "gradstats/gradlabs/2_Description.html#lab-2",
    "title": "Lecture 2 : Describing Data",
    "section": "",
    "text": "Working with Quarto.\n\n\nCreate a code block.\nRender the document.\nPublish the document (as a .pdf or to a website)\n\n\nIn Class : Describing Data.\n\n\nLoad and describe the dataset.\nGraph and summarize a numeric variable; interpret what you learn\nGraph and summarize a categorical variable; interpret what you learn.\n\n\nIn Class : Thinking more deeply about the mean and SD.\n\n\nShow that the mean is the value that minimizes residual error (better than all other values.)\nShow that the median is less sensitive to outliers than the mean.\nShow that nobody is really described by ‚Äúthe mean‚Äù.\nDoes R use n-1 or n equation for calculating SD?\n\n\nPractice In Section : Load & describe the class dataset in R.\n\n\nLoad and describe the dataset.\nGraph and summarize a numeric variable; interpret what you learn\nGraph and summarize a categorical variable; interpret what you learn.\nCreate a report in Quarto based on your R code.\n\n\nPractice On Your Own : Load & describe a dataset in R.\n\n\nMerge two files together; create a variable"
  },
  {
    "objectID": "gradstats/gradlabs/2Lab_DataDescription_KEY.html",
    "href": "gradstats/gradlabs/2Lab_DataDescription_KEY.html",
    "title": "Psych 205 - Lab 2 - Prof.¬†Key",
    "section": "",
    "text": "The goal of this lab is to get practice describing data and working in Quarto.\nDownload this file as a .qmd from the course page.\n\n\n\nOne advantage of Quarto (and R Markdown) is that you can run code in a document. You do this using a ‚Äúcode block‚Äù. In the space below, insert an R code block, type out a math equation that used to give you difficulty as a kid into the code block below, and run the code to see the result. Below the code block, add text for humans to determine whether R got the math problem correct or not. Then render the document as a .pdf and .html file. Did this work?\nLoad the ‚Äúgrad onboarding‚Äù survey into the code block below, and answer the following questions. Note : to successfully render code in the document, you must a) explicitly load the dataset into your Quarto document and b) make sure you have no errors in your code. :)\n\nGraph the variables d$self.skills and d$class.skills side by side using the par() function. Change the formatting of the graph to make it look ready for presentation. Add vertical lines to each graph to illustrate the mean (solid line) and standard deviation (dashed lines).\nBelow each graph, report the mean and standard deviation of both variables, and interpret what these statistics tell you about the individuals in our class. (Who cares? What do these statistics tell us?)\n\n\n\nd &lt;- read.csv(\"~/Downloads/grad_onboard_SP25.csv\", stringsAsFactors = T, na.strings = \"\")\npar(mfrow = c(1,2))\n\nhist(d$self.skills, breaks = c(0:5), \n     col = 'black', bor = 'white', main = \"Computer Skills\\n(Self-Perceptions)\")\nabline(v = mean(d$self.skills), lwd = 4, col = 'red')\nabline(v = mean(d$self.skills) + sd(d$self.skills), \n       lwd = 2, lty = \"dashed\", col = 'red')\nabline(v = mean(d$self.skills) - sd(d$self.skills), \n       lwd = 2, lty = \"dashed\", col = 'red')\n\nhist(d$class.skills, breaks = c(0:5),\n     col = 'black', bor = 'white', main = \"Computer Skills\\n(Perceptions of Classmates)\")\nabline(v = mean(d$class.skills), lwd = 4, col = 'red')\nabline(v = mean(d$class.skills) + sd(d$class.skills), \n       lwd = 2, lty = \"dashed\", col = 'red')\nabline(v = mean(d$class.skills) - sd(d$class.skills), \n       lwd = 2, lty = \"dashed\", col = 'red')\n\n\n\n\n\n\n\n\nI see that students rated their classmates as having higher computer skills (mean = 3.88) than they rated themselves (mean = 3.88. There was also more variation in people‚Äôs self-perceptions (sd = 1.15) than perceptions of the class 0.89. One possibility is that people were coming up with a less differentiated stereotype about the average other student‚Äôs skill, but had access to more information when making their own self-perception of skill.\n\nSplit your graphics window into a 2x5 grid, and graph each of the 10 ‚Äúcan.*‚Äù variables in the dataset (e.g., ‚Äúcan.import‚Äù, ‚Äúcan.clean‚Äù, etc.). Make sure each graph contains the name of the variable and that the graph looks good / is intelligible. Note : you can and should use a for-loop to do this! Then, report the frequencies of these variables - what are some things you observe about the data? Does this make sense given what you know about the participants?\n\nd[,7:16] &lt;- lapply(d[,7:16], factor, levels = c(\"No\", \"Maybe\", \"Yes\"))\n\npar(mfrow = c(2,5), cex = .5)\ngraph.names &lt;- c(\"Can Import Data\", \"Can Clean Data\", \"Can Graph a Variable\", \"Can Render Markdown\", \"Can Define LM\",\n                 \"Can Interpret a LM\", \"Can Interpret NHST\", \"Can Differentiate \\nSD and SE\", \"Can Interpret 95% CI\", \"Can For-Loop\")\nfor(i in c(7:16)){\n  plot(d[,i], main = paste(graph.names[i-6]))\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\nChoose one of the datasets from the class folder (avoid the one(s) labeled [repeated measures] as we will get to these later. I‚Äôll be focusing on the ‚ÄúPerceptions of the Wealthy‚Äù dataset if you want to follow along with my key when it‚Äôs posted). Look over the accompanying article for a guide to the variables, and identify two numeric variables from the dataset that seem interesting to you. Graph these variables, report the relevant descriptive statistics for each variable. Then explain what these statistics and graphs tell you about the individuals in the dataset, and what other questions you might ask about these individuals (e.g., what do the data NOT tell you?)\n\n\nI‚Äôm going to work with the ‚ÄúPerceptions of the Wealthy‚Äù study again. First I‚Äôll load my dataset, and check to make sure it loaded correctly.\n\n\nd &lt;- read.csv(\"~/Dropbox/!GRADSTATS/Datasets/Perceptions of the Wealthy/Dawtry 2015 Study 1a.csv\", # my file path\n              stringsAsFactors = T, # to convert strings into factors\n              na.strings = \"\") # empty data --&gt; NA\nhead(d) # did it work?\n\n   PS PD_15 PD_30 PD_45 PD_60 PD_75 PD_90 PD_105 PD_120 PD_135 PD_150\n1 233    27    48    21     0     0     0      0      0      0      0\n2 157    39     0     0     0     0     0      0      0      0      0\n3 275     0     0    50     0     0    50      0      0      0      0\n4 111     9    14    17    17    17     8      7      5      2      2\n5  52    68    32     0     0     0     0      0      0      0      0\n6  11    10    31     6    14     8     8      6      5      5      7\n  PD_150plus fairness satisfaction SC_15 SC_30 SC_45 SC_60 SC_75 SC_90 SC_105\n1          4        1            1    50    24    26     0     0     0      0\n2         61        5            2    19     0     0     0    53     0     28\n3          0        5            5     0     0    11     0     0     0     35\n4          2        7            7     2     7     7    11    11    13     14\n5          0        4            5     0     0    57     0     0    43      0\n6          0        1            4     8    25     8    16     7    11     11\n  SC_120 SC_135 SC_150 SC_150plus redist1 redist2 redist3 redist4\n1      0      0      0          0       6       3       6       1\n2      0      0      0          0       2       2       3       4\n3      0     54      0          0       5       4       5       5\n4     14     14      5          2       1       3       3       4\n5      0      0      0          0       4       5       4       5\n6      9      3      1          1       6       5       6       6\n  Household_Income Political_Preference age gender\n1               NA                    5  40      2\n2               20                    5  59      2\n3              100                    5  41      2\n4              150                    8  59      2\n5              500                    5  35      1\n6              600                    3  34      2\n  Population_Inequality_Gini_Index Population_Mean_Income\n1                         38.78294                  29715\n2                         37.21451                 123630\n3                         20.75000                  60000\n4                         35.37958                  59355\n5                         16.87500                  15360\n6                         40.92448                  57600\n  Social_Circle_Inequality_Gini_Index Social_Circle_Mean_Income\n1                            28.05674                     21150\n2                            24.32339                     65355\n3                            14.44258                    107100\n4                            26.92590                     86640\n5                            21.40106                     56850\n6                            37.08824                     59835\n\nnames(d) # \n\n [1] \"PS\"                                  \"PD_15\"                              \n [3] \"PD_30\"                               \"PD_45\"                              \n [5] \"PD_60\"                               \"PD_75\"                              \n [7] \"PD_90\"                               \"PD_105\"                             \n [9] \"PD_120\"                              \"PD_135\"                             \n[11] \"PD_150\"                              \"PD_150plus\"                         \n[13] \"fairness\"                            \"satisfaction\"                       \n[15] \"SC_15\"                               \"SC_30\"                              \n[17] \"SC_45\"                               \"SC_60\"                              \n[19] \"SC_75\"                               \"SC_90\"                              \n[21] \"SC_105\"                              \"SC_120\"                             \n[23] \"SC_135\"                              \"SC_150\"                             \n[25] \"SC_150plus\"                          \"redist1\"                            \n[27] \"redist2\"                             \"redist3\"                            \n[29] \"redist4\"                             \"Household_Income\"                   \n[31] \"Political_Preference\"                \"age\"                                \n[33] \"gender\"                              \"Population_Inequality_Gini_Index\"   \n[35] \"Population_Mean_Income\"              \"Social_Circle_Inequality_Gini_Index\"\n[37] \"Social_Circle_Mean_Income\"          \n\nnrow(d)\n\n[1] 305\n\n\n\nOkay, this looks good. Next step will be to focus on the variable Household Income.\n\n\nhist(d$Household_Income, breaks = 20, \n     main = \"Household Income\", \n     xlab = \"Household Income (USD)\")\n\n\n\n\n\n\n\nboxplot(d$Household_Income)\n\n\n\n\n\n\n\nsummary(d$Household_Income, na.rm = T)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n     20   25000   42000   54732   75000  350000       4 \n\nsd(d$Household_Income, na.rm = T)\n\n[1] 47238.63\n\n\nI See the Following :\n\nmean = NA) =\nmedian = NA =\nstandard deviation = NA)\nrange = NA)\n\nThings I Learned and Questions I Had :\n\n\n\nFor the categorical variable, I‚Äôll focus on\n\nhist(d$fairness)\n\n\n\n\n\n\n\n\n\n\n\n\nPsychologists often want to combine information from multiple questions (that measure the same construct) into one variable. There are lots of different ways to do this, but the first that we will talk about is just the humble average. For example, the Rosenberg (1965) self-esteem scale has 10 questions all related to self-esteem; rather than work with 10 different variables, it might be nice to just average these 10 and report one average number. These are called ‚Äúlikert‚Äù scales. It‚Äôs technically pronounced ‚Äòlick-ert‚Äô, but no one says that because it sounds kind of gross.\n\nHere‚Äôs an overview (from my 101 class notes) of how to work with such likert scales conceptually and computationally (in R). You‚Äôll need to load the psych library into R; to do this, run the following code in your console.\ninstall.packages(\"psych\") # installs the \"psych\" package. you only need to do this once.\nlibrary(psych) # loads the library. you need to do this every R session.\nUse these notes to answer the following questions. Let me know if anything is unclear!\nUse the self-esteem dataset (from the class datasets folder). You should find a codebook that describes these data in the same folder.\nCheck to make sure the data loaded correctly. (Note that 0s in this dataset mean the person was missing data.) Report the sample size of the dataset.\n\nselfes &lt;- read.csv(\"../datasets/Self-Esteem Dataset/data.csv\",\n                   stringsAsFactors = T,\n                   na.strings = \"0\", sep = \"\\t\")\nhead(selfes) # checking to make sure it loaded okay\n\n  Q1 Q2 Q3 Q4 Q5 Q6 Q7 Q8 Q9 Q10 gender age source country\n1  3  3  1  4  3  4  3  2  3   3      1  40      1      US\n2  4  4  1  3  1  3  3  2  3   2      1  36      1      US\n3  2  3  2  3  3  3  2  3  3   3      2  22      1      US\n4  4  3  2  3  2  3  2  3  3   3      1  31      1      US\n5  4  4  1  4  1  4  4  1  1   1      1  30      1      EU\n6  4  4  1  3  1  3  4  2  2   1      2  25      1      CA\n\nsummary(as.factor(selfes$Q1)) # making sure zeros got turned into NAs\n\n    1     2     3     4  NA's \n 3011  8647 21018 15200    98 \n\nnrow(selfes) # sample size\n\n[1] 47974\n\n\nCreate a self-esteem scale from the 10-items. Make sure to reverse-score the negatively-keyed items so that for every question, higher numbers measure higher self-esteem. Graph this variable as a histogram, and make the graph look nice (ready for publication).\n\nnames(selfes) # \n\n [1] \"Q1\"      \"Q2\"      \"Q3\"      \"Q4\"      \"Q5\"      \"Q6\"      \"Q7\"     \n [8] \"Q8\"      \"Q9\"      \"Q10\"     \"gender\"  \"age\"     \"source\"  \"country\"\n\nposkey.df &lt;- selfes[,c(1:2,4,6,7)] # pos-keyed items (from the codebook)\nnegkey.df &lt;- selfes[,c(3,5,8:10)] # neg-keyed items (from the codebook)\nnegkeyR.df &lt;- 5-negkey.df # reverse scoring the neg-keyed items\nSELFES.DF &lt;- data.frame(poskey.df, negkeyR.df) # bringing it all 2gether.\n\nlibrary(psych) # loading the library\nalpha(SELFES.DF) # alpha reliability.\n\n\nReliability analysis   \nCall: alpha(x = SELFES.DF)\n\n  raw_alpha std.alpha G6(smc) average_r S/N     ase mean  sd median_r\n      0.91      0.91    0.92      0.52  11 0.00058  2.6 0.7     0.52\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.91  0.91  0.91\nDuhachek  0.91  0.91  0.91\n\n Reliability if an item is dropped:\n    raw_alpha std.alpha G6(smc) average_r  S/N alpha se  var.r med.r\nQ1       0.90      0.90    0.91      0.51  9.5  0.00064 0.0089  0.51\nQ2       0.91      0.91    0.91      0.52  9.7  0.00063 0.0085  0.52\nQ4       0.91      0.91    0.91      0.53 10.3  0.00061 0.0081  0.53\nQ6       0.90      0.90    0.90      0.50  9.2  0.00067 0.0087  0.51\nQ7       0.90      0.90    0.91      0.51  9.3  0.00066 0.0089  0.51\nQ3       0.90      0.90    0.91      0.51  9.3  0.00066 0.0094  0.51\nQ5       0.90      0.91    0.91      0.52  9.6  0.00065 0.0098  0.51\nQ8       0.91      0.91    0.92      0.54 10.7  0.00059 0.0064  0.54\nQ9       0.90      0.91    0.91      0.52  9.6  0.00065 0.0085  0.52\nQ10      0.90      0.90    0.90      0.51  9.3  0.00067 0.0086  0.51\n\n Item statistics \n        n raw.r std.r r.cor r.drop mean   sd\nQ1  47876  0.76  0.77  0.75   0.70  3.0 0.87\nQ2  47658  0.73  0.74  0.71   0.66  3.1 0.79\nQ4  47751  0.66  0.68  0.62   0.59  2.9 0.81\nQ6  47809  0.81  0.81  0.79   0.75  2.6 0.92\nQ7  47758  0.79  0.79  0.77   0.74  2.4 0.93\nQ3  47751  0.79  0.79  0.76   0.73  2.7 0.95\nQ5  47781  0.76  0.76  0.72   0.69  2.6 0.98\nQ8  47797  0.64  0.63  0.56   0.54  2.3 0.96\nQ9  47728  0.76  0.75  0.73   0.69  2.2 0.99\nQ10 47772  0.81  0.80  0.78   0.74  2.4 1.07\n\nNon missing response frequency for each item\n       1    2    3    4 miss\nQ1  0.06 0.18 0.44 0.32 0.00\nQ2  0.04 0.13 0.50 0.33 0.01\nQ4  0.05 0.21 0.50 0.24 0.00\nQ6  0.14 0.33 0.37 0.17 0.00\nQ7  0.18 0.34 0.35 0.14 0.00\nQ3  0.13 0.28 0.37 0.22 0.00\nQ5  0.14 0.32 0.32 0.22 0.00\nQ8  0.21 0.41 0.24 0.14 0.00\nQ9  0.27 0.40 0.20 0.14 0.01\nQ10 0.24 0.33 0.22 0.22 0.00\n\nselfes$SELFES &lt;- rowMeans(SELFES.DF, na.rm = T) # creating the scale\nhist(selfes$SELFES, col = 'black', bor = 'white', # the graph\n     main = \"Histogram of Self-Esteem\", \n     xlab = \"Self-Esteem Score\", breaks = 15)\n\n\n\n\n\n\n\n\nReport the alpha reliability, mean, and standard deviation of this variable. Below your graph, describe what these statistics tell you about the self-esteem of the participants. What other questions do you have about this variable, or the output?\nThen, graph the variable gender as a categorical factor, and report the number of people who identified as ‚Äúfemale‚Äù, ‚Äúmale‚Äù, and ‚Äúother‚Äù. You will need to do some data cleaning here.\n\nselfes$gender\n\n    [1]  1  1  2  1  1  2  1  1  1  2  1  2  2  1  1  1  2  1  1  2  2  2  1  1\n   [25]  1  2  1  1  1  2  1  1  1  2  1  2  2  2 NA  2  1  2  1  1  2  1  2  1\n   [49]  1  2  1  1  1  3  1  2  1  1  2  1  2  1  2  1  1  1  1  2  1  1  2  2\n   [73]  2  2  1  2  1  1  2  1  1  1  2  2  1  2  2  2  2  1  1  1  2  1  1  2\n   [97]  2  2  1  2  2  2  2  1  2  2  2  2  2  1  2  1  2  1  1  2  2  2  2  1\n  [121]  1  2  2  1  2  1  2  2  1  2  1  1  1  1  2  1  1  2  1  2  1  1  2  1\n  [145]  2  1  2  2  2  2  2  2  1  1  1  2  2  1  2  2  2  2  1  2  1  1  1  1\n  [169]  1  1  1  1  2  2  1  1  2  1  1  1  2  2  1  1  2  1  2  2  1  1  2  2\n  [193]  2  1  1  1  1  2  2  1  2  2  2  2  2  2  2  1  2  1  1  2  2  2  2  1\n  [217]  1  2  1  1  2  2  1  1  2  1  2  2  2  1  2  1  1  2  2  1  1  2  1  2\n  [241]  1  2  2  1  2  2  1  1  2  1  2  1  2  1  2  2  2  2  1  2  2  1  2  2\n  [265]  2  1  2  2  2  2  1  2  2  1  1  1  2  1  2  1  2  2  2  1  1  1  2  1\n  [289]  2  2  2  1  1  1  2  2  2  2  2  1  2  1  2  1  1  1  2  1  1  1  1  1\n  [313]  1  2  2  2  2  2  2  2  1  2  1  1  2  1  2  1  2  1  1  1  2  2  1  1\n  [337]  2  1  1  2  2  2  1  2  2  1  2  2  2  1  2  2  2  1  1  1  1  2  1  2\n  [361]  2  2  1  2  2  2  2  2  2  2  2  1  2  1  2  1  2  2  2  2  2  1  1  1\n  [385]  1  1  2  2  1  1  1  1  1  2  1  2  1  1  2  2  2  2  2  1  1  1  1  2\n  [409]  1  1  1  1  2  1  1  2  2  1  1  2  1  1  1  1  2  1  2  1  2  2  2  1\n  [433]  2  1  2  2  1  2  2  2  2  2  2  1  1  1  1  2  2  2  2  2  1  1  1  2\n  [457]  1  2  2  2  2  1  1  1  1  1  1  2  2  2  1  2  2  2  2  1  2  2  1  1\n  [481]  1  2  1  2  2  2  1  1  2  2  1  1  1  2 NA  1  1  1  1  1  1  1  2  1\n  [505]  2 NA  1  2  1  2  1  2  2  1  2  2  2  1  2  2  2  2  2  2  1  2  2  2\n  [529]  2  2 NA  1  2  2  1  2  2  1  1  2  2  2  2  2  2  1  2  2  2  2  2  1\n  [553]  2  2  2  2  2  1  1  2  2  2  2  2  2  2  2  2  1  2  1  2  2  2  2  1\n  [577]  1  1  2  2  2  2  2  2  2  2  2  2  2  1  1  2  2  2  2  1  2  2  2  1\n  [601]  2  2  2  2  2  2  2  2  2  2  1  1  2  1  2  1  2  2  2  1  1  2  2  1\n  [625]  1  2  2  2  1  2  2  1  1  2  2  1  2  1  1  1  1  1  1  2  1  2  2  1\n  [649]  2  1  1  2  1  2  1  2  2  2  1  2  2  1  2  1  2  2  2  1  1  2  1  2\n  [673]  1  1  1  2  2  2  2  2  2  1  2  2  1  2  1  1  2  2  1  1  1  1  1  1\n  [697]  1  1  1  1  2  1  2  2  2  2  2  1  1  2  2  1  2  1  2  2  1  1  2  1\n  [721]  1  1  2  2  2  2  3  2  2  2  1  2  1  1  2  1  1  2  1  2  1  1  1  1\n  [745]  2  2  1  1  1  1  1  1  1  1  2  2  2  2  2  2  2  2  2  2  1  1  1  2\n  [769]  1  2  2  2  1  1  1  2  1  1  2  1  1  2  2  2  2  1  2  1  2  2  2  2\n  [793]  1  1  2  2  1  1  2  1  2  1  2  1  2  1  1  2  2  2  2  2  1  2  2  2\n  [817]  1  2  2  2  2  1  1  1  1  2  2  2  1  2  2  1  1  2  2  1  1  2  2  1\n  [841]  2  1  2  1  1  2  1  1  2  1  2  2  1  2  2  2  1  2  2  2  1  1  2  1\n  [865]  1  2  1  1  2  1  2  2  1  2  2  2  1  2  2  2  1  1  2  1  1  2  1  2\n  [889]  1  2  1  2  1 NA  1  2  1  2  2  2  2  1  1  2  1  1  1  2  2  2 NA  1\n  [913]  2  2  2  1  2  1  1  2  1  3  1  2  2  1  2  2  2  1  2  1  2 NA NA  2\n  [937]  2  2  1  1  1  1  1  2  2  2  1  2  2  2  1  1  1  2  2  2  2  2  2  2\n  [961]  2  1  2  2  1  2  2  2  1  1  2  2  2  1  1  1  1  2  1  2  1  2  2  1\n  [985]  1  1  2  1  2  1  1  1  1  1  2  1  1  3  2  2  1  2  1  1  2  1  2  1\n [1009]  1  1  2  2  1  2  1  2  1  1  2  2  2  2  1  2  1  1  1  2  1  1  2  2\n [1033]  2  2  1  1  2  1  2  2  2  1  2  1  2  2  1  2  2  2  1  1  2  2  2  1\n [1057]  1  2  2  2  1  2  1  2  2  2  2  2  1  1  2  1  2  2  2  1  2  2  2  2\n [1081]  2  2  2  2  1  1  2  1  1  2  2  1  2  1  1  2  2  2  1  2  1  2  2  2\n [1105]  1  2  2  1  2  2  2  1  1  1  1  1  1 NA  1  2  2  2  1  1  1  2  2  1\n [1129]  1  2  1  2  2  2  2  2  2  2  1  1  2  1  1  2  2  1  1  1  2  2  2  2\n [1153]  2  2  2  1  2  1  2  1  2  2  2  2  2  1  2  2  1  2  1  1  1  2  1  1\n [1177]  1  1  1  2  2  1  2  1  1  2  1  2  2  2  2  1  2  2  1  1  2  1  1  1\n [1201]  2  2  1  2  2  2  2  2  2  2  1  1  1  1  1  2  2  1  2  1  2  1  1  1\n [1225]  1  2  1  1  1  2  2  1  2  2  2  2  2  2  2  2  2  2  2  2  2  1  1  2\n [1249]  2  2  1  2  2  1  1  2  2  2  2  1  1  1  2  1  1  2  1  1  2  2  2  1\n [1273]  2  1  2  1  2  2  1  2  2  1  2  1  2  2  1  2  1  2  2  1  2  2  2  2\n [1297]  1  1  2  2  1  2  2  2  2  2  2  2  2  2  2  2  2  2  1  2  2 NA  1  1\n [1321]  1  1  2  2  1  1  2  2  1  2  2  2  2  2  3  2  2  1  1  2  2  1  1  1\n [1345]  2  2  1  3  1  1  1  1  2  1  2  1  2  1  2  2  1  1  2  1  1  2  2  2\n [1369]  2  2  2  2  1  2  2  1  2  2  2  2  1  1  2  1  2  1  2  1  2  1  1  2\n [1393]  2  2  2  1  2  2  2  2  2  2  1  2  2  2  1  2  1  2  2  1  1  2  2  1\n [1417]  1  2  2  1  1  2  1  1  2  1  1  2  1  1  1  1  2  1  2  1  2  1  1  2\n [1441]  1  2  1  1  2  1  2  2  2  2  2  2  2  1  2  1  2  1  1  1  1  1  2  2\n [1465]  2  1  1  1  2  1  1  2  1  1  2  2  2  2  2  1  2  1  2  2  1  1  2  2\n [1489]  2  2  2  2  1  1  1  1  1  1  1  2  2  1  1  2  1  1  2  1  1  1  2  2\n [1513]  2  2  2  2  1  1  2  1  2  2  2  2  1  1  1  1  2  2  2  1  1  2  1  1\n [1537]  1  1  1  1  1  2  2  2  2  2  1  2  2  2  2  2  1  1  1  1  1  2  1  1\n [1561]  2  2  2  1  2  1  2  2  2  1  2  1  2  2  1  2  2  2  2  2  1  1  2  1\n [1585]  2  2  2  1  2  2  1  1  1  2  2  2  2  2  2  1  2  2  1  2  1  1  2  2\n [1609]  2  2  1  2  3  2  2  2  1  1  2  1  1  2  1  1  1  2  1  1  1  1  1  1\n [1633]  1  1  1  2  1  1  1  1  1  1  1  1  1  1  1  1  3  1  1  1  1  1  1  1\n [1657]  1  1  1  1  1  1  1  1  1  1  2  1  1  1  1  1  1  1  2  1  1  1  1  1\n [1681]  1  1  1  1  1  2  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n [1705]  1  1  1  1  1  1  1  1  1  1  1  1  1  2  1  1  1  1  1  1  1  1  1  1\n [1729]  3  1  1  2  1  1  2  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n [1753]  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  2  2  1  1  1  1\n [1777]  2  1  1  1  1  2  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n [1801]  2  2  1  1  1  1  3  2  2  1  1  1  2  1  2  1  2  1  2  1  2  1  1  1\n [1825]  2  2  2  2  2  1  1  2  2  2  2  2  1  1  2  1  2  2  1  2  1  1  1  2\n [1849]  2  1  2  2  2  1  2  2  2  2  2  2  1  1  1  1  1  1  1  1  2  2  1  1\n [1873]  1  1  1  1  2  2  2  1  2  1  1  1  2  2  2  1  1  1  1  1  1  2  2  2\n [1897]  1  2  1  1  1  1  1  1  1  1  2  2  1  1  2  2  1  2  1  1  2  2  1  1\n [1921]  2  2  2  2  1  2  1  1  1  2  2  1  1  2  2  2  1  1  1  1  2  2  2  2\n [1945]  1  1  2  1  1  2  2  1  2  2  1  2  2  1  2  2  1  2  2  2  2  2  1  1\n [1969]  2  1  2  1  2  1  2  1  2  1  2  2  1  2  1  2  1  2  2  1  1  1  2  2\n [1993]  2  2  1  2  2  1  2  2  1  2  2  2  2  2  1  2  1  1  1  2  1  2  2  2\n [2017]  2  1  2  1  1  2  1  1  1  2  1  1  1  2  2  2  1  2  1  1  1  2  2  2\n [2041]  1  1  2  1  1  2  1  2  2  2  1  2  1  1  2  2  1  2  2  2  2  2  2  1\n [2065]  2  1  1  2  2  2  2  1  1  1  1  2  2  1  2  2  2  2  2  1 NA  2  2  2\n [2089]  2  2  2  2  1  2  1  2  1  1  2  2  1  1  2  1  2  1  2  2  2  2  1  2\n [2113]  1  1  1  1  1  1  1  1  2  1  1  2  1  1  1  1  2  2  2  2  1  2  2  1\n [2137]  2  1  2  2  2  2  2  2  2  1  1  2  1  1  1  1  1  1  1  1  1  1  2  2\n [2161]  1  1  1  1  2  1  1  1 NA  1  1  1  2  1  2  1  1  1  2  1  2  1  1  1\n [2185]  2  1  1  2  1  1  1  1  1  2  2  1  1  1  1 NA  2  1  2  2  1  1  1  1\n [2209]  1  2  2  2  1  2  1  2  1  2  1  1  1  2  1  1  2  1  1  1  2  2  1  1\n [2233]  2  1  1  1  2  1  2  1  1  1  2  2  2  1  1  1  1  1  2  2  1  2  2  1\n [2257]  1  2  1  2  2  1  1  1  1  2  2  1  1  1  2  1  2  1  1  1  2  1  1  1\n [2281]  1  2  1  2  2  1  2  1  1  2  2  1  1  1  2  1  1  1  1  1  2 NA  1  2\n [2305]  1  1  1  2  2  1  1  1  2  1  2  1  2  1  1  2  1  1  2  2  1 NA  2  2\n [2329]  2  1  2  1  2  1  1  1  1  2  1  2  2  1  1  1  2  1  2  2  2  1  1  1\n [2353]  1  1  1  2  2  1  2  2  2  1  1  2  2  1  2  1  1  1  1  2  1  2  1  1\n [2377]  1  1  1  2  1  1 NA  1  1  2  1  1  1  1  1  1  1  1  1  1  1  2  2  1\n [2401]  1  2  1  2  1  2  2  1  1  1  1  2  1  1  2  2  2  1  1  2  1  1  2  1\n [2425]  2  2  1  2  1  1  1  1  1  2  2  2  1  2  1  2  2  1  1  1  2  1  2  1\n [2449]  2  2  2  1  2  1  2  2 NA  1  1  2  2  1  2  1  2  2  2  1  1  2  1  2\n [2473]  1  2  1  1  1  1  1  1  1  2  2  2  2  2  2  2  1  2  2  1  1  1  2  1\n [2497]  1  1  2  1  1  2  1  2  1  1  2  2  2  2  2  2  2  1  2  2  1  2  2  2\n [2521]  1  2  1  2  2  2  1  1  2  3  2  2  2  2  1  1  2  2  2  2  2  2  2  2\n [2545]  2  1  1  2  2  2  2  1  1  1  2  1  1  1  2  2  2  2  1  1  2  1  2  1\n [2569]  2  1  2  1  1 NA  2  2  1  1  2  2  1  1  2  2  1  2  1  1  1  2  1  1\n [2593]  1  2  2  2  1  2  2  2  1  2  1  2  1  2  2  1  2  2  2  1  1  2  1  1\n [2617]  1  1  2  2  1  2  2  2  1  2  2  1  2  1  1  1  1  2  2  2  2  2  2  2\n [2641]  1  2  3  1  1  2  1  1  1  2  1  2  1  1  1  2  2  2  2  1  1  2  2  1\n [2665]  1  2  1  2  1  1  1  2  2  2  2  1  2  2  1  1  2  2  2  2  1  2  2  2\n [2689]  1  2  1  2  1  2  2  2  2  2  1  2  2  2  2  2  2  2  1  2  1  2  1  2\n [2713]  2  2  2  2  2  2  2  1  2  1  2  3  1  1  1  2  1  2  2  2  1  2  1  2\n [2737]  2  2  2  2  1  1  1  2  1  2  2  2  2  1  1  2  1  1  2  2  2  2  2  2\n [2761]  1  2  2  1  1  2  2  2  1  2  2  1  1  2  2  1 NA  1  1  1  2  1  2  1\n [2785]  1  2  1  1  2  1  1  1  2  2  2  2  2  1  1  1  2  2  2  1  2  1  1  2\n [2809]  2  2  1  1  2  2  2  2  1  1  1  1  2  1  2  1  1  1  2  1  2  1  1  1\n [2833]  1  2  2  2  2  1  1  2  1  2  2  2  2  1  1  2  2  1  1  2  1  2  2  2\n [2857]  1  1 NA  1  2  1  1  1  2  2  1  1  1  1  2  2  2  2  2  2  2  1  1  1\n [2881]  2  2  2  1  2  2  2  1  1  2  1  2  1  1  1  2  1  1  1 NA  2  2  1  1\n [2905]  1  1  2  1  2  2  1  1 NA  1  1  1  1  2  2  2  2  2  1  2  1  2  1  2\n [2929]  1  1  1  1  2  1  1  2  2  2  2  2  2  1  2  2  2  2  2  2  2  1  1  2\n [2953]  2  1  2  2  2  2  1  1  1  2  2  2  2  1  2 NA  2  2  1  2  1  2  1  2\n [2977]  1  1  1  1  2  2  1  1  1  2  1  1  1  1  2  1  2  1  1  1  2  2  2  2\n [3001]  1  2  1  1  2  1  2  1  2  2  1  2  2  1  2  1  2  2  1  2  2  2  2  2\n [3025]  2  2  1  1  1  2  1  1  1  1  1  2  2  1  1  2  2  1  2  1  1  2  2  1\n [3049]  2  2  1  2  1  1  2  2  2  1  3  1  1  2  2  2  1  1  2  2 NA  2  2  1\n [3073]  1  1  2  1  1  2  2  2  1  1  1  1  2  1  2  1  1  1  1 NA  2  1  2  2\n [3097]  2  2  1  1  2  1  1  1  1  1  2  2  1  2  2  2  2  1  2  2  2  1  2  1\n [3121]  2  1  2  1  2  1  2  2  2  2  2  2  1  1  2  1  2  2  1  2  2  2  2  2\n [3145]  2  1  1  2  2  2  1  1  1  2  2  2  1  2  1  2  2  3  2  1  1  2  2  1\n [3169]  1  2  2  1  2  2  2  2  1  2  1  1  1  2  2  1  1  1  2  1  1  1  1  1\n [3193]  1  1  1  2  1  2  2  2  1  2  2  1  1  1  1  2  1  1  1  1  2  2  1  2\n [3217]  2  2  2  2  2  2  1  2  1  1  1  2  1  1  1  1  2  3  2  1  2  2  2  2\n [3241]  2  1  2  2  2  1  2  1  2  3  2  2  2  2  2  1  2  2  2  2  2  2  1  2\n [3265]  1  2  1  1  2  2  2  1  2  1  2  2  2  1  2  1  2  1  2  2  2 NA  1  1\n [3289]  2  1  2  1  2  2  2  2  2  2  2  1  2  2  1  3  2  2  2  2  1  2  2  2\n [3313]  1  2  2  2  2  1  2  1  1  1  1  2  2  1  1  2  2  1  2  2  2  2  2  2\n [3337]  1  2  2  2  2  1  2  2  2  1  2  2  2  1  1  2  1  1  1  2  2  1  1  2\n [3361]  2  1  1  2  2  1  2  2  2  2  2  1  2  1  1 NA  1  1  1  2  1  1  1  2\n [3385]  1  1  2  2  1  2  2  1  2  2 NA  2  2  2  1  3  2  2  1 NA  2  2  2  2\n [3409]  2  1  2  2  2  2  2  1  2  2  1  2  2  1  2  1  2  1  1  2  2  2  2  2\n [3433]  1  2  2  2  2  1  2  2  1  2  2  2  2  2  1  1  2  2  2  2  2  1  1  2\n [3457]  2  2  1  1  1  2  1 NA  2  1  2  2  2  2  1  1  2  2  2  2  2  2  1  2\n [3481]  2  2  1  2  1  1  2  2  2 NA  2  2  2  2  1  2  1  2  2  2  1  2  1  1\n [3505]  2  1  2  2  2  2  2  1  1  1  2  2  2  2  1  1  2  2  2  2  2  2  1  2\n [3529]  2  1  1  2  2  2  1  2  2  2  2  2  2  2  2  2  2  1  2  2  2  2  1  2\n [3553]  2  2  2  2  1  2  2  2  2  2  2  2  1  1  1  2  1  1  2  1  1  1  2  2\n [3577]  2  2  1  1  2  2  2  1  1  2  2  2  2  1  2  2  2  1  2  1  2  2  2  1\n [3601]  2  2  2  2  2  2  2  2  1  2  2  2  2  1  2  2  2  2  2  1  2  2  2  1\n [3625]  2  2  2  2  1 NA NA NA  1  2  1  2  1  1  2  2  1  2  2  2  2  2  1  1\n [3649]  1  2  2  1  1  1  1  1  1  1  2  1  2  1  1 NA  2  1  1  1  2  1  2  2\n [3673]  2  2  2  2  1  1  2  2  1  2  2  2  1  1  2  2  2  1  1  1  2  2  2  1\n [3697]  2  2 NA NA  2  2  2  2  2  2  2  2  1  2  2  1  2  2  2  2  1  1  1  2\n [3721]  2  2  1  1  2  2  1  2  1  2  2  1  1  2  2  2  1  1  1  2  2  2  2  1\n [3745]  2  1  2  2  1  2  2  2  2  2  2  1  2  1  2  2  1  1  2  1  2  1  1  1\n [3769]  2  1  1  1  2  2  2  2  2  2  2  1  2  2  2  1  2  2  2  1  1  2  2  3\n [3793]  1  2  2  1  1  1  1  2  1  2  3  1  1  2  2  2  2  1  2  1  2  2  1  2\n [3817] NA  2  2  2  1  2  1  1  1  1  2  2  2  2  1  2  1  2  2  2  1  2  2  2\n [3841]  2  1  1  1  2  2  2  2  1  2  1  2  2  1  2  1  2  1  1  2  2  1  2  2\n [3865]  2  1  2 NA  2  2  1  1  2  2  2  2  2  1  1  2  2  2  1  1  1  1  1  2\n [3889]  2  1  2  2  1  1  2  2  2  1  2  2  2  2  2  1  1  2  3  1  1  1  2  2\n [3913]  2  2  2  2  1  2  2  2  2  1  1  3  1  1  2  1  2  1  2  1  2  2  1  1\n [3937]  2  1  2  2  1  1  2  1  2  1  2  1  1  2  2  2  1  2  2  1  2  1  1  1\n [3961]  2  1  2  2  1  2  1  2  2  1  2  2 NA  2  2  2  1  2  2  1  2  2  1  1\n [3985]  2  2  1  1  1  2  3  2  1  2  2  1  2  1  1  2  2  2  2  2  2  1  1  2\n [4009]  1  2  2  1  1  2  1  2  1  2  1  1  1  2  1  1  1  1  2  2  1  1  1  1\n [4033]  1  2  2  1  1  2  1  1  1  1  2  2  2  2  2  2  2  1  2  2  1  2  1  1\n [4057]  2  2  2  1  1  2  1  1  2  2  2  1  2  2  1  2  1  2  1  2  2  2  2  2\n [4081]  1  2  1  2  2  1  2 NA  2  2  1  1  2  1  3  1  2  2  2  2  2  1  2  1\n [4105]  2  2  1  1  1  1  2  2  1  1  2  2  2  1  2  2  1  2  1  1  1  2  1  2\n [4129]  1  1  2  1  1  2  2  1  2  1  2  2  1  2  2  2  2  1  2  1  2  1  2  1\n [4153]  2  2  1  2  1  1  1  1  1  1  2  2  2  1  2  1  1  2  2  2  1  2  2  2\n [4177]  2  2  2  2  2  1  3  1  2  1  2  2  2  2  2  2  2  2  2  1  2  2  2  2\n [4201]  2  2  2  1  2  1  1  1  2  2  2  1  2  1  2  1  2  1  2  2  2  2  2  1\n [4225]  2  2  2  2  1  1  2  2  2  1  2  1  2  2  2  2  2  1  2  1  1  2  1  2\n [4249]  2  2  2  2  1  1  1  1  1  2  2  2  2  2  2  2  2  2  1  2  1  2  2  2\n [4273]  1  2  2  2  1  2  2  2  2  1  2  2  2  2  2  1  2  2  1  1  1  2  2  1\n [4297]  1  1  1  2  1  2  2  1  1  2  2  1  2  2  2  2  1  1  2  2  2  2  2  2\n [4321]  1  2  2  2  2  2  2  2  2  2  2  1  2  2  2  1  2  1  1  1  2  2  2  2\n [4345]  2  2  1  2  1  2  2  2  1  1  1  2  2  1  2  2  1  1  2  1  1  1  2  1\n [4369]  1  1  2  2  1  1  2  1  2  2  2  2  1  1  1  3  1  2  2  2  2  2  2  2\n [4393]  1  2  2  2  1  1  1  1  1  2  2  2  1  1  2  1  1  2  2  1  1  2  1  1\n [4417]  2  2  1  1  1  2  1  2  1  1  1  1  1  2  1  2  2  1  2  2  2  2  2  1\n [4441]  1  1  2  2  2  1  1  2  1  2  2  2  2  1  1  1  1  2  2  1  1  2  1  2\n [4465]  1  2  2  2  1  2  2  2  1  2  2  2  2  1  2  1  2  1  1  1  2  1  1  2\n [4489]  2  1  2  1  1  1  1  1  1  1  2  2  2  1  2  1  1  2  2  2  2  1  1  1\n [4513]  2  1  2  2  2  1  3  2  2  2  2  2  1  2  2  1  1  1  1  1  2  1  1  1\n [4537]  1  2  1  2  1  2  2  3  2  2  2  2  1  2  2  1  2  2  2  1  1  1  2  2\n [4561]  1  2  2  1  2  1  1  1  1  2  1  1  2  1  2  2  2  2  1  1  1  2  3  1\n [4585]  2  1  2  2  2  2  1  2  1  1  2  2  2  2  1  1  1  2  2  1  1  1  1  1\n [4609]  1  2  2 NA  1  2  2  2  2  2  2  1  2  2  2  1  1  2  2  2  1  2  2  1\n [4633]  1  1  1  2  1  1  2  1  2  2  1  1  2  1  2  1  1  1  1  2  1  1 NA  1\n [4657]  1  1  1  1  2  1  2  2  1  1  2  2  2  2  2  2  1  1  2  1  1  2 NA  2\n [4681]  2  1  2  2  1  2  2  2  2  2  2  2  1  2  2  1  2  1  1  2  1  2  2  2\n [4705]  2  2  2  1  2  2  2  2  1  1  2  2  1  1  2  1  2  2  2  2  2  1  1  1\n [4729]  2  2  1  2  2  2  2  1  1  1  1  1  1  2  1  1  2  1  1  1  2  2  1  2\n [4753]  2  2  1  2  1  1  1  2  2  1  2  2  1  1  1  2  1  1  2  2  2  1  2  1\n [4777]  2  2  2  1  1  2  1  1  2  2  1  1  2  2  2  1  2  1  1  2  2  2 NA  1\n [4801]  1  1  2  2  2  1  1  1  1  1  2  2  1  2  2  2  2  1  2  1  2  1  2  2\n [4825]  1  2  2  1  2  2  1  1  1  2  1  2  1  1  1  2  2  1  2  1  2  1  2  2\n [4849]  3  2  2  1  2  1  1  2  2  1  2  2  2  2  2  1  1  1  2  2  2  2  2  1\n [4873]  1  2  1  1  1  2  2  2  1  1  2  2  2  2  2  1  2  1  1  2  1  3  1  2\n [4897]  2  1  1  2  2  1  1  1  2  2  1  2  2  1  2  1  1  2  2  1  2  2  1  1\n [4921]  1  1  2  2  1  1  1  2  2  1  1  1  1  1  2  1  2  2  1  2  1  2  1  2\n [4945]  1  1  2  1  2  1  1  1  2  2  2  1  1  1  2  1  1  2  1  1  2  3  2  1\n [4969]  1  1  2  2  1  2  2  1  2  2  2  1  2  2  2  2  1  1  1  1  2  2  2  2\n [4993]  2  1  1  2  2  2  2  2  1  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2\n [5017]  2  1  2  2  2  2  1  2  2  2  2  1  2  1  2  1  2  2  2  2  1  2  1  1\n [5041]  2  2  1  2  2  1  1  2  2  1  2  1  1  2  1  1  2  2  2  2  1  2  2  2\n [5065]  1  2  2  1  2  2  1  2  2  2  2  2  1  1  1  1  2  2  1  2  1  2  1  2\n [5089]  2  1  1  2  2  2  2  1  1  1  2  1  1  1  1  2  2  2  2  2  2  2  1  2\n [5113]  1  1  2  2  2 NA  2  2  1  1  1  2  1  1  1  2  2  2  2  1  1  1  1  2\n [5137]  1  2  2  2  2  2  1  2  2  1  2  1  2  2  1  1  2  1  2  1  2  1  2  1\n [5161]  1  1  1  2  1  2  1  1  2  1  2  2  2  2  1  1  1  1  2  1  2  2  2  2\n [5185]  1  1  2  2  2  2  2  1  2  2  1  2  1  2  1  2  1  2  2  2  1  1  1  1\n [5209]  2  2  1  1  1  2  2  1  2  1  1  2  2  1  2  1  1  2  1  1  2  2  2  2\n [5233]  2  1  1  1  1  2  1  2  2  1  1  1  1  2  1  2  1  2  1  2  1  2  2  1\n [5257]  1  2  2  2  1  2  2  1  1  2  2  1  1  2  2  1  2  1  2  2  2  1  2  1\n [5281]  1  2  2  3  2  2  2  1  1  1  2  2  2  1  1  1  1  2  2  2  1  1  1  2\n [5305]  1  2  2  1  2  2  1  1  2  2  1  2  1  2  2  2  1  2  1  1  1  2  2  2\n [5329]  2  2  2  2  2  2  3  1  1  1  2  2  2  1  2  2  2  2  2  2  2  1  2  2\n [5353]  2  2  2  2  2  2  2  2  2  1  3  2  2  1  3  2  2  1  2  2  2  3  1  2\n [5377]  2  2  2  2  1  2  2  3  2  2  2  1  2  2  3  2  2  2  2  2  1  1  2  2\n [5401]  2  2  2  2  2  2  2  2  2  2  2  1  2  2  1  2  1  3  2  2  2  2  2  2\n [5425]  2  1  2  1  1  3  2  2  2  2  2  2  2  2  2  2  2  2  1  2  2  2  2  2\n [5449]  2  2  2  2  2  2  2  2  2  2  2  2  2  2  1  2  2  3  2  2  2  2  1  2\n [5473]  2  2  2  1  2  2  2  2  1  2  2  2  1  2  2  2  2  1  2  2  2  3  2  3\n [5497]  2  2  2  2  3  1  2  2  2  2  2  2  1  2  2  2  2  1  2  2  2  2  2  2\n [5521]  2  2  2  2  2  2  1  2  2  1  2  2  2  2  2  2  1  2  2  2  2  2  1  2\n [5545]  1  2  2  2  2  2  2  2  2  2  2  2  2  2  1  2  2  2  2  3  2  2  2  1\n [5569]  2  1  2  2  3  2  2  2  2  2  1  2  2  2  2  2  2  2  2  1  2  2  2  1\n [5593]  2  2  2  1  1  2  2  2  3  2  2  2  1  2  2  2  3  2  2  2  2  1  2  1\n [5617]  2  2  2  2  2  2  1  1  1  2  2  2  1  2  2  2  2  2  2  2  2  2  3  2\n [5641]  1  2  2  2  2 NA  2  2  2  2  2  2  2  2  2  2  2  1  1  2  1  2  2  2\n [5665]  2  2  2  1  2  2  2  2  2  1  2  2  2  1  2  2  2  2  2  2  2  2  2  2\n [5689]  1  1  3  2  2  2  2  2  2  2  2  3  2  3  2  2  2  2  2  2  2  3  2  2\n [5713]  2  2  2  1  3  2  2  2  1  2  2  2  1  2  2  2  2  2  2  1  2  2  1  2\n [5737]  1  2  2  2  2  1  2  2  2  3  1  2  2  2  2  1  2  2  2  1  2  2  2  1\n [5761]  2  2  3  2  2  2  2  1  2  2  2  2  2  2  2  2  2  1  2  2  2  2  1  1\n [5785]  1  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  1  2\n [5809]  2  2  2  1  2  2  2  2  1  2  2  2  2  2  1  2  1  2  2  2  2  2  2  2\n [5833]  2  2  1  1  3  2  1  2  2  2  2  2  1  2  2  1  1  2  2  2  1  2  1  2\n [5857]  2  2  2  2  2  1  2  2  2  2  2  2  3  2  2  2  2  2  2  2  2  2  2  2\n [5881]  1  1  2  2  2  1  2  3  2  1  2  2  2  2  1  2  2  2  2  2  2  2  1  1\n [5905]  3  2  1  2  2  3  1  1  1  2  2  2  2  2  2  2  2  2  1  1  2  1  1  1\n [5929]  2  1  2  2  1  2  2  2  1  2  1  2  1  2  2  2  2  2  1  1  2  2  1  1\n [5953]  2  1  1  2  1  1  1  1  2  2  2  1  2  2  2  2  2  2  2  2  2  2  2  2\n [5977]  2  1  2  2  1  2  2  1  1  2  2  1  2  2  2  2  2  2  1  2  2  2  2  2\n [6001]  2  2  1  2  2  2  2  2  2  3  2  2  3  2  1  2  2  2  2  2  2  2  1  2\n [6025]  2  2  2  1  2  3  2  1  2  2  1  2  2  2  2  2  2  2  2  2  2  2  1  1\n [6049]  1  1  2  2  1  2  3  2  2  2  2  2  2  1  2  1  2  2  2  1  2  2  2  2\n [6073]  2  2  2  2  2  3  2  2  2  2  2  1  2  2  2  2  2  2  2  2  1  1  2  2\n [6097]  2  2  2  2  2  2  2  2  2  1  1  3  2  2  2 NA  3  3  2  2  2  2  1  2\n [6121]  2  2  1  2  1 NA NA  2  2  2  2  2  2  2  2  2  1  1  3  2  2  2  1  2\n [6145]  2  1  2  2  1  2  1  1  2  2  2  2  2  2  2  2  2  2  3  2  2  2  2  2\n [6169]  2  2  2  2  2  1  2  1  2  2  2  2  2  2  2  2  2  2  2  2  1  2  2  1\n [6193]  2  2  2  2  2  1  2  2  2  2  2  2  1  3  2  3  1  2  2  2  2  2  2  2\n [6217]  3  3  2  2  2  2  1  1  1  1  2  1  2  2  2  2  2  2  1  1  1  2  2  1\n [6241]  2  2  2  1  3  2  2  2  1  2  1  1  2  2  1  2  2  2  1  1  2  1  1  2\n [6265]  2  2  3  2  2  1  1  2  2  2  2  1  2  1  2  1  2  1  2  2  2  1  1  2\n [6289]  2  1  2  1  2  2  1  1  2  1  2  2  1  2  1  2  2  2  1  2  1  2  2  3\n [6313]  2  1  2  2  2 NA  1  2  2  2  1  1  1  2  2  2  1  2  2  2  3  2  1  2\n [6337]  3  2  1  2  2  3  2  1  2  2  2  2  2  1  2  2  2  2  2  2  2  2  2  2\n [6361]  1  1  3  2  1  2  2  2  2  1  2  1  2  2  3  2  2  2  2  2  1  2  2  2\n [6385]  2  2  2  2  2  2  2  2  2  2  2  2  2  2  1  2  2  1  2  2  2  1  1  2\n [6409]  2  2  2  2  2  1  2  2  2  2  2  2  2  2  2  1  2  1  2  2  2  1  1  1\n [6433]  1  2  2  2  2  2  1  1  2  2  2  2  1  2  2  2  2  2  2  1  2  2  1  2\n [6457]  1  2  2  2  2  2  3  2  2  2  1  1  2  1  2  1  2  2  2  1  1  1  2  1\n [6481]  1  2  2  2  1  2  1  1  1  1  1  1  1  1  1  1  2  2  2  2  2  1  2  2\n [6505]  2  1  2  2  2  2  2  1  1  2  2  1  2  2  2  1  2  1  1  2  2  2  2  2\n [6529]  2  2  2  2  1  1  2  1  2  1  2  1  2  2  2  2  2  2  1  2  2  2  2  2\n [6553]  1  1  1  2  1  1  2  2  2  1  1  2  2  1  1  1  1  1  2  2  2  1  1  2\n [6577]  1  1  2  2  1  1  1  1  2  1  1  2  2  1  2  1  1  2  1  1  2  2  2  2\n [6601]  1  2  2  1  2  2  2  2  2  1  2  1  2  2  2  1  1  1  2  1  1  1  1  2\n [6625]  1  1  1  1  1  1  1  2  2  2  2  1  1  1  2  2  2  1  1  2  2  2  2  2\n [6649]  2  2  2  1  2  2  2  2  2  2  2  2  2  2  2  1  2  1  2  2  2  2  2  2\n [6673] NA  1  2  2  2  1  1  1  1  1  1  2  2  2  2  1  1  2  1 NA  2  3  1  2\n [6697]  1  2  2  2  2  2  2  2  2  1  1  2  2  1  1  2  2  1  2  2  2  1  2  2\n [6721]  2  2  1  2  2  2  2  2  1  1  2  1  1  1  1  2  1  2  2  2  2  1  2  2\n [6745]  2  1  1  2  2  2  2  2  1  2  2  2  2  2  1  1  2  2  1  2  2  1  1  2\n [6769]  1  2  2  2  2  1  2  3  2  2  2  1  2  2  2  2  2  2  2  2  2  2  2  2\n [6793]  2  1  1  1  1  1  1  2  1  2  1  1  1  3  2  1  1  2  2  2  2  1  1  2\n [6817]  1  2  2  2  2  2  1  2  1  1  1  1  2  2  2  2  2  2  2  1  1  1  2  2\n [6841]  2  2  2  2  1  2  1  2  1  2  2  2  2  1  2  2  2  1  2  2  2  1  2  2\n [6865]  2  1  2  2  1  2  1  1  2  1  1  2  1  2  2  1  2  1  2  1  2  1  1  1\n [6889]  2  2  2  1  1  1  2  1  1  1  1  1  1  2  2  1  1  2  1  2  1  1  2  1\n [6913]  1  1  1  2  1  1  2  1  1  1  1  2  2  1  1  1  2  1  1  1  2  2  1  1\n [6937]  1  2  3  1  2  2  1  1  2  2  2  1  2  1  2  2  2  1  1  2  2  2  1  1\n [6961]  2  2  2  2  2  2  2  2  2  2  2  2  1  2  1  2  2  1  2  2  1  1  2  2\n [6985]  2  2  2  1  2  2  2  2  2  2  1  2  2  2  2  2  2  2  2  2  2  2  2  2\n [7009]  1  2  2  2  1  2  1  2  1  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2\n [7033]  2  2  2  2  2  1  2  1  2  2  2  2  2  2  2  1  2  1  2  2  1  2  1  2\n [7057]  1  2  2  2  2  1  1  2  2  2  1  2  2  2  2  2  1  1  2  1  2  1  1  1\n [7081]  1  2  2  1  2  2  2  2  1  1  2  1  2  2  2  2  2  2 NA  2  2  2  1  1\n [7105]  2  2  2  2  1  2  2  2  2  2  2  2  2  2  1  3  2  2  2  2  2  2  1  1\n [7129]  1  1  2  2  2  2  2  2  2  1  2  2  1  2  2  2  1  1  2  2  2  2  2  2\n [7153]  2  3  2  1  1 NA  2  2  1  2  1  2  1  1  2  2  2  2  2  1  1  2  1  2\n [7177]  2  1  2  2  2  2  1  1  2  2  2  2  1  1  2  2  2  2  1  2  1  2  2  2\n [7201]  2  2  1  2  1  1  2  2  1  2  2  2  2  1  2  1  3  2  2  2  2  2  2  2\n [7225]  2  2  1  1  2  2  3  1  1  1  1  2  1  2  2  2  2  2  2  2  2  1  1  2\n [7249]  2  2  2  1  2  2  2  2  2  2  1  1  1  2  1  1  2  1  2  2  1  1  2  2\n [7273]  2  2  2  2  1  2  2  2  2  2  2  2  1  2  1  1  2  1  2  2  2  1  2  2\n [7297]  2  2  2  2  2  2  1  1  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2\n [7321]  2  1  2  2  2  2  2  1  1  1  2  2  1  2  2  1  1  1  1  2  2  2  1  1\n [7345]  2  2  3  2  2  2  2  2  2  2  2  2  2  2 NA  2  2  1  2  1  2  2  2  2\n [7369]  2  2  2  2  2  2  1  2  2  1  2  1  1  2  2  2  2  2  2  2  1  1  2  1\n [7393]  2  2  1  2  2  1  2  2  2  1  2  2  2  1  2  2  2  1  2  1  1  2  1  1\n [7417]  2  1  2  2  2  1  2  2  1  2  2  1  1  2  3  2  1  2  1  1  1  1  2  2\n [7441]  2  1  1  1  2  2  2  1  2  1  2  1  2  2  2  2  2  1  2  2  2  3  1  2\n [7465]  2  2  1  2  2  2  2  3  2  2  2  2  2  1  2  2  2 NA  2  2  2  2  1  2\n [7489]  2  1  2  2  1  2  2 NA  2  2  1  2  2  2  2  2  2  2  2  1  2  2  2  2\n [7513]  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  1  2  1  2  2  2  2\n [7537]  1  1  2  3  2  2  2  2  2  2  2  3  2  2  2  2  2  2  2  1  2  2  2  2\n [7561]  2  2  1  2  2  2  2  2  2  2  2  2  2  2  2  2  2  3  2  2  2  2  2  2\n [7585]  2  1  1  3  2  2  2  2  2  2  1  1  2  2  2  2  2  2  2  2  2  2  3  2\n [7609]  1  2  2  2  2  2  2  2  2  2  2  2  2  1  2  2  1  2  2  2  2  2  1  2\n [7633]  1  2  2  2  2  2  2  2  2  2  2  2  3  2  2  2  2  2  2  1  2  2  2  1\n [7657]  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  1  1  2  1  2  1  2  1\n [7681]  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2\n [7705]  2  2  2  2  2  2  2  1  2  1  2  2  2  2  2  2  2  1  2  2  2  2  2  2\n [7729]  1  2  2  2  2  2  2  3  1  1  2  2  2  3  2  2  1  1  2  1  3  2  2  2\n [7753]  2  2  2  3  2  2  2  2  2  2  1  2  1  2  2  1  2  1  2  2  2  1  2  2\n [7777]  2  2  2  2  2  2  1  2  2  2  2  1  2  3  2  2  2  2  2  2  2  2  2  2\n [7801]  2  1  2  2  2  2  2  2  1  2  2  2  2  2  2  2  1  2  2  2  2  2  2  1\n [7825]  2  2  2  2  2  2  2  1  1  2  2  1  2  2  2  2  2  1  2  2  2  2  2  2\n [7849]  2  2  2  2  1  2  1  2  2  2  2  2  2  2  2  2  1  2  2  2  2  3  2  1\n [7873]  2  2  2  2  2  1  2  2  2  2  2  2  2  2  2  2  1  2  3  2  2  2  2  2\n [7897]  2  2  2  2  2  1  1  2  2  2  3  1  1  2  2  2  1  2  2  2  2  2  2  2\n [7921]  2  2  2  2  2  2  2  2  1  2  2  1  2  2  2  2  2  2  2  2  2  2  2  2\n [7945]  2  2  2  1  2  2  2  1  2  1  2  2  1  2  1  1  1  1  2  2  2  2  2  3\n [7969]  1  1  1  2  2  2  2  2  2  2  1  2  1  2  2  1  2  2  2  1  3  2  2  1\n [7993]  2  2  2  2  2  2  2  2  1  1  2  2  2  2  2  1  2  2  2  3  2  2  2 NA\n [8017]  2  2  1  2  2  2  1  1  2  2  2  2  2  2  1  2  1  2  2  1  2  2  1  2\n [8041]  2  2  1  2  2  2  2  1  2  2  3  2  2  2  2  2  1  2  2  2  2  2  1  2\n [8065]  2  1  2  3  2  1  2  1  2  2 NA  2  1  2  2  1  1  2  2  2  1  1  2  2\n [8089]  1  2  2  2  2  2  2  1  3  1  2  1  1  1  1  2  1  2  1  2  2  2  2  1\n [8113]  2  2  2  2  1  2  1  1  2  2  2  2  3  2  2  2  2  2  2  2  1  2  2  2\n [8137]  2  3  1  1  1  2  2  1  2  2  2  2  2  2  1  2  2  2  1  2  2  2  2  2\n [8161]  1  2  1  2  2  1  1  2  2  2  2  2  2  1  2  2  1  2  2  2  2  2  2  1\n [8185]  1  1  1  1  2  2  1  2  2  2  2  2  2 NA  2  1  1  2  1  2  2  2  2  2\n [8209]  1  2  2  2  1  1  1  1  2  1  2  2  2  2  2  2  1  2  1  2  2  2  2  1\n [8233]  2  1  2  1  1  2  2  2  1  2  2  1  2  2  2  1  2  2  2  2  1  2  2  2\n [8257]  2  2  2  1 NA  1  2  1  2  2  2  2  2  2  2  1  2  2  1  1  2  2  2  2\n [8281]  2  2  2  2  2  2  2  2  1  1  2  2  2  2  2  2  2  2  1  3  1  2  2  2\n [8305]  2  2  1  1  1  2  2  2  2  2  2  1  2  2  1  2  2  2  2  2  2  1  2  2\n [8329]  2  2  1  2  2  1  2  1  2  2  2  2  1  1  2  1  1  2  2  1  2  2  1  2\n [8353]  2  2  1  2  2  3  2  2  2  1  1  1  2  2  2  1  1  2  1  1  1  2  2  2\n [8377]  1  2  2  1  2  2  2  2  1  2  2  1  1  2  2  1  2  2  2  2  2  1  2  2\n [8401]  2  2  2  2  2  2  2  2  2  2  1  2  2  1  2  1  2  1  2  1  2  2  2  1\n [8425]  1  2  2  2  1 NA  1  2  2  2  2  2  2  1  2  2  1  2  1  1  1  2  2  1\n [8449]  2  2  2  2  1  2  2  1  2  2  1  1  2  2  2  1  2 NA  1  2  1  2  2  2\n [8473]  2  2  1  1  1  2  1  1  1  1  2  2  1  1  2  2  2  2  2  1  1  2  1  1\n [8497]  1  2  1  2  2  2  2  1  2  2  1  2  1  2  1  2  2  1  1  1  1  1  2  1\n [8521]  2  1  2  1  2  1  1  1  2  1  2  1  1  1  1  2  1  2  2  2  2  2  2  1\n [8545]  2  1  1  2  1  2  1  1  1  2  1  1  1  2  2  1  2  2  2  2  1  2  2  2\n [8569]  1  2  1  2  1  2  1  1  1  2  2  2  1  3  2  2  1  2  1  1  1  2  2  1\n [8593]  2  2  2  1  2  1  2  2  1  2  1  1  2  1  2  2  1  2  2  2  2  2  2  1\n [8617]  2  1  2  2  1  2  2  2  1  1  2  2  2  2  1  2  2  1  1  2  2  2  2  2\n [8641]  1  1  2  1  2  1  1  2  2  1  1  1  1  2  2  1  2  2  1  1  1  2  1 NA\n [8665]  1  1  1  1  2  3  2  2  2  2  2  1  1  2  2  2  1  2  2  2  2  2  1  3\n [8689]  1  2  2  2  3  1  2  2  1  1  1  1  2  2  2  3  2  1  1  2  1  2  2  2\n [8713]  2  1  1  1  2  2  2  1  2  1  1  1  2  2  2  1  2  1  2  2  1  1  2  2\n [8737]  2  1  2  1  2  1  2 NA  1  2  1  2  2  1 NA  1  2  1  2  1  2  1  1  2\n [8761]  2  2  2  1 NA  2  2  1  2  1  2  1  2  1  2  2  2  2  2  1  2  2  2  2\n [8785]  2  2  2  1  2  2  2  1  2  1  1  2  2  1  2  1  2  1  2  2  1  1  1  1\n [8809]  2  1  2  2  2  2  2  1  2  2  1  1  2  1  1  2  2 NA  2  2  2  1  3  2\n [8833]  2  2  1  1  2  2  2  1  2  3  2  2  2  1  1  2  2  1  2  3  1  1  2  1\n [8857]  2  2  1  2  1  1  2  1  2  2  1  2  1  1  1  1  1  1  2  2  1  1  2  2\n [8881]  2  2  1  1  2  2  2  2  2  1  1  2  2  2  1  2  2  2  2  2  2  2  1  2\n [8905]  2  2  1  1  2  1  2  1  1  2  2  1  2  2  1  2  2  1  2  2  1  1  2  2\n [8929]  3  2  2  2  1  2  1  2  2  2  2  2  1  1  2  2  1  2  1 NA  2  2  2  1\n [8953]  1  2  1  1  1  1  2  1  1  1  1  1  2  1  1  2  1  2  2  1  2  1  2  2\n [8977]  2  2  1  1  1  2  1  2  1  2  2  1  1  2  2  2  1  2  1  2  2  2  1  2\n [9001]  1  2  2  2  2  2  1  2  2  2  1  1  1  1  2  3  2  1  2  2  1  2  1  1\n [9025]  1  2  2  1  1  1  1  2  2  2  2  1  1  1  2  1  2  2  1  2  2  2  2  2\n [9049]  2  2  1  2  1  1  2  2  1  2  1  1  1  2  2  1  2  1  2  2  1  2  2  2\n [9073]  1  1  1  2  2  2  1  2  1  1  1  2  1  1  2  1  2  2  2  1  2  2  2  2\n [9097]  2  2  1  1  1  2  1  2  2  2  2  2  2  2  2  1  1  1  2  1  2  1  1  1\n [9121]  2  2  1  1  1  1  2  1  2  1  1  2  2  2  2  2  1  1  1  2  1  1  1  1\n [9145]  2  1  2  2  2  2  1  1  1  1  1  1  2  1  2  1  1  1  2  1  1  1  1  1\n [9169]  1  1  2  1  1  1  2  2  1  1  1  1  1  1  2  1  2  1  2  1  1  1  2 NA\n [9193]  2  2  2  2  2  3  1  2  2  2  2  1  2  2  1  2  2  2  1  2  1  1  2  1\n [9217]  2  1  2  2  1  2  2  1  2  1  2  2  2  2  1  1  2  2  1  2  1  1  2  2\n [9241]  2  1  2  2  2  1  1  2  2  2  2  2  2  2  1  2  1  1  2  2  2  2  2  1\n [9265]  1  1  2  2  2  1  3  2  1  2  1  1  2  1  1  1  2  2  2  2  2  1  1  3\n [9289]  2  2  1  1  1  1  2  2  1  1  1  2  1  2  2  1  2  1  3  1  2  1  2  1\n [9313]  2  1  2  1  1  2  1  1  1  2  2  1  1  2  1  1  1  1  1  1  2  2  1  1\n [9337]  1  1  1  2  1  1  2  2  1  1  2  2  1  1  2  1  1  1  2  2  2  1  1  1\n [9361]  2  2  2  1  2  2  2  1  2  2  2  1  2  1  1  1  1  1  1  1  1  1  1  2\n [9385]  1  1  2  1  2 NA  2  1  2  2  1  2  2  2  2  1  2  2  2  2  2  1  1  2\n [9409]  2  1  2  1  2  2  2  1  1  1  2  2  2  1  1  1  2  1  1  1  2  1  1  2\n [9433]  2  1  2  1  1  2  1  1  2  1  2  2  1  3  1  2  1  2  2  1  1  1  2  2\n [9457]  2  2  2  1  1  2  2  2  2  1  2  1  2  2  2  2  2  2  1  1  2  2  1  2\n [9481]  2  1  1  2  2  2  2  2 NA  1  2  1  1  2  2  2  1  2  1  1  2  1  2  1\n [9505]  1  2  1  2  2  1  2  2  1  2  1  1  2  2  2  2  1  2  1  1  2  1  2  1\n [9529]  1  1  2  2  3  2  2  3  2  1  2  2  1  2  1  1  2  2  2  1  2  2  1  2\n [9553]  3  1  1  1  2  2  2  2  2  2  1  2  2  2  1  2  2  1  2  1  3  2  1  1\n [9577]  1  2  2  2  2  2  2  2  1  2  2  1  2  2  1  2  2  1  1  3  2  2  2  1\n [9601]  2  2  2  2  2  2  2  2  2  1  2  2  2  2  2  1  2  2  2  1  1  2  1  1\n [9625]  2  1  1  2  2  2  2  1  2  1  1  1  2  2  2  1  3  1  1  2  1  2  1  2\n [9649]  2  1  2  2  2  2  1  1  2  2  1  2  2  1  2  1  2  2  1  2  2  2  2  2\n [9673]  2  1  2  2  1  2  1  2  1  2  2  1  1  2  1  1  1  1  1  2  1  2  2  1\n [9697]  1  2  2  1  2  2  1  1  1  2  2  2  2  2  1  2  1  1  2  2  1  1  2  2\n [9721]  2  2  2  2  1  1  2  1  1  2  3  1  1  1  2  2  2  2  2  2  2  2  2  1\n [9745]  2  2  1  1  2  2  2  2  2  2  1  1  2  1  2  2  2  1  2  2  2  2  2  2\n [9769]  1  1  2  2  2  2  1  2  1  1  2  2  2  2  2  1  1  2  2  1  2  2  2  2\n [9793]  2  1  1  1  2  2  2  1  2  2  2  2  2  1  1  2  1  1  2  2  3  1  1  1\n [9817]  2  1  1  1  2  2  2  1  2  1  2  2  1  1  2  2  2  2  1  2  1  2  2  2\n [9841]  2  1  1  1  2  2  1 NA  2  1  1  2  1  1  2  1  1  1  2  2  2  1 NA  1\n [9865]  2  2  1  2  2  2  2  1  2  1  2  1  2  2  2  1  2  1  1  2  1  2  2  1\n [9889]  1  1  1  2  2  1  2  2  1  2  2  2  1  1  2  1  2  2  1  2  1  1  1  2\n [9913]  1  2  2  2  2  2  2  2  1  2  2  1  2  1  2  2  1  2  1  2  2  2  2  2\n [9937]  2  2  1  2  2  2  2  2  2  2  2  2  2  1  2  2  2  2  1  2  2  2  1  2\n [9961]  1  1  1  2  1  1  1  2  1  2  1  1  2  1  1  2  1  1  2  1  2  1  2  1\n [9985]  1  2  1  1  1  1  1  2  1  1  2  2  1  2  2  1  2  2  1  2  1  2  1  1\n[10009]  1  3  1  2  2  2  1  2  2  2  1  1  2  2  2  1  2  2  1  2  1  1  2  2\n[10033]  1  1  1  2  2  2  2  2  2  1  2  2  2  2  2  1  2  2  2  2  1  1  2  1\n[10057]  2  2  2  1  2  2  1  1  2  2  2  2  1  2  2  2  2  2  2  1  1  1  2  2\n[10081]  2  2  2  2  2  2  1  2  1  1  3  2  2  2  2  1  2  2  2  2  2  1  1  2\n[10105]  1  2  2  1  2  1  2  2  1  2  1  1  2  1  1  2  1  2  2  2  2  1  2  2\n[10129]  1  1  1  1  1  1  1  2  1  2  1  2  2  2  1  2  1  2  2  2  1  2  2  1\n[10153]  2  2  2  1 NA  2  2  2  2  1  2  2  1  1  2  1  2  2  2  2  1  2  2  1\n[10177]  2  2  2  1  2  2  2  1  2  1  1  1  2  3  2  2  1  1  1  2  2  2  1  1\n[10201]  2  2  2  2  2  2  1  2  2  1  2  2  1  1  1  2  2  2  2  2  2  2  2  1\n[10225]  1  1  1  2  1  1  2  1  2  2  1  1  2  2  2  2  2  1  1  2  2  2  2  2\n[10249]  2  2  2  2  2  2  2  1  2  1  2  1  2  1  1  1  2  1  1  2  2  2  2  2\n[10273]  2  2  1  2  2  2  1  1  2  1  2  2  1  1  1  2  2  2  2  1  2  1  2  1\n[10297]  1  1  1  2  1  1  1  1  2  2  1  2  2  1  2  1  2  2  1  2  1  1  1  1\n[10321]  2  1  2  2  1  2  2  1  1  2  2  2  2  1  1  1  2  3  2  2  1  2  2  2\n[10345]  2  1  2  2  1  1  1  2  2  1  2  1  1  2  2  2  2  1  2  1  1  2  1  1\n[10369]  1  3  2  2  2  2  2  1  1  2  1  2  2  2  1  2  3  1  1  2  2  2  2  2\n[10393]  2  1  2  2  3  2  2  2  2  2  2  2  2  2  2  2  2  2  2  1  2  2  1  2\n[10417]  2  2  2  2  2  1  2  1  1  1  2  2  1  2  2  1  2  2  1  2  2  1  1  2\n[10441]  1  2  2  1  2  2  1  2  2  2  2  2  1  2  2  2  1  2  2  2  2  1  2  2\n[10465]  1  2  2  2  2  1  1  2  2  1  2  2  2  2  2  1  1  1  2  2  1  2  1  1\n[10489]  1  2  1  1  1  2  1  1  2  1  1  1  1  1  2  1  1  2  1  2  1  2  2  2\n[10513]  2  2  1  2  2  2  1  2  2  2 NA  1  1  1  1 NA  2  2  2  1  2  2  2  1\n[10537]  1  2  1  1  1  3  2  2  1  2  2  2  2  1  2  2  2  1  2  1  1  1  2  2\n[10561]  1  1  1  1  2  2  2  2  1  1  2  1  2  1  2  1  1  2  2  1  2  2  2  2\n[10585]  1  2  1  2  1  2  2  1  1  2  1  1  1  1  1  2  2  1  2  1  2  2  2  1\n[10609]  1  1  1  1  1  1  2  2  2  1  2  1  1  2  2  1  1  2  2  2  2  2  2  2\n[10633]  2  2  1  2  2  1  1  2  1  2  1  1  2  2  1  1  1  1  2  2  2  2  2  1\n[10657]  1  2  2  1  2  1  1  1  2  2  1  2  2  2  2  1  2  2  2  2  2  2  2  2\n[10681]  2  1  2  1  1  2  2  1  1  2  2  2  1  1  2  2  1  2  1  2  1  2  1  1\n[10705]  1  2  1  2  1  2  1  2  1  1  2  2  2  2  1  1  1  2  2  1  2  2  2  2\n[10729]  2  1  1  2  2  2  2  2  2  2  1  2  1  1  2  2  1  1 NA  2  1  2  1  1\n[10753]  2  2  2  2  2  2  2  2  2  2  2  2  1  2  2  2  2  2  2  2  2  2  2  1\n[10777]  1  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  1  2  1  1  1\n[10801]  2  3  1  1  2  1  2  2  1  1  2  1  1  2  1  2  2  2  2  1  2  1  3  2\n[10825]  2  1  1  1  1  1  2  2  1  2  2  2  1  2  2  1  2  2  1  1  2  2  1  2\n[10849]  1  2  2  1  2  2  2  1 NA  2  2  2  2  1  2  2  2  2  1  2  1  2  2  2\n[10873]  2  1  2  1  1  1  2  2  2 NA  2  3  2  1  2  1  1  1  2  2  2  1  1  1\n[10897]  2  2  1  2  2  1  1  1  2  2  2  1  2  2  2  2  2  1  2  2  1  1  1  1\n[10921]  3  1  2  2  1  2  2  1  1  2  2  2  2  1  2  1  1  2  2  2  1  1  2 NA\n[10945]  1  1  2  2  2  1  2  2  2  1  2  2  1  2  1  1  2  2  2  1  2  2  2  2\n[10969]  1  2  1  2  2  1  1  2  2  2  1  2  2  2  2  2  2  2  2  1  2  2  1  1\n[10993]  1  1  1  1  1  1  2  1  1 NA  2  2  1  1  1  1  1  1  1  1  1  1  1  1\n[11017]  1  1  2  1  2  1  1  1  2  1  2  1  1  1  1  2  2  1  1  1  2  2  1 NA\n[11041]  1  2  2  2  2  1  2  2  2 NA  2  2  2  1  1  2  1  1  1  2  1  2  2  1\n[11065]  1  2  1  2  2  2  2  1  2  2  2  1  3 NA  2  1  1  2  2  1  2  2  2  1\n[11089]  2  1  1  1  1  2  1  2  1  2  2  1  2  1  2  1  2  2  1  2  2  2  2  2\n[11113]  1  1  1  2  2  2  1  1  1  1  2  1  1  1  2  2  2  2  2  1  1  2  1  1\n[11137]  2  2  2  1  2  2  2  1  2  2  2  1  2  1  2  2  2  1  2  2  2  2  2  2\n[11161]  2  1  2  2  1  1  1  2  2  2  1  2  2  1  2  1  2  1  2  2  2  1  1  2\n[11185]  2  1  2  2  2  1  2  2  1  1  2  2  1  2  2  1  2  2  2  2  2  2  1  2\n[11209]  2  1  2  1  1  2  2  2  1  1  1  1  1  1  2  2  2  2  1  1  2  2  2  2\n[11233]  2  1  2  1  2  2  2  2  2  2  2  1  2  2  1  2  1  2  1  2  2  1  1  1\n[11257]  1  1  1  2  2  2  2  2  1  1  2  2  2  1  2  2  2  1  1  2  2  2  1  2\n[11281]  2  2  2  1  1  2  2  2  2  1  2  1  1  2  2  1  1  2  2  2  1  1  2  1\n[11305]  2  2  2  2  1  2  2  2  2  1  2  2  2  2  2  2  2  2  1  1  1  2  2  2\n[11329]  2  2  2  2  1  2  2  1  2  2  2  2  2  1  2  2  2  2  2  2  1  2  2  1\n[11353]  1  2  2  2  2  2  2  2  2  2  2  2  1  1  1  2  2  2  1  2  1  2  2  1\n[11377]  2  2  1  2  2  2  2  2  2  2  2  2  2  2  2  2  3  2  1 NA  1  2  2  2\n[11401]  2  2  1  1  2  1  2 NA  2  1  2  1  1  2  2  2  2  1  1  2  1  2  2  2\n[11425]  1  1  1  2  2  2  1  1  1  2  2  2  1  2  1  2  2  2  2  1  2  1  2  2\n[11449]  2  1  2  1  2  1  2  1  2  2  2  2  2  3  2  1  2  2  1  1  2  1  2  1\n[11473]  2  2  2  2  2  1  2  1  1  1  2  2  2  2  2  2  2  2  2  2  1  2 NA  2\n[11497]  2  1  2  1  2  1  2  2  2  1  2  1  2  2  2  2  2  2  1  2  1  1  2  1\n[11521]  2  1  2  2  2  1  2  2  1  2  2  2  2  1  1  2  2  2  2  2  1  1  2  1\n[11545]  1  1  2  2  2  2  1  2  1  2  1  2  2  2  1  2  1  2  2  2  2  2  2  2\n[11569]  3  2  2  1  2  1  2  2  1  1  2  1  2  2  2  2  2  1  2  2  1  2  2  2\n[11593]  1  2  2  2  1  1  2  2  2  2  2  2  2  2  1  2  2  2  1  3  1  2  3  2\n[11617]  2  2  2  2  2  2  2  2  3  1  2  2  2  2  2  2  2  2  2  2  1  2  1  2\n[11641]  2  2  2  1  2  2  1  2  2  1  2  2  2  2  2  1  2  1  2  1  2  1  2  2\n[11665]  2  2  2  1  2  2  1  2  3  2  2  2  2  1  1  2  2  2  2  2  2  2  2  2\n[11689]  1  2  2  1  2  1  2  2  2  1  1  1  2  2  2  1  2  1  2  2  2  2  2  2\n[11713]  2  3  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  1  2  1\n[11737]  2  2  1  1  2  1  2  2  2  2  2  2  1  1  1  2  2  2  2 NA  1  3  1  1\n[11761]  2  2  2  1  2  2  2  2  2  2  1  1  2  1  1  2  1  2  2  1  2  2  2  1\n[11785]  2  2  1  2  2  2  2  1  2  1  1  1  1  2  2  1  2  2  1  1  1  1  2  2\n[11809]  1  2  1  2  2  2  1  1  2  2  1  2  2  2  2  2  2  2  2  1  2  2  1  2\n[11833]  2  1  2  1  2  1  2  2  1  1  2  1  1  1  2  2  2  2  2  1  1  2  2  1\n[11857]  2  1  2  2  2  2  2  2  1  1  3  1  2  2  2  2  2  2  2  2  2  2  2  1\n[11881]  2  2  1  1  1  1  2  2  1  2  1  2  2  2  1  2  2  2  2  1  2  2  2  1\n[11905]  2  2  2  1  2  1  2  2  1  2  2  2  2  2  1  1  1  1  2  1  2  1  2  2\n[11929]  2  1  2  1  2  2  1  2  1 NA  1  2  1  2  2  2  2  3  1  1  2  1  1  2\n[11953]  2  2  1  1  2  1  1  2  2  2  2  1  1  1  2  2  1  1  1  2  1  2  2  1\n[11977]  1  2  1  2  1  2  1  1  2  2  2  1  1  1  2  2  1  2  2  1  1  1  2  1\n[12001]  2  1  1  1  1  1  2  1  1  2  1  1  1  2  2  1  2  2  2  2  1  1  2  2\n[12025]  1  1  2  2  2  1  1  2  1  1  2  3  2  2  1  2  2  1  2  2  1  2  2  2\n[12049]  1  2  1  2  2  2  2  1  2  1  2  2  2  2  2  2  3  2  2  2  1  1  2  1\n[12073]  2  2  1  1  1  1  2  2  2  2  2  2  2  2  2  2  2  2  1  2  1  2  2  2\n[12097]  2  2  2  1  2  1  2  2  2  1  1  2  2  1  1  2  1  2  2  2  1  2  1  3\n[12121]  2  2  2  1  1  2  2  2  2  1  2  2  2  2  1  1  2  2  2  2  1  2  1  1\n[12145]  1  2  2  2  2  2  1  2  1  3  1  1  2  1  2  2  2  2  2  2  1  2  2  1\n[12169]  2  2  1  2  2  1  2  2  1  2  2  2  2  1  1  2  2  2  2  2  2  1  2  1\n[12193]  1  1  2  2  2  2  2  3  1  2  2  2  2  2  1  2  2  2  1  2  2  2  3  1\n[12217]  2  1  2  2  1  2  1  2  2  2  2  2  2  1  2  2  1  1  2  2  1  2  1  2\n[12241]  2  2  2  2  1  1  2  2  1  2  1  1  2  1  2  1  1  1  2  1  2  2 NA  2\n[12265]  1  2  2  2  2  1  1  1  1  2  2  2  2  2  1  1  2  1  2  1  3  2  1  2\n[12289]  1  2  1  1  2  2  1  2  2  1  1  2  1  2  2  2  2  1  2  2  1  2  1  2\n[12313]  2  1  2  1  1  2  2  1  2  2  1  1  1  2  1  2  2  2  2  1  3  2  3  2\n[12337]  2  1  1  1  2  2  1  2  2  2  1  1  2 NA  2  2  2  2  2  1  2  2  1  1\n[12361]  1  1  2  1  2  2  1  1  1  3  2  1  1  1  2  2  2  2  1  2  2  1  2  1\n[12385]  1  2  1  1  2  1  1  1  2  1  1  2  2  2  2  2  2  1  2  1  2  2  2  1\n[12409]  2  1  2  1  3  1  2  1  2  1  1  3  2  2  1  2  2  1  1  2  1  2  2  1\n[12433]  1  2  2  1  1  2 NA  2  2  2  2  2  2  2  2  2  1  2  2  2  1  1  2  2\n[12457]  2  2  1  2  2  1  1  1  2  2  2  1  2  1  1  2  1  1  2  2  1  2  2  2\n[12481]  2  1  1  1  1  1  1  2  2  2  2  1  2  1  1  1  1  2  1  2  1  1  1  2\n[12505]  2  2  2  2  2  1  1  2  2  1  2  1  1  1  2  1  1  1  1  1  1  1  2  1\n[12529]  1  1  2  1  2  2  2  2  1  2  2  1  1  2  2  2  2  1  2  1  2  2  2  1\n[12553]  2  1  2  2  1  1  2  2  2  1  2  1  2  1  1  1  1  1  1  2  2  1  2  2\n[12577]  1  2  1  2  2  1  2  1  1  1  2  2  2  2  1  2  2  2  1  2  1  2  2  2\n[12601]  2  2  1  2  1  2  2  1  2  1  2  2  2  1  2  2  2  2  1  2  1  2  2  2\n[12625]  1 NA  1  1  1  2  2  1  2  1  1  2  2  1  2  2  2  2  2  1  2  2  1  2\n[12649]  2  2  2  2  1  2  1  2  1  2  1  1  2  2  2  2  2  3  2  2  2  2  1  2\n[12673]  1  2  1  1  1  2  2  1  1  2  2  2  1  1  2  1  2  2  2  1  2  2  2  1\n[12697]  2  2  1  2  2  1  1  2  2  2  2  1  2  2  2  2  2 NA  2  2  1  2 NA  1\n[12721]  2  2  2  1  2  1  2  2  2  1  2  2  2  2  2  2  1  1  2  2  1  1  2  2\n[12745]  2  1  1  1  2  2  1  2  1  1  1  2  2  2  2  2  1  2  1  2  2  1  2  1\n[12769]  2  1  2  1  2 NA  2  2  1  2  2  1  1  1  2  2  2  1  2  1  2  2  2  2\n[12793]  1  2  1  2  2  1  2  2  1  2  2  2  2  2  2  2  1  1  1  2  1  2  2  1\n[12817]  2  2  1  2  2  1  2  2  1  1  2  1  2  2  1  1  2  2  2  2  1  2  1  2\n[12841]  1  1  1 NA  2  1  1  2  2  1  1  2  2  1  1  2  1  1  1  1  2  2  1  1\n[12865] NA  1  2  2  1  2  2  2  2  2  2  1  2  2  2  2  1  2  1  1  2  2  2  2\n[12889]  2  2  2  2  2  2  1  2  1  2  1  2  2  2  2  2  2  2  1  2  1  2  2  1\n[12913]  2  2  2  2  1  2  2  1  2  1  2  2  2  1  2  2  2  2  2  2  1  2  2  2\n[12937]  1  2  2  1  1  2  1  1  2  1  2  1  2  2  1  2  2  1  3  2  2  1  1  2\n[12961]  1  2  2  2  2  2  2  2  1  1  2  1  2  2  1  2  2  2  1  2  1  2  2  2\n[12985]  2  1  2  2  1  1  1  2  2  2  1  2  1  2  1  1  2  1  1  2  2  2  1  1\n[13009]  2  2  1  1  1  1  1  2  1  1  2  1  2  2  2  1  2  2  1  2  1  1  2  1\n[13033]  2  1  1  1  2  1  2  1  2  1  1  2  2  2  1  2  1  2  1  1  1  1  2  3\n[13057]  2  1  2  1  2  2  2  2  1  3  1  1  1  1  2  1  2  1  2  2  3  2  1  2\n[13081]  2  2  1  2  1  2  1  1  2  2  1  2  2  2  2  2  2  2  1  1  2  2  1  1\n[13105]  1  1  1 NA  1  2  1  1  2  2  2  1  2  2  2  1  1  1  1  2  1  2  2  1\n[13129]  1  2  2  2  2  2  2  2  2  2  1  1  2  2  2  2  3  2  1  2  1  1  2  1\n[13153]  2  2  2  1  2  2  2  2  2  1  1  2  1  1  2  2  1  1  2  2  2  1  1  2\n[13177]  1  1  2  2  1  1  1  1  2  1  1  1  1  2  2  1  1  2  1  1  2  2  1  1\n[13201]  1  2  2  1  2  2  1  2  2  2  2  1  1  1  2  2  2  2  2  1  1  2  2  2\n[13225]  2  1  2  3  2  2  1  1  1  1  1  1  1  1  2  2  2  2  2  1  2  1  2  1\n[13249]  2  1 NA  2  2  1  2  2  2  2  2  2  2  2  2  1 NA  2  1  1  1  2  2  2\n[13273]  1  1  2  1  2  2  1  2  2  2  2  2  1  2  2  1  2  1  2  1  2  3  1  1\n[13297]  2  2  2  2  2  2  2  2  2  2  1  2  1  2  1  2  1  1  1  2  1  2  2  1\n[13321]  2  1  1  2  2  2  2  1  3  2  2  1  1  1  2  2  1  1  1  2  2  1  1  2\n[13345]  2  2  2  2  1  1  1  1  1  2  1  2  1  1  2  1  2  2  2  2  1  1  1  2\n[13369]  2  2  2  2  2  1  1  1  2  2  1  1  2  1  2  2  2  1  1  2  1  1  1  1\n[13393]  2  1  2  2  2  1  1  1  1  1  1  1  1  1  1  2  2  2  2  1  2  1  1  1\n[13417]  1  2  2  1  1  2  1  2  1  2  2  2  2  2  2  1  2  2  2  1  3  1  1  1\n[13441]  1  2  2  1  2  2  2  1  2  2  1  2  1  1  3  1  2  2  1  2  1  1  1  2\n[13465]  1  1  2  2  1  2  1  2  1  2  1  1  2  1  2  2  2  1  1  1  2  1  2  1\n[13489]  2  1  2  2  2  1  2  1  2  2  1  2  1  2  2  2  2  1  1  1  2  2  1  2\n[13513]  2  2  2  2  2  2  2  1  2  2  1  1  1  2  2  1  1  1  2  2  2  2  2  1\n[13537]  1  1  2  2  2  2  2  2  1  2  1  1  2  1  2  2  1 NA  1  2  2  1  1  1\n[13561]  1  1  2  1  2  2  2  2  1  1  2  2  2  2  2  2  2  1  2  2  1  2  1  2\n[13585]  1  1  1  1  2  2  1  1  2  2  2  1  1  1  2  2  2  2  1  1  1  2  2  1\n[13609]  2  1  2  1  1  1  2  1  2  2  2  1  2  2  1  1  1  2  2  2  1  2  1  2\n[13633]  2  2  2  2  1  2  2  2  3  1  1  2  2  2  1  1  1  2  2  2  2  1  1  1\n[13657]  2  2  2  1  2  1  1  2  1  2  1  1  1  2  2  2  1  1  1  2  2  2  1  2\n[13681]  2  2  2  2  3  2  2  1  1  1  1  2  1  2  2  2  2  2  2  2  2  2  1  2\n[13705]  1  2  2  1  2  1  1  1  1  1  2  1  2  1  2  1  1  2  1  1  2  2  2  2\n[13729]  2  1  2  2  2  1  1  2  1  2  2  2  2  1  1  2  2  2  2  2  2  1  1  2\n[13753]  2  2  1  2  2  1 NA  1  2  2  2  1  2  2  2  2  2  2  2  2  2  1  1  1\n[13777]  2  2  2  1  2  2  2  2  1  1  2  2  2  1  2  2  2  1  1  1  1  2  2  2\n[13801]  2  1  2  2  2  1  2  2  1  2  2  1  2  2  1  2  1  1  2  2  2  1  2  2\n[13825]  1  3  2  1  2  1  2  1  1  1  2  2  1  2  2  2  2  1  1  2  1  1  2  2\n[13849]  1  2  2  2  2  1  2  2  2  2  2  1  2  2  3  2  1  2  2  2  1  1  2  2\n[13873]  2  2  1  2  1  2  2  2  2  1  2  2  2  1  2  1  1  1  2  1  1  2 NA  2\n[13897]  2  2  2  2  2  2  2  2  1  1  1  2  1  2  1  2  2  2  2  2  2  2  2  2\n[13921]  1  1  2  2  2  2 NA  2  2  2  1  2  2  1  1  2  1  2  2  2  1  2  1  2\n[13945]  2  2  1  1  2  1  2  2  1  1  2  1  1  2  2  1  2  1  2  2  1  2  2  2\n[13969]  1  2  2  2  2  2  2  1  2  1  2  1  2  2  3  2  2  2  1  2 NA  2  2  1\n[13993]  2  2  2  2  2  2  2  2  2  2  1  1  1  2  1  2  2  2  2  1  1  1  2  2\n[14017]  1  1  2  2  1  1  1  1  2  1  2  2  2  2  1  1  1  1  1  1  2  2  2  2\n[14041]  1  2  1  2  1  1  2  1  2  2  1  2  1  2  1  2  1  1  2  2  1  2  2  2\n[14065]  1  2  2  2  1  2  1  2  1  2  2  1  2  1  1  2  2  1  1  1  1  1  1  2\n[14089]  2  2  1  2  1  1  2  2  1  1  2  2  2  2  2  2  2  1  3  1  2  1  2  2\n[14113]  2  2  1  2  2  2  2  2  1  2  2  1  1  2  3  1  1  2  1  2  1  1  1  2\n[14137]  1  2  2  1  2  2  2  2  2  2  2  2  2  2  2  2  2  2  1  1  1  1  1  2\n[14161]  1  2  2  2  2  1  1  1  2  2  2  2  1  1  1  1  1  2  2  2  2  2  1  1\n[14185]  2  1  1  3  2  2  2  2  2  2  2  2  2  2  2  2  2  1  2  2  2  2  1  1\n[14209]  1  1  2  2  1  2  2  2  1  2  2  1  1  1  2  2  2  1 NA  1  2  1  2  1\n[14233]  2  1  1  1  2  2  1  1  3  2  1  1  2  1  2  1  2  1  2  1  2  2 NA  2\n[14257]  1  2  2  2  1  2  2  2  2  2  1  1  2  2  2  2  2  2 NA  2  2  2  1  2\n[14281]  1  1  1  2  1  1  2  1  2  2  2  2  2  2  2  2  2  2  1  1  2  2  2  2\n[14305]  1  1  2  2  1  1  2  1  2  2  1  2  1  2  2  2  2  2  2  1  2  2 NA  1\n[14329]  3  2  1  2  2  2  2  2  1  1  3  1  1  1  2  1  3  2 NA  2  2  1  3  2\n[14353]  2  2  1  1  2  1  2  1  2  2  2  2  1  2  2  1  1  1  1  1  2  1  2  2\n[14377]  1  2  2  2  1  1  2  2  1  2  2  1  1  2  2  2  1  1  1  1  1  2  2  2\n[14401]  2  2  1  1  2  2  1  2  2  2  2  2  1  2  2  1  2  2  2  1  2  2  1  2\n[14425]  2  2  2  2  1  2  2  2  1  2  2  2  2  2  1  2  1  1  2  1  1  1  1  2\n[14449]  2  2  2  2  2  2  2  2  1  2  2  2  2  1  1  1  1  1  1  2  2  1  1  2\n[14473]  1  1  2  3  2  2  1  2  2  2  1  1  1  1  1  1  1  1  1  1  2  1  1  1\n[14497]  1  1  1  1  1  1  1  1  2  1  1  1  1  2  1  1  1  1  1  1  1  1  1  1\n[14521]  2  2  1  1  2  2  2  1  2  1  2  1  2  2  1  2  1  2  2  2  2  2  2  3\n[14545]  2  2  2  2  2  1  1  2  1  2  2  2  2  1  2  1  2  2  1  2  2  2  2  1\n[14569]  2  1  2  2  1  2  2  2  1  2  2  2  1  1  2  1  2  2  1  2  1  2  1  2\n[14593]  2  2  2  2  1  2  2  2  1  1  2  2  2  2  2  1  1  1  1  1  1  2  1  1\n[14617]  1  1  2  2  1  1  2  1  2  2  2  2  1  1  2  1  1  2  2  2  1  2  2  2\n[14641]  1  2  2  2  2  2  2  2  2  1  2  2  1  1  2  1  2  1  1  1  2  2  1  2\n[14665]  1  2  2  2  1  2  2  1  2  2  1  1  2  2  1  2  2  2  2  2  1  2  2  1\n[14689]  1  2  2  1  2  2  2  2  1  2  1  1  2  2  2 NA  2  2  1  2  1  2  2  1\n[14713]  1  2  1  1  1  2  1  1  1  2  1  1  1  2  2  2  2  2  2  1  2  2  1  2\n[14737]  2  2  2  1  1  2  2  2  1  2  1  2  2  2  2  1  2  1  1  1  2  2  1  1\n[14761]  2  2  2  2  2  1  2  2  1  2  1  2  1  1  1  1  2  2  1  2  1  1  1  1\n[14785]  2  2  2  2  1  2  1  2  2  2  1  2  1  1  2  2  2  1  2  2  2  2  1  2\n[14809]  2  1  2  2  3  1  2  2  2  1  2  2  2  2  2  2  2  2  2  1  2  2  2  2\n[14833]  1 NA  2  1  1  2  2  1  2  1  1  2  2  2  2  1  2  2  2  2  2  2  1  1\n[14857]  1  1  2  2  2  2  2  1  1  2  2  1  1  2  2  2  2  2  2  2  2  2  1  2\n[14881]  1  1  2  2  2  1  2  2  1  1  2  1  2  2  1  1  2  2  2  2  2  2  2  2\n[14905]  2  2  1  2  2  1  2  2  2  1  2  2  1  2  2  1  2  1  2  2  1  1  2  2\n[14929]  2  1  2  2  1  1  2  2  2  2  1  2  2  1  2  2  2  2  1  2  2  2  1  2\n[14953]  2  2  2  2  1  1  2  1  1  1  2  2  2  2  1  2  1  1  1  1  2  2  2  1\n[14977]  1  2  2  1  2  2  1  2  1  2  2  2  2  2  2  2  2  1  2  2  1  1  2  2\n[15001]  2  2  2  2  2  1  2  1  2  2  1  2  2  2  1  2  2  1  2  1  1  1  2  2\n[15025]  1  2  1  2  2  1  2  1  3  2  1  2  2  2  2  2  1  2  1  1  1  2 NA  1\n[15049]  2  2  2  2  2  2  2  1  2  1  2  1  2  1  1  2  2  2  2  2  2  2  2  2\n[15073]  2  2  1  2  2  1  2  2  2  1  2  2  2  2  2  1  1  2  2  1  1  1  1  2\n[15097]  1  1  2  1  2  2  1  1  1  2  1  1  2  2  2  2  2  3  2  2  2  2  2  2\n[15121]  2  1  1  1  2  1  2  2 NA  1  1  2  1  1  2  2  2  2  1  2  1  2  2 NA\n[15145]  2  2  2  2  2  1  2  2  2  1  1  2  2  2  1  2  2  2  1  2  2  2  2  1\n[15169]  2  1  2  2  1  2  2  1  1  2  2  2  1  1  2  1  2  2  2  2  1  2  1  2\n[15193]  1  2  2  2  2  2  1  2  2  1  2  2  2  1  1  2  1  1  1  2  1  1  1  2\n[15217]  1  2  1  2  2  1  1  1  2  2  2  1  1  1  2  2  1  2  1  2  1  1  2  1\n[15241]  1  1  1  1  1  2  2  2  1  1  2  1  2  1  1  2  2  2  2  2  2  1  2  2\n[15265]  2  1  1  2  1  2  1  2  2  2  2  2  2  2  1  2 NA  2  2  2  2  1  2  2\n[15289]  3  2  2  2  1  2  2  2  1  2  2  2  2  1  2  2  2  2  1  2  2  1  1  2\n[15313]  1  1  1  1  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2\n[15337]  2  2  2  2  2  2  1  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2\n[15361]  1  2  2  1  2  2  2  1  1  2  2  1  1  2  2  1  1  2  2  1  2  2  1  2\n[15385]  2  2  1  2  2  2  2  1  2  2  2  2  2  1  1  1  2  1  2  2  1  1  2  2\n[15409]  1  1  2  1  1  2  2  2  2  2  1  1  1  2  2  2  2  2  2  2  2  2  2  2\n[15433]  2  2  2  2  2  2  2 NA  2  2  2  1  2  2  2  1  1  2  2  2  2  1  1  1\n[15457]  2  1  1  1  2  1  1  2  2  2  2  2  2  1  2  1  2  2  2  1  1  1  2  2\n[15481]  2  2  2  1  1  1  1  2  2  2  2  1  1  1  2  2  1  2  1  1  2  1  2  2\n[15505]  2  2  2  2  2  2  2  1  2  2  2  1  2  1  1  2  2  2  1  1  2  2  2  2\n[15529]  2  2  2  2  2  2  2  2  1  2  2  1  1  1  2  1  2  2  1  2  1  2  2  2\n[15553]  2  1  2  2  1  1  2  2  1  1  1  1  2  2  2  1  2  2  3  2  1  2  2  2\n[15577]  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  1  2 NA  2  2  2\n[15601]  1  1  3  2  1  1  1  2  2  2  2  2  2  2  2  2  2  2  2  1  1  2  2  1\n[15625]  1  1  1  2  1  2  2  2  2  2  1  1  2  2  2  2  1  1  2  2  1  2  1  2\n[15649]  1  2  2  2  1  2  2  1  2  2  2  2  2  2  2  1  1  2  2  2  1  3  1  2\n[15673]  2  2  2  2  2  1  2  2  2  1  2  2  2  2  2  2  2  2  1  2  2  2  2  2\n[15697]  2  2  2  2  2  2  2  2  2  1  2  1  2  1  2  2  1  1  2  2  2  1  2  1\n[15721]  2  1  2  1  2  1  1  2  1  2  1  2  1  2  1  2  2  2  2  1  2  3 NA  2\n[15745]  2  1  2  1  2  2  2  1  2  1  2  2  2  2  2  2  1  1  1  2  2  2  2  2\n[15769]  1  1  2  1  1  1  1  1  3  1  1  2  1  2  2  2  3  1  2  1  2  2  2  2\n[15793]  2  2  2  2  2  2  3  2  2  2  1  2  2  1  2  2  1  2  2  2  1  2  1  1\n[15817]  2  1  2  2  2  2  1  1  2  3  2  2  2  1  2  2  2  1  1  2  1  2  1  2\n[15841]  1  2  2  2  1  2  2  1  2  2  2  2  2  2  2  1  2  2  2  2  2  2  1  2\n[15865]  2  1  1  2  2  2  2  1  1  2  1  2  2  1  2  1  2  2  1  2  2  1  2  1\n[15889]  1  1  1  2  3  2  2  1  3  2  1  2  1  2  1  1  2  2  2  2  2  2  2  2\n[15913]  2  2  1  1  2  2  1  2  1  2  2  2  1  1  2  2  1  1  1  1  2  2  2  2\n[15937]  2  1  1  1  1  2  2  2  1  2  2  1  2  1  2  1  2  2  2  1  2  1  2  2\n[15961]  2  2  2  2  2  2  1  2  1  2  1  2  2  2 NA  1  1  1  2  2  1  2  1  2\n[15985]  1  2  2  2  2  1  2  2  2  1  2  1  2  2  2  2  2  2  1  2  1  2  2  2\n[16009]  2  1  2  1  2  2  1  2  2  2  1  2  2  2  2  3  2  2  2  2  2  2  1  1\n[16033]  2  2  2  1  2  2  1  2  1  2  2  2  2  1  1  2  1  2  2  2  2  2  2  2\n[16057]  2  1  2  2  2  2  1  1  2  2  1  2  1 NA  2  2  2  2  2  2  2  1  2  2\n[16081]  1  1  2  2  2  2  2  2  2  2  1  2  2  1  2  1  1  2  1  1  1  2  1  2\n[16105]  2  2  1  2  1  1  1  2  2  2  2  2  2  2  2  2  2  2  1  2  2  2  1  1\n[16129]  1  2  2  2  2  2  2  2  2  2  2  2  1  2  1  2  1  1  2  2  2  2  2  1\n[16153]  1  2  2  1  1  1  2  1  1  1  2  2  2  1  2  1  2  2  1  1  2  1  2  2\n[16177]  2  2  1  2  1  2  1  2  2  2  2  2  2  1  1  2  1  2  2  2  2  1  2  2\n[16201]  2  2  1  1  2  2  2  2  1  2  2  2  1  1  2  2  2  2  1  1  2  1  2  1\n[16225]  1  1  2  2  2  2 NA  2  2  2  2  1  1  2  1  2  2  2  1  1  2  1  1  2\n[16249]  1  1  2  2  2  1  1  2  2  2  2  2  2  2  2  2  2  2  2  1  2  2  1  1\n[16273] NA  2  2  1  2  1  2  2  2  1  2  1  2  2  2  2  2  1  2  1  2  2  2  1\n[16297]  2  1  2  3  2  1  2  2  2  2  2  1  1  1  1  2  1  2  1  2  2  2  2  2\n[16321]  2  1  2  2  2  2  2  2  1  1  2  1  2  2  2  1  2 NA  2  1  2  1  2  2\n[16345]  1  1  2  1  2  1  2  2  1  2  2  2  1  2  2  2  2  2  1  2  2  2  2  2\n[16369]  1  2  1  1  2  2  1  2  2  2  2  1  2  2  2  1  2  2  2  2  1  2  1  1\n[16393]  2  2  2  2  2  2  1  2  2  1  1  2  2  2 NA  2  1  1  2  2  2  1  1  2\n[16417]  2  2  2  2  2  1  1  2  2  2  1  1  1  2  1  2  1  2  2  1  2  2  2  2\n[16441]  2  2  2  2  2  2  3  2  2  2  1  1  2  2  2  2  2  1  2  2  2  1  1  2\n[16465]  1  1  1  1  1  2  2  2  1  1  2  1  1  2  2  2  1  2  2  1  2  2  2  2\n[16489]  2  2  2  2  1  1  2  3  2  1  1  2  2  2  1  2  2  2  2  1  2  2  2  1\n[16513]  2  2  1  2  2  1  2  2  2  1  2  2  2  2  1  1  2  1  2  2  2  2  2  1\n[16537]  2  2  2  2  2  1  1  1  1  2  2  2  2  2  1  2  2  2  2  1  2  2  1  2\n[16561]  2  1  1  1  2  2  1  2  2  2  2  2  1  2  2  2  1  2  2  2  1  2  2  1\n[16585]  2  1  2 NA  2  2  2  2  2  3  3  1  2  2  2  2  2  1  2  2  1  1  2  1\n[16609]  1  2  1  2  2  2  2 NA  2  1  1  2  2  2  2  1  1  1  2  2  1  2  2  2\n[16633]  2  1  1  1  2  1  2  1  1  2  2  2  2  2  2  2  2  2  2  2  1  2  2  1\n[16657] NA  2  2  2  1  2  1  2  1  1  2  1  2  1  2  1  1  1  1  2  1  1  2  2\n[16681]  2  1  2  2  2  2  1  2  1  1  1  2  2  2  1  2  1  2  2  2  2  2  1  2\n[16705]  1  2  1  2  2  2  1  2  2  1  2  1  2  2  2  2  2  2  1  2  2  1  1  2\n[16729]  2  2  1  2  2  2  1  2  1  2  1  1  2  1  2  1  1  2  2  2  2  2  1  2\n[16753]  2  1  2  1  2  2  2  2  1  2  2  2  2  1  1  1  2  2  1  1  2  2  1  1\n[16777]  2  1  1  2  2  1  1  1  2  2  2  2  1  2  2  2  2  1  1  2  1  2  2  1\n[16801]  1  2  1  2  1  2  2  2  2  2  3  2  3  2  1  2  1  1  2  1  1  1  2  2\n[16825]  2  1  2  2  1  2  1  1  1  2  2  1  1  1  1  1  1  1  2  2  2  2  2  1\n[16849]  2  1  2  2  1  1  2  2  2  1  2  2  1  1  1  2  1  2  3  2  2  2  2  1\n[16873]  1  2  2  2  2  1  1  2  2  2  2  2  2  1  2  2  1  1  1 NA  2  1  1  2\n[16897]  2  1  2  2  2  2  2  2  2 NA  1  1  2  2  1 NA  2  1  2  1  1  1  1  2\n[16921]  1  2  3  1  1  1  2  1  1  1  1  1  2  1  1  1  2  1  2  1  2  1  2  1\n[16945]  2  1  1  2  1  2  1  1  1  2  2  2  1  1  2  2  2  1  2  2  2  2  2  1\n[16969]  2  2  1  2  2  2  2  1  2  1  2  2  2  2  2  1  2  2  2  2  1  1  1  2\n[16993]  2  2  1  2  1  2  1  2  1  2  2  2  1  2  1  2  2  2  2  1  1  1  1  2\n[17017]  1  2  2  2  3  2  1  1  1  2  2  2  2  2  1  2  2  2  1  2  2  2  1  1\n[17041]  1  1  1  2  1  2  1  2  1  2  2  2  2  2  2  2  2  2  2  2  2  1  2  2\n[17065]  2  2  2  1  2  2  1 NA  2  2  2  2  2  1  2  2  2  2  2  2  2  1  1  2\n[17089]  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  1  2  2  2  1  1  1  1\n[17113]  2  2  2  2  1  2  2  2  1  2  2  2  1  2  2  2  2  2  1  2  1  2  2  2\n[17137]  1  1  2  1  2  2  2  2  2  2  2  1  2  2  2  2  1  1  1  1  2  1  2  1\n[17161]  1  2  1  2  1  1  2  1  1  1  1  2  2  2  2  2  2  2  1  1  2  1  2  1\n[17185]  1  2  2  1  1  1  2  2  2  2  2  2  1  2  2  2  1  2  2  2  2  2  1  2\n[17209]  2  2  2  1  1  2  2  1  2  1  1  2  2  2  2  2  2  1  2  1  1  1  2  1\n[17233]  1  1  2  1  2  1  3  2  2  2  2  2  2  2  1  2  2  2  2  1  1  2  2  2\n[17257]  2  2  2  2  1  2  1  2  1  2  2  2  1  2  1  1  1 NA  1  2  1  1  2  1\n[17281]  1  1  2  1  2  2  2  2  2  2  1  1  1  2  2  2  2  2  1  1  2  2  1  2\n[17305]  2  2  1  2  2  2  2  2  1  2  2  2  1  2  2  2  2  2  2  1  2  1  2  1\n[17329]  2  2  2  2  2  1  2  2  2  2  1  2  1  2  2  2  2  1  1  1  2  1  2  2\n[17353]  1  1  2  2  2  2  2  1  2  1  2  1  1  1  2  2  2  1  2  1  2  2  2  1\n[17377]  2  1  2  1  1  1  1  2  2  1  1  2  1  1  2  1  1  1  1  1  1  1  2  1\n[17401]  1  2  2  2  1  2  1  1  2  1  2  2  1  1  1  1  2  1  2  2  1  1  2  2\n[17425]  1  2  2  2  2  2  2  2  2  1  2  1  2  1  1  2  2  2  2  1  2  2  1  2\n[17449]  1  2  1  1  2  1  2  2  2  2  2  2  2  2  1  2  2  1  1  1  1  1  2  2\n[17473]  2  2  2  2  1  1  1  1  2  1  2  1  1  2  1  2  2  2  1  1  1  2  2  2\n[17497]  2  2  1  2  2  1  2  2  2  1  1  1  2  1  1  1  2  2  1  2  1  1  1  2\n[17521]  2  2  2  1  2  2  1  1  2  2  1  1  2  2  2  1  2  1  1  2  2  1  1  1\n[17545]  2  1  2  1  2  2  1  1  2  1  2  1  2  1  1  1  1  2  1  1  2  1  1  1\n[17569]  2  2  2  2  1  2  2  2  2  2  2  2  2  2  2  2  2  2  2  1  1  1  2  2\n[17593]  2  1 NA  1  1  1  2  1  2  2  2  1  1  1  2  2  2  2  2  1  2  2  2  2\n[17617]  3  1  2  2  2  2  1  2  1  2  1  2  2  2  2  1  1  1  2  2  2  2  2  2\n[17641]  1  2  2  2  1  2  2  2  2  1  2  2  2  2  2  1  1  1  2  2  2  1  2  2\n[17665]  2  2  2  2  2  1  1  2  1  2  2  1  1  1  1  2  2  2  2  2  1  2  2  2\n[17689]  2  1  2  1  1  2  2  2  2  2  1  1  2  1  2  1  2  1  2  1  2  2  1  1\n[17713]  2  2  1  2  2  2  2  2  2  1  2  1  2  1  1  2  2  2  2  2  2  1  2  1\n[17737]  1  1  2  1  2  2  2  2  2  2  2  1  2  2  1  1  1  2  2  2  2  2  1  1\n[17761]  2  1  2  1  2  2  1  1  1  1  1  2  1  2  2  1  2  1  1  2  1  2  1  1\n[17785]  1  2  2  1  2  1  2  1  1  2  2  2  1  1  2  2  2  2  2  1  2  1  1  2\n[17809]  2  2  2  2  1  1  2  2  2  1  2  2  2  2  2  1  2  2  2  2  1  1  1  2\n[17833]  2  1  2  2  2  2  2  2  1  1  2  2  2  2  1  1  1  1  1  1  1  1  2  2\n[17857]  1  1  2  2  1  2  1  1  1  2  1  2  2  1  2  2  2  1  1  2  2  2  2  2\n[17881]  2  2  1  2  2  2 NA  2  2  2  2  2  2  2 NA  2  1  2  1  1  1  2  2  2\n[17905]  1  2  1  1  2  2  2  1  2  1  1  1  2  1  2  2  1  1  1  2  2  2  1  1\n[17929]  2  1  1  2  1  2  2  2  2  2  1  2  2  1  1  1  2  1  1  1  1  1  1  2\n[17953]  2  1  2  2  2  2  1  1  1  2  1  1  1  2  1  2  1  2  2  2  1  2  1  2\n[17977]  1  1  1  2  2  2  2  1  2  1  1  2  2  1  2  2  1  1  1  1  1  2  2  2\n[18001]  2  1  1  2  1  2  2  1  2  2  2  1  2  2  2  1  2  2  2  1  2  2  2  2\n[18025]  2  2  2  2  1  2  2  1  1  1  1  1  1  2  1  2  2  1  2  1  2  2  1  1\n[18049]  1  1  1  2  2  2  2  2  1  2  2  1  1  1  2  3  1  2  2  2  2  1  1  2\n[18073]  2  2  2  2  2  2  1  2  2  1  2  1  2  2  1  1  2  1  2  2  2  2  1  2\n[18097]  2  2  2  1  2  2  2  1  2  2  2  1  1  2  2  1  2  1  1  2  2  1  1  1\n[18121]  2  1  1  2  1  2  1  1  1  1  2  2  1  1  2  1  2  2  1  1  2  2  2  2\n[18145]  2  2  2  1  1  2  1  2  2  2  2  1  1  2  2  2  2  2  1  2  2  2  1  1\n[18169]  1  1  2  2  2  1  2  2  1  2  2  2  2  2  1  2  1  2  2  1  1  1  2  2\n[18193]  2  2  2  2  1  2  2  2  2  2  1  2  2  2  1  2  1  1  2  2  2  2  2  2\n[18217] NA  2  1  2  2  2  1  2  2  2  2  1  2  1  2  2  2  2  1  1  2  2  2  3\n[18241]  2  1  1  1  1  1  1  1  2  1  1  1  1  2  2  2  1  2  1  2  2  1  1  2\n[18265]  1  2  2  1  2  2  2  2 NA  2  2  2  2  2  2  2  1  1  1  2  1  1  1  2\n[18289]  1  2  1  2  1  2  2  2  2  2  2  1  2  2  1  2  2  2  2  1  1  2  2  2\n[18313]  2  2  2  2  2  1  1  1  1  1  2  2  1  1  2  1  1  1  2  2  2  1  2  2\n[18337]  2  2  2  2  2  1  2  2  2  1  2  2  2  2  1  2  1  2  2  2  2  2  2  2\n[18361]  1  2  2  1  2  2  2  1  1  1  1  1  1  1  2  2  2  2  1  1  2  2  2  2\n[18385]  1  1  1  2  1  2  2  1  2  2  2  1  1  1  1  2  2  2  1  1  2  1  2  2\n[18409]  1  2  1  1  1  1  1  1  1  2  2  2  2  2  1  1  2  1  2  2  1  2  1  2\n[18433]  2  2  1  2  2  2  2  2  2  2  2  2  2  1  2  2  1  1  1  1  1  1  1  2\n[18457]  1  2  2  1  1  2  2  2  2  2  1  2  1  1  2  1  1  2  1  1  2  1  2  1\n[18481]  2  1  2  1  1  1  1  2  3  2  1  1  2  1  2  2  1  2  2  1  2  2  2  2\n[18505]  1  2  1  1  2  1  1  2  1  2  1  1  2  1  1  1  2  2  1  2  2  1  2  2\n[18529]  2  1  2  1  1  2  2  1 NA  2  2  2  2  2  1  2  1  2  1  2  1  2  1  2\n[18553]  2  1  1  2  1  1  2  1  2  2  2  2  2  2  2  2  2  2  2  2  2  1  1  1\n[18577]  2  2  1  1  2  2  2  1  1  2  2  2  1  1  2  2  2  2  2  1  1  2  2  2\n[18601]  2  2  2  2  1  2  1  2  2  2  2  2  1  2  1  1  2  2  2  1  1  2  1  2\n[18625]  2  2  2 NA  2  1  1  2  2  1  2  2  1  2  2  2  1  1  2  2  2  1  1  2\n[18649]  1  1  2  2  2  2  1  2  1  2  2  1  2  1  2  2  1  2  1  1  1  1  1  1\n[18673]  1  1  2  2  2  1  1  1  2  3  1  2  3  1  1  2  2  1  1  2  2  3  2  2\n[18697]  2  2  2  2  2  1  2  2  2  1  2  2  1  2  2  2  2  2  2  1  2  2  2  1\n[18721]  1  2  2  2  1  2  1  2  2  2  2  2  1  1  2  2  1  2  2  1  1  2  1  1\n[18745]  1  1  2  1  2  1  1  2  1  2  1  1  1  2  2  2  2  1  2  1  1  2  1  2\n[18769]  2  2  1  1  2  2  2  2  2  1  1  2  2  2  2  2  1  1  1  2  2  1  2  1\n[18793]  1  1  2  2  2  2  1  2  2  2 NA  2  1  1  1  2  1  2  1  1  2  1  2  2\n[18817]  1  2  2  2  2  1  2  2  1  1  1  1  2  2  2  2  2  1  1  1  2  1  1  1\n[18841]  2  1  1  2  2  1  2  2  2  2  2  2  1  2  1  2  2  2  2  2  2  1  2  1\n[18865]  1  1  1  2  1  1  1  1  1  2  1  2  2  1  2  2  1  2  2  2  1  2  2  2\n[18889]  1  2  2  1  1  2  2  2  2  3  1  2  2  1  1  1 NA  2  1  1  1  1  2  1\n[18913]  2  2  2  1  2  1  2  2  2  1  1  1  1  2  2  2  1  1  2  2  1  1  2  2\n[18937]  1  2  2  2  2  2  1  2  2  1  1  2  2  2  2  2  1  1  2  1  2  2  1  2\n[18961]  2  2  1  1  2  1  1  2  2  2  1  2  2  1  1  2  2  1  1  1  2  1  2  2\n[18985]  1  2  1  2  2  2  2  2  2  1  2  2  2  1  1  1  1  1  2  2  2  2  2  2\n[19009]  2  2  2  2  1  1  1  1  2  1  2  2  1  2  2  2  2  1  2  2  1  2  1  2\n[19033]  1  2  1  2  1  2  1  2  2  2  2  2  1  2  2  1  2  1  1  2  1  1  2  1\n[19057]  2  2  2  2  2  1  1  2  2  1  2  2  2  1  2  2  2  1  2  1  2  2  2  1\n[19081]  1  1  2  1  2  2  1  2  2  2  2  2  2  1  2  2  2  1  2  1  1  3  2  2\n[19105]  2  2  2  2 NA  2  2  2  1  2  2  2  2  2  2  2  2  2  2  1  2  2  1  2\n[19129]  1  2  1  2  1  2  2  1  2  2  2  2  1  1  2  2  1  2  2  1  1  1  1  2\n[19153]  2  1  1  2  2  1  2  2  1  2  2  2  2  2  1  1  1  1  1  2  1  2  2  2\n[19177]  1  2  2  1  2  1  2  2  1  2  2  1  2  2  2  2  2  1  2  2  1  2  1  2\n[19201]  2  1  1  2  3  2  1  2  2  1  2  2  1  2  2  1  2  1  1  3  2  2  2  2\n[19225]  2  1  1  1  2  2  2  2  2  2  2  2  3  1  1  2  2  2  1  1  2  2  2  1\n[19249]  2  2  2  2  2  2  1  2  1  2  2  2  1  1  2  2  2  1  2  1  1  1  1  2\n[19273]  1  2  1  2  1  2  2  2  1  1  2  2  2  1  2  2  2  1  2  2  2  2  1  2\n[19297]  1  2  2  2  2  1  1  1  1  1  1  2  1  2  1  2  1  2  1  2  1  1  2  2\n[19321]  3  2  2 NA  1  1  1  2  2  1  1  1  2  1  1  2  2  2  1  2  2  2  2  1\n[19345]  2  2  2  2  2  2  2  2  2  2  2  1  1  2  2  1  2  2  2  2  2  2  2  2\n[19369]  2  2  2  2  2  1  2  2  2  2  2  2  2  2  2  2  1  2  2  2  1  1  1  2\n[19393]  2  1  2  2  3  2  2  2  2  2  2  2  2  2  1  1  1  2  2  2  2  2  1  1\n[19417]  1  2  1  2  2  2  1  2  1  2  1  2  2  1  1  1  2  2  2  2  1  2  2  2\n[19441]  1  1  1  2  1  2  2  2  2  2  1  2  2  2  1  2  2  1  2  2  2  1  2  2\n[19465]  2  1  2  2  1  2  1  2  1  1  2  1  1  1  2  2  2  2  2  1  2  1  1  2\n[19489]  2  2  2  2  2  2  2  2  1  2  1  2  1  2  3  1  2  1  2  1  2  2  2  2\n[19513]  2  2  2  2  1  2  2  2  2  1  2  2  2  2  2  2  2  2  2  3  2  2  2  2\n[19537]  2  2  2  1  2  2  1  2  2  2  2  2  2  1  1  2  2  2  2  2  1  2  2  2\n[19561]  2  2  3  2  3  2  2  3  1  1  2  2  2  1  2  1  1  2  2  2  2  2  2  2\n[19585]  2  2  2  2  2  2  2  2  2  2  1  2  2  2  2  1  3  2  2  2  2  2  2  3\n[19609]  2  2  1  2  1  2  1  2  2  2  2  2  2  2  2  1  1  2  2  2 NA  1  2  1\n[19633]  2  2  3  1  1  2  2  2  1  1  1  1  2  3  2  2  2  2  2  2  2  1  2  2\n[19657]  1  1  2  1  1  1  1  2  1  1  1  2  2  2  2  2  2  1  2  2  2  2  2  2\n[19681]  3  2  2  2  2  2  1  2  2  2  1  2  1  2  1  1  2  2  2  2  2  2  2  2\n[19705]  1  2  2  2  2  1  2  1  1  1  2  1  2  2  2  1  1  2  2  1  2  2  2  1\n[19729]  1  2  2  2  2  2  3  2  1  2  2  1  2  1  2  1  2  2  2  2  2  2  2  1\n[19753]  1  1  2  2  2  2  1  2  2  2  1  2  2  2  1  2  2  1  1  1  1  1  2  2\n[19777]  1  2  1  2  2  2  2  2  1  2  2  2  2  1  1  2  1  2  2  2  1  2  2  1\n[19801]  1  2  2  2  2  2  2  2  2  2  2  2  1  2  2  2  1  1  1  1  2  2  1  2\n[19825]  3  2  1  1  2  1  1  1  1  2  2  2  2  2  2  2  2  1  2  2  1  1  2  2\n[19849]  1  2  1  1  2  2  1  1  1  1  2  2  2  2  2  2  1  2  1  2  2  1  1  1\n[19873]  2  1  2  2  2  2  2  2  1  2  2  1  1  1  2  2  2  1  1  1  2  2  1  2\n[19897]  1  2  2  2  1  1  2  1  2  2  1  2  2  2  1  1  2  1  2  1  1  2  1  2\n[19921]  2  2  2  2  2  2  2  2  2  2  2  2  1  1  1  2  2  2  2  2  2  2  2  2\n[19945]  2  2  2  2  2  1  2  2  1  1  2  1  2  2  1  2  1  2  2  2  1  2  2  1\n[19969]  2  2  2  2  2  2  2  2  1  1  2  2  1  2  2  2  1  2  1  1  1  1  2  2\n[19993]  1  1  1  2  2  2  1  2  2  2  1  1  1  1  2  1  2  1  2  1  2  1  2  1\n[20017]  1  1  2  2  2  1  2  2  2  2  2  1  2  2  2  2  2  2  1  2  1  1  2  2\n[20041]  2  1  1  2  1  2  2  2  2  1  1  1  2  2  1  2  1  1  1  1  2  2  2  2\n[20065]  1  2  2  1  1  1  2  1  1  1  1  2  1  2  2  2  2  2  1  1  1  2  2  1\n[20089]  2  1  1  2  2  1  2  2  2  2  2  1  1  1  2  2  2  2  2  3  1  2  1  2\n[20113]  2  1  2  2  2  2  2  2  1  2  2  2  2  1  2  2  1  1  1  2  1  2  2  1\n[20137]  1  2  1  2  1  1  1  2  2  1  1  2  2  1  2  2  1  2  2  2  2  2  1  1\n[20161]  2  1  1  2  2  1  1  1  2  2  2  1  1  2  1  2  1  3  1  1  1  2  2  2\n[20185]  1  2  2  2  2  1  1  2  1  1  2  2  2  2  1  2  1  2  2  2  1  2  2  2\n[20209]  2  1  1  1  2  2  2  2  2  1  2  1  1  1  1  1  2  2  2  2  2  2  1  2\n[20233]  2  2  1  1  2  1  2  2  2  1  1  2  2  2  2  2  1  2  2  2  2  1  2  2\n[20257]  1  2  2  2  2  1  1  1  3  2  1  2  1  1  2  2  2  2  1  2  2  2  2  2\n[20281]  1  2 NA  2  2  2  2  2  1  1  2  2  2  1  2  1  1  2  2  2  2  1  1  2\n[20305]  1  2  1  2  3  2  2  2  3  2  1  2  2  1  1  2 NA  1  1  2  2  2  2  2\n[20329]  2  1  1  1  2  2  2  1  2  2  1  1  2  2  2  2  1  1  1  2  1  2  2  2\n[20353]  2  1  2  1  1  2  1  2  1  2  2  1  2  2  2  2  2  2  2  2 NA  2  3  2\n[20377]  1  1  2  1  2  1  2  2  2  2  2  2  1  2  2  2  1  2  1  2  2  2  2  1\n[20401]  2  1  1  2  2  1  2  2  1  2  3  2  1  2  1  1  1  2  2  2  2  1  2  1\n[20425]  2  2  2  1  2  1  2  1  2  2  1  2  2  2  2  1  1  1  2  1  1  1  3  2\n[20449]  2  2  1  2  2  2  2  2  2  2  2  2  2  1  2  2  2  1  2  2  2  2  2  1\n[20473]  1  2  2  2  2  2  2  2  1  2  2  1  3  1  2  2  1  2  2  2  2  2  1  2\n[20497]  1  1  1  2  1  1  2  2  1  1  2  1  2  2  2  1  2  2  2  1  1  2  2  2\n[20521]  2  2  2  2  2  1  2  1  2  2  1  2  1  2  2  1  2  1  1  2  1  1  2  2\n[20545]  2  2  2  1  1  2  2  2  2  2  1  2  1  2  1  2  2  1  2  2  2  2  1  1\n[20569]  1  2  1  2  1  2  1  2  2  1  1  2  1  2  1  2  2  1  1  2  2  2  1  1\n[20593]  1  1  1  2  1  2  2  2  1  2  1  2  2  1  2  2  2  2  1  2 NA  2  1  2\n[20617]  1  2  2  2  2  1  1  2  1  2  1  2  1  1  2  1  1  1  2  1  2  2  1  2\n[20641]  1  1  2  2  2  2  2  1  2  2  1  1  2  2  2  1  2  2  2  1  2  2  1  1\n[20665]  1  1  1  2  1  2  2  1  2  2  2  1  2  2  1  2  2  2  2  2  2  2  1  1\n[20689]  2  1  1  1  2  1  2  2  2  2  2  2  2  2  1  2  1  2  2  2  2 NA  2  2\n[20713]  2  1  1  2  1  1  2  1  2  2  2  2  2  2  1  2  2  2  1  1  2  1  2  1\n[20737]  1  2  1  2  1  2  1 NA  1  1  2  2  2  2  2  2  2  1  2  1  2  1  1  2\n[20761]  1  1  2  2  1  2  2  1  2  2  2  1  2 NA  1  2  1  2  1  2  1  2  2  2\n[20785]  2  2  1  1  1  1  2  2  1  2  1  2  2  1  2  2  2  1  1  2  2  2  2  2\n[20809]  1  1  2  1  1  1  2  2  1  3  2  1  1  1  1  1  2  1  1  2  1  2  1  2\n[20833]  1  1  1  1  1  2  2  1  1  2  2  1  2  1  2  1  2  2  2  2  2  2  1  1\n[20857]  2  2  2  2  2  2  1  1  2  2  2  2  2  2  2  2  1  1  1  2  2  2  2  2\n[20881]  2  2  1  2  1  2  1  2  1  1  2  2  2  2  2  2  2  2  1  2  1  2  2  1\n[20905]  2  2  2  2  2  2  1  2  1  2  1  2  1  2  2  2  2  1  1  2  1  1  2  1\n[20929]  2  2  2  1  1  2  2  2  2  1  2  2  1  2  2  1 NA  1  1  1  2  1  1  2\n[20953]  1  2  2  1  2  1  1  1  1  2  1  2  2  2  1  1  2  1  1  2  2  2  2  1\n[20977]  2  2  2  1  1  2  2  1  2  2  1  2  2  2  2  3  1  2  2  1  2  2  2  2\n[21001]  2  2  1  2  2  1  1  2  2  2  1  2  2  1  2  2  1  1  2  2  2  1  3  2\n[21025]  2  1  1  2  1  1  2  1  2  1  1  2  1  2  1  2  2  1  1  2  2  2  2  2\n[21049]  2  2  1  2  2  2  1  2  2  2  2  2  1  1  2  1  2  2  1  2  1  1  2  2\n[21073]  1  1  3  2  1  1  1  1  2  1  2  1  2  2  2  2  1  2  2  2  2  2  2  1\n[21097]  1  2  1  2  2  2  1  1  1  1  1  2  2  2  2  2  1  2  2  2  1  1  1  1\n[21121]  2  1  2  1  3  2  2  2  2  2  2  2  2  2  2  1  1  1  1  2  2  1  2  2\n[21145]  2  2  1  1  1  1  2  2  2  2  2  1  1  1  2  2  2  2  1  2  2  2  1  1\n[21169]  2  1  2  2  2  1  3  1  1  2  1  2  1  1  1  2  2  1  2  2  2  2  2  1\n[21193]  1  2  2  2  2  1  2  2  2  2  1  2  2  1  2  2  2  2  2  2  2  2  2  2\n[21217]  2  1  2  2  2  2  2  2  1  1  2  2  2  2  2  1  1  1  2  1  2  1  2  1\n[21241]  2  1  1  2  1  2  1  1  1  1  2  2  1  2  2  2  2  1  2  3  2  2  2  2\n[21265]  2  2  2  2  2  2  2  2  1  2  1  2  1  2  2  2  1  1  2  1  2  2  2  2\n[21289]  1  1  2  2  1  2  1  2  1  3  2  2  1 NA  1  2  2  2  3  2  1  1  2  1\n[21313]  2  1  2  2  2  2  2  2  1  2  2  1  2  1  1  2  2  2  2  1  2  2  1  2\n[21337]  2  2  2  2  2  1  2  2  2  1  2  2  2  2  1  2  2  1  2  2  1  2  2  2\n[21361]  1  1  2  2  2  2  1  2  2  1  1  2  1  2  1  2  2  2  2  2  1  2  2  1\n[21385]  2  2  2  2  1  2  1  2  2  1  1  1  2  1  2  2  2  1  2  1  1  1  2  2\n[21409]  2  2  2  1  2  2  2  2  2  1  2  2  1  1  2  2  1  1  1  1  2  2  1  2\n[21433]  1  1  1  2  2  2  2  1  2  1  1  2  2  2  2  1  1  2  2  2  1  2  2  1\n[21457]  3  2  2  2  2  2  2  2  2  2  3  1  1  1  1  2  2  2  1  2  2  3  2  2\n[21481]  2  2  2  2  1  2  1  2  2  2  2  2  2  2  1  1  2  1  1  1  2  1  2  2\n[21505]  1  1 NA  1  2  2  2  2  1  1  2  2  1  1  1  1  2  2  2  2  2  2  2  2\n[21529]  2  2  2  2  2  2  1  2  2  1  2  2  2  2  2  2  3  1  1  1  2  2  1  1\n[21553]  1  2  2  1  1  2  2  1  2  1  2  2  1  2  2  2  2  2  1  1  2  2  1  2\n[21577]  2  2  2  1  2  2  2  2  2  2  2  2  1  2  1  2  2  2  2  1  1  1  2  1\n[21601]  1  1  2  1  2  2  2  1  1  1  1  2  2  2 NA  2  2  2  2  1  1  1  1  2\n[21625]  1  1  1  2  3  2  2  2  1  1  2  1  2  1  2  1  2  2  2  2  2  2  2  2\n[21649]  2  1  2  2  2  2  2  1  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2\n[21673]  2  1  2  2  2  1  1  2  2  2  2  1  1  1  2  1  1  2  1  1  2  2  2  2\n[21697]  1  2  2  2  2  2  2  2  2  2  1  2  1  2  2  2  2  2  2  2  1  1  2  2\n[21721]  2  2  2  1  1  3  2  2  2  2  1  1  1  1  2  2  1  2  1  2  2  1  1  2\n[21745]  2  2  2  2  1  1  2  2  2  2  1  2  2  2  2  2  2  2  1  2  2  1  2  2\n[21769]  1  2  1  1  2  2  2  2  2  1  2  2  2  1  2  2  1  1  2  1  2  1  2  2\n[21793]  1  2  2  2  2  2  2  1  2  2  1  1  1  1  2  1  2  2  2  2  2  2  2  2\n[21817]  2  1  2  2  2  2  2  2  1  2  1  1  1  2  1  1  1  1  2  1 NA  2  2  2\n[21841]  2  1  2  2  2  2  1  2  2  2  2  1  2  2  2  1  3  2  1  1  2  2  2  1\n[21865]  2  1  1  1  1  1  1  2  1  2  1  1  2  2  2  1  2  1  2  2  2  2  1  2\n[21889]  2  2  2  1  1 NA  2  2  2  1  2  2  2  2  2  2  2  2  2  1  2  2  2  2\n[21913]  2  1  2  1  2  2  2  2  2  2  2  2  2  2  1  1  1  2  1  1  2  2  2  1\n[21937]  1  1  2  1  1  2  2  2  2  1  2  2  2  2  1  1  2  1  1  2  2  1  1  1\n[21961]  1  1  1  2  2  2  2  2  2  2  1  1  1  1  1  1  1  2  2  1  2  1  2  1\n[21985]  1  1  2  2  2  1  1  2  2  2  2  2  1  1  2  2  1  2  1  2  1  1  2  1\n[22009]  1  1  1  3  3  2  2  1  2  2  1  1  1  1  2  1  2  1  2  1  1  2  2  2\n[22033]  2  2  1  1  2  1  2  1  1  2  1  1  2  2  2  2  2  2  2  2  2  2  1  2\n[22057]  2  2  1  2  2  1  2  1  2  1  2  2  1  2  1  2  1  1  2  2  1  1  2  2\n[22081]  1  3  2  2  3  2  2  2  1  2  1  2  2  1  2  2  2  1  2  2  2  2  2  2\n[22105]  2  2  1  2  2  2  2  2  1  1  2  2  1  1  1  2  1  1  2  2  2  1  2  1\n[22129]  2  1  2  1  2  2  2  2  1  2  1  1  1  2  2  1  1  2 NA  2  2  2  2  1\n[22153]  1  2  2  2 NA  2  2  1 NA  2  2  2  3  2  2  1  2  1  2  1  3  1  2  2\n[22177]  2  2  2  2  1  2  2  2  2  2  2  2  2  2  1  1  2  1  2  3  2  1  1  2\n[22201]  1  1  1  1  1  2  1  1  2  1  2  2  1  2  1  1  1  2  2  2  2  2  2  2\n[22225]  2  2  1  2  2  2  2  2 NA  2  2  1  2  1  1  2  2  1  2  1  1  1  2  1\n[22249]  2  2  2  2  2  1  2  2  2  1  1  1  2  2  2  2  2  2  2  2  2  2  2  2\n[22273]  2  3  2  2  1  2  2  1  2  1  2  2  2  2  1  1  1  2  2  2  1  2  2  1\n[22297]  2  2  1  2  1  2  1  2  1  1  2  2  2  1  2  2  2  2  2  1  1  1  2  2\n[22321]  1  2  2  2  2  1  2  1  2  2  2  2  2  1  2  1  1  1  1  1  1  1  2  2\n[22345]  2  2  3  1  1  1  2  2  2  1  2  2  2  2  1  1  1  2  1  2  2  1  2  1\n[22369]  2  1  2  2  1  2  2  2  2  2  1  1  1  1  1  1  2  1  2  2  1  2  2  2\n[22393]  1  1  1 NA  2  1  1  2  2  1  1  2  2  2  2  2  1 NA  1  2  1  2  2  1\n[22417]  2  1  1  2  2  1  2  2  2  1  1  1  1  2  1  1  2  2  2  2  2  2  2  1\n[22441]  2  2  2  2  2  2  2  2  1  2  1  2  1  2  2  2  2  2  2  2  2  2  2  2\n[22465]  2  1  2  2  1  2  2  1  2  1  2  2  2  1  1  2  2  1  1  2  2  2  2  2\n[22489]  2  1  2  2  2  2  2  1  1  1  2  2  2  1  2  2  2  2  1  3  2  2  2  2\n[22513]  1  2  2  1  2  2  2  2  1  2  1  2  1  2  1  1  2  2  1  2  2  2  1  2\n[22537]  1  1  1  2  2  1  1  1  1  2  1  1  2  1  2  1  2  2  1  1  1  1  2  1\n[22561]  1  2  2  2  2  2  1  1  2  2  2  2  2  2  1  1  2  1  2  1  2  2  2  2\n[22585]  2  1  1  2  1  2  1  2  2  2  2  2  2  1  1  1 NA  2  2  2  2  2  1  1\n[22609]  2  2  2  2  1  2  2  2  2  1  2  2  2  2  1  1  2  1  2  1  2  2  2  1\n[22633]  2  1  1  2  2  1  1  1  2  1  1  2  1  1  2  2  2  2  2  2  1  1  2  2\n[22657]  2  2  2  2  1  2  1  3  2  2  2  2  2  2  1  1  2  2  1  1  2  1  2  2\n[22681]  1  2 NA  2  2  2  1  2  2  1  2  2  2  2  2  1  2  1  2  1  1  1  2  2\n[22705]  1  2  2  2  2  2  2  1  1  2 NA  1  1  2  1  2  1  2  2  1  2  2  2  1\n[22729]  1  1  1  2  2  2  2  1  1  1  1  2  1  1  2  2  2  2  1  1  2  1  1  1\n[22753]  2  2  1  2  2  2  2  2  2  1  1  1  1  1  2  2  2  1  2  1  1  2  1  2\n[22777]  2  2  1  2  2  2  2  2  1  2  1  1  1  1  1  1  2  2  2  2  2  1  2  2\n[22801]  1  1  1  2  2  1  1  1  2  2  2  1  2  2  2  1  1  2  2  1  2  2  1  1\n[22825]  1  1  1  1  2  2  2 NA  2  2  2  1  2  1  2  2  2  1  1  2  2  2  2  1\n[22849]  2  2  1  2  1  2  1  2  1  2  1  2  2  2  1  2  2  2  2  2  2  2  2  1\n[22873]  2  2  2  2  2  2  1  2  2  1  2  2  1  2  1  2  2  2  2  2  1  2  2  1\n[22897]  2  1  1  1  1  2  1  1  1  2  2  2  2  2  2  2  2  2  2  1  1  2  2  2\n[22921]  1  1  2  1 NA  2  2  2  2  2  2  2  2  2  1  2  1  1  2  1  2  1  1  2\n[22945]  2  1  2  1  2  1  2  2  2  2  2  1  1  2  2  2  2  1  1  2  2  1  1  2\n[22969]  1  2  2  2  1  2  2  2  2  2  2  2  2  2  2  1  2  1  2  2  1  2  1  1\n[22993]  1  2  1  1  2  2  1  2  2  2  2  2  2  2  2  2  2  1  1  1  2  1  2  1\n[23017]  2  1 NA  1  2 NA  2  2  2  1  2  2  2  2  2  1  2  1  1  2  1  1  2  1\n[23041]  2  1  2  1  2  2  2  2  2  2  1  1  2  1  1  3  2  2  1  2  1  1  2  2\n[23065]  2  2  1  2  2  2  2  2  2  2  1  2  1  2  2  1  1  2  1  2  2  2  2  2\n[23089]  1  1  2  1  2  2  2  1  2  2  1  2  1  2  2  1  2  2  2  2  1  2  2  1\n[23113]  1  2  2  2  2  1  1  2  2  3  1  2  2  2  2  2  2  2  1  2  2  2  2  2\n[23137]  1  2  2  1  2  2  2  2  2  1  2  2  2  2  2  1  2  2  1  1  2  2 NA  2\n[23161]  2  2  2  1  1  2  1  1 NA  1  1  1  2  1  2  3  1  2  2  2  2  1  2  2\n[23185]  1  1  1  2  2  2  2  1  1  2  1  2  2  2  3  1  1  2  3  1  2  2  2  2\n[23209]  1  1  1  2  2  2  2  1  2  2  1  2  2  2  2  1  2  1  1  2  1 NA  2  1\n[23233]  1  1  1  2  1  2  2  1  1  2  2  1  2  2  2  1  2  2  2  2  1  2  1  1\n[23257]  1  1  1  1  2  2  2  2  1  1  1  2  2  2  1  1  2  2  2  2  1  2  2  2\n[23281]  2  2  2  1  1  1  2  2  2  2  2  2  2  2  2  1  2  2  2  2  2  2  1  2\n[23305]  2  1  2  2  2  1  1  1  1  2  1  2  2  2  2  1  2  2  2  2  2  2  1  1\n[23329]  2  1  2  2  1  1  2  2  2  1  2  2  2  2  3  1  2  2  2  1  2  1  2  1\n[23353]  2  1  2  2  1  2  1  1  1  2  1  1  2  2  2  1  2  2  1  2  2  2  2  1\n[23377]  2  1  1  2  1  2  2  2  1  1  2  2  1  2  1  1  1  2  1  1  2  1  1  1\n[23401]  2  1  1  2  2  2  2  1  1  2  1  1  1  3  2  2  1  1  1  2  1  1  2  2\n[23425]  2  2 NA  1  2  2  1  2  2  2  2  1  2  2  2  1  2  2  1  2  2  1  1  2\n[23449]  1  2  2  1  2  2  1  1  1  1  2  2  1  2 NA  1  2  1  2  2  1  2  2  2\n[23473]  1  1  1  2  2  2  1  2  2  2  2  1  2  2  2  2  1  1  2  1  1  1  2  2\n[23497]  2  1  2  2  2  2  3  1  2  2  1  2  1  1  1  1  2  1  2  2  1  2  2  2\n[23521]  2  2 NA  2  1  2  2  1  2  2  2  2  1  2  2  2  1  1  1  2  2  1  2  1\n[23545]  2  1  2  2  2  1  1  2  1  2  1  1  1  2  2  2  2  2  2  1  1  2  1  1\n[23569]  2  1  2  2  2  2  2  1  2  2  2  1  2  2  1  2  1  2  1  2  2  2  2 NA\n[23593]  2 NA  1  2  2  2  2  2  2  1  1  2  2  1  1  2  1  2  2  2  2  2  1  2\n[23617]  2  2  2  1  2  1  2  2  1  2  2  2  2  1  2  2  2  2  2  1  2  1  1  2\n[23641]  1  2  2  2  2  1  2  2  2  2  2  1  1  1  2  2  2  2  2  2 NA  1  2  1\n[23665]  2  2  1  2  2  2  2  1  2  1  2  2  2  2  2  2  1 NA  2  2  2  2  2  1\n[23689]  2  2  1  1  1  1  2  2  2  2  2  1  1  2  2  2  2  2  1  2  2  2  1  1\n[23713]  2  2  1  1  2  2  1  2  1  2  2  2  2  1  2  2  2  2  1  1  2  2  1  2\n[23737]  1  1  1  2  2  2  1  2  2  1  1  1  1  1  2  2  2  2  2  1  2  1  2  3\n[23761]  1  1  2  2  1  2  2  2  2  2  2  2  2  1  2  1  2  2  2  2  1  2  1  1\n[23785]  2  1  2  1  2  2  1  2  2  2  1  1  1  2  1  2  2  1  2  2  2  2  2  2\n[23809]  2  2  1  1  1  2  2  1  3  1  2  1  2  2  1  1  1  1  2  2  1  2  1  2\n[23833]  2  2  2  1  2  2  1  1  1  2  2  1  1  2  2  1  1  2  1  2  1  2  2  2\n[23857]  2  3  2  2  1  2  2  2  2  2  1  2  1  1  2  2  2  1  2  1  1  2  3 NA\n[23881]  2  1  1  2  2  2  2  2  1  2  2  1  2  1  1  2  1  2  1  2  2  2  2  2\n[23905]  1  2  1  1  1  2  1  1  2  1  1  2  2  1  2  2  2  2  2  2  2  2  2  1\n[23929]  2  2  1  3  2  2  2  1  1  1  1  1  1  2  2  2  1  2  2  2  2  2  1  2\n[23953]  1  2  2  2  2  2  1  2  1  2  2  1  2  1  1  2  1  1  1  1  1  3  2  2\n[23977]  1  1  1  2  2  1  2  1  2  1  1  1  1  2  2  1  1  1  2  2  1  1  1  2\n[24001]  2  1  2  2  1  1  2  1  2  2  2  3  2  1  2  2  2  1  2  2  2  1  1  1\n[24025]  2  1  2  1  2  2  2  2  2  2  2  2  2  2  2  2  2  1  2  1  2  2  2  2\n[24049]  2  1  2  2  2  1  2  2  2  2  1  2  1  1  2  2  2  1  2  2  2  1  2  2\n[24073]  2  2  1  2  1  1  2  2  2  2  2  2  1  1 NA  2  1  1  2  2  2  2  2  1\n[24097]  1  1  1  1  2  1  1  2  2  1  2  1  2  2  2  1  1  2  3  1  1  1  2  2\n[24121]  2  1  2  2  2  1  2  1  2  2  2  2  1  1  1  1  1  2  1  2  1  2  2  1\n[24145]  1  1  2  1  2  1  1  1  2  1  2  1  2  2  2  1  2  2  1  2  2  2  2  1\n[24169]  2  1  1  1  2  2 NA  1  2  1  2  2  2  1  1  2  2  1  2  1  3  2  1  2\n[24193]  1  2  3  2  2  2  1  2  2  2  2  2  2  2  2  1  1  2  2  1  1  2  2  1\n[24217]  1  1  2  2  2  2  2  1  2  1 NA  2  1  2  1  2  1  1  1  2  2  2  2  1\n[24241]  2  2  2  1  2  1  1  1  2  1  1  1  1  2  2  1  2  2  2  2  2  2  2  1\n[24265]  2  1  1  1  2  2  2  2  1  2  2  2  1  1  2  2  2  2  1  2  1  1  1  1\n[24289]  1  2  1  2  1  2  1  2  2  2  2  1  1  1  2 NA  2  1  1  1  2  1  1  1\n[24313]  1  1  2  1  1  1  2  2  2 NA  1  1  1  1  2  1  2  2  1  1  2  2  2  1\n[24337]  2  2  2  1  2  2  2  2  2  2  1  2  2  2  1  2  1  2  2  2  1  1  2  2\n[24361]  1  2  1  2  1  1  1  1  1  3  1  1  2  1  2  2  2  1  1  2  1  1  2  1\n[24385]  1  2  1  2  1 NA  1  2  2  2  2  3  2  1  2  2  2  1  1  1  1  2  2  1\n[24409]  2  2  2  2  2  1  2  2  1  2  2  1  2  2  1  1  1  1  2  2  2  1  1  1\n[24433]  2  1  2  3  2  1  1  2  1  2  2  2  2  2  1  1  2  1  1  1  2  1  1  1\n[24457]  2  1  2  1  1  2  2  2 NA  1  1  1  2  1  1  2  2  2  1  1  2  2  1  2\n[24481]  2  1  1  1  2  2  2  1  2  1  2  2  1  2  2  2  2  2  1  2  1  2  1  2\n[24505]  1  2  2  1  1  1  1  1  2  1  2  2  1  1  1  2 NA  2  2  2  1  2  1  2\n[24529]  2  2  1  1  2  1  1  1  2  2  1  1  1  1  1  2  1  1  1  1  2  2  2  2\n[24553]  1  2  1  1  2  1  2  2  2  2  1  2  1  2  2  2  1  1  1  1  2  2  2  2\n[24577]  2  1  1  1  1  2  2  2  2  1  2  2  2  1  2  2  1  1  1  1  1  2  1  2\n[24601]  2  2  2  2  2  1  1  2  2  1  2  2  1  2  2  1  2  1  2  2  1  1  2  2\n[24625]  2  1  1  1  1  1  2  2  2  1  1  1  2  2  1  1  2  2  1  1  2  1  1 NA\n[24649]  1  2  2  2  1  2  2  2 NA  2  1  1  2  2  1  2  2  2  1 NA  3  2  2  2\n[24673]  2  1  2  2  2  2  2  3  2  2  2  1  2  2  1  2  1  2  1  1  2  2  2  1\n[24697]  1  1  1  2  2  2  2  2  2  1  2  3  2  2  2  2  2  1  2  1  2  1  2  2\n[24721]  2  2  1  1  1  2  2  1  2  2  2  2  1  1  1  1  2  2  1  1  2  1  1  1\n[24745]  2  1  1  2  2  1  1  1  1  1  2  2  2 NA  2  1  1  2  1  1  2  1  2  1\n[24769]  2  2  2  2  2  2  1  1  1  2  1  2  1  2  1  1  2  2  2  2  2  1  1  1\n[24793]  1  2  1  2  1  2  2  2  1  2  1  2  1  1  2  2  2  2  2  2  1  1  2  1\n[24817]  2  2  1  2  2  2  2  2  1  1  2  1  2  2  2  2  1  1  1  2  2  2  2  2\n[24841]  1  2  1  2  1  2  2  2  1  2  2  2  1  2  1  2  1  2  1  2  2  2  2  1\n[24865]  1  2  1  1  2  2  2  1  1  1  2  1  2  1  1  2  1  2  1  1  2  1  2  1\n[24889]  1  2  1  2  1  1  2  1  1  1  1  1  1  2  2  2  1  1  2  1  1  2  1  1\n[24913]  1  1  1  1  2  2  1  1  1  1  2  1  1  1  2  2  2  2  2  1  2  1  1  1\n[24937]  1 NA  2  1  2  2  1  1  2  2  2  1  1  1  1  1  1  1  2  1  2  1  2  2\n[24961]  2  2  1  2  2  2  2  1  2  1  2  1  1  2  1  2 NA  2  1  2  2  1  1  1\n[24985]  1  2  2  2  2  2  1  2  1  1  2  2  2  2  2  2  1  2  2  2  2  1 NA  2\n[25009]  1  2  2  1  2  1  2  2  2  2  2  2  2  2  2  2  1  2  1  2  2  2  1  2\n[25033]  1  1  2  2  2  2  2  1  2  2  2  2  1  2  2  2  1  2  2  2  2  2  2  2\n[25057]  1  2  2  1  2  1  1  1  2  2  2  2  2  2  2  1  2  1  2  1  2  1  1  1\n[25081]  1  1  2  1  1  1  2  1  2  2  2  2  2  2  2  2  2  1  2  2  1  2  2  1\n[25105]  1  2  2  2  2  1  2  1  2  2  1  2  1  2  2  2  1  1  2  1  1  2  2  1\n[25129]  2  1  1  2  2  2  1  2  2  2  2  2  2  2  2  2  2  1  2  2  1  1  2  2\n[25153]  2  2  2 NA  1  1  2  2  1  1  2  2  1  1  2  2  1  2  2  2  1  2  1  2\n[25177]  1  1  1  1  1  2  2  1  1  2  2  2  1  1  1  1  2  1  1  2  2  1  2  2\n[25201]  2  1  2  2  2  2  2  2  2  1  2  2  2  1  2  1  2  2  1  1  2  2  2  2\n[25225]  2  2  1  2  1  1  2  2  2  2  1  2  2  2  2  1  2  2  1  2  2  2  2  1\n[25249]  2  2  2  1  1  2  2  2  2  2  2  1  2  1  1  1  2  2  1  1  1  1  2  2\n[25273]  2  1  1  2  2  2  2  2  2  1  1  1  1  1  1  1  1  2  1  2  2  1  1  2\n[25297]  1  2  1  2  2  1  2  1  1  2  1  2  2  2  2  2  1  1  1  1  2  1  1  1\n[25321]  1  2  1  2  1  2  2  2  2  2  2  2  1  2  1  1  2  2  2  1  1  3  2  2\n[25345]  2  2  2  2  1  1  2  1  1  2  1  1  2  2  1  2  1  2  2  2  1  1  1  2\n[25369]  2  1  2  2  2  2  2  2  2  1  1  1  1  2  1  1  2  2  2  2  2  1  2  2\n[25393]  1  2  2  2  2  1  2  1  2  2  2  2  2  2  2  1  2  1  1  2  2  2  2  1\n[25417]  2 NA  2  2  2  1  2  2  2  2  2  2  2  2  2  2  2  2  1  1  2  2  2  2\n[25441]  2  1  1  2  1  1  1  1  1  2  2  2  1  2  2  2  1  2  2  2  2  2  2  2\n[25465]  2  1  2  1  2  2  1  2  1  1  1  1  2  2  1  2  2  2  2  2  1  2  2  2\n[25489] NA  2  1  2  1  2  2  2  1  2  2  2  1  1  1  2  2  2  1  2  2  2  2  2\n[25513]  1  2  2  2  1  2  2  2  2  2  2  1  1  1  2  2  1  2 NA  2  1  1  2  1\n[25537]  1 NA  1  2  2  2  1  2  2  2  1  1  1  2  2  2  3  1  2  2  1  2  2  2\n[25561]  1  2  2  2  2  1  2  2  2  2  2  1  2  1  2  2  2  2  2  2  1  2  1  1\n[25585]  1  2  1  1  2  1  1  1  1  1  1  2  2  2  2  1  2  1  1  2  2  1  1  1\n[25609]  2  2  2  3  2  1  2  2  2  1  2  2  1  2  2  2  1  2  2  2  2  2  1  2\n[25633]  1  2  2  2  2  2  2  2  1  1  2  2  1  2  2  2  2  2 NA  1  1  2  2  2\n[25657]  1  2  1  2  2  2  2  1  1  2  2  2  2  1  1  2  1  1  3  1  2  1  2  1\n[25681]  2  1  2  1  1  2  2  2  2  1  1  1  1  1  1  1  1  2  2  2  1  1  1  1\n[25705]  1  2  2  1  1  1  1  2  1  2  2  2  2  2  2  2  1  1  1  1  1  1  2  2\n[25729]  2  1  2  1  1  2  1  2  2  1  2  2  2  2  2  2  2  2  2  2  2  2  1  1\n[25753]  2  1  2  2  2  1  1  2  2  1  1  1  2  2  3  1  2  2  2  2  2  2  1  1\n[25777]  2  1  2  1  1  2  1  2  2  2  1  1  1  1  1  2  2  1  2  2  1  2  1  2\n[25801]  2  2  2  2  2  2  2  1  2  2  2  2  2  2  1  1  2  2  2  2  2  1  1  2\n[25825]  1  2  1  2  1  2  2  1  2  2  1  2  1  1  1  2  2  2  1  1  1  1  1  2\n[25849]  1  2  1  1  1  2  2  2  2  2  2  2  2  2  2  2  1  1  2  1  1  2  2  1\n[25873]  1  1  2  1  2  2  2  2  2  2  2  2  2  2  2  1  2  1  2  1  1  2  2  2\n[25897]  2  2  1  1  1  1  2  1  2  2 NA  1  2  1  1  2  1  2  2  1  2  2  2  2\n[25921]  2  2  2  1  2  1  1  2  1  1  2  2  2  2  1  1  2  1  2  2  1  2  2  2\n[25945]  2  1  2  1  1  2  1  2  1  2  2  1  1  1  2  2  1  2  2  1  1  1  2  3\n[25969]  1  2  2  2  2  2  1  1  2  1  2  2  1  1  1  2  1  2  1  1  1  1  1  1\n[25993]  1  1  2  1  2  1  1  2  1  2  2  2  2  2  2  1  2  1  2  1  1  1  1  1\n[26017]  2  2  2  2  2  2  2  2  2  2  1  2  1  2  1  2  1  2  2  2  1  2  1  2\n[26041]  1  2  2  2  2  1  2  1  2  2  1  1  2  1  2  2  2  1  1  2  1  2  1  2\n[26065]  2  2  1  2  2  2  2  2  2  2  1  1  1  2  2  2  2  2  1  2  1  1  2  2\n[26089]  2  2 NA  1  1  1  2  1  1  2 NA  2  1  1  2  2  2  3  2  2  1  1  2  1\n[26113]  2 NA  2  1  1  2  2  1  1  2  1  2  2  2  1  1  2  1  1  1  2  2  1  2\n[26137]  1  2  2  1  2  2  2  2  1  2  2  2  2  1  2  2  2  2  2  2  2  1  2  1\n[26161]  2  1  1  2  2  2  1  1  2  2  1  1  2  2  2  1  2  1  2  2  1  2  2  2\n[26185]  2  2  2  2  1  1  2  2  1  1  1  2  2  1  1  2  2  2  1  2  1  2  1  2\n[26209]  1  2  2  1  1  1  1  2  2  1  2  2  1  2  2  2  2  1  2  2  1  2  2  2\n[26233]  2  2  2  2  2  1  1  1  2  2  1  2  1  1  2  2  1  2  1  2  1  2  2  2\n[26257]  1  1  1  2  1  2  2  2  1  2  2  2  1  1  1  1  1  2  1  1  2  2  1  2\n[26281]  1  2  2  2  2  2  2  1 NA  2  1  2  1  2  2  2  1  2  2  2  1  2  2  2\n[26305]  2  2  2  2  2  2  1  1  2  2  2  1  2  2 NA  2  1  2  1  2  2  2  1  2\n[26329]  2  1  2  2  1  2  1  1  2  2  2  2  1  1  2  1  2  1  1  2  1  2  1  2\n[26353]  1  2  2  1  2  2  1  2  2  1  1  2  2  1  1  2  2  1  1  2  2  1  2  1\n[26377]  1  1  2  2  2  2  1  1  1  2  1  2  2  1  2  2  2  2  1  2  2  2  2  2\n[26401]  2  1  1  2  2  2  2  2  2  2  1  2  2  1  2  1  1  1  2  2  2  1  2  1\n[26425]  2  2  1  1  2  2  2  1  2  2  1  2  1  2  2  2  1  1  1  2  1  2  2  2\n[26449]  1  2  1  2  1  2  2  1  1  1  1  1  1  2  1  2  2  2  2  2  1  2  1  1\n[26473]  2  2  2  2  2  2  1  1  2  2  2  2  2  2  1  2  2 NA  1  1  2  2  2  2\n[26497]  2  1  2  2  1  2  2  1  2  1  2  2  1  2  2  2  2  1  1  2  2  1  2  2\n[26521]  1  1  1  2  2  1  2  1  2  1  1  2  2  1  1  2  1  2  2  2  1  2  2  2\n[26545]  2  1  2  1  2  1  2  2  2  1  1  2  2  2  2  2  1  1  1  2  1  2  2  1\n[26569]  2  2  1  2  2  1  2  2  2  2  2  2  1  2  2  2  1  1  1  2  2  2  2  1\n[26593]  2  1  2  1  1  1  2  1  2  3  2  1  2  1  2  1  2  1  2  1  2  2  1  2\n[26617]  2  2 NA  2  2  2  1  2  1  2  2  1  2  2  2  2  1  2  1  2  2  1  1  1\n[26641]  2  2  2  2  1  2  2  2  2  2  2  1  2  2  2  2  1  1  2  2  2  1  2  2\n[26665]  2  2  2  2  1  2  2  2  2  1  2 NA  1  1  2  2  2  1  2  2  2  2 NA  2\n[26689]  2  1  2  2  2  1  2  2  1  1  2  2  1  1  2  2  2  2  1  2  1  2  2  1\n[26713]  1  2  1  2  2  2  2  1  2  2  1  2  2  2  2  1  2  1  2  2  2  2  2  2\n[26737]  2  2  2  2  2  2  2  1  2  1  2  1  1  2  2  1  2  1  2  1  2  1  1  1\n[26761]  1  1  2  2  1  1  2  1 NA  1  1  1  2  1  2  1  1  2  2  2  2  2  2  2\n[26785]  2 NA  1  1  2  2  2  1  1  2  2  1  1  2  2  1  2  2  1  1  2  2  2  2\n[26809]  2  2  2  2  2  2  2  1  2  1  2  2  2  1  2  1  1  2  2  1  1  1  2  2\n[26833]  2  2 NA  2  1  1  1  2  2  2  1  2  2  1  1  2  2  2  2  1  2  2  1  1\n[26857]  1  2  1  1  2  2  1  2  1  2  2  1  2  2  1  2  1  2  2  2  1  2  2  1\n[26881]  2  2  2  1  1  2  1  2  2  2  2  1  2  1  1  2  2  2  2  2  2  1  2  1\n[26905]  2  2  1  2  2  1  1  2 NA  1  2  1  1  2  2  2  2  2  2  2  2  2  1  2\n[26929]  2  1  2  2  1  2  1  2  2  2  2  1  1  2  2  2  2  2  1  2  1  2  1  1\n[26953]  1  1  1  2  2  1  1  1  2  2  1  1  1  1  1  2  1  1  1  2  1  2  2  2\n[26977] NA  2  2  2  2  1  2  2  2  2  2  1  1  1  2  1  1  1  1  1  2  2  2  2\n[27001]  1  2  2  2  1  1  2  2  2  2  2  2  1  2  2  2  2  1  1  2  2  1  2  2\n[27025]  1  1  1  2  2 NA  2  1  2  1  1  2  1  2  1  2  2  2  1  2  2  1  2  1\n[27049]  1  1  1  2  1  1  1  2  2  1  1  1  2  1  2  1  2  1  1  2  2  2  2  2\n[27073]  2  1  1  2  1  1  2  2  2  2  1  2  2  2  2  1  2  2  1  1  1  2  1  2\n[27097]  2  2  2  2  1  2  1  1  1  2  1  2  2  2  1  1  2  2  1  2  2  3  1  1\n[27121]  1  2  1  2  2  2  1  1  1  2  1  2  1  2  1  1  2  1  1  2  1  1  1  2\n[27145]  3  2  2  2  1  2  1  1  1  1  1  2  1  1  1  1  1  1  2  1  2  1  2  1\n[27169]  1  2  1  1  2  1  2  1  2  2  2  1  1  2  2  2  2  2  1  2  1  1  1  2\n[27193]  1  2  1  2  1  2  1  2  2  1  2  2  2  2  2  2  1  2  2  1  2  2  1  2\n[27217]  2  2  2  1  1  2  2  2  2  2  1  1  1  2  1  1  2  1  1  1  2  2  2  2\n[27241]  1  2  2  1  2  2  1  1  2  1  1  1  1  1  2  1  2  2  1  2  2  1  1  1\n[27265]  1  2  1  2  2  2  2  1  1  2  2  2  1  2  2  3  2  2  2  2  2  2  1  2\n[27289]  1  2  1  2  2  1  2  1  2  1  2  1  2  1  1  2  2  2  2  2  1  1  1  2\n[27313]  2  1  2  2  1  1  2  1  1  2  2  2  2  2  2  1  1  1  1  2  1  2  1  1\n[27337]  1  2  1  1  2  2  2  1  1  1  1  2  2  2  2  2  2  2  2  1  2  2  2  1\n[27361]  1  1  1  2  1  2  2  2  1  1  2  2  2  2  1  1  2  2  1  1  2  2  2  2\n[27385]  1  1  2  2  2  1  1  1  2  2  2  1  2  2  2  2  2  2  2  2  1  2 NA  2\n[27409]  1  1  1  1  2  1  1  2  1  2  1  2  2  1  2  2  1  1  1 NA  1  1  1  2\n[27433]  2  1  2  1  1  2  2  2  2  2  2  1  2  2  1  2  2  1  1  1  1  1 NA  2\n[27457]  2  1  2  2  2  1  2  2  1  2  2  1  2  2  1  2  1  1  2  1  2  2  2  1\n[27481]  2  1 NA  2  2  2  2  2  2  1  2  2  1  1  1  2  2  2  2  1  2  1  2  3\n[27505]  1  2  2  2  1  2  1  2  2  1  2  1  2  1  2  1  2  1  2  2  2  2  2  2\n[27529]  1  2  2  2  2  2  2  2  2  1  2  2  1  1  1  2  2  1  1  2  1  2  1  2\n[27553]  1  2 NA  2  1  2  1  1  2  1  2  1  1  1  2  1  1  1  2  1  1  1  2  1\n[27577]  2  1  1  2  1  2  2  1  2  1  2  2  2  1  2  2  2  2  2  1  1  2  2  1\n[27601]  2  1  2  2  1  2  2  3  2  2  1  1  2  1  2  1  2  1  1  1  1  1  2  2\n[27625]  2  2  2  2  2  2  2  2  1  2  1  2  1  1  3  1  2  2  2  2  2  1  2 NA\n[27649]  1  1  2  1  2 NA  2  1  1  2  2  2  2  1  2  2  1  2  2  2  2  2  2  1\n[27673]  1  2  2  1  2  1  2  2  2  1  2  1  2  2  2  2  1  2  2  2  2  2  2  1\n[27697]  1  1  1  2  2  1  2  1  1  2  2  2  2  2  1  1  1  1  2  1  2  2  2  1\n[27721]  2  1  2  1  1  1  1  1  2  2  1  2  2  2  1  1  2  2  1  3  2  2  2  2\n[27745]  2  2  2  2  2  1  2  2  1  1  2  1  2  2  1  1  1  2  1  2  1  1  1  1\n[27769]  1  2  2  2  1  3  2  1  1  2  1  2  2  2  1  1  1  2  1  2  2  2  2  2\n[27793]  2  2  2  1  2  2  2  2  2  1  1  1  1  1  1  2  2  2  1  2  1  1  2  1\n[27817]  1  2  2  2  1  2  2  1  1  1  2  2  2  1  2  1  2  2  1  2  1  2  2  2\n[27841]  2  2  2  2  2  1  2  2  2  1  2  1  2  1  1  2  2  2  1  2  1  1  2  1\n[27865]  1  2  2  1  2  1  2  2  2  2  2  1  2  2  1  2  2  1  2  2  2  2  2  1\n[27889]  2  2  1  1  2  1  1  1  2  1  2  2  1  1  2  1  2  2  2  1  2  2  1  2\n[27913]  1  1  1  1  2  2  2  2  2  1  2  2  1  1  2  2  2  2  1  2  1  2  2  1\n[27937]  2  2  2  2  2  2  2  2  2  1  1  2  2  1  2  1  2  2  2  1  2  2  2  2\n[27961]  2  1  1  2  2  2  2  2  2  2  1  2  1  2  1  1  1  1  1  2  2  1  2  2\n[27985]  2  1  3  2  1  2  2  1 NA  2  2  2  2  2  1  2  1  2  2  1  1  2  2  1\n[28009]  2  2  2  2  1  1  1  2  2  2  1  2  2  2  2  2  1  2  1  1  2  2  2  2\n[28033]  2  2  2  1  2  2  2  2  2  2  2  1  2  1  1  2  2  2  1  2  3  2  1  2\n[28057]  2  1  2  1  2  2  2  2  1  1  2  2  2  2  1  2 NA  2  2  1  2  1  1  1\n[28081]  2  2  1  2  1  2  2  1  1  2  1  1  1  2  1  2  1  2  1  1  2  2  1  1\n[28105]  2  2  2  1  1  2  1  2  2  1  2  2  1  1  2  2  1  2  2  2  2  1  2  2\n[28129]  1  2  1  2  1  1  1  1  2  1  1  1  2  1  1  2  1  2  2  1  1  2  1  1\n[28153]  1  2  2  2  2  1  2  2  2  2  2  1  2  1  1  1  1  3  1  2  1  2  2  2\n[28177]  1  1  1  2  1  1  2  1  1  2  2  1  2  1  2  2  1  1  1  1  1  1  2  2\n[28201]  2  1  2  2  2  2  2  2  1  1  2  2  2  1  2  1  2  2  2  1  1  1  2  2\n[28225] NA  2  1  2  2  2  2  2  2  2  2  1  1  2  2  1  2  1  2  2  1  2  2  2\n[28249]  1  1  1  1  1  2  2  1  2  1  2  1  1  2  1  1  1  2  1  1  2  2  1  2\n[28273]  1  2  1  2  2  1  2  2  2  1  2  2  1  2  2  1  1  3  2  1  2  1  1  2\n[28297]  1  2  2  1  2 NA  2  1  2  1  2  2  1  2  2  2  2  2  2  1  2  2  3  2\n[28321]  2  2  1  1  1  1  2  2  2  2  2  2 NA  2  2  1  1  1  2  1  1  2  1  1\n[28345]  1  1  1  1  2 NA  1  1  2  2  2  2  2  2  1  2  2  2  2  1  1  1  2  2\n[28369]  1  2  2  2  2  2  1  2  1  2  2  1  2  2  2  2  2  1  2  1  1  2  1  2\n[28393]  2  2  2  2  1  2  2  2  2  1  1  2  2  2  2  1  2  2  1  2  1  2  2  2\n[28417]  1  1  2  2  1  2  2  2  2  2  2 NA  2  2  2  2  2  2  2  2  2  1  1  2\n[28441]  2  1  1  2  2  1  1  2  2  2  1  1  2  2  2  1  2  2  2  1  2  2  2  2\n[28465]  2  2  2  2  1  2  1  2  2  2  2  2  2 NA  1  2  1  2  2  1  1  2  1  2\n[28489]  2  1  2  2  1  2  2  2  2  1  1  2  2  2  2  2  2  3  2 NA  2  2  2  1\n[28513]  1  2  2  2  1  2  2  2  1 NA  2  2  1  1  1  2  1  2  2  1  1  1  1  2\n[28537]  1  1  2  2  2  1  2  2  2  1  2  1  3  1  2  2  1  2  1  1  2  2  2  2\n[28561]  1  1  1  2  1  1  2  1  2  2  1  1  2  1  2  2  2  1  1  2  1  2  1  1\n[28585]  2  1  2  1  1  1  2  2  2  1  1  2  1  2  1  2  2  1  1  1  1  2 NA  1\n[28609]  1  2  1  1  2  1  1  2  1  1  1  2  2  1  1  1  2  2  2  2  2  1  2  2\n[28633]  2  2  2  1  1  2  2  2  2  1  2  1  2  3  2  2  2  2  2  2  1  2  2  2\n[28657]  1  2  2  2  1  1  1  1  2  1  2  1  1  2  2  1  2  2  1  2  2  1  2  1\n[28681]  2  2  1  1  2  2  2  1  1  2  2  2  1  2  2  2  1  2  2  1  2  1  1  2\n[28705]  2  2  2  2  2  1  2  2  2  2  1  1  1  2  2  1  2  2  1  2  1  2  2  2\n[28729]  2  2  2  2  2  2  1  1  2  2  2  2  1  2  1  2  2  2  2  2  1  1  2  2\n[28753]  1  1  2  2  1  1  1  2  1  1  1  1  2  1  2  2  1  1  2  1  2  2  2  2\n[28777]  1  1  2  2  2  2  2  2  2  2  2  1  2  2  1  2  2  2  1  2  1  2  3  1\n[28801]  2  1  2  2  1  2  2  2  1  2  1  2  2  1  1  2  1  2  2  2  2  2  1  2\n[28825]  2  1  2  2  2  2  2  2  2  1  2  1  2  2  2  2  2  1  2  2  2  2  1  2\n[28849]  1  2  2  2  2  2  1  1  2  2  2  2  1  2  2  2  1  1  2  2  1  2  2 NA\n[28873]  1  2  2  2  2  1  2  2  2  2  1  2  1  2  2  2  2  2  2  1  2  1  1  1\n[28897]  1  2  1  2  2  1  3  2  1  2  2  1  1  1  1  2  2  2  1  2  2  1  2  1\n[28921]  1  2  2  2  2  2  2 NA  2  2  1  2  2  2  1  2  2 NA  2  1  1  2  1  2\n[28945]  1  2  2  1  1  1  2  1  2  1  2  2  1  2  1  2  1  2  1  2  2  2  2  2\n[28969]  2  2  1  2  2  2  2  2  1  2  2  2  2  2  2  1  2  1  2  2  1  2  2  2\n[28993]  2  2  2  1  1  1  2  2  1  2  2  1  2  1  2  2  2  1  1  2  2  1  2  2\n[29017]  1  1  1  1  1  2  2  2  2  1  1  2  2  2  1  2  1 NA  1  2  2  2  1  2\n[29041]  2  1  1  2  1  1  2  1  2  1  2  2  2  2  2  2  2  2  2  2  1  1  2  2\n[29065]  2  2  2  1  2  2  2  2  2  1  2  2  1  2  2  1  1  2  2  2  1  2  1  1\n[29089]  2  2  2  2  1  2  2  2  2  2  2  1  1  2  2  2  2  2  1  1  2  2  2  2\n[29113]  2  1  1  1  2 NA  1  2  1 NA  1  2  2  2  1  2  2  2  1  1  1  2  2  2\n[29137]  2  2  2  1  1  2  2  1  2  2  2  2  2  2  2  1  2  2  2  2  1  1  1  1\n[29161]  2  2  2  2  2  1  2  1  2  2  2  1  1  1  2  1  2  2  2  1  1  2  1  2\n[29185]  2  1  1  2  2  2  2  2  1  1  1  2  1  2  2  2  2  2  1  2  1  2  2  2\n[29209]  2  1  2  2 NA  2  1  1  2  2  2  2  2  2  1  2  1  2  2  2  2  2  2  2\n[29233]  2  2  1  2  1  2  1  1  2  2  2  1  1  1  2  2  1  2  2  2  1  1  2  2\n[29257]  2  1  2  2  2  2  2  2  1  2  2  1  2  2  2  2  2  1  1  2  1  2  2  2\n[29281]  2  1  2  1  1  1  2  1  2  2  2  1  2  2  2  2  2  1  1  1  1  2  1  3\n[29305]  2  2  2  1  2  2  2  1  2  2  1  2  1  1  1  2  1  2  2  2  1  2  1  1\n[29329]  2  1  2  2  2  1  2  1  2  1  1  2  2  2  2  1  3  2  1  1  2  2  2  2\n[29353]  2  1  1  2  2  1  1  2  1  2  2  1  2  2  2  2  2  2  2  3  1  1  2  2\n[29377]  2  2  2  2  2  2  2  2  2  2  1  2  2  1  2  2  2  2  2  2  2  2  2  2\n[29401]  2  2  2  2  2  2  2  2  2  2  2  2  3  2  2  2  1  2  2  2  2  2  2  2\n[29425]  2  2  2  2  2  2  2  2  2  1  1  2  2  2  2  1  2  1  2  1  2  2  2  1\n[29449]  2  2  2  2  2  2  2  2  1  1  1  2  1  2  2  1  1  1  2  2  3  3  2  2\n[29473]  2  2  2  2  2  2  1  2  2  1  1  2  3  2  2  2  2  2  2  2  2  2  2  2\n[29497]  2  2  2  2  2  2  3  2  2  2  2  2  2  2  2  2  2  2  2  2  2  1  1  2\n[29521]  2  2  2  3  1  2  2  2  1  2  2  2  2  2  2  1  2  2  2  2  2  2  2  1\n[29545]  2  2  2  2  2  2  2  2  2  2  2  1  2  1  2  2  2  1  2  2  2  2  2  2\n[29569]  1  2  2  2  1  2  2  2  2  2  2  2  1  2  2  1  1  2  3  2  2  2  1  1\n[29593]  2  2  2  2  1  3  2  2  2  2  2  2  2  2  2  2  1  2  2  3  2  1  2  2\n[29617]  1  2  1  1  2  2  2  1  2  2  2  2  3  2  1  2  2  1  2  2  2  2  2  2\n[29641]  2  2  2  2  1  2  2  1  2  2  1  1  2  2  1  2  1  2  1  1  1  1  1  1\n[29665]  1  2  1  1  1  2  1  2  2  1  2  1  2  1  2  2  2  2  1  2  1  2  2  1\n[29689]  1  1  2  2  1  2  1  2  2  2  2  2  2  1  2  2  2  1  1  2  2  2  2  2\n[29713]  2  2  1  2  2  2  1  2  2  2  2  2  1  1  1  2  2  2  2  2  1  2  2  1\n[29737]  1  2  2  2  2  3  1  1  1  2 NA  1  2  2  2  2  3  1  2  2  2  1  1  2\n[29761]  1  1  2  1  2  1  2  2  1  2  2  2  2  1  1  2  2  1  1  2  2  1  1  2\n[29785]  1  2  2  1  2  2  1  2  2  2  1  1  2  2  1  3  2  1  1  1  2  2  2  1\n[29809]  2  2  1  1  1  2  2  1  1  2  1  2  1  1  2  2  2  1  1  2  2  2  1  2\n[29833]  2  2  2  2  2  2  2  2  2  2  1  1  1  2  2  2  1  1  1  2  2  2  2  3\n[29857]  1  1  2  1  2  2  2  2  2  1  2  2  2  1  1  2  1  2  2  2  1  2  2  2\n[29881]  2  2  1  2  2  2  2  2  1  2  2  2  2  1  1  2  2  2  1  2  1  2  1  1\n[29905]  1  2  1  1  2  2  2  1  2  2  2  2  2  1  2  2  2  2  1  2  2  2  2  2\n[29929]  2  2  2  1  2  2  2  1  2  2  2  1  2  2  1  1  2  2  1  1  1  2  2  2\n[29953]  2  2  1  2  1  1  2  2  2  2  2  2  2  2  2  1  2  2  1  1  2  2  2  2\n[29977]  1  2  2  1  2  2  2  2  1  2  2  1  2  1  2  2  2  2  1  2  2  2  2  2\n[30001]  2  2  1  1  1  3  1  2  1  1  2  1  2  2  2  2  1  1  2  2  1  2  2  2\n[30025]  2  2  2  2  2  1  1  1  2  1  1  2  2  1  1  1  1  2  2  1  2  2  2  2\n[30049]  2  1  2  2  1  1  2  2  2  1  2  2  2  2  2  2  1  1  2  2  2  2  1  2\n[30073]  2  2  2  1  2  2  2  1  1  1  2  2  2  2  2  2 NA  2  2  2  1  2  2  1\n[30097]  2  1  1  1  2  2  1 NA  1  2  2  1  2  1  2  2  2  2  2  2  1  2  1  3\n[30121]  2  2  2  2  1  1  1  2  2  2  2  2  2  1  1  1  2  1  2  2  2  2  2  2\n[30145]  1  2  1  2  1  2  2  1  1  2  2  2  2  1  2  2  2  2  2  2  2  2  2  2\n[30169]  1  2  1  2  2  2  2  1  2  1  2  2  2  2  1  2  2  2  2  1  2  1  1  1\n[30193]  1  2  2  1  2  1  1  1  1  2  2  2  1  2  1  2  1  2  1  1  1  1  2  2\n[30217]  1  2  1  2  2  2  1  2  1  2  1  2  2  1  2  2  2  1  2  2  2  2  1  1\n[30241]  2  2  1  1  2  1  1  2  2  2  2  2  2  2  2  2  2  2  2  2  1  1  2  2\n[30265]  2  1  2  1  2  2  1  2  2  2  2  1  1  2  1  2  2  1  1  2  2  2  1  1\n[30289]  1  1  2  2  2  2  1  1  2  2  2  1  2  2  1  1  2  1  1  1  2  2  2  2\n[30313]  2  2  1  1  3  1  1  2  2  1  1  1  2  1  2  1  2  2  2  1  2  1  2  2\n[30337]  1  1  1  1  1  1  2  2  2  2  2  2  2  2  2  2  2  1  2  2  2  1  2  1\n[30361]  2  2  2  2  2  2  2  1  2  2  2  1  1  1  2  2  2  1  1  1  2  1  2  2\n[30385]  3  2  2  2  1  1  2  2  2  1  2  2  2  2  1  1  2  2  1  2  1  1  1  1\n[30409]  2  2  2  1  1  2  1  2  2  1  1  2  1  2  2  2  2  2  2  2  2  2  2  2\n[30433]  1  1  1  1  1  2  1  1  2  1  1  2  2  1  1  1 NA  1  2  2  2  1  2  2\n[30457]  1  2  2  2  2  1  2  2  2  1  2  1  1  2  2  2  2  1  1  2  2  1  1  2\n[30481]  1  2  2  1  2  2  1  2  1  2  2  1  2  2  1  2  2  2  2  2  2  1  2  2\n[30505]  2  2  1  3  3  1  2  2  2  2  2  2  2  2  2  1  2  2  2  1  2  2  1  2\n[30529]  2  2  1  2  2  1  2  2  1  2  3  1  1  1  2  2  2  2  2  2  1  2  2  2\n[30553]  2  1  1  2  2  1  1  2  1  2  1  2  2  2  1  2  2  2  1  2  1  1  2  2\n[30577]  2  2  2  2  2  1  2  2  2  2  1  1  1  1  2  1  2  1 NA  2  2  1  2  2\n[30601]  2  2  2  1  2  2  2  1  2  2  2  1  2  2  1  1  2  1  2  2  1  2  2  2\n[30625]  2  2  2  2  2  1  1  1  2  1  2  2  2  2  2  2  1  2  1  2  2  2  1  2\n[30649]  2  2  1  2  1  1  1  3  2  2  1  1  2  2  1  2  2  2  2  1  2  2  1  1\n[30673]  2  2  2  1  2  1  1  2  2  2  2  2  2  2  1  2  3  1  1  1  1  2  2  2\n[30697]  2  2  2  2  1  1  2  1  2  2  2  2  2  1  2  2  1  2  1  2  2  2  1  1\n[30721]  2  2  2  2  2  2  2  2  2  1  2  2  2  2  2  2  2  1  1  2  1  2  1  1\n[30745]  2  1  2  1  2  2  2  1  2  1  1  2  2  2  1  2  1  2  2  2  1  2  2  2\n[30769]  2  1  2  2  2  2  1  2  1  1  1  2  2  2  2  1  2  2  1  2  2  1  2  2\n[30793]  2  2  1  2  2  2  1  2  2  2  2  1  2  2  1  2  2  2  2  2  2  2  2  1\n[30817]  2  1  1  1  1  2  1  2  1  2  2  1  1  2  1  2  1  2  2  2  1  2  1  2\n[30841]  1  2  2  1  2  1  2  1  2  1  2  2  2  2  2  2  1  1  2  2  2  1  1  2\n[30865]  2  1  1  2  1  2  2  1  2  2  2  1  2  2  1  1  2  2  2  2  1  1  1  2\n[30889]  2  2  2  2  1  2  2  2  1  2  2  2  1  1  2  1  1  1  1  1  2  2  1  1\n[30913]  2  1  1  2  1  2  2  2  2  2  1  1  2  2  2  2  1  1  2  1  2  1  2  1\n[30937]  2  2  2  1  2  2  2  2  2  2  2  2  2  1  2  2  1  2  1  1  2  2  2  2\n[30961]  2  2  1  1  2  1  1  1  2  2  2  1  2 NA  1  2  2  2  1  2  2 NA  2  1\n[30985]  2  1  2  2  1  1  2  2  2  1  2  1  2  1  2  2  2  2  2  1  1  1  1  2\n[31009]  1  1  2  2  2  2  1  2  1  1  1  2  2  1  2  2  1  2  2  1  1  2  2  1\n[31033]  2  2 NA  2  2  2  2  1  2  1  2  2  1  2  2  1  2  2  1  1  1  2  1  1\n[31057]  2  1  2  1  1  2  3  2  1  2  2  1  2  1  1  2  2  2  1  2  2  2  1  2\n[31081]  2  2  2  2  1  2  2  2  2  2  1  2  2  2  2  2  1  2  1  2  2  2  1  2\n[31105]  1  2  1  1  2  1  2  2  1  2  2  1  1  2  2  1  2  2  2  2  1  2  1  2\n[31129]  2  2  1  1  2  2  2  1  2  2  2  1  1  2  2  2  2  1  1  2  2  2  2  1\n[31153]  1  2  2  1  2  2  2  2  2  2  1  1  2  1  2  2  2  1  1  2  2  1  1  2\n[31177]  2  1  2  2  1  1  2  2  1  2  1  2  2  1  1  1  1  1  2  2  1  1  1  2\n[31201]  2  2  2  2  2  2  2  2  2  2  2  2  1  2  2  1  1  2  1  2  2  2  2  1\n[31225]  2  2  2  2  2  1  2  2  3  2  2  2  2  1  2  1  2  1  2  2  2  2  2  2\n[31249]  1  1  1  2  2  2  2  1  1  1  2 NA  1  2  2  2  1  2  2  2  2  1  1  2\n[31273]  1  1  2  1  2  2  2  2  2  1  2  2  2  1  1  2  2  2  1  2  1  2  1  2\n[31297]  2  2  2  1  2  2  1  2  1  1  2  2  2  2  3  2  2  1  2  2  1  2  2  2\n[31321]  1  2  1  1  2  1  2  1  2  1  2  2  1  2  1  1  2  2  1  2 NA NA  2  2\n[31345]  1  1  1  1  2  2  1  2  1  1  2  1  2  2  2  3  1  2  2  1  1  2  2  3\n[31369]  1  2  2  1  1  1  2  1  1  2  1  2  2  2  1  2  1  2  2  2  1  2  2  1\n[31393]  2  2  2  2  1  1  2  1  2  2  2  2  2  2  2  2  2  1  2  2  2  2  3  2\n[31417]  2  2  2  2  2  2  1  2  2  2  2  2  2  1  1  1  2  2  1  2  2  2  2  1\n[31441]  2  1  2  2  2  2  2  1  1  1  2  3  1  1  1  2  1  2  2  2  2  1  2  2\n[31465]  2  2  2  2  1  2  2  1  2  2  1  2  2  1  2  2  2  1  2  2  1  2  1  1\n[31489]  2  2  2  2  2  2  1  2  1  2  2  1  1  2  1  1  2  1  1  2  2  2  1  2\n[31513]  2  2  1  2  1  2  2  2  2  2  2  1  2  2  2  2  1  2  2  1  1  2  2  2\n[31537]  2  1  2  1  2  2  2  1  1  1  1  2  2  2  1  2  2  2  2  1  2  2  2  2\n[31561]  2  2  1  2  2  1  1  1  2  1 NA  2  2  2  1  2  2  2  2  2  2  2  2  2\n[31585]  1  1  2  1  2  2  2 NA  1  2  2  2  1  2  1  2 NA  2  1  2  2 NA  2  2\n[31609]  2  2  2  1  2  2  1  1  1  2  2  1  2  2  1  1  1  2  2  2  2  1  1  2\n[31633]  1 NA  2  1  1  2  2  2  2  1  2  2  2  1  1  2  2  1  2  2  2  2  1  1\n[31657]  2  2  2  1  2  2  1  2  2  2  1  2  2  2  2  2  2  2  1  2  1  2  2  2\n[31681]  1  1  2  2  2  1  1  2  1  1  1  1  2  1  1  1  2  2  2  2  1  1  2  2\n[31705]  2  2  1  2  2  2  2  2  1  1  1  2  2  2  2  2  2  2  1  1  2  2  1  2\n[31729]  2  1  2  1  1  1  2  2  2  1  1  2  2  1  2  2  2  2  2  2  2  2  1  2\n[31753]  2  2  2  1  2  1  2  2  2  2  1  2  1  1  1  2  1  1  1  2  2  2  2  2\n[31777]  1  2  2  1  2  2  2  2  1  1  2  2  2  2  2  1  2  2  1  1  1  2  2  2\n[31801]  1  1  2  2  1  2  2  1  1  2  1  1  1  2  2  1  2  2  2  2  1  2  2  2\n[31825]  2  2  1  2  2  1  1  3  2  2  2  2  2  2  2  1  2  1  2  2  2  2  2  2\n[31849]  1  2  1  2  2  1  2  2  1  1  2  1  2  2  2  1  2  1  1  2  2  2  2  2\n[31873]  1  2  1  2  2  1  2  2  2  1  1  1  2  2  2  1  1  2  1  2  2  2  1  2\n[31897]  2  2  1  1  2  1  2  2  2  1  2  2  2  1  2  1  2  2  2  2  2  2  2  1\n[31921]  2  2  2  2  1  2  2  1  1  2  2  1  2  1  1  1  1  1  2  2  1  2  2  1\n[31945] NA  2  2  2  2  2  2  2  1  2  2  1  2  1  2  2  1  1  1  2  1  1  2  2\n[31969]  2  2  2  2  2  1  2  1  2  1  1  2  1  2  2  1  1  2  1  2  2  2  2  2\n[31993]  2  2  1  2  1  1  3  2  2  1  1  2  2  2  2  1  1  2  2  2  1  2  2  1\n[32017]  2  2  2  2  2  2  1  2  1  2  2  2  1  2  2  2  1  2  1  2  1  1  1  2\n[32041]  1  2  2  2  2  2  2  1  2  2  2  3  2  2  1  1  2  2  1  2  1  1  2  1\n[32065]  1  2  1  1  2  2  1  2  2  2  2  1  2  2  2  1  1  2  2  1  1  1  1  2\n[32089]  1  2  2  2  2  2  1  2  2  2  2  2  1  1  1  2  2  1  2  1  1  2  2  2\n[32113]  2  1  1  2  1  2  2  2  2  2  2  2  1  1  1  1  2  2  1  2  1  2  2  2\n[32137]  2  1  2  2  2  2  1  2  2  2  1  1  1  2  2  2  1  2  1  2  2  2  2  2\n[32161]  1  1  2  2  1  2  2  2  3  1  1  2  2  2  2  2  2  2 NA  1  1  2  2  1\n[32185]  1  1  1  2  1  1  2  2  1  2  2  2  2  2  3  2  2  1  2  2  2  1  1  1\n[32209]  2  1  2  1  1  2  1  1  2  1  2  1  2  2  2  1  1  1  2  2  1  2  2  2\n[32233]  1  2  2  2  1  1  1  2  2  1  2  2  1  2  1  1  2  2  2  2  1  1  2  2\n[32257]  1  2  2  2  2  2  1  1  1  2  2  2  1  1  2  2  1  1  2  1  1  1  2  2\n[32281]  2  2  1  1  2  2  2  2  2  1  2  2  2  2  2  2  2  1  1  1  1  2  1  1\n[32305]  2  1  2  1  1  1  2  2  2  1  1  1  1  1  2  1  1  1  2  1  2  1  1  2\n[32329]  2  1  2  2  1  1  2  2  2  1  2  2  2  2  2  1  2  1  1  2  2  1  2  2\n[32353]  2  2  1  2  1  2  2  2  3  2  2  2  1  1  2  2  1  1  2  2  2  2  1  2\n[32377]  2  2  2  2  2  2  1  2  2  2  2  2  2  2  1  2  1  2  2  1  2  2  2  1\n[32401]  1  1  2  2  2  2  2  1  1  1  2  2  2  2  2  2  2  2  3  2  1  1  2  2\n[32425]  2  2  2  1  1  2  2 NA  2  1  2  2  2  1  2  2  1  2  2  1  2  2  2  2\n[32449]  2  2  2  2  1  1  2  1  1  2  2  2  2  2  1  2  1  1  1  1  1  2  1  2\n[32473]  1  2  2  2  2  2  1  2  2  1  2  2  2  3  2  2  2  2  2  1  2  2  2  2\n[32497]  1  2  2  2  2  2  2  1  2  1  2  2  2  2  2  2  1  1  2  1  1  2  2  2\n[32521]  2  1  2  1  2  2  2  2  2  1  1  2  1  2  2  1  1  2  2  2  2  2  1  2\n[32545]  2  1  1  2  2  1  1  2  2  1  1  2  2  2  1  2  1  2  2  2  1  2  2  1\n[32569]  1  2  2  1  1  2  2  2  2  1  1  2  1  2  2  1  2  2  1  2  2  2  2  2\n[32593]  2  2  2  2  1  1  1  1  2  2  2  2  2  2  2  1  1  1  2  2  2  2  1  2\n[32617]  2  2  2  1  2  2  2  2  2  2  2  2  2  2  2  1  2  1  1  2  2  2  2  1\n[32641]  2  2  1  2  1  2  2  2  2  2  2  1  2  2  2  2  2  2 NA  2  1  2  2  2\n[32665]  2  2  2  2  2  2  2  3  1  2  2  2  2  2  2  2  2  1  2  2  2  2  2  2\n[32689]  2  2  2  1  2  2  2  2  2  2  2  2  2  2  2  2  1  2  2  2  2 NA  1  2\n[32713]  2  2  2  1  2  2  2  2  2  2  2  2  2  2  1  2  2  2  2  1  2  2  2  2\n[32737]  2  2  2  2  2  2  1  2  1  2  2  2  1  1  2  2  2  2  2  2  1  2  2  2\n[32761]  2  2  2  2  2  2  1  1  2  2  2 NA  2  2  1  2  2  1  2  2  2  2  2  2\n[32785]  2  2  3  2  3  1  1  1  2  1  2  2  2  2  2  2  1  2  2  1  3  2  2  2\n[32809]  2  2  2  1 NA  3  1  2  2  2  2  2  2  1  2  2  2  2  2  2  2  2  2  2\n[32833]  2  1  1  2  2 NA  2  1  2  2  1  2  2  2  2  2  2  2  2  2  1  2  2  1\n[32857]  2  1  1  2  1  2  2  1  2  2  2  1  2  2  2  1  2  2  2  2  2  2  2  2\n[32881]  2  2  2  1  2  2  2  2  2  2  2  1  2  2  1  2  2  1  1  2  2  2  2  1\n[32905]  2  1  2  2  1  2  1  1  1  1  2  2  2  2  1  2  2  1  1  1  1  1  1  1\n[32929]  1  1  1  2  2  2  2  1  2  1  2  2  2  1 NA  2  2  2  1  2  2  1  2  2\n[32953]  1  1  2  2  2  2  1  2  2  2  1  2  2  1  2  2  2  2  1  1  1  2  2  2\n[32977]  2  1  2  2  2  1  2  2  1  2  2  2  2  2  2  2  2  2  2  2  2  2  2  1\n[33001]  1  2  2  2  2  2  2  2  2  2  1  2  2  2  2  1  2  2  2  1  2  2  2  2\n[33025]  2  2  2  2  2  1  2  2  1  2  1  1  2  1  2  2  2  2  2  1  2 NA  1  2\n[33049]  2  2  2  1  2  2  2  2  2  1  2  2  2  2  2  2  1  3  2  1  1  1  1  1\n[33073]  1  2  2  2  1  1  2  1  2  2  2  1  1  2 NA  1  2  2  1  2  2  2  2  2\n[33097]  2  1  1  2  2  2  2  2  2  2  2  2  2  2  1  2  2  2  2  2  2  2  2  2\n[33121]  2  2  1  1  1  2  1  2  2  2  1  2  1  2  1  1  2  2  2  2  2  2  2  2\n[33145]  2  2  1  1  1  1  2  2  2  2  2  2  2  2  2  2  2  1  1 NA  2  1  1  2\n[33169]  1  2  1  2  2  1  2  2  2  2  1  2  2  2  2  1  2  2  1  2  2  2  1  2\n[33193]  2  2  2  2  2  2  1  1  2  2  2  1  2  2  2  2  2 NA  2  2  2  2  2  2\n[33217]  1  2  1  2  2  2  2  2 NA NA  2  2  2  2  2  1  2  1  2  1  1  2  1  2\n[33241]  1  2  2  2  2  2 NA  2  2  2  2  1  2  2  1  2  1  1  2  2  1  2  2  2\n[33265]  1  2  2  2  2  2  2  2  1  2  2  1  2  2  1  2  2  2  2  2  2  1  2  2\n[33289]  2  2  2  2  2  1  2  1  1  2  2  1  2  2  1  1  2  1  1  1  2  1  2  1\n[33313]  2  1  2  2  2  2  1  2  2  1  2  2  1  2  2  2  1  2  1  2  2  2  2  2\n[33337]  2  1  1  2  2  1  2  2  2  2 NA  1  2  2  1  2  2  2  2  2  2  2  2  1\n[33361]  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  1  2  1  2  2  2  2  2  2\n[33385]  2  1  1  2  2  2  1  2  2  2  1  2  1 NA  2  2  1  2  2  2  3  2  2  2\n[33409]  1  2  1  2  2  1  2  2  2  2  2  1  1  2  1  1  1  2  2  2  2  1  2  2\n[33433]  2  1  2  2  1  2  2  2  2  2  2  2  1  1  2  2  3  2  2  2  2  2  2  2\n[33457]  2  2  1  1  2  2  2  1  2  1  2  1  2  2  2  2  2  2  1  2  2  1  1  1\n[33481]  2  2  2  2  1  2  2  2 NA  1  2  1  1  1  1  2  1  2  1  2  2  1  2  1\n[33505]  2  2  1  2  1  2  2  1  1  2  2  2  1  2  1  2  1  1  1  2  2  1  2  2\n[33529]  1  2  2  2  2  2  2  2  2  2  2  1  1  1  2  1  1  1  2  1  1  1  1  1\n[33553]  1  2  1  1  2  2  1  2  1  2  2  2  1  2  2  2  1  2  1  2  2  2  2  1\n[33577]  1  2  1  1  2  1  1  2  2  2  1  1  1  2  2  2  2  2  1  2  2  2  2  2\n[33601]  2  2  1  2  1  2  2  2  2  2  2  2  2  2  2  1  1  2  2  2  2  2  1  2\n[33625]  2  2  1  1  1  1  1  1  1  1  1  2  1  1  2  2  2  2  2  2  2  2  1  2\n[33649]  2  2  1  1  1  2  2  1  2  2  2  2  2  2  2  2  2  1  1  2  2  1  2  1\n[33673]  2  1  1  2  2  2  2  2  2  2  2  2  2  3  1  1  1  2  1  2  2  1  1  1\n[33697]  1  1  2  1  1  1  1  2  2  1  1  2  2  1  1  2  1  2  2  2  1  2  2  2\n[33721]  1  1  2  2  1  2  2  2  2  2  2  2  2  1  2  2  2  2  2  2  1  1  1  2\n[33745]  2  2  2  1  1  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  1  2\n[33769]  2  2 NA  2  1  2  2  2  2  2  2  2  2  2  2  2  2  2  2  1  1  2  2  2\n[33793]  2  2  2  1  1  2  1  1  2  1  1  2  2  1  2  2  1  2  2  2  2  1  2  1\n[33817]  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  1  2  2  2  2  1\n[33841]  2  2  1  1  2  1  2  2  2  2  1  1  2  2  2  2  2  2  1  1  1  2  2 NA\n[33865]  1  2  1  2  2  2  2  2  1  2  2  2  1  1  2  2  2  2  1  2  1  1  1  2\n[33889]  2  2  2  2  1  2  2  2  1  2  1  2  2  2  1  2  2  2  2  2  2  2  2  2\n[33913]  1  1  2  1  2 NA  2  2  1  1  1  2  1  1  1  2  2  1  1  2  1  2  1  2\n[33937]  2  1  2  2  1  1  1  1  2  2  2  2  1  2  2  2  2  1  1  2  2  2  1  1\n[33961]  2  1  1  2  2  2  1  2  2  2  1  2  2  1  2  2  1  1  2  2  2  2  1  2\n[33985]  1  1  1  2  2  1  2  1  1  1  2  2  2  2  2  2  2  2  2  1  2  2  2  2\n[34009]  1  2  2  1  1  2  2  2  2  2  2  2  2  1  1  1  2  2  1  1  1  2  1  2\n[34033]  2  2  1  1  2  2  1  2  1  2  2  1  2  2  2 NA  1  2  1  1  2  2  2  2\n[34057]  2  2  2  1  2  2  1  1  1  1  1  2 NA  1  2  2  1  2  1  2  2  1  2  2\n[34081]  2  1  1  2  1  2  2  1  2  2  1  2  2  2  2  2  1  1  1  1  2  2  2  2\n[34105]  3  1  2  2  1  2  1  1  1  1  1  2  2  2  2  2  2  2  2  2  2  1  2  1\n[34129]  1  2  2  2  2  2  2  1  2  1  2  1  1  2  2  2  2  2  2  2  1  1  1  1\n[34153]  2  1  2  2  1  2  2  1  2  1  1  2  2  2  2  2  2  2  2  2  2  1  2  1\n[34177]  2  2  2  1  1  2  2  2  2  2  2  2  2  2  1  1  2  1  2  1  2  3  2  2\n[34201]  1  1  2  2  1  2  2  1  2  2  1  2  1  2  1  2  2  2  1  2  1  2  2  2\n[34225]  2  2  2  1  2  2  2  1  2  1  2  2  2  2  2  2  2  1  1  2  2  1  2  1\n[34249]  1 NA  2  2  1  2  1  2  2  1  2  2  2  1  2  2  1  1  2  2  2  1  1  2\n[34273]  2  2  2  2  2  1  1  2  1  2  2  1  1  2  2  1  1  1  1  2  2  1  2  2\n[34297]  2  1  1  2  2  1  2  1  2  2  1  2  1  1  2  2 NA  1  1  1  2  2  2  1\n[34321]  2  1  2  1  2  2  2  1  2  2  1  1  2  1  1  1  2  2  1  2  2  2  1  2\n[34345]  2  1  2  1  2  2  1 NA  1  1  2  1  2  1  1  1  2  2  2  2  1  2  1  1\n[34369]  2  1  1  1  2  2  2  1  1  2  2  2  1  2  2  2  2  1  2  2  2  2  2  2\n[34393]  2  2  2  2  2  1  2  2  2  2  2  1  1  2  2  2  2  1  2  2  1  1  2  2\n[34417]  2  1  2  2  1  1  1  2  2  1  2  2  2  2  2  1  1  2  2  2  2  2  2  2\n[34441]  2  1  2  1  2  2  2  1  2  2  2  1  2  1  1  2  2  1  2  2  2  1  2  2\n[34465]  1  2  1  2  2  2  2  2  2  1  2  2  2  2  2  2  1  2  2 NA  1  2  1  1\n[34489]  2  2  2  1  2  2  2  2  1  2  2  1  1  2  2  2  2  2  2  2  2  1  2  2\n[34513]  2  1  1  3  2  2 NA  2  2  2  2  2  1  1  2  1 NA  2  2  2  1  1  1  2\n[34537]  2  2  2  1  2  2  2  2  1  2  2  2  2  2  1  1  2  1  1  2  2  2  2  2\n[34561]  2  2  2  1  2  1  1  2  2  1  1  2  1  1  1  1  2  2  1  2  2  2  1  1\n[34585]  1  2  2  2  2  2  1  2  2  1  1  1  2  2  2  1  2  1  1  1  2  2  1  2\n[34609]  1  1  1  2  2  2  2  1  1  2  2  1  2  2  1  2  1  2  2  1  1  2  1  2\n[34633]  2  2  2  2  2  2  2  2  2  1  1  2  2  1  1  1  1  1  2  2  2  2  1  2\n[34657]  2  2  2  1  2  2  2  1  1  1  2  2 NA NA  2  1  2  1  2  2  2  2  1  1\n[34681]  1  1  2  1  1  1  2  1  1  1  1  1  1  2  1  1  1  2  1  1  1  2  2  2\n[34705]  2  2  1  1  1  2  1  2  2  2  2  1  2  2  2  2  2  2  2  2  2  2  2  2\n[34729]  1  2  1  1  1  2  1  2  2  1  2  2  2  1  2  2  2  1  1  2  2  2  2  1\n[34753]  2  1  2  1  2  1  1  1  2  1  2  1  1  2  2  1  1  1  2 NA  1  2  1  2\n[34777]  2  2  2  1  1  1  1  2  2  2  2  2  1  1  1  1  2  1  1  1  2  1  2  2\n[34801]  2  1  1  1  1  1  2  1  1  1  2  1  1  1  1  1  1 NA  1  1  1  2  1  1\n[34825]  1  1  1  2  2  2  2  1  1  1  2  1  1  1  1  2  1  1  2  1  1  2  2  1\n[34849]  2  2  1  2  1  1  1  1  2  2  1  1  1  2  1  2 NA  1  2  2  2  2  2  1\n[34873]  1  2  1  1  2  1  2  1  1  2  2 NA  2  2  2  3  2  2  1  2  2  2  2  1\n[34897]  1  1  2  2  2  2  2  2  2  2  1  1  1  1  2  2  1  2  2  2  1  2  2  2\n[34921]  2  2  2  1  2  1  1  2  2  2  2  1  2  2  1  2  1  1  2  2  1  2  2  1\n[34945]  1  1  2  1  1  2  1  2  2  2  1  2  1  2  1  2  1  2  1  2  2  1  1  1\n[34969]  2  1  2  1  2  2  1  1  1  2  1  1  1  1  1  2  1  2  1  1  1  2  2  2\n[34993]  1  2  2  1  1  1  2  1  2  2  2  1  2  1  2  1  2  2  2  2  2  1  2  2\n[35017]  1  2  1  2  1  2  2  2  2  1  1  2  1  2  2  1  2  1  1  2  1  2  2  1\n[35041]  2  1  2  2  1  2  2  2  2  2  1  2  2  1  1  1  2  2  1  2  2  2  2  1\n[35065] NA  2  2  1  2  2  1  2  2  2  1  2  2  2  2  2  2  2  2  1  1  1  2  1\n[35089]  1  3  1  2  2  1 NA NA  2  2  2  1  2  1  2  1  2  2  2  1  2  2  1  2\n[35113]  2  2  2  2  2  2  2  1  2  2  2  1  2  2  2  2  2  1  2  1  2  2  2  2\n[35137]  2  1  2  1  2  1  2  2  2  1  2  2  2  1  1  2  2  1  1  2  2  2  1  1\n[35161]  2  2  2  2  1  1  2  2  1  1  1  2  2  2  1  2  2  1  2  2  1  2  2  2\n[35185]  2  2  2  2  2  2  2  2  1  2  1  1  3  1  2  1  2  2  1  2  2  1  1  1\n[35209]  2  2  2  2  1  1  2  2  2  2  1  2  1  2  2  1  2  2  2  2  2  2  1  2\n[35233]  2  3  2  1  2  1  2  2  2  2  1  2  2  1  2  2  3  2  2  1  2  1  1  2\n[35257]  1  2 NA  1  1  1  2  2  1  2  2  2  2  2  2  1  2  2  2  1  2  2  2  2\n[35281]  2  1  2  2  1  1  2  2  2  2  1  2  2  2  2  1  2  1  2  2  1  2  1  2\n[35305]  2  2  2  1  1  2  2  2  1  2  1  1  2  2  2  2  1  2  1  2  2  2  1  2\n[35329]  2  1  2  2  1  2  2  1  1  1  2  1  2  1  2  1  2  2  2 NA  2  2 NA  2\n[35353]  1  2  2 NA  2  2  2  2  1  1  2  1  2  1  2  2  2  3  2  1  2  2  1  2\n[35377]  2  2  2  2  2  2  1  2  2  1  2  2  2  2  1  1  2  2  2  1  1  2  2  2\n[35401]  2  2  1  1  2  1  2  2  2  2  1  2  1  1  1  2  2  2  2  1  1  2  1  2\n[35425]  2  2  2  2  2  2  2  1  1  1  2  2  2  1  1  1  2  2  1  2  2  2  2  2\n[35449]  2  2  1  1  2  1  2 NA  2  2  1  2  2  2  2 NA  2  2  1  3  1  2  2  1\n[35473]  2  1  2  2  2  2  2  2  2  2  2  1  2  2  1  2  2  2  2  1  1  2  2  1\n[35497]  1  2  2  2  1  1  2  2  2  2  2  2  1  1  1  1  2  2  2  1  2  1  2  1\n[35521]  2  2  1  2  1  2  1  2  1  2  1  2  1  2  1  2  2  2  2  2  2  2  2  1\n[35545]  2  2  2  2  1  2  2  1  2  1  2  1  2  2  2  2  2  2  1  2  2  2  2  2\n[35569]  2  1  1  1  2  1  2  1  1  2  2  2  1  1  2  1  2  2  1  2  1  3  2  2\n[35593]  1  2  2  1  2  2  2  2  1  1  2  2  1 NA  2  2  2  2  2  2  1  2  2  1\n[35617]  2  2  2  2  1  2  2  1  2  1  2  2  2  2  2  1  1  1  1  1  2  1  2  2\n[35641]  1  2  1  1  2  2  1  2  1  2  2  2  1  2  2  1  2  2  2  2  1  1  1  2\n[35665]  2  2  2  1  1  2  2  2  2  1  1  1  1  1  2  2  1  2  1  2  1  1  1  2\n[35689]  2  1  1  2  1  1  1  2  1  2  1  2  2  2  2  1  2  2  2  2  2  2  2  2\n[35713]  2  1  2  2  2  1  2  2  1  2  1  2  2  2  2  1  2  2  2  2  1  2  2  1\n[35737]  1  1  2  1  1  2  1  2  2  2  2  1  1  1  2  2  2  1  2  2  1  2  1  2\n[35761]  2  2  1  1  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2\n[35785]  2  2  2  2  2  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n[35809]  1  1  1  1  1  1  2  1  2  2  2  2  1  2  2  2  2  1  2  2  2  2  2  1\n[35833] NA  2  1  1  2  1  2  1  1  2  1  1  1  2  2  2  2  1  2  2  2  2  2  1\n[35857]  2  2  1  1  2  2  1  1  1  1  1  2  2  1  1  2  2  2  1  1  2  2  2  1\n[35881]  2  2  1  2  1  2  2  2  2  2  1  1  1  2  2  2  2  2  1  2  1  2  2  2\n[35905]  1  2  2  2  2  2  2  2  2  2  1  2  2  1  1  2  2  1  1  2  2  2  1  2\n[35929]  2  1  1  1  2  2  2  2  2  2  1  1  1  2  2  2  2  1  2  2  2  1  2  2\n[35953]  2  2  2  1  2  2  2  1  2  2  1  2  2  2  2  2  2  2  1  2  2  2  2  2\n[35977]  2  2  2  2  2  2  2  2  1  2  1  1  2  2  1  2  2  2  2  2  2  2  2  2\n[36001]  1  1  1  2  2  1  2  2  2  2  2  1 NA  2  3  1  1  2  1  2  2  2  1  1\n[36025]  2  2  2  2  1  2  2  2  2  2  1  2  2  2  1  1  2  1  1  2  2  2  1  2\n[36049]  2  2  2  2  2 NA  2  2  1  2  2  1  1  2  2  2  1  2  2  1  2  1  2 NA\n[36073]  2  2  2  1  1  2  2  1  2  2  2  1  2  1  2  1  2  2  2  2  2  2  2  2\n[36097]  1  2  2  2  1  1  1  1  1  2  1  1  2  2  1  2  2  1  2  1  1  1  2  1\n[36121]  2  1  2  2  1  1  2  2  2  2  1  2  2  2  2  2  2  2  1  2  2  1  1  1\n[36145]  2  1  2  2  2  2  2  2  2  1  2  1  1  1  2  2  2  1  2  1  2  1  2  1\n[36169]  1  1  1  2  2  1  2  2  1  2  2 NA  1  1  1  2  2  1  2  2  2  2  2  2\n[36193]  2  2  2  2  1  1  2  1  2  2  2  2  2  2  2  2  2  2  1  2  1  2  2  2\n[36217]  1  2  2  2  2  1  2  1  1  2  2  2  2  2  1  1  2  2  2  2  2  2  2  2\n[36241]  2  2  2  2  2  2  2  1  2  2  2  2  2  2  2  2  2  2  1 NA  2  2  2  2\n[36265]  2  2  1  2  1  2  2  1  2  2  1  1  1  2  2  2  2  2  2  2  2  1  1  2\n[36289]  1  1  2  2  2  2  2  1  2  2  2  2  2  1  2  1  2  1  2  1  2  2  2  1\n[36313]  2  2  2  2  2  2  1  2  2  1  2  2  1  1  2  1  2  1  2  1  1  2  2  2\n[36337]  1  2  1  2  2  2  2  2  2  1  2  2  2  2  2  1  1  1  1  1  2  1  1  2\n[36361]  2  1 NA  2  2  2  2  1  2  1  2  2  2  2  2  2  1 NA  2 NA  2  2  2  2\n[36385]  2  2  2  2  2  2  2  2  2  1  2  2  2  2  1  1  2  1  2  2  2  2  1  2\n[36409]  1  1  1  1  2  2  1  1  1  2  2  2  1  2  2  1  2  2  2  1  2  2  1  1\n[36433]  1  1 NA  2  2  2  1 NA  2  1  2  2  2  1  1  2  2  2  2  2  1  2 NA  2\n[36457]  2  2  2  2  2  2  2  2  2  2  2  1  2  2  2  1  2  1  2  1  1  1  2  1\n[36481]  2  1  2  2  1  2  2  2  1  2  2  2  2  2  2  1  2  2  2  2  2  1  2  2\n[36505]  2  2  2  1  1  1  1  2  2  2  2  1  2  2  1  1  2  2  2  1  2  1  1  2\n[36529]  2  1  1  1  3  2  2  1  1  2  2  2  2  2  1  2  2  2  2  1  1  1  1  2\n[36553]  1  1  1  2  2  1  2  1  1  1  2  2  2  1  2  2  1  1  2  2  1  2  2  2\n[36577]  1  1  2  2  1  2  2  1  2  1  2  1  2  1  2  1  2  2  1  1  2  2  2  1\n[36601]  2  1  2  2  2  3  2  2  2  1  1  1  2  2  2  1  2  1  1  1  2  2  2  2\n[36625]  1  2  2  2  2  2  2  2  2  1  2  2  1  1  2  2  2  2  2  2  1  2  2  1\n[36649]  2  2  1  1  1  2  2  1  1  1  2  1  2  2  2  1  1  1  2  1  2  1  1  2\n[36673]  2  1  1  2  1  2  2  2  2  2  2  2  2  2  1  2  2  2  1  1  2 NA  2  2\n[36697]  1  1  2  2  2  2  1  1  1  2  2  2  1  1  2  2  2  1  2  1  1  1  2  2\n[36721]  2  2  1  2  2  2  2  1  1  1  1  2  1  1  2  1  1  2  2  1  1  2  1  2\n[36745]  2  2  2  1  2  2  1  1  1  1  2  2  2  1  2  2  1  3  1  2  1  2  1  2\n[36769]  2  2  2  2  2  1  2  1  2  1  2  2  2  2  2  2  2  2  1  2  2  1  1  2\n[36793]  2  1  1  1  2  2  2  2  2  2  1  2  2  2  1  2  2  2  2  2  2  2  2  2\n[36817]  2  2  2  2 NA  2  1  1  2  2  2  2  2  2  1  1  1  1  1  2  2  1  1  2\n[36841]  1  2  1  2  2  1  2  2  2  2  1  2  2  2  2  2  2  1  2 NA  2  2  1  2\n[36865]  2  2  1  1  1  1  1  1  2  2  1  1  1  2  1  1  1  1  1  1  2  2  2  1\n[36889] NA  2  2  1  2  2  2  2  2  1  2  2  2  2  1  2  1 NA  2  2  2  2  2  1\n[36913]  2  2  1  1  2  1  2  1  2  1  2  1  2  1  1  2  1  2  1  2  2  1  1  2\n[36937]  2  1  2  2  2 NA  1  1  1  2  2  1  3  1  2  1  1  2  2  2  2  2  2  2\n[36961]  2  1  1  2  1  2  1  2  2  1  1  2  1  2  1  1  1  2  2  2  2  2  1  1\n[36985]  2  2  2  1  1  2  2  2  2  2  2  2  1  2  2  2  1  2  2  2  2  2  1  1\n[37009]  2  2  2  2  1  2  2  2  2  2  1  2  1  2  1  1  2  2  2  1  1  2  2  2\n[37033]  2  2  2  2  2  1  2  1  1  2  2  2  2  1  1  2  1  2  2  2  2  2  1  2\n[37057]  2  1  2  1  2  2  2  2  2  2  2  1  2  1  2  2  1  1  2  2  1  1  2  1\n[37081]  2  1  1  2  2  1  1  2  1  2  2  2  2  2  1  2  1  2  2  1  2  2  2  2\n[37105]  2  1  2  2  2  2  2  2  1  1  2  2  1  2  2  2  1  2  1  2  1  2  1  2\n[37129]  2  2  1  2  1  2  2  1 NA  2  1  1  2  2  2  2  1  1  2  2  1  2  1  2\n[37153]  2  1  2  2  2  1  1  2  2  2  1  2  1  1  1  1  1  2  1  2  1  2  2  1\n[37177]  2  1  1  1  1  1  2  2  2  2  1  2  2  2  2  2  1  1  1  2  1  2  2  2\n[37201]  2  2  2  1  3  2  2  2  2  2  1 NA  2  1  1  1  2  1  2  1  2  2  2  1\n[37225]  1  2  2  2  1  2  2  2  2  1  2  2  1  2  2  2  2  2  1  2  2  2  2  2\n[37249]  1  2  2  2  2  2  2  2  1  2  2  2  1  1  2  2  2  2  2  2  2  2  2  1\n[37273]  2  2  2  1  2  2  2  2  2  2  3  2  2  2  2  1  1  3  1  2  1  1  2  1\n[37297]  2  2  2  2  1  1  2  2  2  1  2  1  2  2  2  1  2  2  2  2  2  1  2  2\n[37321]  2  1 NA  2  2  2  2  2  2  1  2  1  2  1  2  1  2  2  1  1  2  2  2  2\n[37345]  1  1  2  2  2  1  2  1  2  2  2  1  1  2  2  3  2  2  2  2  1  1  1  2\n[37369]  2  2  1  2  1  1  1  2  2  2  2  2  2  2  2  2  2  1  1  2  1  2  2  2\n[37393]  2  1  1  2  2 NA  1 NA  2  1  1  1  2  1 NA  2  2  1  2  2  2  1  2  2\n[37417]  2  2  1  1  1  1  2  1  1  1  2  1  2  1  1  2  1  2  2  1  1  2  1  1\n[37441]  2  2  2  1  2  1  1  1  2  1  2  2  1  1  2  2  2  2  2  1  2  2  1  2\n[37465]  2  2  1  2  2  2  1  3  2  2  1  2  1  2  2  2  2  2  1  2  2  2  2  2\n[37489]  1  2  2  1  1  2  2  1  2  2  2  2  1  2  1  2  2  2  2  1  1  2  2  1\n[37513]  2  3  2  2  2  2  2  1  2  2 NA  2  1  2  2  1  2  2  2  2  2  1  1  2\n[37537]  2  2  2  1  2  2  1  2  2  1  1  1  1  2  1  1  2  1  2  2  2  1  2  2\n[37561]  1  1  1  2  2  2  2  2  1  2  2  1  1  1  2  2  1  2  1  2  1  2  1  2\n[37585]  1  1  2  2  1  2  2  2  1  2  1  1  1  2  1  1  1  2  2  2  1  2  2  2\n[37609]  1  2  2  1  2  1  2  2  2  2  2  1 NA  1  2  2  2  2  2  2  1  2  2  2\n[37633] NA  1  1  2  1  1  2  2  1  2  2  2  1  2  1  1  2  2  1  2  1  2  1  1\n[37657]  2  2  2  2  2  2  2  2  1  2  1  2  1  1  2  2  2  2  1  1  2  2  3  2\n[37681]  1  2  1  2  2  2  1  1  1  1  3  1  2  1  2  2  2  3  1  1  2  2  1  1\n[37705]  1  2  1  2  2  2  1  2  1  2  2  2  1  1  2  2  1  1  1  2  2  2  2  2\n[37729]  2  1  2  2  1  2  2  2  2  1  2  2  2  2  2  2  2  2  1  1  1 NA  2  3\n[37753]  1  2  2  2  1  2  2  2  3  1  2  2  2  2  2  2  1  2  2  1  1  2  1  3\n[37777]  1  2  1  1  2  1  1  2  1  1  2  2  2  2  1  2  1  2  2  1  1  2  1  2\n[37801]  1  1  2  1  2  1  1  1  2  1  1  2  2  2  2  2  2  2  1  2  1  2  2  2\n[37825]  1  2  2  1  2  1  2  1  1  2  2  2  1  1  2  1  1  2  1  2  1  1  1  1\n[37849]  1  2  2  1  2  2  2  2  2  1  2  2  2  2  2  1  1  2  2  2  2  2  1  2\n[37873]  2  2  1  1  2  1  2  1  2  1  1  2  2 NA  1  1  2  2  2  3  2  2  3  2\n[37897]  2  2  2  2  2  1  1  2  1  1  2  1  2  2  2  1  2  2  1 NA  2  1  1  2\n[37921]  1  1  2  2  2  2 NA  2  1  2  2  2  2  2  2  2  2  2  2  2 NA  1  2  2\n[37945]  2  1  1  2  1  3 NA  2  1  1  1  2  2  1  1  2  1  1  2  2  2  2  1  2\n[37969]  2  2  1  1  2  1  1  1  1  2  2  2  2  2  2  1  1  3  2  2  1  1  1  2\n[37993]  2  1  2  1  2  2  2  2  2  1  2  2  1  2  2  2  1  1 NA  1  1  2  2  1\n[38017]  2  1  2  1  2  2  2  2  1  2  1  2  2  1  1  2  1  2  1  1  1  1  2  2\n[38041]  2  2  2  2  2  2  2  2  1  2  2  2  1  2  1  1  2  1  2  2  2  1  2  2\n[38065]  1  1  1  1  2  1  2  2  2  1  2  1  1  2  1  2  1  2  1  2  2  1  2  1\n[38089]  1 NA  2  2  2  1  2  1  2  2  2  1  2  2  1  1  2  1  2  2  1  1  2  1\n[38113]  2  2  1  2  2  1  2  1  1  1  2  2  2  1  1  1  2  1  1  2  1  1  1  1\n[38137]  1  1  1  1  1  2  1  1  2  2  2  1  2  1  2  2 NA  2  2  1  2  3  2  2\n[38161]  2  2  2  1  2  2  1  1  2  1  2  2  2  1  1  2  2  2  2  2  2  2  1  1\n[38185]  1  2  2  2  2  2  1  2  1  1 NA  2  1  1  2  2  2  2  2  1  2  2  2  1\n[38209]  2  2  2  1  2  1  2  2  1  2  2  2  2  1  1  2  2  2  2  1  2  2  2  1\n[38233]  2  1  2  1  1  2  1  1  2  2  1  2 NA  2  2  2  1  2  1  2  1  2  1  1\n[38257]  2  2  2  2  2  1  2  1  2  2  2  2  1  2  2  2  2  1  1  2  2  2  1  1\n[38281]  2  2  2  1  2  1  1  1  2  1  2  1  2  2  1  2  1  2  2  2  2  2  2  1\n[38305]  2  2  2  2  2  2  2  2  2  2  2  1  1  2  2  1  1  1  2  1  1  2  1  2\n[38329]  1  2  2  1  2  2  2  2  1  2  1  1  2  2  2  1  2  2  1  2  2  1  2  1\n[38353]  2  2  1  1  1  2  2  2  1  1  1  2  1  1  2  2  1  1  1  2  2  1  2  2\n[38377]  1  2  2  2  1  2  1  1  1  1  1  2  1  2  1  2  1  2  1  1  2  2  1  2\n[38401]  2  2  2  2  2  2  1  1  2  1  1  2  1  2  2  1  1  2  2  1  2  2  2  2\n[38425]  1  1  2  1  2  1  1  2  2  1  2  2  2  2  1  2  1  2  2  2  2  2  1  2\n[38449]  2  2  2  2  1  2  1  2  2  1  1  1  1  1  2  1  2  2  1  1  1  1  2  2\n[38473]  2  2  1  1  1  1  2  1  2  2  2  3  1  1  2  2  2  2  1  1  1  2  2  1\n[38497]  2  1  1  2  2  2  2  1  1  2  2  1  1  2  2  1  2  1  2  1  1  1  1  2\n[38521]  1  1  1  1  1  1  1  2  2  2  2  1  2  2  2  1 NA  1  2  2  1  2  2  2\n[38545]  1  1  1  2  1  2  2  1  2  2  2  1  1  1  2  1  2  2  2  2  2  2  2  2\n[38569]  2  2  1  1  2  1  2  2  1  2  2  1  2  2  2  1  1  2  2  1  2  2  2  2\n[38593]  2  2  1  2  2  2  2  1  3  1  2  2  1  2  2  2  2  2 NA  1  2  1  2  2\n[38617]  1  2  2  2  2  2  2  1  1  2  1  2  1  2  2  2  2  2  2  1  2  1  1  2\n[38641]  2  2  3  1  2  1  1  2  1  1  1  2  2  1  1  1  1  1  1  1  1  2  1  1\n[38665]  2  2  1  1  2  2  2  1  2  2  1  2  1  2  1  2  2  2  2  1  2  1  2  1\n[38689]  1  1  1  2  2  2  2  1  1  1  1  1  2  2  1  2  1  1  2  2  1  2  1  2\n[38713]  2  1  1  2  2  1  1  1  2  2  1  2  2  1  1  1  2  2  1  2  2  1  1  1\n[38737]  1  1  2  2  2  2  1  2  2  1  1  2  1  1  2  2  2  1  1  2  1  2  2  1\n[38761]  2  2  2  1  1  1  2  1  2  1  3  2  1  2  2  2  2  2  2  1  2  1  2  1\n[38785]  1  1  2  1  2  1  1  2  1  1  2  2  2  1  1  2  1  2  2  1  1  1  2  1\n[38809]  1  1  2  1  1  1  1  1  1  2  2  1  2  1  2  2  1  2  1  3  2  1  2  1\n[38833]  2  2  2  1  1  2  2  2  2  1  2  2  1  1  1  1  2  3  2  1  2  2  2  2\n[38857]  1  1  2  3  1  2  2  1  1  1  2  1  1  1  2  1  1  2  2  1  2  2  2  2\n[38881]  2  2  1  1  2  1  2  1  1  2  2  1  2  2  2  1  2  2  2  1  2  1  1  1\n[38905]  2  1  2  2  1  2  2  2  2  2  2  2  2  2  2 NA  1  1  2  1  2  2  1  2\n[38929]  2  1  2  2 NA  2  1  2 NA  2  1  2  2  2  2  1  2  2  2  2  1  2  2  2\n[38953]  1  2  1  1  2  1  2  2  2  1  2  2  2  2  1  1  2  1  1  1  1  2  1  1\n[38977]  2  2  1  1  2  2  1  1  1  1  1  2  1  1  2  2  3  2  1  2  2  1  2  2\n[39001]  2  1  2  2  2  1  2  2  1  2  1  2  1  2  2  2  2  1  1  1  1  1  2  1\n[39025]  2  1  2  1  2  2  2  1  1  2  2  2  2  2  1  2  2  2  2  2  1  2  2  2\n[39049]  2  2  2  2  2  1  2  1  2  2  2  2  1  2  2  2  2  2  1  1  1  2  2  1\n[39073]  1  2  1  2  1  1  1  1  2  1  1  1  1  1  1  1  2  2  2  2  2  2  1  2\n[39097]  2  1  2  2  1  2  1  2  2  2 NA  1  2  1  2  1  1  2  2  2  2  2  1  2\n[39121]  1  1  2  1  2  2  1  2  2  2  2  1  2  1  1  1  2  2  1  2  1  1  1  1\n[39145]  2  2  2  1  1  2  2  2  2  2  2  2  2  2  1  1  1  2  1  1  1  2  1  2\n[39169]  2  2  1  1  1  2  1  2  1  1  3  1  3  1  1  2  2  2  2  2  1  2  2  2\n[39193]  2  2  1  1  2  1  1  2  1  2  2  1  1  2  2  1  2  2  2  1  1 NA  2  1\n[39217]  2  2  2  2  2  2  2  1  1  1  2  1  2  2  2  1  1  1  2  2  1  2  1  3\n[39241]  2  2  2  2  2  1  1  2  1  2  2  2  2  1  2  3  1  2  2  1  2  2  1  1\n[39265]  1  1  2  1  2  2  2  2  1  2  1  1  1  1  2  2  2  2  1  1  2  1  2  2\n[39289]  2  2  2  1  2  2  1  1  2  1  2  2  2  2  2  2  1  2  2  2  2  2  1  1\n[39313]  2  2  2  1  1  2  2  2  2  2  1  1  2  2  1  1  1  1  2  1  2  2  2  2\n[39337]  1  1  2  2  1  1  1  2  2  2  2  2  2  2  2  1  2  2  1  1  1  1  2  2\n[39361]  1  2  2  1  1  1  1  2  2  2  1  2  1  2  2  2  2  1  1  2  2  1  2  1\n[39385]  2  2  1  1  1  2  2  2  2  2  2  1  1  2  1  1  2  2  1  1  2  1  1  1\n[39409]  2  1  2  2  1  1  1  2  2  2  1  2  2  2  2  2  1  2  2  2  2  2  1  1\n[39433]  2  2  1  2  1  2 NA  3  2  2  2  2  1  1  1  2  1  2  2  2  2  1  1  2\n[39457]  2  1  1  2  1  2  2  1  2  2  1  1  2  2  1  1  2  1  2  1  2  1  1  1\n[39481]  2  2  1  1  2  2  2  1  1  2  1  1  1  1  2  1  2  2  1  3  2  2  2  2\n[39505]  1  1  1  2  2  1  2  2  2  2  2  2  2  2  2  1  2  2  1  2  2  1  2  2\n[39529]  2  2  2  1  2  1  1  1  2  2  2  1  2  1  2  2  2  2  2  1 NA  2  1  1\n[39553]  1  1  2  2  2  2  1  1  1  1  1  1  1  2  2  1  2  2  2  1  2  1  2  1\n[39577]  1  2  1  2  2  2  2 NA  2  2  2  2  1  1  1  2  2  2  2  2  1  2  1  2\n[39601]  2  1  1  2  2  2  1  2  1  1  2  2  2 NA  2  2  2  2  1  2  1  2  1  1\n[39625]  2  1  2  1  1  2  2  2  2  2  1  1 NA  1  1  2  2  2  1  1  1  1  2  1\n[39649]  1  1  1  2  1  2  2  2  2  1  2  2  2  2  2  2  2  2  1  2  3  2  2  2\n[39673]  2  2  2  1  2  2  1  1  1  2  2  2  1  2  1  2  1  2  1  1  1  2  1  2\n[39697]  2  2  2  2  2  2  1  2  1  1  1  2  1  3  2  2  1  2  1  2  2  1  1  2\n[39721]  2  1  2  2  1  1  2  2  2  1  2  1  2  2  2  2  2  2  2  2  2  1  1  1\n[39745]  2  2  2  2  2  1  1  1  2  1  2  2  1  1  1  1  2  2  1  2  2  2  2  2\n[39769]  1  2  2  1  2  1  1 NA  2  3  2  2  1  2  2  2  1  2  2  1  2  2  1  2\n[39793]  1  1  2  1  2  2  2  2  2  2  1  1  2  2  2  1  2  2  2  1  2  2  2  2\n[39817]  2  1  1  2  2  2  1  1  1  1  1  2  2  1  1  1  2  2  2  2  2  1  1  2\n[39841]  2  2  2  2  1  2  2  2  2  1  1  2  2  1  1  2  2  2  2  2  2  2  1  2\n[39865]  2  1  1  2  2  2  2  1  2  2  2  2  2  1  2  2  2  1  2 NA  2  1  2  1\n[39889]  1  1  2  2  2  1  2  2  2  2  2  2  2  2  2  2  2  2  2  2  1  2  2  2\n[39913]  1  1  2  1  2  3  2  2  2  1  2  1  1  2  2  1  1  2  1  2  1  1  2  2\n[39937]  2  1  2 NA  2  2  1  2  2  2  1  2  1  2  2  2  1  1  1  2  1  2  1  1\n[39961]  2  2  2  2  2  1  1  2  2  2  2  1  2  1  1  2  1  1  1  2  2  1  3  2\n[39985]  2  2  2  2  3  2  2  2  2  2  1  2  2  2  1  1  1  2  2  2  2  2  1  1\n[40009]  2  1  2  2  3  2  2  1  2  1  1  2  2  1 NA  2  1  2  1  2  2  2  2  2\n[40033]  1  1  2  1  2  2  1  1  2  2  2  2  2  1  2  1  1  1  1  2  2  1  2  1\n[40057]  2  2  1  2  2  2  2  2  1  2  2  1  2  2  2  2  1  2  1  2  2  2  1  2\n[40081]  2  2  2  2  2  1  2  2  1  2  1  2  2  2 NA  2  2  2  2  1  2  2 NA  2\n[40105]  1  2  2 NA  2  2  1  2  2  2  1  1  2  2  2  2  2  1  1  1  2  2  1  2\n[40129]  2  1  2  1  2  2  2  1  2  2  2  2  2  1  1  2  2  2  1  2  2  2  1  2\n[40153]  2  1  2  2  2  2  2  2  1  2  2  1  1  2  2  2  1  2  2  1  1  1  2  1\n[40177]  1  2  2  1  2  2  1  1  2  1  1  1  1  2  1  1  2  2  1  1  2  2  1  2\n[40201]  2  2  2  2  2  1  2  1  2  1  2 NA  2  2  1  1  2  3 NA  2  1  1  1  1\n[40225]  1  2  1  1  2  1  1 NA  2  1  1  1  3  2  2  2  1  2  1  1  2  2  1  1\n[40249]  1  2  2  1  1  2  1  2  1  2  2  2  1  2  2  2  2  2  2  1  2  1  2  2\n[40273]  2  1  2  1  2  1  2  2  2  1 NA  2  1  1  2  2  1  1  2  1  1  1  2  2\n[40297]  2  3  2  2  1  1  2  1  1  1  1  1  1  2  2  1  1  2  3  2  1  2  1  2\n[40321]  1  2  2  2  2  1  1  2  2  2  1  2  2  2  2  1  2  1  2  1  1  1  2  2\n[40345]  2  2  2  2  1  1  2  1  1  1  2  1  2  1  1  1  2  2  1  2  2  2  2  2\n[40369]  2  2  2  2  1  2  1  1  2  2  1  2  2  2  2  2  2  1  2  2  2  2  2  2\n[40393]  2  1  1  2  2  2  2  2  1  2  2  2  2  1  1  2  1  2  2  2  1  1  2  1\n[40417]  1  1  1  2  1  1  1  1  2  2  1  1  1  1  1  1  2  1  1  2  1  2  2  2\n[40441]  2  2  1  2  1  2  1  2  1  1  2  1  2  1  2  2  2  1  1  2  2  1  2  1\n[40465]  2  1  2  2  2  2  2  2  2  1  1  1  2  2  2  2  2  2  2  2  1  2  2  1\n[40489]  2  2  2  1  2  2  1  1  2  2  2  2  1  2  2  1  2 NA  2  1  2  1  1  2\n[40513]  2  2  2  1  2  2  1  1  1  1  2  1  1  2  2  1  1  2  1  1  1  2  1  2\n[40537]  1  1  1  2  2  2  2  2  2  1  1  1  2  2  1  2  2  2  2  1  1  2  1  1\n[40561]  1  2  1  2  1  2  2  2  2  1  1  2  1  1  1  1  2  2  1  1  1  1  2  1\n[40585]  2  1  1  2  2  2  3  1  2  2  2  1  2  1  1  2  2  1  1  1  2  1  2  2\n[40609]  2 NA  2  2  1  1  1  2  1  1  2  2  2  1  2  2  1  2  1  2  1  2  2  1\n[40633]  1  1  2  2  1  2  1  2  1  2  1  1  1  2  1  1  2  1  1  2  2  2  1  2\n[40657]  1  2  2  3  2  1  2  1  1  2  2  2  2  1  2  2  1  2  1  2  2  2 NA  2\n[40681]  2 NA  2  3  2  1  1  2  2  2  2  2  2  2  2  2  1  1  2  2  2  1  2  2\n[40705]  2  2  2  1  1  2  2  2  2  2  1  2  2  2  2  1  1  1  2  2  2  2  2  1\n[40729]  2  2  1  2  2  1  2  1  2  2  1  2  1  2  2  2  2  1  2  2  2  3  2  2\n[40753]  2  2  1  2  2  2  1  2  2  2  2  1  1  2  2  1  2  2  1  2  2  1  2  2\n[40777]  2  1 NA  2  2  1  1  1  2  1  1  1  1  1  2  2  2  1 NA  2  2  2  2  1\n[40801]  1  2  1  1  2  1  2  2  2  2  2  2  2  3  2  2  1  1  2  1  2  2  2  2\n[40825]  1  1  1  1  2  2  2  1  2  2  2  2  2  2  2  2  2  2  1  2  1  1  2  2\n[40849]  1  2  2  1  2  2  2  2 NA  2  1  2  2  2  1  2  2  2  2 NA  2  2  2  1\n[40873]  2  2  2  2  1  1  2  2  3  3  1  2  2  1  2  1  2  2  2  2  2  2  1  2\n[40897]  2  1  2  1  2  2  2  2  2  2  1  1  2  1  2  2  2  2  1  1  2  2  1  1\n[40921]  2  2  1  2  2  2 NA  1  2  2  1  2  2  1  2  2 NA  2  1  2  2  2  2  2\n[40945]  1  2  2  2  2  1  2  2  2  1  2  2  2  2  2  2  2  1  2  2  2  2  1  2\n[40969]  2  1  1  1  1  2  2  2  2  2  1  2  2  2  1  1  1  2  2  2 NA  2  1  2\n[40993]  2  1  2  1  2  1  2  3  2  1  1  2  1  2  2  2  1  1  2  1  2  1  2  2\n[41017]  2  1  2  1  1  1  1  1  1  1  2  2  2  2  2  2  2  2  2  1  1  1  1  2\n[41041]  2  2  2  2  1  2  2  1  2  2  1  2  1  2  1  2  1  2  1  1  1  1  1  1\n[41065]  1  1  1  2  2  2  2  1  2  2  1  2  1  2  1  1  2  2  1  2  1  2  1  2\n[41089]  2  2  1  1  1  1  2  1  2  1  1  2  1  1  1  2 NA  2  2 NA  2  2  3  2\n[41113]  2  2  2  2  2  2  2  2  1  2  1  2  1  1 NA  2  1  2  2  1  1  2  1  2\n[41137]  1  2  1  2  2  1  2  2  2  2  1  2  1  2  1  2  1  1  2  1  1  2  2  1\n[41161]  2  2  2  2  2  1  1  2  1  1  2  1  1  2  2  2  1  1  1  2  2  2  1  2\n[41185]  2  1  2  1  1  2  2  1  2  2  2  3  2  2  1  2  3 NA  1  2  2  1  2  2\n[41209]  2  1  2  1  1  1  2  2 NA  2  1  1  1  2  2 NA  2  2  1  2  2  2  1  2\n[41233]  2  1  2  2  2  2  2  1  1  2  2  2  2  2  1  1  1  1  1  2  2  2  1  2\n[41257]  1  1  2  2  2  2  1  1  1  2  1  1  1  2  1  1  2  1  2  2  2  2  1  2\n[41281]  1  2  1  2  2  2  2  1  2  1  2  1  1  2  1  1  2  1  1  2  2  2  1  1\n[41305]  2  2  2  1  1  1  2  1  1  2  2  1  1  1  2  1  2  2  2  1  1  1  2  2\n[41329]  2  2  2  1  2  1  1  2  2  1  1  2  1  2  2  2  2  2  1  1  2  1  2  2\n[41353]  1  2  2  1  2  2  2  2  1  2  2  2  1  1  1  3  1  2  1  2  1  2  1  2\n[41377]  2  2  1  2  2  2  2  2  2  1  2  1  2  1  2  2  1  1  2  2  2  2  1  1\n[41401]  2  2  2  2  1  2  2  2  2  2  2  1  2  1  1  2  1  2  2  2  2  2  1  2\n[41425]  1  1  1  1  1  1  2  2  2  1  2  1  1  2  1  2  1  2  2  2  2  2  1  1\n[41449]  1  2  2  2  1  2  2  1  2  1  1  2  1  1  1  2  1  1  1  2  1  2  1  2\n[41473]  2  2  2 NA  2  1  1  1  2  2  2  1  2  2  2  2  2  2  1  2  1  2  1 NA\n[41497]  2  2  1  2  1  1  1  2  2  2  2  2  2  2  1  2  1  2  2  2  1  2  2  1\n[41521]  2  1  2  1  2  1  2  1  2  2  2  1  3  1  2  2  1  2  1  2  1  2  2  2\n[41545]  2  1  1  2  2  2  1  2  2  2  1  1  2  1  2  2  2  2  2 NA  1  1  2  2\n[41569]  2  1  1  1  1  1  1  2  2  2  2  2  1  1  2  1  1  1  1  2  1  2  1  1\n[41593]  1  2  1  1  1  1  1  1  2  1  2  2  2  1  2  2  1  2  1  1  1  2  2  1\n[41617]  1  1  1  1  1  1  3  2  2  2  1  2  2  2  2  2  2  2  2  2  1  2  2  2\n[41641]  1  2  2  2  2  2  1  2  1  2  2  2  1  2  1  1  2  2  1  1  2  1  2  2\n[41665]  1  2  2  1  2  2  1  2  1  1  1  2  1  1  2  1  1  2  1  2  1  2  1  2\n[41689]  1  1  1  1  1  2  1  2  2  1  1  2  1  2  2  2  1  2  1  2  1  2  2  2\n[41713]  2  2  2  1  1  2  2  1  2  2  2  1  1  1  1  1  2  1  2  1  1  1  2  2\n[41737]  1  2  2 NA  2  2  2  1  2  1  1  2  1  2  2  2  2  1  2  2  2  2  1  2\n[41761]  2  2  2  2  1  1  2  1 NA  1  1  2  1  2  1  1  2  2  1 NA  2  2  1  1\n[41785]  2  2  2  2  1  1  2  2  1  2  1  2  1  2  1  2  2  2  2  2  2  1  2  2\n[41809]  1  1  2  1  2  1  1  1  1  1 NA  2  1  1  2  2  2  2  2  1  1  2 NA  1\n[41833]  1  1  2  1  2  2  2  2  2  2  2  2  3  2  2  2  1  2  1  2  1  1  2  1\n[41857]  1  2  2  2  2  1  1  2  1  2  2  2  2  1  2  2  2  2  2  2  1  2  2  1\n[41881]  2  1  2  1  1  1  2  1  2  2  1  2  2  2  2  2  2  2  2  2  1  2  1  1\n[41905]  2  2  2  2  1  2  2  2  1  1  1  2  2  2  1  2  1  2  2  2  2  2  2  1\n[41929]  2  1  2  1  2  2  2  2  1  2  2  1  1  2  2  2  1  1  2  1  2  2  1  1\n[41953]  2  1  2  2  1  2  2  2  1  2  1  2  1  2  1  2  1  2  1  1  2  2  1  2\n[41977]  1  2  2  2  2  3  2  2  2  2  2  1  2  1  2  1  1  1  1  2  1  2  2  1\n[42001]  2  1  2  2  2  2  1  2  2  2  2  2  1  2  2  2  2  1  2  2  2  2  2  2\n[42025]  2  1  2  2  1  1  2  1  2  2  2  1  2  2  1  2  2  2  2  2  2  2  2  2\n[42049]  2  2  2  2  2  2  1  2  2  2  1  2  2  2  2  1  2  2  1  2  2  2  2  2\n[42073]  2  2  1  2  1  1  2  2  1  1  2  2  1  2  2  1  2  2  2  2  1  2  2  1\n[42097]  1  2  2  2  1  2  2  2  2 NA  1  2  2  1  1  2  2  2  1  1  2  2  1  2\n[42121]  1  2  1  1  1  2  2  2  2  2  2  2  2  2  2  2  2  1  1  2  2  2  2  2\n[42145]  1  1  2  1  1  2  2  2 NA  2  1  2  1  2  1  2  2  2  2  2  2  2  2  1\n[42169]  1  1  1  1  2  2  1  2  1  1  1  2  2  1  2  1  2  2  2  1  2  2  2  2\n[42193]  2  2  2  2  1  2  2  2  2  3  2  1  1  1  2  2  1 NA  2  2  2  2  2  2\n[42217]  1  1  2  2  2  2  2  2  2  2  2  2  1  1  2  3  2  2  1  1  3  1  2  2\n[42241]  2  1  2  1  1  2  2  2  1  2  2  2  2  1  2  1  2  2  2  2  1  1  1  1\n[42265]  2  2  2  1  1  2  2  1  2  2  2  2  2  2  1  1  2  1 NA  2  1  2  1  2\n[42289]  1  2  1  1  1  1  1  1  2  2  2  2  1  2  2  2  2  2 NA  1  2  2  1  2\n[42313]  2  2  1  1  2  1  2  1  2  2  2  2  1  2 NA  1  2  2  2  1  2  1  2  2\n[42337]  2  2  1  2  2  2  1  1  2  2  2  1  3  2  1  2  2  2  1  2  2  1  1  2\n[42361]  1  2  2  2  2  1  2  2  1  2  1  2  1  2  2  2  1  2  1  1  2  2  2  3\n[42385]  1  1  2  2  1  2  2  1  2  2  1  1  1  1  2  1  1  2  2  2  2  2  1  2\n[42409]  2  1  2  2  2  2  1  1  1  2  2  1  2  2  2  1  2  2  1  2  2  2  2  1\n[42433]  1  2  2  2  2  2  1  2  1  2  2  1  2  1  2  1  2  1  2  1  1  2  2  1\n[42457]  2  2  1  1  2  2  1  2  1  2  2  2  1  1  1  2  1  1  2  2  1  1  2  2\n[42481]  1  1  2  1  2  1  2  2  2  1  2  2  2  2  1  1  1  1  2  2  1  2  1  2\n[42505]  2  1  2  2  2  1  2  2  2  2  1  1  2  2  2  2  2  1  2  1  2  1  1  2\n[42529]  2  1  1  1  2  2  2  2  1  2  2  1  2  2  2  1  1  1  2  2  2  2  2  1\n[42553]  1  1  1  1  2  1  1  1  1  1  2  1  2  3  1  2  2  1  2  1  2  2  2  2\n[42577]  1  1  1  2  2  2  2  2  2  2  2  1  2  2  2  2  2  2  1  2  2  2  1  1\n[42601]  2  2  2  2  1  2  1  1  2  1  2  2  2  1  1  2  2  2  2  1  2  2  2  1\n[42625]  1  2  2  2  2  1  2  1  1  1  2 NA  1  1  2  1  1  1  1  2  2  1  1  2\n[42649]  1  2  1  2  1  1  2  2  2  2  2  1  1  1  1  1  2  2  2  2  2  2  2  2\n[42673]  2  1  2  2  2  2  2  2  2  2  2  2  1  1  2  2  2  2  1  1  1  2  2  2\n[42697]  2  1  3 NA  2  1  2  2  1  2  1  2  1  2  2  2  2  1  2  2  2  2  1  2\n[42721]  1  1  2  1  2  1  2  2  2  2  2  1  2  1  1  1  2  2  2  2  1  2  2  2\n[42745]  2  2  1  2  1  2  1  2  3  2  2  2  2  2  2  2  2  2  2  1  2  2  2  2\n[42769]  2  2  2  1  2  2  1  2  1  1  2  1  1  1  1  1  2  1  1  2  2  2  2  2\n[42793]  2  2  1 NA  3  1  2  2  1  1  1  1  1  2  1  2  2  1  2  2  2  2  2  1\n[42817]  2  1  2  2  2  2  1  2  2  2  1  3  2  2  1  1  2  2  2  1  2  1  2  2\n[42841]  2  1  1  2  2  2  2  1  2  2  2  2  1  2  2  1  2  2  2  2  1  2  2  2\n[42865]  1  1  1  2  2  2  2  2  1  2  1  2  2  1  2 NA  1  1  1  2  2  1  1  2\n[42889]  1  1  1  1  1  2  1  2  2  1  2  2  1  3  2  1  1  1  2  2  2  1  1  2\n[42913]  2  1  2  2  2  2  1  2  2  1  1  2  2  2  1  2  2  2  1  1  2  2  2  1\n[42937]  1  1  1  2  2  2  1  1  1  1  2  1  1  2  2  1  1  1  2  2  1  2  2  1\n[42961]  2  2  2  2  2  2  2  2  2  2  1  1  1 NA  2  2  1  1  2  2  2  2  2  2\n[42985]  2  2  2  2  2  1  2  1  2  2  2  2  2  1  2  2 NA  2  2  1  2  1  2  2\n[43009]  2  2  2  1  2  2  1  2  2  1  2  2 NA  2  2  2  2  1  1  1  3  2  2  1\n[43033]  2  2  1  1  1  1  2  2  2  1  2  1  1  2  1  1  2  2  2  2  1  1  1  2\n[43057]  2  2  2  1  1  2  2  2  2  2  2  1  2  2  2  1  1  2  2  1  3  1  2  2\n[43081]  1  2  3  2  2  1  2  2  2  1  2  2  1  2  2  2  1  2  2  1  2  2  2  2\n[43105]  1  1  1  2  2  1  2  1  2  2  1  2  2  2  1  2  2  2  2  2  2  2  2  2\n[43129]  2  1  1  1  2  2  2  2  1  2  1  1  1  2  2  1  2  1  1  1  2  1  1  1\n[43153]  1  2  2  1  2  2  2  1  1  2  2  2  2  2  2  2  1  2  2  2  2  1  1  1\n[43177]  1  1  2  1  1  1  2  1  1  2  1  2  1  2  1  2  1  2  1  2  2  2 NA  1\n[43201]  2  2  1  2  2  2  2  3  2  2  1  1  1  2  2  2  2  1  1  2  2  2  1  2\n[43225]  2  1  2  2  2  2  1  2  2  1  1  1  2  1  2  1  2  2  1  1  1  1  2  2\n[43249]  2  2  2  2  1  2  3  2  2  2  2  1  2  1  2  2  1  2  2  1  2  2  1  2\n[43273]  1  2  2  1  2  2  2  2  1  2  2  2  2  2  1  1  2  1  2  1  2  2  1  1\n[43297]  2  2  2  1  2  2  2  2  2  1  2  1  1  2  1  2  2  2  2  1  2  1  2  2\n[43321]  3  2  2  1  1  2  2  1  2  1  2  1  1  2  1  1  2  2  2  1  1  1  1  1\n[43345]  2  2  1  2  2  1  2  2  1  2 NA  1  2  2  1  2  2  2  2  2  1  1  1  1\n[43369]  1  2  1  1  1  2  2  1  2 NA  1  1  1  2  2  2  2  2  2  2  1  2  1  1\n[43393]  2  2  2  2  1  2  2  2  2  2  2  2  1  1  1  2  2  1  2  2  1  2  2  2\n[43417]  2  1  2  2  1  1  2  1  1  2  1  1  1  2  2  2  2  1  2  1  2  2  3  1\n[43441]  1  2  2  2  1  1  1  1  2  1  2  2  2  2  2  2  2  1  2  1  1  1  1  2\n[43465]  2  1  2  2  2  2  1  2  2  2  1  2  1  1  2  1  2  2  2  2  2  2  2  2\n[43489]  1  1  2  2  2  2  2  2  1  2  1  2  2  1  2  2  2  1  2  2  1  1  1  1\n[43513]  2  1  1  1  1  2  2  2  1  2  2  1  2  2  2  2  2  2  2  1  2  2  2  1\n[43537]  2  2  2  2  2  2  2  1  2  2  2  1  2  1  2  2  2  1  1  1  2  2  2  2\n[43561]  1  2  2  2  2  1  2  2  2  2  2  1  1  1  2  2  2  2  3  3  2  1  1  2\n[43585]  2  2  1  2  1  1  2  1  1  1  1  2  2  2  1  1  2  1  2  2  2  2  1  1\n[43609]  2  2  2  1  2  2  2  1  1  2  1  2  2  2  1  1  1  1  2  1  1  1  1  1\n[43633]  2  2  2  1  2  1  2  1  1  2  2  1  1  2  2  2  2  2  2  2  1  2  2  2\n[43657]  2  1  1  2  2  2  1  2  1  1  1  1  1  1  2  2  2  2  2  2  2  2  2  2\n[43681]  2  2  2  2  2  1  2  2  2  1  2  2  1  2  2  2  2  1  2  2  1  2  1  1\n[43705]  2  2  2  2  1  2  2  1  2  2  1  1  1  2  1  1  2  1  1  2 NA  2  1  1\n[43729]  2  2  2  1  2  1  2  1  2  2  2  1  1  2  2  2  2  3  2  1  2  2  2  2\n[43753]  1  1  2  2  2  2  2  1  2  2  2  2  2  1  2  1  1  2  2  2  2  1  1  1\n[43777]  1  1  2  1  2  1  1  1  1 NA  2  2  1  2  2  2  1  2  1  2  2  1  2  2\n[43801]  1  2  1  1  2  1  2  2  2  2  1  2  1  2  1  1  2  1  2  2  2  1  1  1\n[43825]  2  2  2  2  2  2  1  2  1  1  2  1  1  1  2  2  2  2  2  1  2  2  1  2\n[43849]  2  1  2  1  2  2  2  1  1  2  2  2  2  2  1  1  1  2  1  2  2  1  2  1\n[43873]  2  2  1  1  1  2  2  2  2  2  2  2  2  2  1  2  1  2  2  1  2  2  2  1\n[43897]  2  1  2  1  2  1  2  1  1  2  1  2  2 NA  1  1  2  2  2  1  2  2  1  1\n[43921]  2  2  2  2  2  1  1  2  2  2  2  1  1  1  2  2  2  2  2  1  2  2  1  2\n[43945] NA  2  2  2  2  1  1  2  1  2  1  1  1  2  2  2  2  2  2  2  1  2  2  1\n[43969]  2  2  1  1  1  1  1  2  2  2  2  2  2  2  2  2  1  2  2  1  1  2  1  1\n[43993]  2  2  1  1  1  2  2  2  1  2  2  2  2  1  1  1  2  1  1  1  1  2  1  2\n[44017]  1  1  2  2  1  1  2  1  1  1  1  2  2  1  2  2  2  1  1  2  1  1  1  2\n[44041]  2  2  1  1  2  2  1  2  2  1  1  2  2  2  1  1  1  2  1  2  1  1  2  1\n[44065]  1  1  1  2  1  1  2  2  1  1  1  2  2  2  3  2  1  1  1  2  2  1  1  1\n[44089]  2  1  2  2  1  2  2  1  2  2  1  2  2  2  2  1  2  2  2  2  2  1  1  2\n[44113]  2  1  1  2  2  2  1  2  2  2  2  2  2  1  1  1  2  1  1  2  1  2  1  1\n[44137]  2  1  2  1  2  1  2  1  2  2  2  2  1  1  1  2  1  1  1  2  1  2  1  2\n[44161]  2  2  2  2  2  2  1  2  2  2  2  1  1  2  1  1  2  1  2  1  2  1  1  2\n[44185]  2  2  2  2  2  1  2  1  1  1  2  2  2  2  2  2  2  2  2  2  2  2  1  2\n[44209]  1  2  1  2  2  2  1  1  2  2  2  2  2  2  1  1  2  1  2  2  2  3  1  2\n[44233]  1  2  1  1  2  2  1  2  1  2  2  2  2  2  2  2  2  2  2  2  2  1  1  2\n[44257]  2  2  2  1  1  2  2  2  1  2  2  1  2  1  1  2  2  2  1  2  2  2  2  2\n[44281]  1  2  1  2 NA  2  2  2  1  1  1  1  1  1  2  1  1  2  2  2  2  2  1  1\n[44305]  2  1  2  2  2  2  2  2  2  2  1  1  1  2  2  2  1  2  1  2  2  1  2  1\n[44329]  1  1  1  2  2  2  2  1  1  2  2  2  1  1  2  1  3  1  1 NA  2  2  2  2\n[44353]  1  2  2  1  1  2 NA  1  2  1  1  2  1  1 NA  2  2  2  2  2  2  1  2  2\n[44377]  2  2  1  2  1  1  2  2  1  2  1  2  2  1  2  2  2  2  2  2  1  2  2  2\n[44401]  1  1  2  2  1  2  2  1  1  2  2 NA  2  2  1  2  2  2  2  2  1  2  1  2\n[44425]  2  1  2  2  1  2  2  2  1  1  2  2  2  1  2  1  2  2  1  2  2  1  1  1\n[44449]  2  2  2  2  2  1  2  2  1  2  2  2  1  1  1  1  1  1  2  1  2  2  2  2\n[44473]  2  2  1  2  2 NA  2  1  1  1  2  3  2  1  2  2  2  1  2  2  2  1  2  2\n[44497]  2  2  2  1  1  2  2  2  2  1  2  2  2  2  2  1  2  1  2  1  1  1  2  1\n[44521]  2  2  2  2  2  1  1  2  2  2  2  2  1  1  2  1  2  2  2  1  1  1  2  2\n[44545]  2  1  2  2  1  2  1  2  1  2  1  2  2  1  2  2  1  2  2  1  2  2  1  2\n[44569]  1  1  2  1  1  1  1  1  2  2  2  1  1  2  2  1  2  2  2  2  2  2  2  1\n[44593]  1  2  1  2  2  2  1  2  1  1  2  2  2  1  2  1  3  3  2  2  2  2  1  1\n[44617]  1  2  2  1  2  1  1  1  1  1  2  2  2  2  2  2  2  1  1  2  2  2  2  1\n[44641]  2  1  1  2  1  2  1  3  2  2  2  1  2  2  2  2  2  1  1  2  2  2  2  2\n[44665]  2  2  2  2  1  2  1  2  1  1  2  2  2  1  1  1  2  2  2  1  2  1  2  2\n[44689]  1  1  2  3  1  1  2  2  2  1  2  2  2  2  1  2  2  1  2  1  2  2  1  2\n[44713]  2  2  1  1  2  1  1  2 NA  2  1  1  2  2  2  1  2  2  1  2  2  1  2  1\n[44737]  1  1  1  2  1  2  2  1  2  1  1  2  2  2  1  2  2  2  2  2  1  2  2  1\n[44761]  2  2  2  1  2  2  2  2  2  2  3  2  1  2  2  2  2  2  2  2  1  1  2  2\n[44785]  2  2  2  1  2  1  1  2  1  1  1  2  1  2  2  2  1  1  2  1  1  1 NA  2\n[44809]  2  1  2  2  2  2  1  2  1  2  1  1  2  2  1  2  2  2  1  1  1  1  2  2\n[44833]  2  2  2  2  2  1  1  2  2  2  1  1  1  3  1  2  2  2  1  2  2  2  1  2\n[44857]  2  2  2  1  2  2  2  1  2  2  1  2  2  2  2  2  2  1  2  1  1  3  1  2\n[44881]  2  1  2  1  1  1  1  2  2  1  2  1  1  2  1  2  2  2  1  1  1  2 NA  2\n[44905]  2  1  1  2  2  1  1  2  1  2  2  1  2  2  2  1  2  2  2  2 NA  1  2  1\n[44929]  2  2  1  2  2  1  2  1  2  1  2  1  2  2  2  2  2  2  1  2  2  2  3  2\n[44953]  1  2  2  2  2  2  2  2  1  1  1  1  2  2  2  1  2  2  1  2  1  2  2  2\n[44977]  2  2  1  1  2  2  2  2  1  2  2  3  2  1  2  1  1  1  2  2  2  1  1  1\n[45001]  2  2  2  2  2  1  1  2  1  1  1  2  1  1  1  2  1  2  1  2  1  2  2  2\n[45025]  1  2 NA  2  2  2  1  1  1  1  2  2  1  3  2  1  2  1  1  2  2  2  2  2\n[45049]  1  2  1  2  2  2  2  1  1  2  1  2  1  1  2  2  2  2  1  2  1  2  2  1\n[45073]  2  2  2  1  2  2  2  2  2  2  2  1  2  1  2  2  2  2  1  1  2  2  1  2\n[45097]  2  2  2  2  2  2  1  1  1  2  2  1  2  2  2  2  1  1  1  2  1  2  1  2\n[45121]  2  2  1  2  2  2  1  2  2  2  1  2  2  2  2  2  2  2  2  2  2  1 NA  2\n[45145]  2  2  2  1  2  2  2  1  1  2  2  2  2  1  2  2  1  2  2  2  1  1  1  1\n[45169]  2  1  1  2  2  2  2  2  1  1  1  2  2  2  2  1  2  2  2  2  1  2  2  1\n[45193]  2  2  2  2  2  1  2  1  1  2  1  1  1  1  1  1  1  2  2  1  2  1  1  2\n[45217]  2  1  1  1  2  2  2  2  2  2  2  2  2  2  2  1  2  2  2  2  1  1  2  1\n[45241]  2  2  1  1  2  2  1  2  2  2  2  1  1  1  2  2  2  2  2  2  1  2  1  2\n[45265]  2  2  2  1  2  1  1  1  2  2 NA  2  2  2  1  1  2  2  2  1  1  2  1  1\n[45289]  1  1  2  1  1  2  2  2  2  2  1  1  2  2  2 NA  2  1  1  2  2  2  2  2\n[45313]  1  1  2  2  2  1  2  2  2  2  1  2  2  2  2  2  1  2  2  2  2  2  2  2\n[45337]  2  2  2  2  2  1  1  2  1  1  2  2  2  2  1  1  1  2  2  1  2  2  2  2\n[45361]  1  1  1  1  2  1  1  1  1  2  1  1  1  1  1  2  2  2  1  2  2  2  2  2\n[45385]  2  2  2  2  2  2  2  2  1  1  1  1  2  2  1  1  1  1  2  1  1  2  2  1\n[45409]  2  1  2  2  2  1  2  2  1  1  2  1  1  2  2  2  2  1  1  2  2  2  2  2\n[45433]  2  2  1  2  2  1  1  1  2  1  2  2  1  2  1  1  2  2  2  1  2  1  1  3\n[45457]  1  2  2  2  2  1  2  1  2  2  1  2  2  1  2  1  2  1  1  2  2  1  2  2\n[45481]  1  2  2  1  2 NA  1  2  2  2  1  2  2  2  2  2  2  1  2  2  1  1  1  1\n[45505]  2  1  2  1  1  1  2  1  2  2  1  1  1  2  2  1  2  2  2  1  2  2  2  1\n[45529]  2  1  2  2  2  2  1  2  2  2  2  2  2  1  2  2  1  2  2  2  2  2  2  2\n[45553]  2  2  2  2  2  1  2  1  1  2  2  2  1  2  1  1  2  2  1  2  2  2  1  2\n[45577]  1  1  1  2  2  1  1  1  1  2  2  2  1  2  2  1  2  2  1  1  2  2 NA  1\n[45601] NA  2  2  1  2  1  2  2  1  2  2  1  1  2  2  2  2  2  2  1  2  1  2  2\n[45625]  1  1  2  2  2 NA  2  1  2  2  2  2  2  2  2  1  1  2  2  2  1  1  1  1\n[45649]  1  1  1  1  1  2  1  2  1  1  1  2  1  1  1  2  1  2  2  1  2  2  1  2\n[45673]  2  2  2  2  1  2  2  1  2  1  2  1  2  2  1  1  2  2  1  2  2  2  2  2\n[45697]  2  1  2  2  1  2  2  1  2  2  2  1  2  2  2  2  1  2  2  1  1  1  1  1\n[45721]  2  2  2  2  1  1  2  1  2  1  1  2  1  2  2  2  2  2  2  2  2  2  2  2\n[45745]  2  2  1  1  2  2  2  2  1 NA  1  2  2  2  2  1  2  2  1  1  1  1  2  2\n[45769]  2  2  2  2  2  2  2  1  1  2  1  1  2  2  2  1  2  2  2  1  1  1  2  2\n[45793]  1  2  1  2  2  1  1  2  1  2  1  1  2  1  2  1  1  1  1  2  2  2  2  2\n[45817]  1  2  1  2  1  2  2  2  2  2  1 NA  2  2  2  2  1  2 NA  2  1  1  2  1\n[45841]  2  2  2  2  1  2  1  1  2  1  2  2  2  2  2  1  3  2  1  1  1  1  2  2\n[45865]  1  2  2  2  1  2  2  1  2  2  1  2  2  2  2  2  1  1  2  2  1  1  1  2\n[45889]  2  2  2  3  2  2  1  1  1  2  1  2  1  1  2  2  2  2  2  1  2  2  2 NA\n[45913]  1  1  2  1  1  1  2  2  2  2  2  1  1  2  1  2  2  2  2  2  1  1  1  2\n[45937]  2  2  2  3  1  2  2  2  1  1  2  2  1  2  2  2  1  2  2  2  1  2  2  2\n[45961]  2  2  1  1  2  2  2  2  1  2  2  2  2  1  2  1  1  2  1  2  2  2  2  1\n[45985]  2  1  2  2  2  1  1  1  2  1  2  2  1  2  1  2  2  1  2  1  1  2  1  2\n[46009]  1  2  2  2  2  1  2  2  2  2  2  2  2  2  2  1  2  2  1  2  2  2  1  1\n[46033]  2  1  1  2  2  1  1  1  2  1  2  2  2  1  2  1 NA  2  2  2  2  2  1  2\n[46057] NA  2  2  1  2  2  2  1  2  1  2  2  2  1  1  1  2  2  2  1  2  1  2  2\n[46081]  2  2  2  3  2  2  1  2  1  2  1  1  2  2  2  2  2  2  2  2  1  1  2  2\n[46105] NA  2  1  2  1  2  2  1  2  2  1  2  2  1  2  2  2  2  2  2  2  2  2  2\n[46129]  2  1  1  1  2  2  1  2  1  2  2  1  2  1  1  2  2  2  2  1  2  2  2  2\n[46153]  1  2  2  2  1  2  1  2  2  2  1  2  1  1  2  1  2  2  2  3  2  1  1  1\n[46177]  1  1  1  2  2  1  2  1  2  1  1  2  1  2  1  1  2 NA  2  1  2  1  1  2\n[46201]  2  1  2  2  2  2  2  2  2  1  2  1  2  1  1  2  2  1  3  1  2  1  1  2\n[46225]  2  2  1  2  2  2  1  2  1  1  2  1  2  1  2  2  2  2  1  2  2  1  2  2\n[46249]  1  1  2  2  2  2  1  2  2  2  1  1  1  1  1  1  2  2  2  2  1  2  1  2\n[46273]  2  1  2  2  1  2  1  2  2  1  2  2  2  1  2  1  2  2  1  1  2  2  1  1\n[46297]  2  1  1  2  2  2  2  1  2  2  2  1  2  2  1  2  2  2  1  2  2  2  2  2\n[46321]  2  2  1  2  2  1  2  2  1  2  2  2  2  1  1  2  1  2  1  2  2  2  1  1\n[46345]  2  3  2  2  2  2 NA  2  2  2  2  2  1  1  2  1  3  2  2  2  2  2  2  2\n[46369]  1  2  1  2  1  1  2  2  2  2  2  1  1  1  2  2  2  1  2  2  2  1  2  1\n[46393]  1  2  1  2  2  2  2  2  2  3  2  2  1  2  2  1  2  2  2  2  2  2  1  1\n[46417]  1  1  1  2  1  1  1  1  2  2  2  1  1  1  2  1  2  1  1  2  2  2  2  1\n[46441]  2  2  2  1  1  2  2  2  2  1  2  2  1  1  1  2  2  2  2  2  2  2  2  2\n[46465]  1  2  2  1  2  1  1  2  1  2  3  2  2  1  2  2  2  2  2  1  2  2  2  2\n[46489]  1  1  2  2  1  2  1  2  2  2  2  2  2  2  1  1  2  2  2  1  2  2  2  3\n[46513]  1  2  1  1  1  1  1  2  2  2  2  2  1  2  2  2  1  2  1  2  1  2 NA  2\n[46537]  2  1  1  1  1  1  2  2  2  2  2  2  2  2  1  2  1  1  1  2  2  1  1  2\n[46561]  1  2  2  3  2  1  2  2  1  1  2  2  1  2  2  2  2  2  2  2  1  1  1  2\n[46585]  2  1  2  2  1  1  2  1  2  1  1  1  2  1 NA  2  2  1  2  1  1  1  2  2\n[46609]  2  2  1  2  1  2  2  1  2  1  2  2  2  2  1  2  1  2  1  2  1  1 NA  2\n[46633]  2  2  2  2 NA  2  2  2  2  2  1  2  2  1  2  2  2  1  2  1  2  1  2  1\n[46657]  2  2  2  2  2  2  2  2  2  1  2  2  2  1  2  1  1  2  1  2 NA  1  2  2\n[46681]  2  2  2  2  2  2  1  2  2  1  1  2  1  2  1  1  1  2  2  2  2  2  1  2\n[46705]  2  2  1  1  2  1  2  1  2  1  2  1 NA  1  1  1  1  2  1  2  2  2  2  2\n[46729]  2  2  2  2  1  2  1  1  2  2  2  2  1  2  2  2  1  2  2  2  2  2  2  2\n[46753]  2  1  2  2  1  1  2  2  1  1  2  1  1  1  1  1  2  2  2  2  2  2  1  2\n[46777]  1  2  2  2  2  1  1  2  1  1  1  2  1  2  2  2  2  2  2  2  2  2  2  2\n[46801]  2  1  2  1  1  1  2  2  2  3  2  2  2  1  3  2  2  1  1  2  2  2  1  2\n[46825]  2  2  2  2  1  2  2  1  2  2  2  2  2  1  2  2  2  1  2  2  1  1  2  1\n[46849]  2  1  2  1  2  1  2  2  1  2  2  2  1  2  2  2  1  1  2  2  2  2  2  2\n[46873]  2  1  1  1  2  2  2  1  2  1  1  2  1  2  1  2  2  2  1  2  2  2  2  2\n[46897]  2  1  2  1  2  1  2  2  2  1  1  1  2  2  2  1  1  1  2  2  2  1  1  1\n[46921]  1  2  1  1  2  2  1  2  2 NA  2  2  2  1  1  1  2  2  1  2  2  2  2  1\n[46945]  2  2  1  2  2  2  1  2  1  2  2  2  1  2  1  1  2  2  2  1  2  2  1  2\n[46969]  2  1  2  2  2  2  2  2  2  1  2  1  2  1  2  2  2  2  1  2  2  2  2  1\n[46993]  1  1  1  2  2  1  1  1  2  2  2  2  2  1  2  1  1  2  2  2  1  1  2  2\n[47017]  2  2  2  2  2  1  2  1  2  1  2  2  2  2  1  2  2  2  1  1  1  2  2  2\n[47041]  2  1  2  2  2  2  2  2  2  1  1  1  2  2  2  1  1  2  2  2  2  2  1  1\n[47065]  2  2  1  2  2  2  2  2  1  2  1  2  2  2  2  2  2  2  2  2  2  1  1  2\n[47089]  2  2  2  2  2  2  2  2  2  2  1  2  1  2  2  1  2  2  1  2  1  2  2  2\n[47113]  1  2  1  2  1  2  2  2  1  2  2  1  2  2  2  2  2  2  2  2  2  1  2  2\n[47137]  1  1  2  1  2  2  2  1  2  2  2  2  2  1  2  1  2  1  2  1  2  2  2  2\n[47161]  2  2  2  2  1  2  1  2  2  2  2  2  2  2  2  2  2  1  2  2  2  2 NA  1\n[47185]  2  2  2  1  2  1  2  2  2  2  2  2  3  2  2  2  1  2  1  2  2  2  1  1\n[47209]  2  1  1  2  2  2  2  2  1  1  2  1  2  2  1  2  2  1  1  2  2  2  2  2\n[47233]  1  1  1  1  2  1  2  1  1  2  2  2  2  1  1  1  1  2  2  2  2  2 NA  2\n[47257]  1  2  1  2  2  2  2  2  2  1  2  1  1  2  2  1  2  1  2  1  2  2  2  1\n[47281]  2  2  1  1  2  1 NA  1  2  1  3  2  2  2  1  2  1  2  1  3  1  2  2  1\n[47305]  3  1  2  1 NA  1  2 NA  2  2  1  2  2  2  2  2  1  2  2  2  2  2  2  2\n[47329]  2  2  2  2  1  2  2  2  1  2  1  2  1  2  2  1  2  2  2  2  1  1  2  2\n[47353]  2  1  2  1  1  2  2  2  2  1  1  1  2  2  2  1  2  2  2  2  1  1  2  1\n[47377]  1  2  1  2  1  1  1  1  2  1  1  2  1  1  2  2  1  2  2  2  1  2  2  1\n[47401]  1  1  2  2  2  2 NA  2  1  2  1  1  2  2  1  2  1  2  1  1  2  1  2  2\n[47425]  2  1  2  2  2  1 NA  1  2  1  2  2  1  1  2  2  1  2  2  1  1  2  2  1\n[47449]  2  2  1  2  2  2  2  2  1  2  2  2  2  1  1  2  2  1  1  1  2  1  2  2\n[47473]  2  1  1  2  2  1  2  2  2  1  1  2  1  1  2  1  2  2  2  2  2  2  2  2\n[47497]  2  2  2  1  2  2  1  1  2  2  1  1  2  2  2  2  2  2  2  1  2  1  2  2\n[47521]  2  1  2  2  2  1  2  1  2  1  2  2  1  1  2  2  2  2  1  1  1  1  2  2\n[47545]  1  2  2  1  2  2  2  2  2 NA  2  2  2  2  2  2  2  1  1  2  2  2  2  2\n[47569]  1  2  1  2  2  2  2  1  1 NA  3  1  1  2  1  2  2  2  2  2  1  2  1  2\n[47593]  1  1  1  2  1  3  2  2  1  2  2  1  2  1  2  2  1  2  2  2  2  2  2  2\n[47617]  2  2  2  1  2  2  1  2  3  2  2  2  2  2  1  2  1  2  2  1  2  1  2  1\n[47641]  2  1  2  2  2  2  2  1  2  2  2  2  1  1  2  2  2  2  2  2  2  1  1  2\n[47665]  1  1  2  2  2  1  2  1  1  1  1  1  2  2  2  2  1  1  1  2  2  2  2  1\n[47689]  2  1  2  2  2  2  1  2  2  1  2 NA  2  2  2  2  2  1  2  2  2  2  1  2\n[47713]  2  1  2  1  1  2  2  2  2  1  2  2  2  2  2  1  1  2  1  2  1  2  1  2\n[47737]  2  2  2  2  2  2  2  2  1  2  2  2  2  2  2  1  1  2  1  2  2  2  1  2\n[47761]  1  2  2  1  2  2  1  1  1  2  2  2  2  2  1  1  2  2  1  1  2  1  2  1\n[47785]  1  2  2 NA  2  2  2  1  2  2  1  2  1  2  2  1  2  2  1 NA  2  1  2  3\n[47809]  2  2  1  2  1  2  1  1  2  1  2  2  2  1  2  2  3  2  2  2  2  1  2  1\n[47833]  2  2  2  2  2  1  1  2  1  2  2  1  2  1  2  1  1  2  2  1  2 NA  2  2\n[47857]  2  2  2  2  2  2  2  2  2  1  1  1  2  2  2  1  2  2  2  2  2  2  2  1\n[47881]  1  2  2  1  2  2  1  2  2  1  1  2  2  2  2  2  1  2  1  1  2  1  2  2\n[47905]  2  2  2  2  2  2  2  2  1  2  2  1  2  2  2  2  1  1  2  1  2  1  2  2\n[47929]  1  1  2  2  2  1  2  1  2  2  2  1  2  2  2  2  1  2  1  1  2  1  1  2\n[47953]  2  1  2  2  1  2  1  2  2  2  1  2  1  2  1  3  1  1  1  2  1  2\n\nselfes$genF &lt;- as.factor(selfes$gender)\nlevels(selfes$genF) &lt;- c(\"Male\", \"Female\", \"Other\")\nplot(selfes$genF, col = 'black', xlab = \"Gender (Self-Reported)\")\n\n\n\n\n\n\n\nsummary(selfes$genF)\n\n  Male Female  Other   NA's \n 17801  29182    529    462 \n\n\nChallenge Problem (Optional, but Encouraged!) Report the mean and standard deviation of self-esteem for people who are identified as female, male, and ‚Äúother‚Äù in the dataset.\nNote : there are MANY ways to do this in R; you can use the subset function or indexing to divide the dataset into three groups - ‚Äúfemales‚Äù, ‚Äúmales‚Äù, and people who reported ‚Äúother‚Äù. Or other fancier methods we will talk about later. See how many different ways you can do it.\n\nnames(selfes)\n\n [1] \"Q1\"      \"Q2\"      \"Q3\"      \"Q4\"      \"Q5\"      \"Q6\"      \"Q7\"     \n [8] \"Q8\"      \"Q9\"      \"Q10\"     \"gender\"  \"age\"     \"source\"  \"country\"\n[15] \"SELFES\"  \"genF\"   \n\n## The indexing approach, broken up into clear steps.\nselfesF &lt;- selfes[selfes$genF == \"Female\",] # creating new datasets for each group\nselfesM &lt;- selfes[selfes$genF == \"Male\",]\nselfesO &lt;- selfes[selfes$genF == \"Other\",]\n\npar(mfrow = c(1,3)) # checking my work; should see only one bar per graph\nplot(selfesF$genF)\nplot(selfesM$genF)\nplot(selfesO$genF) # yep!\n\n\n\n\n\n\n\nmean(selfesF$SELFES, na.rm = T)\n\n[1] 2.574468\n\nmean(selfesM$SELFES, na.rm = T)\n\n[1] 2.731565\n\nmean(selfesO$SELFES, na.rm = T)\n\n[1] 2.258706\n\nsd(selfesF$SELFES, na.rm = T)\n\n[1] 0.6921987\n\nsd(selfesM$SELFES, na.rm = T)\n\n[1] 0.6956196\n\nsd(selfesO$SELFES, na.rm = T)\n\n[1] 0.7222484\n\n## indexing another way\nmean(selfes[selfes$genF == \"Female\", ]$SELFES, na.rm = T) # as one line of code.\n\n[1] 2.574468\n\n## the subset function\nsF &lt;- subset(selfes, genF == \"Female\")\nmean(sF$SELFES, na.rm = T) # another way.\n\n[1] 2.574468\n\n## the tapply function.\ntapply(selfes$SELFES, selfes$genF == \"Female\", mean, na.rm = T) # another way\n\n   FALSE     TRUE \n2.717905 2.574468 \n\n## and then there's tidyverse...\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nsF2 &lt;- group_by(selfes, selfes$genF)\nsF2 %&gt;% summarise(mean(SELFES, na.rm = T))\n\n# A tibble: 4 √ó 2\n  `selfes$genF` `mean(SELFES, na.rm = T)`\n  &lt;fct&gt;                             &lt;dbl&gt;\n1 Male                               2.73\n2 Female                             2.57\n3 Other                              2.26\n4 &lt;NA&gt;                               2.61\n\n## feel like I'm forgetting an obvious one? Point is there are a LOT of ways to do this in R, and it matters less how you do it than getting the correct answer."
  },
  {
    "objectID": "gradstats/gradlabs/2Lab_DataDescription_KEY.html#in-lecture.",
    "href": "gradstats/gradlabs/2Lab_DataDescription_KEY.html#in-lecture.",
    "title": "Psych 205 - Lab 2 - Prof.¬†Key",
    "section": "",
    "text": "One advantage of Quarto (and R Markdown) is that you can run code in a document. You do this using a ‚Äúcode block‚Äù. In the space below, insert an R code block, type out a math equation that used to give you difficulty as a kid into the code block below, and run the code to see the result. Below the code block, add text for humans to determine whether R got the math problem correct or not. Then render the document as a .pdf and .html file. Did this work?\nLoad the ‚Äúgrad onboarding‚Äù survey into the code block below, and answer the following questions. Note : to successfully render code in the document, you must a) explicitly load the dataset into your Quarto document and b) make sure you have no errors in your code. :)\n\nGraph the variables d$self.skills and d$class.skills side by side using the par() function. Change the formatting of the graph to make it look ready for presentation. Add vertical lines to each graph to illustrate the mean (solid line) and standard deviation (dashed lines).\nBelow each graph, report the mean and standard deviation of both variables, and interpret what these statistics tell you about the individuals in our class. (Who cares? What do these statistics tell us?)\n\n\n\nd &lt;- read.csv(\"~/Downloads/grad_onboard_SP25.csv\", stringsAsFactors = T, na.strings = \"\")\npar(mfrow = c(1,2))\n\nhist(d$self.skills, breaks = c(0:5), \n     col = 'black', bor = 'white', main = \"Computer Skills\\n(Self-Perceptions)\")\nabline(v = mean(d$self.skills), lwd = 4, col = 'red')\nabline(v = mean(d$self.skills) + sd(d$self.skills), \n       lwd = 2, lty = \"dashed\", col = 'red')\nabline(v = mean(d$self.skills) - sd(d$self.skills), \n       lwd = 2, lty = \"dashed\", col = 'red')\n\nhist(d$class.skills, breaks = c(0:5),\n     col = 'black', bor = 'white', main = \"Computer Skills\\n(Perceptions of Classmates)\")\nabline(v = mean(d$class.skills), lwd = 4, col = 'red')\nabline(v = mean(d$class.skills) + sd(d$class.skills), \n       lwd = 2, lty = \"dashed\", col = 'red')\nabline(v = mean(d$class.skills) - sd(d$class.skills), \n       lwd = 2, lty = \"dashed\", col = 'red')\n\n\n\n\n\n\n\n\nI see that students rated their classmates as having higher computer skills (mean = 3.88) than they rated themselves (mean = 3.88. There was also more variation in people‚Äôs self-perceptions (sd = 1.15) than perceptions of the class 0.89. One possibility is that people were coming up with a less differentiated stereotype about the average other student‚Äôs skill, but had access to more information when making their own self-perception of skill.\n\nSplit your graphics window into a 2x5 grid, and graph each of the 10 ‚Äúcan.*‚Äù variables in the dataset (e.g., ‚Äúcan.import‚Äù, ‚Äúcan.clean‚Äù, etc.). Make sure each graph contains the name of the variable and that the graph looks good / is intelligible. Note : you can and should use a for-loop to do this! Then, report the frequencies of these variables - what are some things you observe about the data? Does this make sense given what you know about the participants?\n\nd[,7:16] &lt;- lapply(d[,7:16], factor, levels = c(\"No\", \"Maybe\", \"Yes\"))\n\npar(mfrow = c(2,5), cex = .5)\ngraph.names &lt;- c(\"Can Import Data\", \"Can Clean Data\", \"Can Graph a Variable\", \"Can Render Markdown\", \"Can Define LM\",\n                 \"Can Interpret a LM\", \"Can Interpret NHST\", \"Can Differentiate \\nSD and SE\", \"Can Interpret 95% CI\", \"Can For-Loop\")\nfor(i in c(7:16)){\n  plot(d[,i], main = paste(graph.names[i-6]))\n}"
  },
  {
    "objectID": "gradstats/gradlabs/2Lab_DataDescription_KEY.html#on-your-own",
    "href": "gradstats/gradlabs/2Lab_DataDescription_KEY.html#on-your-own",
    "title": "Psych 205 - Lab 2 - Prof.¬†Key",
    "section": "",
    "text": "Psychologists often want to combine information from multiple questions (that measure the same construct) into one variable. There are lots of different ways to do this, but the first that we will talk about is just the humble average. For example, the Rosenberg (1965) self-esteem scale has 10 questions all related to self-esteem; rather than work with 10 different variables, it might be nice to just average these 10 and report one average number. These are called ‚Äúlikert‚Äù scales. It‚Äôs technically pronounced ‚Äòlick-ert‚Äô, but no one says that because it sounds kind of gross.\n\nHere‚Äôs an overview (from my 101 class notes) of how to work with such likert scales conceptually and computationally (in R). You‚Äôll need to load the psych library into R; to do this, run the following code in your console.\ninstall.packages(\"psych\") # installs the \"psych\" package. you only need to do this once.\nlibrary(psych) # loads the library. you need to do this every R session.\nUse these notes to answer the following questions. Let me know if anything is unclear!\nUse the self-esteem dataset (from the class datasets folder). You should find a codebook that describes these data in the same folder.\nCheck to make sure the data loaded correctly. (Note that 0s in this dataset mean the person was missing data.) Report the sample size of the dataset.\n\nselfes &lt;- read.csv(\"../datasets/Self-Esteem Dataset/data.csv\",\n                   stringsAsFactors = T,\n                   na.strings = \"0\", sep = \"\\t\")\nhead(selfes) # checking to make sure it loaded okay\n\n  Q1 Q2 Q3 Q4 Q5 Q6 Q7 Q8 Q9 Q10 gender age source country\n1  3  3  1  4  3  4  3  2  3   3      1  40      1      US\n2  4  4  1  3  1  3  3  2  3   2      1  36      1      US\n3  2  3  2  3  3  3  2  3  3   3      2  22      1      US\n4  4  3  2  3  2  3  2  3  3   3      1  31      1      US\n5  4  4  1  4  1  4  4  1  1   1      1  30      1      EU\n6  4  4  1  3  1  3  4  2  2   1      2  25      1      CA\n\nsummary(as.factor(selfes$Q1)) # making sure zeros got turned into NAs\n\n    1     2     3     4  NA's \n 3011  8647 21018 15200    98 \n\nnrow(selfes) # sample size\n\n[1] 47974\n\n\nCreate a self-esteem scale from the 10-items. Make sure to reverse-score the negatively-keyed items so that for every question, higher numbers measure higher self-esteem. Graph this variable as a histogram, and make the graph look nice (ready for publication).\n\nnames(selfes) # \n\n [1] \"Q1\"      \"Q2\"      \"Q3\"      \"Q4\"      \"Q5\"      \"Q6\"      \"Q7\"     \n [8] \"Q8\"      \"Q9\"      \"Q10\"     \"gender\"  \"age\"     \"source\"  \"country\"\n\nposkey.df &lt;- selfes[,c(1:2,4,6,7)] # pos-keyed items (from the codebook)\nnegkey.df &lt;- selfes[,c(3,5,8:10)] # neg-keyed items (from the codebook)\nnegkeyR.df &lt;- 5-negkey.df # reverse scoring the neg-keyed items\nSELFES.DF &lt;- data.frame(poskey.df, negkeyR.df) # bringing it all 2gether.\n\nlibrary(psych) # loading the library\nalpha(SELFES.DF) # alpha reliability.\n\n\nReliability analysis   \nCall: alpha(x = SELFES.DF)\n\n  raw_alpha std.alpha G6(smc) average_r S/N     ase mean  sd median_r\n      0.91      0.91    0.92      0.52  11 0.00058  2.6 0.7     0.52\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.91  0.91  0.91\nDuhachek  0.91  0.91  0.91\n\n Reliability if an item is dropped:\n    raw_alpha std.alpha G6(smc) average_r  S/N alpha se  var.r med.r\nQ1       0.90      0.90    0.91      0.51  9.5  0.00064 0.0089  0.51\nQ2       0.91      0.91    0.91      0.52  9.7  0.00063 0.0085  0.52\nQ4       0.91      0.91    0.91      0.53 10.3  0.00061 0.0081  0.53\nQ6       0.90      0.90    0.90      0.50  9.2  0.00067 0.0087  0.51\nQ7       0.90      0.90    0.91      0.51  9.3  0.00066 0.0089  0.51\nQ3       0.90      0.90    0.91      0.51  9.3  0.00066 0.0094  0.51\nQ5       0.90      0.91    0.91      0.52  9.6  0.00065 0.0098  0.51\nQ8       0.91      0.91    0.92      0.54 10.7  0.00059 0.0064  0.54\nQ9       0.90      0.91    0.91      0.52  9.6  0.00065 0.0085  0.52\nQ10      0.90      0.90    0.90      0.51  9.3  0.00067 0.0086  0.51\n\n Item statistics \n        n raw.r std.r r.cor r.drop mean   sd\nQ1  47876  0.76  0.77  0.75   0.70  3.0 0.87\nQ2  47658  0.73  0.74  0.71   0.66  3.1 0.79\nQ4  47751  0.66  0.68  0.62   0.59  2.9 0.81\nQ6  47809  0.81  0.81  0.79   0.75  2.6 0.92\nQ7  47758  0.79  0.79  0.77   0.74  2.4 0.93\nQ3  47751  0.79  0.79  0.76   0.73  2.7 0.95\nQ5  47781  0.76  0.76  0.72   0.69  2.6 0.98\nQ8  47797  0.64  0.63  0.56   0.54  2.3 0.96\nQ9  47728  0.76  0.75  0.73   0.69  2.2 0.99\nQ10 47772  0.81  0.80  0.78   0.74  2.4 1.07\n\nNon missing response frequency for each item\n       1    2    3    4 miss\nQ1  0.06 0.18 0.44 0.32 0.00\nQ2  0.04 0.13 0.50 0.33 0.01\nQ4  0.05 0.21 0.50 0.24 0.00\nQ6  0.14 0.33 0.37 0.17 0.00\nQ7  0.18 0.34 0.35 0.14 0.00\nQ3  0.13 0.28 0.37 0.22 0.00\nQ5  0.14 0.32 0.32 0.22 0.00\nQ8  0.21 0.41 0.24 0.14 0.00\nQ9  0.27 0.40 0.20 0.14 0.01\nQ10 0.24 0.33 0.22 0.22 0.00\n\nselfes$SELFES &lt;- rowMeans(SELFES.DF, na.rm = T) # creating the scale\nhist(selfes$SELFES, col = 'black', bor = 'white', # the graph\n     main = \"Histogram of Self-Esteem\", \n     xlab = \"Self-Esteem Score\", breaks = 15)\n\n\n\n\n\n\n\n\nReport the alpha reliability, mean, and standard deviation of this variable. Below your graph, describe what these statistics tell you about the self-esteem of the participants. What other questions do you have about this variable, or the output?\nThen, graph the variable gender as a categorical factor, and report the number of people who identified as ‚Äúfemale‚Äù, ‚Äúmale‚Äù, and ‚Äúother‚Äù. You will need to do some data cleaning here.\n\nselfes$gender\n\n    [1]  1  1  2  1  1  2  1  1  1  2  1  2  2  1  1  1  2  1  1  2  2  2  1  1\n   [25]  1  2  1  1  1  2  1  1  1  2  1  2  2  2 NA  2  1  2  1  1  2  1  2  1\n   [49]  1  2  1  1  1  3  1  2  1  1  2  1  2  1  2  1  1  1  1  2  1  1  2  2\n   [73]  2  2  1  2  1  1  2  1  1  1  2  2  1  2  2  2  2  1  1  1  2  1  1  2\n   [97]  2  2  1  2  2  2  2  1  2  2  2  2  2  1  2  1  2  1  1  2  2  2  2  1\n  [121]  1  2  2  1  2  1  2  2  1  2  1  1  1  1  2  1  1  2  1  2  1  1  2  1\n  [145]  2  1  2  2  2  2  2  2  1  1  1  2  2  1  2  2  2  2  1  2  1  1  1  1\n  [169]  1  1  1  1  2  2  1  1  2  1  1  1  2  2  1  1  2  1  2  2  1  1  2  2\n  [193]  2  1  1  1  1  2  2  1  2  2  2  2  2  2  2  1  2  1  1  2  2  2  2  1\n  [217]  1  2  1  1  2  2  1  1  2  1  2  2  2  1  2  1  1  2  2  1  1  2  1  2\n  [241]  1  2  2  1  2  2  1  1  2  1  2  1  2  1  2  2  2  2  1  2  2  1  2  2\n  [265]  2  1  2  2  2  2  1  2  2  1  1  1  2  1  2  1  2  2  2  1  1  1  2  1\n  [289]  2  2  2  1  1  1  2  2  2  2  2  1  2  1  2  1  1  1  2  1  1  1  1  1\n  [313]  1  2  2  2  2  2  2  2  1  2  1  1  2  1  2  1  2  1  1  1  2  2  1  1\n  [337]  2  1  1  2  2  2  1  2  2  1  2  2  2  1  2  2  2  1  1  1  1  2  1  2\n  [361]  2  2  1  2  2  2  2  2  2  2  2  1  2  1  2  1  2  2  2  2  2  1  1  1\n  [385]  1  1  2  2  1  1  1  1  1  2  1  2  1  1  2  2  2  2  2  1  1  1  1  2\n  [409]  1  1  1  1  2  1  1  2  2  1  1  2  1  1  1  1  2  1  2  1  2  2  2  1\n  [433]  2  1  2  2  1  2  2  2  2  2  2  1  1  1  1  2  2  2  2  2  1  1  1  2\n  [457]  1  2  2  2  2  1  1  1  1  1  1  2  2  2  1  2  2  2  2  1  2  2  1  1\n  [481]  1  2  1  2  2  2  1  1  2  2  1  1  1  2 NA  1  1  1  1  1  1  1  2  1\n  [505]  2 NA  1  2  1  2  1  2  2  1  2  2  2  1  2  2  2  2  2  2  1  2  2  2\n  [529]  2  2 NA  1  2  2  1  2  2  1  1  2  2  2  2  2  2  1  2  2  2  2  2  1\n  [553]  2  2  2  2  2  1  1  2  2  2  2  2  2  2  2  2  1  2  1  2  2  2  2  1\n  [577]  1  1  2  2  2  2  2  2  2  2  2  2  2  1  1  2  2  2  2  1  2  2  2  1\n  [601]  2  2  2  2  2  2  2  2  2  2  1  1  2  1  2  1  2  2  2  1  1  2  2  1\n  [625]  1  2  2  2  1  2  2  1  1  2  2  1  2  1  1  1  1  1  1  2  1  2  2  1\n  [649]  2  1  1  2  1  2  1  2  2  2  1  2  2  1  2  1  2  2  2  1  1  2  1  2\n  [673]  1  1  1  2  2  2  2  2  2  1  2  2  1  2  1  1  2  2  1  1  1  1  1  1\n  [697]  1  1  1  1  2  1  2  2  2  2  2  1  1  2  2  1  2  1  2  2  1  1  2  1\n  [721]  1  1  2  2  2  2  3  2  2  2  1  2  1  1  2  1  1  2  1  2  1  1  1  1\n  [745]  2  2  1  1  1  1  1  1  1  1  2  2  2  2  2  2  2  2  2  2  1  1  1  2\n  [769]  1  2  2  2  1  1  1  2  1  1  2  1  1  2  2  2  2  1  2  1  2  2  2  2\n  [793]  1  1  2  2  1  1  2  1  2  1  2  1  2  1  1  2  2  2  2  2  1  2  2  2\n  [817]  1  2  2  2  2  1  1  1  1  2  2  2  1  2  2  1  1  2  2  1  1  2  2  1\n  [841]  2  1  2  1  1  2  1  1  2  1  2  2  1  2  2  2  1  2  2  2  1  1  2  1\n  [865]  1  2  1  1  2  1  2  2  1  2  2  2  1  2  2  2  1  1  2  1  1  2  1  2\n  [889]  1  2  1  2  1 NA  1  2  1  2  2  2  2  1  1  2  1  1  1  2  2  2 NA  1\n  [913]  2  2  2  1  2  1  1  2  1  3  1  2  2  1  2  2  2  1  2  1  2 NA NA  2\n  [937]  2  2  1  1  1  1  1  2  2  2  1  2  2  2  1  1  1  2  2  2  2  2  2  2\n  [961]  2  1  2  2  1  2  2  2  1  1  2  2  2  1  1  1  1  2  1  2  1  2  2  1\n  [985]  1  1  2  1  2  1  1  1  1  1  2  1  1  3  2  2  1  2  1  1  2  1  2  1\n [1009]  1  1  2  2  1  2  1  2  1  1  2  2  2  2  1  2  1  1  1  2  1  1  2  2\n [1033]  2  2  1  1  2  1  2  2  2  1  2  1  2  2  1  2  2  2  1  1  2  2  2  1\n [1057]  1  2  2  2  1  2  1  2  2  2  2  2  1  1  2  1  2  2  2  1  2  2  2  2\n [1081]  2  2  2  2  1  1  2  1  1  2  2  1  2  1  1  2  2  2  1  2  1  2  2  2\n [1105]  1  2  2  1  2  2  2  1  1  1  1  1  1 NA  1  2  2  2  1  1  1  2  2  1\n [1129]  1  2  1  2  2  2  2  2  2  2  1  1  2  1  1  2  2  1  1  1  2  2  2  2\n [1153]  2  2  2  1  2  1  2  1  2  2  2  2  2  1  2  2  1  2  1  1  1  2  1  1\n [1177]  1  1  1  2  2  1  2  1  1  2  1  2  2  2  2  1  2  2  1  1  2  1  1  1\n [1201]  2  2  1  2  2  2  2  2  2  2  1  1  1  1  1  2  2  1  2  1  2  1  1  1\n [1225]  1  2  1  1  1  2  2  1  2  2  2  2  2  2  2  2  2  2  2  2  2  1  1  2\n [1249]  2  2  1  2  2  1  1  2  2  2  2  1  1  1  2  1  1  2  1  1  2  2  2  1\n [1273]  2  1  2  1  2  2  1  2  2  1  2  1  2  2  1  2  1  2  2  1  2  2  2  2\n [1297]  1  1  2  2  1  2  2  2  2  2  2  2  2  2  2  2  2  2  1  2  2 NA  1  1\n [1321]  1  1  2  2  1  1  2  2  1  2  2  2  2  2  3  2  2  1  1  2  2  1  1  1\n [1345]  2  2  1  3  1  1  1  1  2  1  2  1  2  1  2  2  1  1  2  1  1  2  2  2\n [1369]  2  2  2  2  1  2  2  1  2  2  2  2  1  1  2  1  2  1  2  1  2  1  1  2\n [1393]  2  2  2  1  2  2  2  2  2  2  1  2  2  2  1  2  1  2  2  1  1  2  2  1\n [1417]  1  2  2  1  1  2  1  1  2  1  1  2  1  1  1  1  2  1  2  1  2  1  1  2\n [1441]  1  2  1  1  2  1  2  2  2  2  2  2  2  1  2  1  2  1  1  1  1  1  2  2\n [1465]  2  1  1  1  2  1  1  2  1  1  2  2  2  2  2  1  2  1  2  2  1  1  2  2\n [1489]  2  2  2  2  1  1  1  1  1  1  1  2  2  1  1  2  1  1  2  1  1  1  2  2\n [1513]  2  2  2  2  1  1  2  1  2  2  2  2  1  1  1  1  2  2  2  1  1  2  1  1\n [1537]  1  1  1  1  1  2  2  2  2  2  1  2  2  2  2  2  1  1  1  1  1  2  1  1\n [1561]  2  2  2  1  2  1  2  2  2  1  2  1  2  2  1  2  2  2  2  2  1  1  2  1\n [1585]  2  2  2  1  2  2  1  1  1  2  2  2  2  2  2  1  2  2  1  2  1  1  2  2\n [1609]  2  2  1  2  3  2  2  2  1  1  2  1  1  2  1  1  1  2  1  1  1  1  1  1\n [1633]  1  1  1  2  1  1  1  1  1  1  1  1  1  1  1  1  3  1  1  1  1  1  1  1\n [1657]  1  1  1  1  1  1  1  1  1  1  2  1  1  1  1  1  1  1  2  1  1  1  1  1\n [1681]  1  1  1  1  1  2  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n [1705]  1  1  1  1  1  1  1  1  1  1  1  1  1  2  1  1  1  1  1  1  1  1  1  1\n [1729]  3  1  1  2  1  1  2  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n [1753]  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  2  2  1  1  1  1\n [1777]  2  1  1  1  1  2  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n [1801]  2  2  1  1  1  1  3  2  2  1  1  1  2  1  2  1  2  1  2  1  2  1  1  1\n [1825]  2  2  2  2  2  1  1  2  2  2  2  2  1  1  2  1  2  2  1  2  1  1  1  2\n [1849]  2  1  2  2  2  1  2  2  2  2  2  2  1  1  1  1  1  1  1  1  2  2  1  1\n [1873]  1  1  1  1  2  2  2  1  2  1  1  1  2  2  2  1  1  1  1  1  1  2  2  2\n [1897]  1  2  1  1  1  1  1  1  1  1  2  2  1  1  2  2  1  2  1  1  2  2  1  1\n [1921]  2  2  2  2  1  2  1  1  1  2  2  1  1  2  2  2  1  1  1  1  2  2  2  2\n [1945]  1  1  2  1  1  2  2  1  2  2  1  2  2  1  2  2  1  2  2  2  2  2  1  1\n [1969]  2  1  2  1  2  1  2  1  2  1  2  2  1  2  1  2  1  2  2  1  1  1  2  2\n [1993]  2  2  1  2  2  1  2  2  1  2  2  2  2  2  1  2  1  1  1  2  1  2  2  2\n [2017]  2  1  2  1  1  2  1  1  1  2  1  1  1  2  2  2  1  2  1  1  1  2  2  2\n [2041]  1  1  2  1  1  2  1  2  2  2  1  2  1  1  2  2  1  2  2  2  2  2  2  1\n [2065]  2  1  1  2  2  2  2  1  1  1  1  2  2  1  2  2  2  2  2  1 NA  2  2  2\n [2089]  2  2  2  2  1  2  1  2  1  1  2  2  1  1  2  1  2  1  2  2  2  2  1  2\n [2113]  1  1  1  1  1  1  1  1  2  1  1  2  1  1  1  1  2  2  2  2  1  2  2  1\n [2137]  2  1  2  2  2  2  2  2  2  1  1  2  1  1  1  1  1  1  1  1  1  1  2  2\n [2161]  1  1  1  1  2  1  1  1 NA  1  1  1  2  1  2  1  1  1  2  1  2  1  1  1\n [2185]  2  1  1  2  1  1  1  1  1  2  2  1  1  1  1 NA  2  1  2  2  1  1  1  1\n [2209]  1  2  2  2  1  2  1  2  1  2  1  1  1  2  1  1  2  1  1  1  2  2  1  1\n [2233]  2  1  1  1  2  1  2  1  1  1  2  2  2  1  1  1  1  1  2  2  1  2  2  1\n [2257]  1  2  1  2  2  1  1  1  1  2  2  1  1  1  2  1  2  1  1  1  2  1  1  1\n [2281]  1  2  1  2  2  1  2  1  1  2  2  1  1  1  2  1  1  1  1  1  2 NA  1  2\n [2305]  1  1  1  2  2  1  1  1  2  1  2  1  2  1  1  2  1  1  2  2  1 NA  2  2\n [2329]  2  1  2  1  2  1  1  1  1  2  1  2  2  1  1  1  2  1  2  2  2  1  1  1\n [2353]  1  1  1  2  2  1  2  2  2  1  1  2  2  1  2  1  1  1  1  2  1  2  1  1\n [2377]  1  1  1  2  1  1 NA  1  1  2  1  1  1  1  1  1  1  1  1  1  1  2  2  1\n [2401]  1  2  1  2  1  2  2  1  1  1  1  2  1  1  2  2  2  1  1  2  1  1  2  1\n [2425]  2  2  1  2  1  1  1  1  1  2  2  2  1  2  1  2  2  1  1  1  2  1  2  1\n [2449]  2  2  2  1  2  1  2  2 NA  1  1  2  2  1  2  1  2  2  2  1  1  2  1  2\n [2473]  1  2  1  1  1  1  1  1  1  2  2  2  2  2  2  2  1  2  2  1  1  1  2  1\n [2497]  1  1  2  1  1  2  1  2  1  1  2  2  2  2  2  2  2  1  2  2  1  2  2  2\n [2521]  1  2  1  2  2  2  1  1  2  3  2  2  2  2  1  1  2  2  2  2  2  2  2  2\n [2545]  2  1  1  2  2  2  2  1  1  1  2  1  1  1  2  2  2  2  1  1  2  1  2  1\n [2569]  2  1  2  1  1 NA  2  2  1  1  2  2  1  1  2  2  1  2  1  1  1  2  1  1\n [2593]  1  2  2  2  1  2  2  2  1  2  1  2  1  2  2  1  2  2  2  1  1  2  1  1\n [2617]  1  1  2  2  1  2  2  2  1  2  2  1  2  1  1  1  1  2  2  2  2  2  2  2\n [2641]  1  2  3  1  1  2  1  1  1  2  1  2  1  1  1  2  2  2  2  1  1  2  2  1\n [2665]  1  2  1  2  1  1  1  2  2  2  2  1  2  2  1  1  2  2  2  2  1  2  2  2\n [2689]  1  2  1  2  1  2  2  2  2  2  1  2  2  2  2  2  2  2  1  2  1  2  1  2\n [2713]  2  2  2  2  2  2  2  1  2  1  2  3  1  1  1  2  1  2  2  2  1  2  1  2\n [2737]  2  2  2  2  1  1  1  2  1  2  2  2  2  1  1  2  1  1  2  2  2  2  2  2\n [2761]  1  2  2  1  1  2  2  2  1  2  2  1  1  2  2  1 NA  1  1  1  2  1  2  1\n [2785]  1  2  1  1  2  1  1  1  2  2  2  2  2  1  1  1  2  2  2  1  2  1  1  2\n [2809]  2  2  1  1  2  2  2  2  1  1  1  1  2  1  2  1  1  1  2  1  2  1  1  1\n [2833]  1  2  2  2  2  1  1  2  1  2  2  2  2  1  1  2  2  1  1  2  1  2  2  2\n [2857]  1  1 NA  1  2  1  1  1  2  2  1  1  1  1  2  2  2  2  2  2  2  1  1  1\n [2881]  2  2  2  1  2  2  2  1  1  2  1  2  1  1  1  2  1  1  1 NA  2  2  1  1\n [2905]  1  1  2  1  2  2  1  1 NA  1  1  1  1  2  2  2  2  2  1  2  1  2  1  2\n [2929]  1  1  1  1  2  1  1  2  2  2  2  2  2  1  2  2  2  2  2  2  2  1  1  2\n [2953]  2  1  2  2  2  2  1  1  1  2  2  2  2  1  2 NA  2  2  1  2  1  2  1  2\n [2977]  1  1  1  1  2  2  1  1  1  2  1  1  1  1  2  1  2  1  1  1  2  2  2  2\n [3001]  1  2  1  1  2  1  2  1  2  2  1  2  2  1  2  1  2  2  1  2  2  2  2  2\n [3025]  2  2  1  1  1  2  1  1  1  1  1  2  2  1  1  2  2  1  2  1  1  2  2  1\n [3049]  2  2  1  2  1  1  2  2  2  1  3  1  1  2  2  2  1  1  2  2 NA  2  2  1\n [3073]  1  1  2  1  1  2  2  2  1  1  1  1  2  1  2  1  1  1  1 NA  2  1  2  2\n [3097]  2  2  1  1  2  1  1  1  1  1  2  2  1  2  2  2  2  1  2  2  2  1  2  1\n [3121]  2  1  2  1  2  1  2  2  2  2  2  2  1  1  2  1  2  2  1  2  2  2  2  2\n [3145]  2  1  1  2  2  2  1  1  1  2  2  2  1  2  1  2  2  3  2  1  1  2  2  1\n [3169]  1  2  2  1  2  2  2  2  1  2  1  1  1  2  2  1  1  1  2  1  1  1  1  1\n [3193]  1  1  1  2  1  2  2  2  1  2  2  1  1  1  1  2  1  1  1  1  2  2  1  2\n [3217]  2  2  2  2  2  2  1  2  1  1  1  2  1  1  1  1  2  3  2  1  2  2  2  2\n [3241]  2  1  2  2  2  1  2  1  2  3  2  2  2  2  2  1  2  2  2  2  2  2  1  2\n [3265]  1  2  1  1  2  2  2  1  2  1  2  2  2  1  2  1  2  1  2  2  2 NA  1  1\n [3289]  2  1  2  1  2  2  2  2  2  2  2  1  2  2  1  3  2  2  2  2  1  2  2  2\n [3313]  1  2  2  2  2  1  2  1  1  1  1  2  2  1  1  2  2  1  2  2  2  2  2  2\n [3337]  1  2  2  2  2  1  2  2  2  1  2  2  2  1  1  2  1  1  1  2  2  1  1  2\n [3361]  2  1  1  2  2  1  2  2  2  2  2  1  2  1  1 NA  1  1  1  2  1  1  1  2\n [3385]  1  1  2  2  1  2  2  1  2  2 NA  2  2  2  1  3  2  2  1 NA  2  2  2  2\n [3409]  2  1  2  2  2  2  2  1  2  2  1  2  2  1  2  1  2  1  1  2  2  2  2  2\n [3433]  1  2  2  2  2  1  2  2  1  2  2  2  2  2  1  1  2  2  2  2  2  1  1  2\n [3457]  2  2  1  1  1  2  1 NA  2  1  2  2  2  2  1  1  2  2  2  2  2  2  1  2\n [3481]  2  2  1  2  1  1  2  2  2 NA  2  2  2  2  1  2  1  2  2  2  1  2  1  1\n [3505]  2  1  2  2  2  2  2  1  1  1  2  2  2  2  1  1  2  2  2  2  2  2  1  2\n [3529]  2  1  1  2  2  2  1  2  2  2  2  2  2  2  2  2  2  1  2  2  2  2  1  2\n [3553]  2  2  2  2  1  2  2  2  2  2  2  2  1  1  1  2  1  1  2  1  1  1  2  2\n [3577]  2  2  1  1  2  2  2  1  1  2  2  2  2  1  2  2  2  1  2  1  2  2  2  1\n [3601]  2  2  2  2  2  2  2  2  1  2  2  2  2  1  2  2  2  2  2  1  2  2  2  1\n [3625]  2  2  2  2  1 NA NA NA  1  2  1  2  1  1  2  2  1  2  2  2  2  2  1  1\n [3649]  1  2  2  1  1  1  1  1  1  1  2  1  2  1  1 NA  2  1  1  1  2  1  2  2\n [3673]  2  2  2  2  1  1  2  2  1  2  2  2  1  1  2  2  2  1  1  1  2  2  2  1\n [3697]  2  2 NA NA  2  2  2  2  2  2  2  2  1  2  2  1  2  2  2  2  1  1  1  2\n [3721]  2  2  1  1  2  2  1  2  1  2  2  1  1  2  2  2  1  1  1  2  2  2  2  1\n [3745]  2  1  2  2  1  2  2  2  2  2  2  1  2  1  2  2  1  1  2  1  2  1  1  1\n [3769]  2  1  1  1  2  2  2  2  2  2  2  1  2  2  2  1  2  2  2  1  1  2  2  3\n [3793]  1  2  2  1  1  1  1  2  1  2  3  1  1  2  2  2  2  1  2  1  2  2  1  2\n [3817] NA  2  2  2  1  2  1  1  1  1  2  2  2  2  1  2  1  2  2  2  1  2  2  2\n [3841]  2  1  1  1  2  2  2  2  1  2  1  2  2  1  2  1  2  1  1  2  2  1  2  2\n [3865]  2  1  2 NA  2  2  1  1  2  2  2  2  2  1  1  2  2  2  1  1  1  1  1  2\n [3889]  2  1  2  2  1  1  2  2  2  1  2  2  2  2  2  1  1  2  3  1  1  1  2  2\n [3913]  2  2  2  2  1  2  2  2  2  1  1  3  1  1  2  1  2  1  2  1  2  2  1  1\n [3937]  2  1  2  2  1  1  2  1  2  1  2  1  1  2  2  2  1  2  2  1  2  1  1  1\n [3961]  2  1  2  2  1  2  1  2  2  1  2  2 NA  2  2  2  1  2  2  1  2  2  1  1\n [3985]  2  2  1  1  1  2  3  2  1  2  2  1  2  1  1  2  2  2  2  2  2  1  1  2\n [4009]  1  2  2  1  1  2  1  2  1  2  1  1  1  2  1  1  1  1  2  2  1  1  1  1\n [4033]  1  2  2  1  1  2  1  1  1  1  2  2  2  2  2  2  2  1  2  2  1  2  1  1\n [4057]  2  2  2  1  1  2  1  1  2  2  2  1  2  2  1  2  1  2  1  2  2  2  2  2\n [4081]  1  2  1  2  2  1  2 NA  2  2  1  1  2  1  3  1  2  2  2  2  2  1  2  1\n [4105]  2  2  1  1  1  1  2  2  1  1  2  2  2  1  2  2  1  2  1  1  1  2  1  2\n [4129]  1  1  2  1  1  2  2  1  2  1  2  2  1  2  2  2  2  1  2  1  2  1  2  1\n [4153]  2  2  1  2  1  1  1  1  1  1  2  2  2  1  2  1  1  2  2  2  1  2  2  2\n [4177]  2  2  2  2  2  1  3  1  2  1  2  2  2  2  2  2  2  2  2  1  2  2  2  2\n [4201]  2  2  2  1  2  1  1  1  2  2  2  1  2  1  2  1  2  1  2  2  2  2  2  1\n [4225]  2  2  2  2  1  1  2  2  2  1  2  1  2  2  2  2  2  1  2  1  1  2  1  2\n [4249]  2  2  2  2  1  1  1  1  1  2  2  2  2  2  2  2  2  2  1  2  1  2  2  2\n [4273]  1  2  2  2  1  2  2  2  2  1  2  2  2  2  2  1  2  2  1  1  1  2  2  1\n [4297]  1  1  1  2  1  2  2  1  1  2  2  1  2  2  2  2  1  1  2  2  2  2  2  2\n [4321]  1  2  2  2  2  2  2  2  2  2  2  1  2  2  2  1  2  1  1  1  2  2  2  2\n [4345]  2  2  1  2  1  2  2  2  1  1  1  2  2  1  2  2  1  1  2  1  1  1  2  1\n [4369]  1  1  2  2  1  1  2  1  2  2  2  2  1  1  1  3  1  2  2  2  2  2  2  2\n [4393]  1  2  2  2  1  1  1  1  1  2  2  2  1  1  2  1  1  2  2  1  1  2  1  1\n [4417]  2  2  1  1  1  2  1  2  1  1  1  1  1  2  1  2  2  1  2  2  2  2  2  1\n [4441]  1  1  2  2  2  1  1  2  1  2  2  2  2  1  1  1  1  2  2  1  1  2  1  2\n [4465]  1  2  2  2  1  2  2  2  1  2  2  2  2  1  2  1  2  1  1  1  2  1  1  2\n [4489]  2  1  2  1  1  1  1  1  1  1  2  2  2  1  2  1  1  2  2  2  2  1  1  1\n [4513]  2  1  2  2  2  1  3  2  2  2  2  2  1  2  2  1  1  1  1  1  2  1  1  1\n [4537]  1  2  1  2  1  2  2  3  2  2  2  2  1  2  2  1  2  2  2  1  1  1  2  2\n [4561]  1  2  2  1  2  1  1  1  1  2  1  1  2  1  2  2  2  2  1  1  1  2  3  1\n [4585]  2  1  2  2  2  2  1  2  1  1  2  2  2  2  1  1  1  2  2  1  1  1  1  1\n [4609]  1  2  2 NA  1  2  2  2  2  2  2  1  2  2  2  1  1  2  2  2  1  2  2  1\n [4633]  1  1  1  2  1  1  2  1  2  2  1  1  2  1  2  1  1  1  1  2  1  1 NA  1\n [4657]  1  1  1  1  2  1  2  2  1  1  2  2  2  2  2  2  1  1  2  1  1  2 NA  2\n [4681]  2  1  2  2  1  2  2  2  2  2  2  2  1  2  2  1  2  1  1  2  1  2  2  2\n [4705]  2  2  2  1  2  2  2  2  1  1  2  2  1  1  2  1  2  2  2  2  2  1  1  1\n [4729]  2  2  1  2  2  2  2  1  1  1  1  1  1  2  1  1  2  1  1  1  2  2  1  2\n [4753]  2  2  1  2  1  1  1  2  2  1  2  2  1  1  1  2  1  1  2  2  2  1  2  1\n [4777]  2  2  2  1  1  2  1  1  2  2  1  1  2  2  2  1  2  1  1  2  2  2 NA  1\n [4801]  1  1  2  2  2  1  1  1  1  1  2  2  1  2  2  2  2  1  2  1  2  1  2  2\n [4825]  1  2  2  1  2  2  1  1  1  2  1  2  1  1  1  2  2  1  2  1  2  1  2  2\n [4849]  3  2  2  1  2  1  1  2  2  1  2  2  2  2  2  1  1  1  2  2  2  2  2  1\n [4873]  1  2  1  1  1  2  2  2  1  1  2  2  2  2  2  1  2  1  1  2  1  3  1  2\n [4897]  2  1  1  2  2  1  1  1  2  2  1  2  2  1  2  1  1  2  2  1  2  2  1  1\n [4921]  1  1  2  2  1  1  1  2  2  1  1  1  1  1  2  1  2  2  1  2  1  2  1  2\n [4945]  1  1  2  1  2  1  1  1  2  2  2  1  1  1  2  1  1  2  1  1  2  3  2  1\n [4969]  1  1  2  2  1  2  2  1  2  2  2  1  2  2  2  2  1  1  1  1  2  2  2  2\n [4993]  2  1  1  2  2  2  2  2  1  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2\n [5017]  2  1  2  2  2  2  1  2  2  2  2  1  2  1  2  1  2  2  2  2  1  2  1  1\n [5041]  2  2  1  2  2  1  1  2  2  1  2  1  1  2  1  1  2  2  2  2  1  2  2  2\n [5065]  1  2  2  1  2  2  1  2  2  2  2  2  1  1  1  1  2  2  1  2  1  2  1  2\n [5089]  2  1  1  2  2  2  2  1  1  1  2  1  1  1  1  2  2  2  2  2  2  2  1  2\n [5113]  1  1  2  2  2 NA  2  2  1  1  1  2  1  1  1  2  2  2  2  1  1  1  1  2\n [5137]  1  2  2  2  2  2  1  2  2  1  2  1  2  2  1  1  2  1  2  1  2  1  2  1\n [5161]  1  1  1  2  1  2  1  1  2  1  2  2  2  2  1  1  1  1  2  1  2  2  2  2\n [5185]  1  1  2  2  2  2  2  1  2  2  1  2  1  2  1  2  1  2  2  2  1  1  1  1\n [5209]  2  2  1  1  1  2  2  1  2  1  1  2  2  1  2  1  1  2  1  1  2  2  2  2\n [5233]  2  1  1  1  1  2  1  2  2  1  1  1  1  2  1  2  1  2  1  2  1  2  2  1\n [5257]  1  2  2  2  1  2  2  1  1  2  2  1  1  2  2  1  2  1  2  2  2  1  2  1\n [5281]  1  2  2  3  2  2  2  1  1  1  2  2  2  1  1  1  1  2  2  2  1  1  1  2\n [5305]  1  2  2  1  2  2  1  1  2  2  1  2  1  2  2  2  1  2  1  1  1  2  2  2\n [5329]  2  2  2  2  2  2  3  1  1  1  2  2  2  1  2  2  2  2  2  2  2  1  2  2\n [5353]  2  2  2  2  2  2  2  2  2  1  3  2  2  1  3  2  2  1  2  2  2  3  1  2\n [5377]  2  2  2  2  1  2  2  3  2  2  2  1  2  2  3  2  2  2  2  2  1  1  2  2\n [5401]  2  2  2  2  2  2  2  2  2  2  2  1  2  2  1  2  1  3  2  2  2  2  2  2\n [5425]  2  1  2  1  1  3  2  2  2  2  2  2  2  2  2  2  2  2  1  2  2  2  2  2\n [5449]  2  2  2  2  2  2  2  2  2  2  2  2  2  2  1  2  2  3  2  2  2  2  1  2\n [5473]  2  2  2  1  2  2  2  2  1  2  2  2  1  2  2  2  2  1  2  2  2  3  2  3\n [5497]  2  2  2  2  3  1  2  2  2  2  2  2  1  2  2  2  2  1  2  2  2  2  2  2\n [5521]  2  2  2  2  2  2  1  2  2  1  2  2  2  2  2  2  1  2  2  2  2  2  1  2\n [5545]  1  2  2  2  2  2  2  2  2  2  2  2  2  2  1  2  2  2  2  3  2  2  2  1\n [5569]  2  1  2  2  3  2  2  2  2  2  1  2  2  2  2  2  2  2  2  1  2  2  2  1\n [5593]  2  2  2  1  1  2  2  2  3  2  2  2  1  2  2  2  3  2  2  2  2  1  2  1\n [5617]  2  2  2  2  2  2  1  1  1  2  2  2  1  2  2  2  2  2  2  2  2  2  3  2\n [5641]  1  2  2  2  2 NA  2  2  2  2  2  2  2  2  2  2  2  1  1  2  1  2  2  2\n [5665]  2  2  2  1  2  2  2  2  2  1  2  2  2  1  2  2  2  2  2  2  2  2  2  2\n [5689]  1  1  3  2  2  2  2  2  2  2  2  3  2  3  2  2  2  2  2  2  2  3  2  2\n [5713]  2  2  2  1  3  2  2  2  1  2  2  2  1  2  2  2  2  2  2  1  2  2  1  2\n [5737]  1  2  2  2  2  1  2  2  2  3  1  2  2  2  2  1  2  2  2  1  2  2  2  1\n [5761]  2  2  3  2  2  2  2  1  2  2  2  2  2  2  2  2  2  1  2  2  2  2  1  1\n [5785]  1  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  1  2\n [5809]  2  2  2  1  2  2  2  2  1  2  2  2  2  2  1  2  1  2  2  2  2  2  2  2\n [5833]  2  2  1  1  3  2  1  2  2  2  2  2  1  2  2  1  1  2  2  2  1  2  1  2\n [5857]  2  2  2  2  2  1  2  2  2  2  2  2  3  2  2  2  2  2  2  2  2  2  2  2\n [5881]  1  1  2  2  2  1  2  3  2  1  2  2  2  2  1  2  2  2  2  2  2  2  1  1\n [5905]  3  2  1  2  2  3  1  1  1  2  2  2  2  2  2  2  2  2  1  1  2  1  1  1\n [5929]  2  1  2  2  1  2  2  2  1  2  1  2  1  2  2  2  2  2  1  1  2  2  1  1\n [5953]  2  1  1  2  1  1  1  1  2  2  2  1  2  2  2  2  2  2  2  2  2  2  2  2\n [5977]  2  1  2  2  1  2  2  1  1  2  2  1  2  2  2  2  2  2  1  2  2  2  2  2\n [6001]  2  2  1  2  2  2  2  2  2  3  2  2  3  2  1  2  2  2  2  2  2  2  1  2\n [6025]  2  2  2  1  2  3  2  1  2  2  1  2  2  2  2  2  2  2  2  2  2  2  1  1\n [6049]  1  1  2  2  1  2  3  2  2  2  2  2  2  1  2  1  2  2  2  1  2  2  2  2\n [6073]  2  2  2  2  2  3  2  2  2  2  2  1  2  2  2  2  2  2  2  2  1  1  2  2\n [6097]  2  2  2  2  2  2  2  2  2  1  1  3  2  2  2 NA  3  3  2  2  2  2  1  2\n [6121]  2  2  1  2  1 NA NA  2  2  2  2  2  2  2  2  2  1  1  3  2  2  2  1  2\n [6145]  2  1  2  2  1  2  1  1  2  2  2  2  2  2  2  2  2  2  3  2  2  2  2  2\n [6169]  2  2  2  2  2  1  2  1  2  2  2  2  2  2  2  2  2  2  2  2  1  2  2  1\n [6193]  2  2  2  2  2  1  2  2  2  2  2  2  1  3  2  3  1  2  2  2  2  2  2  2\n [6217]  3  3  2  2  2  2  1  1  1  1  2  1  2  2  2  2  2  2  1  1  1  2  2  1\n [6241]  2  2  2  1  3  2  2  2  1  2  1  1  2  2  1  2  2  2  1  1  2  1  1  2\n [6265]  2  2  3  2  2  1  1  2  2  2  2  1  2  1  2  1  2  1  2  2  2  1  1  2\n [6289]  2  1  2  1  2  2  1  1  2  1  2  2  1  2  1  2  2  2  1  2  1  2  2  3\n [6313]  2  1  2  2  2 NA  1  2  2  2  1  1  1  2  2  2  1  2  2  2  3  2  1  2\n [6337]  3  2  1  2  2  3  2  1  2  2  2  2  2  1  2  2  2  2  2  2  2  2  2  2\n [6361]  1  1  3  2  1  2  2  2  2  1  2  1  2  2  3  2  2  2  2  2  1  2  2  2\n [6385]  2  2  2  2  2  2  2  2  2  2  2  2  2  2  1  2  2  1  2  2  2  1  1  2\n [6409]  2  2  2  2  2  1  2  2  2  2  2  2  2  2  2  1  2  1  2  2  2  1  1  1\n [6433]  1  2  2  2  2  2  1  1  2  2  2  2  1  2  2  2  2  2  2  1  2  2  1  2\n [6457]  1  2  2  2  2  2  3  2  2  2  1  1  2  1  2  1  2  2  2  1  1  1  2  1\n [6481]  1  2  2  2  1  2  1  1  1  1  1  1  1  1  1  1  2  2  2  2  2  1  2  2\n [6505]  2  1  2  2  2  2  2  1  1  2  2  1  2  2  2  1  2  1  1  2  2  2  2  2\n [6529]  2  2  2  2  1  1  2  1  2  1  2  1  2  2  2  2  2  2  1  2  2  2  2  2\n [6553]  1  1  1  2  1  1  2  2  2  1  1  2  2  1  1  1  1  1  2  2  2  1  1  2\n [6577]  1  1  2  2  1  1  1  1  2  1  1  2  2  1  2  1  1  2  1  1  2  2  2  2\n [6601]  1  2  2  1  2  2  2  2  2  1  2  1  2  2  2  1  1  1  2  1  1  1  1  2\n [6625]  1  1  1  1  1  1  1  2  2  2  2  1  1  1  2  2  2  1  1  2  2  2  2  2\n [6649]  2  2  2  1  2  2  2  2  2  2  2  2  2  2  2  1  2  1  2  2  2  2  2  2\n [6673] NA  1  2  2  2  1  1  1  1  1  1  2  2  2  2  1  1  2  1 NA  2  3  1  2\n [6697]  1  2  2  2  2  2  2  2  2  1  1  2  2  1  1  2  2  1  2  2  2  1  2  2\n [6721]  2  2  1  2  2  2  2  2  1  1  2  1  1  1  1  2  1  2  2  2  2  1  2  2\n [6745]  2  1  1  2  2  2  2  2  1  2  2  2  2  2  1  1  2  2  1  2  2  1  1  2\n [6769]  1  2  2  2  2  1  2  3  2  2  2  1  2  2  2  2  2  2  2  2  2  2  2  2\n [6793]  2  1  1  1  1  1  1  2  1  2  1  1  1  3  2  1  1  2  2  2  2  1  1  2\n [6817]  1  2  2  2  2  2  1  2  1  1  1  1  2  2  2  2  2  2  2  1  1  1  2  2\n [6841]  2  2  2  2  1  2  1  2  1  2  2  2  2  1  2  2  2  1  2  2  2  1  2  2\n [6865]  2  1  2  2  1  2  1  1  2  1  1  2  1  2  2  1  2  1  2  1  2  1  1  1\n [6889]  2  2  2  1  1  1  2  1  1  1  1  1  1  2  2  1  1  2  1  2  1  1  2  1\n [6913]  1  1  1  2  1  1  2  1  1  1  1  2  2  1  1  1  2  1  1  1  2  2  1  1\n [6937]  1  2  3  1  2  2  1  1  2  2  2  1  2  1  2  2  2  1  1  2  2  2  1  1\n [6961]  2  2  2  2  2  2  2  2  2  2  2  2  1  2  1  2  2  1  2  2  1  1  2  2\n [6985]  2  2  2  1  2  2  2  2  2  2  1  2  2  2  2  2  2  2  2  2  2  2  2  2\n [7009]  1  2  2  2  1  2  1  2  1  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2\n [7033]  2  2  2  2  2  1  2  1  2  2  2  2  2  2  2  1  2  1  2  2  1  2  1  2\n [7057]  1  2  2  2  2  1  1  2  2  2  1  2  2  2  2  2  1  1  2  1  2  1  1  1\n [7081]  1  2  2  1  2  2  2  2  1  1  2  1  2  2  2  2  2  2 NA  2  2  2  1  1\n [7105]  2  2  2  2  1  2  2  2  2  2  2  2  2  2  1  3  2  2  2  2  2  2  1  1\n [7129]  1  1  2  2  2  2  2  2  2  1  2  2  1  2  2  2  1  1  2  2  2  2  2  2\n [7153]  2  3  2  1  1 NA  2  2  1  2  1  2  1  1  2  2  2  2  2  1  1  2  1  2\n [7177]  2  1  2  2  2  2  1  1  2  2  2  2  1  1  2  2  2  2  1  2  1  2  2  2\n [7201]  2  2  1  2  1  1  2  2  1  2  2  2  2  1  2  1  3  2  2  2  2  2  2  2\n [7225]  2  2  1  1  2  2  3  1  1  1  1  2  1  2  2  2  2  2  2  2  2  1  1  2\n [7249]  2  2  2  1  2  2  2  2  2  2  1  1  1  2  1  1  2  1  2  2  1  1  2  2\n [7273]  2  2  2  2  1  2  2  2  2  2  2  2  1  2  1  1  2  1  2  2  2  1  2  2\n [7297]  2  2  2  2  2  2  1  1  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2\n [7321]  2  1  2  2  2  2  2  1  1  1  2  2  1  2  2  1  1  1  1  2  2  2  1  1\n [7345]  2  2  3  2  2  2  2  2  2  2  2  2  2  2 NA  2  2  1  2  1  2  2  2  2\n [7369]  2  2  2  2  2  2  1  2  2  1  2  1  1  2  2  2  2  2  2  2  1  1  2  1\n [7393]  2  2  1  2  2  1  2  2  2  1  2  2  2  1  2  2  2  1  2  1  1  2  1  1\n [7417]  2  1  2  2  2  1  2  2  1  2  2  1  1  2  3  2  1  2  1  1  1  1  2  2\n [7441]  2  1  1  1  2  2  2  1  2  1  2  1  2  2  2  2  2  1  2  2  2  3  1  2\n [7465]  2  2  1  2  2  2  2  3  2  2  2  2  2  1  2  2  2 NA  2  2  2  2  1  2\n [7489]  2  1  2  2  1  2  2 NA  2  2  1  2  2  2  2  2  2  2  2  1  2  2  2  2\n [7513]  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  1  2  1  2  2  2  2\n [7537]  1  1  2  3  2  2  2  2  2  2  2  3  2  2  2  2  2  2  2  1  2  2  2  2\n [7561]  2  2  1  2  2  2  2  2  2  2  2  2  2  2  2  2  2  3  2  2  2  2  2  2\n [7585]  2  1  1  3  2  2  2  2  2  2  1  1  2  2  2  2  2  2  2  2  2  2  3  2\n [7609]  1  2  2  2  2  2  2  2  2  2  2  2  2  1  2  2  1  2  2  2  2  2  1  2\n [7633]  1  2  2  2  2  2  2  2  2  2  2  2  3  2  2  2  2  2  2  1  2  2  2  1\n [7657]  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  1  1  2  1  2  1  2  1\n [7681]  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2\n [7705]  2  2  2  2  2  2  2  1  2  1  2  2  2  2  2  2  2  1  2  2  2  2  2  2\n [7729]  1  2  2  2  2  2  2  3  1  1  2  2  2  3  2  2  1  1  2  1  3  2  2  2\n [7753]  2  2  2  3  2  2  2  2  2  2  1  2  1  2  2  1  2  1  2  2  2  1  2  2\n [7777]  2  2  2  2  2  2  1  2  2  2  2  1  2  3  2  2  2  2  2  2  2  2  2  2\n [7801]  2  1  2  2  2  2  2  2  1  2  2  2  2  2  2  2  1  2  2  2  2  2  2  1\n [7825]  2  2  2  2  2  2  2  1  1  2  2  1  2  2  2  2  2  1  2  2  2  2  2  2\n [7849]  2  2  2  2  1  2  1  2  2  2  2  2  2  2  2  2  1  2  2  2  2  3  2  1\n [7873]  2  2  2  2  2  1  2  2  2  2  2  2  2  2  2  2  1  2  3  2  2  2  2  2\n [7897]  2  2  2  2  2  1  1  2  2  2  3  1  1  2  2  2  1  2  2  2  2  2  2  2\n [7921]  2  2  2  2  2  2  2  2  1  2  2  1  2  2  2  2  2  2  2  2  2  2  2  2\n [7945]  2  2  2  1  2  2  2  1  2  1  2  2  1  2  1  1  1  1  2  2  2  2  2  3\n [7969]  1  1  1  2  2  2  2  2  2  2  1  2  1  2  2  1  2  2  2  1  3  2  2  1\n [7993]  2  2  2  2  2  2  2  2  1  1  2  2  2  2  2  1  2  2  2  3  2  2  2 NA\n [8017]  2  2  1  2  2  2  1  1  2  2  2  2  2  2  1  2  1  2  2  1  2  2  1  2\n [8041]  2  2  1  2  2  2  2  1  2  2  3  2  2  2  2  2  1  2  2  2  2  2  1  2\n [8065]  2  1  2  3  2  1  2  1  2  2 NA  2  1  2  2  1  1  2  2  2  1  1  2  2\n [8089]  1  2  2  2  2  2  2  1  3  1  2  1  1  1  1  2  1  2  1  2  2  2  2  1\n [8113]  2  2  2  2  1  2  1  1  2  2  2  2  3  2  2  2  2  2  2  2  1  2  2  2\n [8137]  2  3  1  1  1  2  2  1  2  2  2  2  2  2  1  2  2  2  1  2  2  2  2  2\n [8161]  1  2  1  2  2  1  1  2  2  2  2  2  2  1  2  2  1  2  2  2  2  2  2  1\n [8185]  1  1  1  1  2  2  1  2  2  2  2  2  2 NA  2  1  1  2  1  2  2  2  2  2\n [8209]  1  2  2  2  1  1  1  1  2  1  2  2  2  2  2  2  1  2  1  2  2  2  2  1\n [8233]  2  1  2  1  1  2  2  2  1  2  2  1  2  2  2  1  2  2  2  2  1  2  2  2\n [8257]  2  2  2  1 NA  1  2  1  2  2  2  2  2  2  2  1  2  2  1  1  2  2  2  2\n [8281]  2  2  2  2  2  2  2  2  1  1  2  2  2  2  2  2  2  2  1  3  1  2  2  2\n [8305]  2  2  1  1  1  2  2  2  2  2  2  1  2  2  1  2  2  2  2  2  2  1  2  2\n [8329]  2  2  1  2  2  1  2  1  2  2  2  2  1  1  2  1  1  2  2  1  2  2  1  2\n [8353]  2  2  1  2  2  3  2  2  2  1  1  1  2  2  2  1  1  2  1  1  1  2  2  2\n [8377]  1  2  2  1  2  2  2  2  1  2  2  1  1  2  2  1  2  2  2  2  2  1  2  2\n [8401]  2  2  2  2  2  2  2  2  2  2  1  2  2  1  2  1  2  1  2  1  2  2  2  1\n [8425]  1  2  2  2  1 NA  1  2  2  2  2  2  2  1  2  2  1  2  1  1  1  2  2  1\n [8449]  2  2  2  2  1  2  2  1  2  2  1  1  2  2  2  1  2 NA  1  2  1  2  2  2\n [8473]  2  2  1  1  1  2  1  1  1  1  2  2  1  1  2  2  2  2  2  1  1  2  1  1\n [8497]  1  2  1  2  2  2  2  1  2  2  1  2  1  2  1  2  2  1  1  1  1  1  2  1\n [8521]  2  1  2  1  2  1  1  1  2  1  2  1  1  1  1  2  1  2  2  2  2  2  2  1\n [8545]  2  1  1  2  1  2  1  1  1  2  1  1  1  2  2  1  2  2  2  2  1  2  2  2\n [8569]  1  2  1  2  1  2  1  1  1  2  2  2  1  3  2  2  1  2  1  1  1  2  2  1\n [8593]  2  2  2  1  2  1  2  2  1  2  1  1  2  1  2  2  1  2  2  2  2  2  2  1\n [8617]  2  1  2  2  1  2  2  2  1  1  2  2  2  2  1  2  2  1  1  2  2  2  2  2\n [8641]  1  1  2  1  2  1  1  2  2  1  1  1  1  2  2  1  2  2  1  1  1  2  1 NA\n [8665]  1  1  1  1  2  3  2  2  2  2  2  1  1  2  2  2  1  2  2  2  2  2  1  3\n [8689]  1  2  2  2  3  1  2  2  1  1  1  1  2  2  2  3  2  1  1  2  1  2  2  2\n [8713]  2  1  1  1  2  2  2  1  2  1  1  1  2  2  2  1  2  1  2  2  1  1  2  2\n [8737]  2  1  2  1  2  1  2 NA  1  2  1  2  2  1 NA  1  2  1  2  1  2  1  1  2\n [8761]  2  2  2  1 NA  2  2  1  2  1  2  1  2  1  2  2  2  2  2  1  2  2  2  2\n [8785]  2  2  2  1  2  2  2  1  2  1  1  2  2  1  2  1  2  1  2  2  1  1  1  1\n [8809]  2  1  2  2  2  2  2  1  2  2  1  1  2  1  1  2  2 NA  2  2  2  1  3  2\n [8833]  2  2  1  1  2  2  2  1  2  3  2  2  2  1  1  2  2  1  2  3  1  1  2  1\n [8857]  2  2  1  2  1  1  2  1  2  2  1  2  1  1  1  1  1  1  2  2  1  1  2  2\n [8881]  2  2  1  1  2  2  2  2  2  1  1  2  2  2  1  2  2  2  2  2  2  2  1  2\n [8905]  2  2  1  1  2  1  2  1  1  2  2  1  2  2  1  2  2  1  2  2  1  1  2  2\n [8929]  3  2  2  2  1  2  1  2  2  2  2  2  1  1  2  2  1  2  1 NA  2  2  2  1\n [8953]  1  2  1  1  1  1  2  1  1  1  1  1  2  1  1  2  1  2  2  1  2  1  2  2\n [8977]  2  2  1  1  1  2  1  2  1  2  2  1  1  2  2  2  1  2  1  2  2  2  1  2\n [9001]  1  2  2  2  2  2  1  2  2  2  1  1  1  1  2  3  2  1  2  2  1  2  1  1\n [9025]  1  2  2  1  1  1  1  2  2  2  2  1  1  1  2  1  2  2  1  2  2  2  2  2\n [9049]  2  2  1  2  1  1  2  2  1  2  1  1  1  2  2  1  2  1  2  2  1  2  2  2\n [9073]  1  1  1  2  2  2  1  2  1  1  1  2  1  1  2  1  2  2  2  1  2  2  2  2\n [9097]  2  2  1  1  1  2  1  2  2  2  2  2  2  2  2  1  1  1  2  1  2  1  1  1\n [9121]  2  2  1  1  1  1  2  1  2  1  1  2  2  2  2  2  1  1  1  2  1  1  1  1\n [9145]  2  1  2  2  2  2  1  1  1  1  1  1  2  1  2  1  1  1  2  1  1  1  1  1\n [9169]  1  1  2  1  1  1  2  2  1  1  1  1  1  1  2  1  2  1  2  1  1  1  2 NA\n [9193]  2  2  2  2  2  3  1  2  2  2  2  1  2  2  1  2  2  2  1  2  1  1  2  1\n [9217]  2  1  2  2  1  2  2  1  2  1  2  2  2  2  1  1  2  2  1  2  1  1  2  2\n [9241]  2  1  2  2  2  1  1  2  2  2  2  2  2  2  1  2  1  1  2  2  2  2  2  1\n [9265]  1  1  2  2  2  1  3  2  1  2  1  1  2  1  1  1  2  2  2  2  2  1  1  3\n [9289]  2  2  1  1  1  1  2  2  1  1  1  2  1  2  2  1  2  1  3  1  2  1  2  1\n [9313]  2  1  2  1  1  2  1  1  1  2  2  1  1  2  1  1  1  1  1  1  2  2  1  1\n [9337]  1  1  1  2  1  1  2  2  1  1  2  2  1  1  2  1  1  1  2  2  2  1  1  1\n [9361]  2  2  2  1  2  2  2  1  2  2  2  1  2  1  1  1  1  1  1  1  1  1  1  2\n [9385]  1  1  2  1  2 NA  2  1  2  2  1  2  2  2  2  1  2  2  2  2  2  1  1  2\n [9409]  2  1  2  1  2  2  2  1  1  1  2  2  2  1  1  1  2  1  1  1  2  1  1  2\n [9433]  2  1  2  1  1  2  1  1  2  1  2  2  1  3  1  2  1  2  2  1  1  1  2  2\n [9457]  2  2  2  1  1  2  2  2  2  1  2  1  2  2  2  2  2  2  1  1  2  2  1  2\n [9481]  2  1  1  2  2  2  2  2 NA  1  2  1  1  2  2  2  1  2  1  1  2  1  2  1\n [9505]  1  2  1  2  2  1  2  2  1  2  1  1  2  2  2  2  1  2  1  1  2  1  2  1\n [9529]  1  1  2  2  3  2  2  3  2  1  2  2  1  2  1  1  2  2  2  1  2  2  1  2\n [9553]  3  1  1  1  2  2  2  2  2  2  1  2  2  2  1  2  2  1  2  1  3  2  1  1\n [9577]  1  2  2  2  2  2  2  2  1  2  2  1  2  2  1  2  2  1  1  3  2  2  2  1\n [9601]  2  2  2  2  2  2  2  2  2  1  2  2  2  2  2  1  2  2  2  1  1  2  1  1\n [9625]  2  1  1  2  2  2  2  1  2  1  1  1  2  2  2  1  3  1  1  2  1  2  1  2\n [9649]  2  1  2  2  2  2  1  1  2  2  1  2  2  1  2  1  2  2  1  2  2  2  2  2\n [9673]  2  1  2  2  1  2  1  2  1  2  2  1  1  2  1  1  1  1  1  2  1  2  2  1\n [9697]  1  2  2  1  2  2  1  1  1  2  2  2  2  2  1  2  1  1  2  2  1  1  2  2\n [9721]  2  2  2  2  1  1  2  1  1  2  3  1  1  1  2  2  2  2  2  2  2  2  2  1\n [9745]  2  2  1  1  2  2  2  2  2  2  1  1  2  1  2  2  2  1  2  2  2  2  2  2\n [9769]  1  1  2  2  2  2  1  2  1  1  2  2  2  2  2  1  1  2  2  1  2  2  2  2\n [9793]  2  1  1  1  2  2  2  1  2  2  2  2  2  1  1  2  1  1  2  2  3  1  1  1\n [9817]  2  1  1  1  2  2  2  1  2  1  2  2  1  1  2  2  2  2  1  2  1  2  2  2\n [9841]  2  1  1  1  2  2  1 NA  2  1  1  2  1  1  2  1  1  1  2  2  2  1 NA  1\n [9865]  2  2  1  2  2  2  2  1  2  1  2  1  2  2  2  1  2  1  1  2  1  2  2  1\n [9889]  1  1  1  2  2  1  2  2  1  2  2  2  1  1  2  1  2  2  1  2  1  1  1  2\n [9913]  1  2  2  2  2  2  2  2  1  2  2  1  2  1  2  2  1  2  1  2  2  2  2  2\n [9937]  2  2  1  2  2  2  2  2  2  2  2  2  2  1  2  2  2  2  1  2  2  2  1  2\n [9961]  1  1  1  2  1  1  1  2  1  2  1  1  2  1  1  2  1  1  2  1  2  1  2  1\n [9985]  1  2  1  1  1  1  1  2  1  1  2  2  1  2  2  1  2  2  1  2  1  2  1  1\n[10009]  1  3  1  2  2  2  1  2  2  2  1  1  2  2  2  1  2  2  1  2  1  1  2  2\n[10033]  1  1  1  2  2  2  2  2  2  1  2  2  2  2  2  1  2  2  2  2  1  1  2  1\n[10057]  2  2  2  1  2  2  1  1  2  2  2  2  1  2  2  2  2  2  2  1  1  1  2  2\n[10081]  2  2  2  2  2  2  1  2  1  1  3  2  2  2  2  1  2  2  2  2  2  1  1  2\n[10105]  1  2  2  1  2  1  2  2  1  2  1  1  2  1  1  2  1  2  2  2  2  1  2  2\n[10129]  1  1  1  1  1  1  1  2  1  2  1  2  2  2  1  2  1  2  2  2  1  2  2  1\n[10153]  2  2  2  1 NA  2  2  2  2  1  2  2  1  1  2  1  2  2  2  2  1  2  2  1\n[10177]  2  2  2  1  2  2  2  1  2  1  1  1  2  3  2  2  1  1  1  2  2  2  1  1\n[10201]  2  2  2  2  2  2  1  2  2  1  2  2  1  1  1  2  2  2  2  2  2  2  2  1\n[10225]  1  1  1  2  1  1  2  1  2  2  1  1  2  2  2  2  2  1  1  2  2  2  2  2\n[10249]  2  2  2  2  2  2  2  1  2  1  2  1  2  1  1  1  2  1  1  2  2  2  2  2\n[10273]  2  2  1  2  2  2  1  1  2  1  2  2  1  1  1  2  2  2  2  1  2  1  2  1\n[10297]  1  1  1  2  1  1  1  1  2  2  1  2  2  1  2  1  2  2  1  2  1  1  1  1\n[10321]  2  1  2  2  1  2  2  1  1  2  2  2  2  1  1  1  2  3  2  2  1  2  2  2\n[10345]  2  1  2  2  1  1  1  2  2  1  2  1  1  2  2  2  2  1  2  1  1  2  1  1\n[10369]  1  3  2  2  2  2  2  1  1  2  1  2  2  2  1  2  3  1  1  2  2  2  2  2\n[10393]  2  1  2  2  3  2  2  2  2  2  2  2  2  2  2  2  2  2  2  1  2  2  1  2\n[10417]  2  2  2  2  2  1  2  1  1  1  2  2  1  2  2  1  2  2  1  2  2  1  1  2\n[10441]  1  2  2  1  2  2  1  2  2  2  2  2  1  2  2  2  1  2  2  2  2  1  2  2\n[10465]  1  2  2  2  2  1  1  2  2  1  2  2  2  2  2  1  1  1  2  2  1  2  1  1\n[10489]  1  2  1  1  1  2  1  1  2  1  1  1  1  1  2  1  1  2  1  2  1  2  2  2\n[10513]  2  2  1  2  2  2  1  2  2  2 NA  1  1  1  1 NA  2  2  2  1  2  2  2  1\n[10537]  1  2  1  1  1  3  2  2  1  2  2  2  2  1  2  2  2  1  2  1  1  1  2  2\n[10561]  1  1  1  1  2  2  2  2  1  1  2  1  2  1  2  1  1  2  2  1  2  2  2  2\n[10585]  1  2  1  2  1  2  2  1  1  2  1  1  1  1  1  2  2  1  2  1  2  2  2  1\n[10609]  1  1  1  1  1  1  2  2  2  1  2  1  1  2  2  1  1  2  2  2  2  2  2  2\n[10633]  2  2  1  2  2  1  1  2  1  2  1  1  2  2  1  1  1  1  2  2  2  2  2  1\n[10657]  1  2  2  1  2  1  1  1  2  2  1  2  2  2  2  1  2  2  2  2  2  2  2  2\n[10681]  2  1  2  1  1  2  2  1  1  2  2  2  1  1  2  2  1  2  1  2  1  2  1  1\n[10705]  1  2  1  2  1  2  1  2  1  1  2  2  2  2  1  1  1  2  2  1  2  2  2  2\n[10729]  2  1  1  2  2  2  2  2  2  2  1  2  1  1  2  2  1  1 NA  2  1  2  1  1\n[10753]  2  2  2  2  2  2  2  2  2  2  2  2  1  2  2  2  2  2  2  2  2  2  2  1\n[10777]  1  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  1  2  1  1  1\n[10801]  2  3  1  1  2  1  2  2  1  1  2  1  1  2  1  2  2  2  2  1  2  1  3  2\n[10825]  2  1  1  1  1  1  2  2  1  2  2  2  1  2  2  1  2  2  1  1  2  2  1  2\n[10849]  1  2  2  1  2  2  2  1 NA  2  2  2  2  1  2  2  2  2  1  2  1  2  2  2\n[10873]  2  1  2  1  1  1  2  2  2 NA  2  3  2  1  2  1  1  1  2  2  2  1  1  1\n[10897]  2  2  1  2  2  1  1  1  2  2  2  1  2  2  2  2  2  1  2  2  1  1  1  1\n[10921]  3  1  2  2  1  2  2  1  1  2  2  2  2  1  2  1  1  2  2  2  1  1  2 NA\n[10945]  1  1  2  2  2  1  2  2  2  1  2  2  1  2  1  1  2  2  2  1  2  2  2  2\n[10969]  1  2  1  2  2  1  1  2  2  2  1  2  2  2  2  2  2  2  2  1  2  2  1  1\n[10993]  1  1  1  1  1  1  2  1  1 NA  2  2  1  1  1  1  1  1  1  1  1  1  1  1\n[11017]  1  1  2  1  2  1  1  1  2  1  2  1  1  1  1  2  2  1  1  1  2  2  1 NA\n[11041]  1  2  2  2  2  1  2  2  2 NA  2  2  2  1  1  2  1  1  1  2  1  2  2  1\n[11065]  1  2  1  2  2  2  2  1  2  2  2  1  3 NA  2  1  1  2  2  1  2  2  2  1\n[11089]  2  1  1  1  1  2  1  2  1  2  2  1  2  1  2  1  2  2  1  2  2  2  2  2\n[11113]  1  1  1  2  2  2  1  1  1  1  2  1  1  1  2  2  2  2  2  1  1  2  1  1\n[11137]  2  2  2  1  2  2  2  1  2  2  2  1  2  1  2  2  2  1  2  2  2  2  2  2\n[11161]  2  1  2  2  1  1  1  2  2  2  1  2  2  1  2  1  2  1  2  2  2  1  1  2\n[11185]  2  1  2  2  2  1  2  2  1  1  2  2  1  2  2  1  2  2  2  2  2  2  1  2\n[11209]  2  1  2  1  1  2  2  2  1  1  1  1  1  1  2  2  2  2  1  1  2  2  2  2\n[11233]  2  1  2  1  2  2  2  2  2  2  2  1  2  2  1  2  1  2  1  2  2  1  1  1\n[11257]  1  1  1  2  2  2  2  2  1  1  2  2  2  1  2  2  2  1  1  2  2  2  1  2\n[11281]  2  2  2  1  1  2  2  2  2  1  2  1  1  2  2  1  1  2  2  2  1  1  2  1\n[11305]  2  2  2  2  1  2  2  2  2  1  2  2  2  2  2  2  2  2  1  1  1  2  2  2\n[11329]  2  2  2  2  1  2  2  1  2  2  2  2  2  1  2  2  2  2  2  2  1  2  2  1\n[11353]  1  2  2  2  2  2  2  2  2  2  2  2  1  1  1  2  2  2  1  2  1  2  2  1\n[11377]  2  2  1  2  2  2  2  2  2  2  2  2  2  2  2  2  3  2  1 NA  1  2  2  2\n[11401]  2  2  1  1  2  1  2 NA  2  1  2  1  1  2  2  2  2  1  1  2  1  2  2  2\n[11425]  1  1  1  2  2  2  1  1  1  2  2  2  1  2  1  2  2  2  2  1  2  1  2  2\n[11449]  2  1  2  1  2  1  2  1  2  2  2  2  2  3  2  1  2  2  1  1  2  1  2  1\n[11473]  2  2  2  2  2  1  2  1  1  1  2  2  2  2  2  2  2  2  2  2  1  2 NA  2\n[11497]  2  1  2  1  2  1  2  2  2  1  2  1  2  2  2  2  2  2  1  2  1  1  2  1\n[11521]  2  1  2  2  2  1  2  2  1  2  2  2  2  1  1  2  2  2  2  2  1  1  2  1\n[11545]  1  1  2  2  2  2  1  2  1  2  1  2  2  2  1  2  1  2  2  2  2  2  2  2\n[11569]  3  2  2  1  2  1  2  2  1  1  2  1  2  2  2  2  2  1  2  2  1  2  2  2\n[11593]  1  2  2  2  1  1  2  2  2  2  2  2  2  2  1  2  2  2  1  3  1  2  3  2\n[11617]  2  2  2  2  2  2  2  2  3  1  2  2  2  2  2  2  2  2  2  2  1  2  1  2\n[11641]  2  2  2  1  2  2  1  2  2  1  2  2  2  2  2  1  2  1  2  1  2  1  2  2\n[11665]  2  2  2  1  2  2  1  2  3  2  2  2  2  1  1  2  2  2  2  2  2  2  2  2\n[11689]  1  2  2  1  2  1  2  2  2  1  1  1  2  2  2  1  2  1  2  2  2  2  2  2\n[11713]  2  3  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  1  2  1\n[11737]  2  2  1  1  2  1  2  2  2  2  2  2  1  1  1  2  2  2  2 NA  1  3  1  1\n[11761]  2  2  2  1  2  2  2  2  2  2  1  1  2  1  1  2  1  2  2  1  2  2  2  1\n[11785]  2  2  1  2  2  2  2  1  2  1  1  1  1  2  2  1  2  2  1  1  1  1  2  2\n[11809]  1  2  1  2  2  2  1  1  2  2  1  2  2  2  2  2  2  2  2  1  2  2  1  2\n[11833]  2  1  2  1  2  1  2  2  1  1  2  1  1  1  2  2  2  2  2  1  1  2  2  1\n[11857]  2  1  2  2  2  2  2  2  1  1  3  1  2  2  2  2  2  2  2  2  2  2  2  1\n[11881]  2  2  1  1  1  1  2  2  1  2  1  2  2  2  1  2  2  2  2  1  2  2  2  1\n[11905]  2  2  2  1  2  1  2  2  1  2  2  2  2  2  1  1  1  1  2  1  2  1  2  2\n[11929]  2  1  2  1  2  2  1  2  1 NA  1  2  1  2  2  2  2  3  1  1  2  1  1  2\n[11953]  2  2  1  1  2  1  1  2  2  2  2  1  1  1  2  2  1  1  1  2  1  2  2  1\n[11977]  1  2  1  2  1  2  1  1  2  2  2  1  1  1  2  2  1  2  2  1  1  1  2  1\n[12001]  2  1  1  1  1  1  2  1  1  2  1  1  1  2  2  1  2  2  2  2  1  1  2  2\n[12025]  1  1  2  2  2  1  1  2  1  1  2  3  2  2  1  2  2  1  2  2  1  2  2  2\n[12049]  1  2  1  2  2  2  2  1  2  1  2  2  2  2  2  2  3  2  2  2  1  1  2  1\n[12073]  2  2  1  1  1  1  2  2  2  2  2  2  2  2  2  2  2  2  1  2  1  2  2  2\n[12097]  2  2  2  1  2  1  2  2  2  1  1  2  2  1  1  2  1  2  2  2  1  2  1  3\n[12121]  2  2  2  1  1  2  2  2  2  1  2  2  2  2  1  1  2  2  2  2  1  2  1  1\n[12145]  1  2  2  2  2  2  1  2  1  3  1  1  2  1  2  2  2  2  2  2  1  2  2  1\n[12169]  2  2  1  2  2  1  2  2  1  2  2  2  2  1  1  2  2  2  2  2  2  1  2  1\n[12193]  1  1  2  2  2  2  2  3  1  2  2  2  2  2  1  2  2  2  1  2  2  2  3  1\n[12217]  2  1  2  2  1  2  1  2  2  2  2  2  2  1  2  2  1  1  2  2  1  2  1  2\n[12241]  2  2  2  2  1  1  2  2  1  2  1  1  2  1  2  1  1  1  2  1  2  2 NA  2\n[12265]  1  2  2  2  2  1  1  1  1  2  2  2  2  2  1  1  2  1  2  1  3  2  1  2\n[12289]  1  2  1  1  2  2  1  2  2  1  1  2  1  2  2  2  2  1  2  2  1  2  1  2\n[12313]  2  1  2  1  1  2  2  1  2  2  1  1  1  2  1  2  2  2  2  1  3  2  3  2\n[12337]  2  1  1  1  2  2  1  2  2  2  1  1  2 NA  2  2  2  2  2  1  2  2  1  1\n[12361]  1  1  2  1  2  2  1  1  1  3  2  1  1  1  2  2  2  2  1  2  2  1  2  1\n[12385]  1  2  1  1  2  1  1  1  2  1  1  2  2  2  2  2  2  1  2  1  2  2  2  1\n[12409]  2  1  2  1  3  1  2  1  2  1  1  3  2  2  1  2  2  1  1  2  1  2  2  1\n[12433]  1  2  2  1  1  2 NA  2  2  2  2  2  2  2  2  2  1  2  2  2  1  1  2  2\n[12457]  2  2  1  2  2  1  1  1  2  2  2  1  2  1  1  2  1  1  2  2  1  2  2  2\n[12481]  2  1  1  1  1  1  1  2  2  2  2  1  2  1  1  1  1  2  1  2  1  1  1  2\n[12505]  2  2  2  2  2  1  1  2  2  1  2  1  1  1  2  1  1  1  1  1  1  1  2  1\n[12529]  1  1  2  1  2  2  2  2  1  2  2  1  1  2  2  2  2  1  2  1  2  2  2  1\n[12553]  2  1  2  2  1  1  2  2  2  1  2  1  2  1  1  1  1  1  1  2  2  1  2  2\n[12577]  1  2  1  2  2  1  2  1  1  1  2  2  2  2  1  2  2  2  1  2  1  2  2  2\n[12601]  2  2  1  2  1  2  2  1  2  1  2  2  2  1  2  2  2  2  1  2  1  2  2  2\n[12625]  1 NA  1  1  1  2  2  1  2  1  1  2  2  1  2  2  2  2  2  1  2  2  1  2\n[12649]  2  2  2  2  1  2  1  2  1  2  1  1  2  2  2  2  2  3  2  2  2  2  1  2\n[12673]  1  2  1  1  1  2  2  1  1  2  2  2  1  1  2  1  2  2  2  1  2  2  2  1\n[12697]  2  2  1  2  2  1  1  2  2  2  2  1  2  2  2  2  2 NA  2  2  1  2 NA  1\n[12721]  2  2  2  1  2  1  2  2  2  1  2  2  2  2  2  2  1  1  2  2  1  1  2  2\n[12745]  2  1  1  1  2  2  1  2  1  1  1  2  2  2  2  2  1  2  1  2  2  1  2  1\n[12769]  2  1  2  1  2 NA  2  2  1  2  2  1  1  1  2  2  2  1  2  1  2  2  2  2\n[12793]  1  2  1  2  2  1  2  2  1  2  2  2  2  2  2  2  1  1  1  2  1  2  2  1\n[12817]  2  2  1  2  2  1  2  2  1  1  2  1  2  2  1  1  2  2  2  2  1  2  1  2\n[12841]  1  1  1 NA  2  1  1  2  2  1  1  2  2  1  1  2  1  1  1  1  2  2  1  1\n[12865] NA  1  2  2  1  2  2  2  2  2  2  1  2  2  2  2  1  2  1  1  2  2  2  2\n[12889]  2  2  2  2  2  2  1  2  1  2  1  2  2  2  2  2  2  2  1  2  1  2  2  1\n[12913]  2  2  2  2  1  2  2  1  2  1  2  2  2  1  2  2  2  2  2  2  1  2  2  2\n[12937]  1  2  2  1  1  2  1  1  2  1  2  1  2  2  1  2  2  1  3  2  2  1  1  2\n[12961]  1  2  2  2  2  2  2  2  1  1  2  1  2  2  1  2  2  2  1  2  1  2  2  2\n[12985]  2  1  2  2  1  1  1  2  2  2  1  2  1  2  1  1  2  1  1  2  2  2  1  1\n[13009]  2  2  1  1  1  1  1  2  1  1  2  1  2  2  2  1  2  2  1  2  1  1  2  1\n[13033]  2  1  1  1  2  1  2  1  2  1  1  2  2  2  1  2  1  2  1  1  1  1  2  3\n[13057]  2  1  2  1  2  2  2  2  1  3  1  1  1  1  2  1  2  1  2  2  3  2  1  2\n[13081]  2  2  1  2  1  2  1  1  2  2  1  2  2  2  2  2  2  2  1  1  2  2  1  1\n[13105]  1  1  1 NA  1  2  1  1  2  2  2  1  2  2  2  1  1  1  1  2  1  2  2  1\n[13129]  1  2  2  2  2  2  2  2  2  2  1  1  2  2  2  2  3  2  1  2  1  1  2  1\n[13153]  2  2  2  1  2  2  2  2  2  1  1  2  1  1  2  2  1  1  2  2  2  1  1  2\n[13177]  1  1  2  2  1  1  1  1  2  1  1  1  1  2  2  1  1  2  1  1  2  2  1  1\n[13201]  1  2  2  1  2  2  1  2  2  2  2  1  1  1  2  2  2  2  2  1  1  2  2  2\n[13225]  2  1  2  3  2  2  1  1  1  1  1  1  1  1  2  2  2  2  2  1  2  1  2  1\n[13249]  2  1 NA  2  2  1  2  2  2  2  2  2  2  2  2  1 NA  2  1  1  1  2  2  2\n[13273]  1  1  2  1  2  2  1  2  2  2  2  2  1  2  2  1  2  1  2  1  2  3  1  1\n[13297]  2  2  2  2  2  2  2  2  2  2  1  2  1  2  1  2  1  1  1  2  1  2  2  1\n[13321]  2  1  1  2  2  2  2  1  3  2  2  1  1  1  2  2  1  1  1  2  2  1  1  2\n[13345]  2  2  2  2  1  1  1  1  1  2  1  2  1  1  2  1  2  2  2  2  1  1  1  2\n[13369]  2  2  2  2  2  1  1  1  2  2  1  1  2  1  2  2  2  1  1  2  1  1  1  1\n[13393]  2  1  2  2  2  1  1  1  1  1  1  1  1  1  1  2  2  2  2  1  2  1  1  1\n[13417]  1  2  2  1  1  2  1  2  1  2  2  2  2  2  2  1  2  2  2  1  3  1  1  1\n[13441]  1  2  2  1  2  2  2  1  2  2  1  2  1  1  3  1  2  2  1  2  1  1  1  2\n[13465]  1  1  2  2  1  2  1  2  1  2  1  1  2  1  2  2  2  1  1  1  2  1  2  1\n[13489]  2  1  2  2  2  1  2  1  2  2  1  2  1  2  2  2  2  1  1  1  2  2  1  2\n[13513]  2  2  2  2  2  2  2  1  2  2  1  1  1  2  2  1  1  1  2  2  2  2  2  1\n[13537]  1  1  2  2  2  2  2  2  1  2  1  1  2  1  2  2  1 NA  1  2  2  1  1  1\n[13561]  1  1  2  1  2  2  2  2  1  1  2  2  2  2  2  2  2  1  2  2  1  2  1  2\n[13585]  1  1  1  1  2  2  1  1  2  2  2  1  1  1  2  2  2  2  1  1  1  2  2  1\n[13609]  2  1  2  1  1  1  2  1  2  2  2  1  2  2  1  1  1  2  2  2  1  2  1  2\n[13633]  2  2  2  2  1  2  2  2  3  1  1  2  2  2  1  1  1  2  2  2  2  1  1  1\n[13657]  2  2  2  1  2  1  1  2  1  2  1  1  1  2  2  2  1  1  1  2  2  2  1  2\n[13681]  2  2  2  2  3  2  2  1  1  1  1  2  1  2  2  2  2  2  2  2  2  2  1  2\n[13705]  1  2  2  1  2  1  1  1  1  1  2  1  2  1  2  1  1  2  1  1  2  2  2  2\n[13729]  2  1  2  2  2  1  1  2  1  2  2  2  2  1  1  2  2  2  2  2  2  1  1  2\n[13753]  2  2  1  2  2  1 NA  1  2  2  2  1  2  2  2  2  2  2  2  2  2  1  1  1\n[13777]  2  2  2  1  2  2  2  2  1  1  2  2  2  1  2  2  2  1  1  1  1  2  2  2\n[13801]  2  1  2  2  2  1  2  2  1  2  2  1  2  2  1  2  1  1  2  2  2  1  2  2\n[13825]  1  3  2  1  2  1  2  1  1  1  2  2  1  2  2  2  2  1  1  2  1  1  2  2\n[13849]  1  2  2  2  2  1  2  2  2  2  2  1  2  2  3  2  1  2  2  2  1  1  2  2\n[13873]  2  2  1  2  1  2  2  2  2  1  2  2  2  1  2  1  1  1  2  1  1  2 NA  2\n[13897]  2  2  2  2  2  2  2  2  1  1  1  2  1  2  1  2  2  2  2  2  2  2  2  2\n[13921]  1  1  2  2  2  2 NA  2  2  2  1  2  2  1  1  2  1  2  2  2  1  2  1  2\n[13945]  2  2  1  1  2  1  2  2  1  1  2  1  1  2  2  1  2  1  2  2  1  2  2  2\n[13969]  1  2  2  2  2  2  2  1  2  1  2  1  2  2  3  2  2  2  1  2 NA  2  2  1\n[13993]  2  2  2  2  2  2  2  2  2  2  1  1  1  2  1  2  2  2  2  1  1  1  2  2\n[14017]  1  1  2  2  1  1  1  1  2  1  2  2  2  2  1  1  1  1  1  1  2  2  2  2\n[14041]  1  2  1  2  1  1  2  1  2  2  1  2  1  2  1  2  1  1  2  2  1  2  2  2\n[14065]  1  2  2  2  1  2  1  2  1  2  2  1  2  1  1  2  2  1  1  1  1  1  1  2\n[14089]  2  2  1  2  1  1  2  2  1  1  2  2  2  2  2  2  2  1  3  1  2  1  2  2\n[14113]  2  2  1  2  2  2  2  2  1  2  2  1  1  2  3  1  1  2  1  2  1  1  1  2\n[14137]  1  2  2  1  2  2  2  2  2  2  2  2  2  2  2  2  2  2  1  1  1  1  1  2\n[14161]  1  2  2  2  2  1  1  1  2  2  2  2  1  1  1  1  1  2  2  2  2  2  1  1\n[14185]  2  1  1  3  2  2  2  2  2  2  2  2  2  2  2  2  2  1  2  2  2  2  1  1\n[14209]  1  1  2  2  1  2  2  2  1  2  2  1  1  1  2  2  2  1 NA  1  2  1  2  1\n[14233]  2  1  1  1  2  2  1  1  3  2  1  1  2  1  2  1  2  1  2  1  2  2 NA  2\n[14257]  1  2  2  2  1  2  2  2  2  2  1  1  2  2  2  2  2  2 NA  2  2  2  1  2\n[14281]  1  1  1  2  1  1  2  1  2  2  2  2  2  2  2  2  2  2  1  1  2  2  2  2\n[14305]  1  1  2  2  1  1  2  1  2  2  1  2  1  2  2  2  2  2  2  1  2  2 NA  1\n[14329]  3  2  1  2  2  2  2  2  1  1  3  1  1  1  2  1  3  2 NA  2  2  1  3  2\n[14353]  2  2  1  1  2  1  2  1  2  2  2  2  1  2  2  1  1  1  1  1  2  1  2  2\n[14377]  1  2  2  2  1  1  2  2  1  2  2  1  1  2  2  2  1  1  1  1  1  2  2  2\n[14401]  2  2  1  1  2  2  1  2  2  2  2  2  1  2  2  1  2  2  2  1  2  2  1  2\n[14425]  2  2  2  2  1  2  2  2  1  2  2  2  2  2  1  2  1  1  2  1  1  1  1  2\n[14449]  2  2  2  2  2  2  2  2  1  2  2  2  2  1  1  1  1  1  1  2  2  1  1  2\n[14473]  1  1  2  3  2  2  1  2  2  2  1  1  1  1  1  1  1  1  1  1  2  1  1  1\n[14497]  1  1  1  1  1  1  1  1  2  1  1  1  1  2  1  1  1  1  1  1  1  1  1  1\n[14521]  2  2  1  1  2  2  2  1  2  1  2  1  2  2  1  2  1  2  2  2  2  2  2  3\n[14545]  2  2  2  2  2  1  1  2  1  2  2  2  2  1  2  1  2  2  1  2  2  2  2  1\n[14569]  2  1  2  2  1  2  2  2  1  2  2  2  1  1  2  1  2  2  1  2  1  2  1  2\n[14593]  2  2  2  2  1  2  2  2  1  1  2  2  2  2  2  1  1  1  1  1  1  2  1  1\n[14617]  1  1  2  2  1  1  2  1  2  2  2  2  1  1  2  1  1  2  2  2  1  2  2  2\n[14641]  1  2  2  2  2  2  2  2  2  1  2  2  1  1  2  1  2  1  1  1  2  2  1  2\n[14665]  1  2  2  2  1  2  2  1  2  2  1  1  2  2  1  2  2  2  2  2  1  2  2  1\n[14689]  1  2  2  1  2  2  2  2  1  2  1  1  2  2  2 NA  2  2  1  2  1  2  2  1\n[14713]  1  2  1  1  1  2  1  1  1  2  1  1  1  2  2  2  2  2  2  1  2  2  1  2\n[14737]  2  2  2  1  1  2  2  2  1  2  1  2  2  2  2  1  2  1  1  1  2  2  1  1\n[14761]  2  2  2  2  2  1  2  2  1  2  1  2  1  1  1  1  2  2  1  2  1  1  1  1\n[14785]  2  2  2  2  1  2  1  2  2  2  1  2  1  1  2  2  2  1  2  2  2  2  1  2\n[14809]  2  1  2  2  3  1  2  2  2  1  2  2  2  2  2  2  2  2  2  1  2  2  2  2\n[14833]  1 NA  2  1  1  2  2  1  2  1  1  2  2  2  2  1  2  2  2  2  2  2  1  1\n[14857]  1  1  2  2  2  2  2  1  1  2  2  1  1  2  2  2  2  2  2  2  2  2  1  2\n[14881]  1  1  2  2  2  1  2  2  1  1  2  1  2  2  1  1  2  2  2  2  2  2  2  2\n[14905]  2  2  1  2  2  1  2  2  2  1  2  2  1  2  2  1  2  1  2  2  1  1  2  2\n[14929]  2  1  2  2  1  1  2  2  2  2  1  2  2  1  2  2  2  2  1  2  2  2  1  2\n[14953]  2  2  2  2  1  1  2  1  1  1  2  2  2  2  1  2  1  1  1  1  2  2  2  1\n[14977]  1  2  2  1  2  2  1  2  1  2  2  2  2  2  2  2  2  1  2  2  1  1  2  2\n[15001]  2  2  2  2  2  1  2  1  2  2  1  2  2  2  1  2  2  1  2  1  1  1  2  2\n[15025]  1  2  1  2  2  1  2  1  3  2  1  2  2  2  2  2  1  2  1  1  1  2 NA  1\n[15049]  2  2  2  2  2  2  2  1  2  1  2  1  2  1  1  2  2  2  2  2  2  2  2  2\n[15073]  2  2  1  2  2  1  2  2  2  1  2  2  2  2  2  1  1  2  2  1  1  1  1  2\n[15097]  1  1  2  1  2  2  1  1  1  2  1  1  2  2  2  2  2  3  2  2  2  2  2  2\n[15121]  2  1  1  1  2  1  2  2 NA  1  1  2  1  1  2  2  2  2  1  2  1  2  2 NA\n[15145]  2  2  2  2  2  1  2  2  2  1  1  2  2  2  1  2  2  2  1  2  2  2  2  1\n[15169]  2  1  2  2  1  2  2  1  1  2  2  2  1  1  2  1  2  2  2  2  1  2  1  2\n[15193]  1  2  2  2  2  2  1  2  2  1  2  2  2  1  1  2  1  1  1  2  1  1  1  2\n[15217]  1  2  1  2  2  1  1  1  2  2  2  1  1  1  2  2  1  2  1  2  1  1  2  1\n[15241]  1  1  1  1  1  2  2  2  1  1  2  1  2  1  1  2  2  2  2  2  2  1  2  2\n[15265]  2  1  1  2  1  2  1  2  2  2  2  2  2  2  1  2 NA  2  2  2  2  1  2  2\n[15289]  3  2  2  2  1  2  2  2  1  2  2  2  2  1  2  2  2  2  1  2  2  1  1  2\n[15313]  1  1  1  1  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2\n[15337]  2  2  2  2  2  2  1  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2\n[15361]  1  2  2  1  2  2  2  1  1  2  2  1  1  2  2  1  1  2  2  1  2  2  1  2\n[15385]  2  2  1  2  2  2  2  1  2  2  2  2  2  1  1  1  2  1  2  2  1  1  2  2\n[15409]  1  1  2  1  1  2  2  2  2  2  1  1  1  2  2  2  2  2  2  2  2  2  2  2\n[15433]  2  2  2  2  2  2  2 NA  2  2  2  1  2  2  2  1  1  2  2  2  2  1  1  1\n[15457]  2  1  1  1  2  1  1  2  2  2  2  2  2  1  2  1  2  2  2  1  1  1  2  2\n[15481]  2  2  2  1  1  1  1  2  2  2  2  1  1  1  2  2  1  2  1  1  2  1  2  2\n[15505]  2  2  2  2  2  2  2  1  2  2  2  1  2  1  1  2  2  2  1  1  2  2  2  2\n[15529]  2  2  2  2  2  2  2  2  1  2  2  1  1  1  2  1  2  2  1  2  1  2  2  2\n[15553]  2  1  2  2  1  1  2  2  1  1  1  1  2  2  2  1  2  2  3  2  1  2  2  2\n[15577]  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  1  2 NA  2  2  2\n[15601]  1  1  3  2  1  1  1  2  2  2  2  2  2  2  2  2  2  2  2  1  1  2  2  1\n[15625]  1  1  1  2  1  2  2  2  2  2  1  1  2  2  2  2  1  1  2  2  1  2  1  2\n[15649]  1  2  2  2  1  2  2  1  2  2  2  2  2  2  2  1  1  2  2  2  1  3  1  2\n[15673]  2  2  2  2  2  1  2  2  2  1  2  2  2  2  2  2  2  2  1  2  2  2  2  2\n[15697]  2  2  2  2  2  2  2  2  2  1  2  1  2  1  2  2  1  1  2  2  2  1  2  1\n[15721]  2  1  2  1  2  1  1  2  1  2  1  2  1  2  1  2  2  2  2  1  2  3 NA  2\n[15745]  2  1  2  1  2  2  2  1  2  1  2  2  2  2  2  2  1  1  1  2  2  2  2  2\n[15769]  1  1  2  1  1  1  1  1  3  1  1  2  1  2  2  2  3  1  2  1  2  2  2  2\n[15793]  2  2  2  2  2  2  3  2  2  2  1  2  2  1  2  2  1  2  2  2  1  2  1  1\n[15817]  2  1  2  2  2  2  1  1  2  3  2  2  2  1  2  2  2  1  1  2  1  2  1  2\n[15841]  1  2  2  2  1  2  2  1  2  2  2  2  2  2  2  1  2  2  2  2  2  2  1  2\n[15865]  2  1  1  2  2  2  2  1  1  2  1  2  2  1  2  1  2  2  1  2  2  1  2  1\n[15889]  1  1  1  2  3  2  2  1  3  2  1  2  1  2  1  1  2  2  2  2  2  2  2  2\n[15913]  2  2  1  1  2  2  1  2  1  2  2  2  1  1  2  2  1  1  1  1  2  2  2  2\n[15937]  2  1  1  1  1  2  2  2  1  2  2  1  2  1  2  1  2  2  2  1  2  1  2  2\n[15961]  2  2  2  2  2  2  1  2  1  2  1  2  2  2 NA  1  1  1  2  2  1  2  1  2\n[15985]  1  2  2  2  2  1  2  2  2  1  2  1  2  2  2  2  2  2  1  2  1  2  2  2\n[16009]  2  1  2  1  2  2  1  2  2  2  1  2  2  2  2  3  2  2  2  2  2  2  1  1\n[16033]  2  2  2  1  2  2  1  2  1  2  2  2  2  1  1  2  1  2  2  2  2  2  2  2\n[16057]  2  1  2  2  2  2  1  1  2  2  1  2  1 NA  2  2  2  2  2  2  2  1  2  2\n[16081]  1  1  2  2  2  2  2  2  2  2  1  2  2  1  2  1  1  2  1  1  1  2  1  2\n[16105]  2  2  1  2  1  1  1  2  2  2  2  2  2  2  2  2  2  2  1  2  2  2  1  1\n[16129]  1  2  2  2  2  2  2  2  2  2  2  2  1  2  1  2  1  1  2  2  2  2  2  1\n[16153]  1  2  2  1  1  1  2  1  1  1  2  2  2  1  2  1  2  2  1  1  2  1  2  2\n[16177]  2  2  1  2  1  2  1  2  2  2  2  2  2  1  1  2  1  2  2  2  2  1  2  2\n[16201]  2  2  1  1  2  2  2  2  1  2  2  2  1  1  2  2  2  2  1  1  2  1  2  1\n[16225]  1  1  2  2  2  2 NA  2  2  2  2  1  1  2  1  2  2  2  1  1  2  1  1  2\n[16249]  1  1  2  2  2  1  1  2  2  2  2  2  2  2  2  2  2  2  2  1  2  2  1  1\n[16273] NA  2  2  1  2  1  2  2  2  1  2  1  2  2  2  2  2  1  2  1  2  2  2  1\n[16297]  2  1  2  3  2  1  2  2  2  2  2  1  1  1  1  2  1  2  1  2  2  2  2  2\n[16321]  2  1  2  2  2  2  2  2  1  1  2  1  2  2  2  1  2 NA  2  1  2  1  2  2\n[16345]  1  1  2  1  2  1  2  2  1  2  2  2  1  2  2  2  2  2  1  2  2  2  2  2\n[16369]  1  2  1  1  2  2  1  2  2  2  2  1  2  2  2  1  2  2  2  2  1  2  1  1\n[16393]  2  2  2  2  2  2  1  2  2  1  1  2  2  2 NA  2  1  1  2  2  2  1  1  2\n[16417]  2  2  2  2  2  1  1  2  2  2  1  1  1  2  1  2  1  2  2  1  2  2  2  2\n[16441]  2  2  2  2  2  2  3  2  2  2  1  1  2  2  2  2  2  1  2  2  2  1  1  2\n[16465]  1  1  1  1  1  2  2  2  1  1  2  1  1  2  2  2  1  2  2  1  2  2  2  2\n[16489]  2  2  2  2  1  1  2  3  2  1  1  2  2  2  1  2  2  2  2  1  2  2  2  1\n[16513]  2  2  1  2  2  1  2  2  2  1  2  2  2  2  1  1  2  1  2  2  2  2  2  1\n[16537]  2  2  2  2  2  1  1  1  1  2  2  2  2  2  1  2  2  2  2  1  2  2  1  2\n[16561]  2  1  1  1  2  2  1  2  2  2  2  2  1  2  2  2  1  2  2  2  1  2  2  1\n[16585]  2  1  2 NA  2  2  2  2  2  3  3  1  2  2  2  2  2  1  2  2  1  1  2  1\n[16609]  1  2  1  2  2  2  2 NA  2  1  1  2  2  2  2  1  1  1  2  2  1  2  2  2\n[16633]  2  1  1  1  2  1  2  1  1  2  2  2  2  2  2  2  2  2  2  2  1  2  2  1\n[16657] NA  2  2  2  1  2  1  2  1  1  2  1  2  1  2  1  1  1  1  2  1  1  2  2\n[16681]  2  1  2  2  2  2  1  2  1  1  1  2  2  2  1  2  1  2  2  2  2  2  1  2\n[16705]  1  2  1  2  2  2  1  2  2  1  2  1  2  2  2  2  2  2  1  2  2  1  1  2\n[16729]  2  2  1  2  2  2  1  2  1  2  1  1  2  1  2  1  1  2  2  2  2  2  1  2\n[16753]  2  1  2  1  2  2  2  2  1  2  2  2  2  1  1  1  2  2  1  1  2  2  1  1\n[16777]  2  1  1  2  2  1  1  1  2  2  2  2  1  2  2  2  2  1  1  2  1  2  2  1\n[16801]  1  2  1  2  1  2  2  2  2  2  3  2  3  2  1  2  1  1  2  1  1  1  2  2\n[16825]  2  1  2  2  1  2  1  1  1  2  2  1  1  1  1  1  1  1  2  2  2  2  2  1\n[16849]  2  1  2  2  1  1  2  2  2  1  2  2  1  1  1  2  1  2  3  2  2  2  2  1\n[16873]  1  2  2  2  2  1  1  2  2  2  2  2  2  1  2  2  1  1  1 NA  2  1  1  2\n[16897]  2  1  2  2  2  2  2  2  2 NA  1  1  2  2  1 NA  2  1  2  1  1  1  1  2\n[16921]  1  2  3  1  1  1  2  1  1  1  1  1  2  1  1  1  2  1  2  1  2  1  2  1\n[16945]  2  1  1  2  1  2  1  1  1  2  2  2  1  1  2  2  2  1  2  2  2  2  2  1\n[16969]  2  2  1  2  2  2  2  1  2  1  2  2  2  2  2  1  2  2  2  2  1  1  1  2\n[16993]  2  2  1  2  1  2  1  2  1  2  2  2  1  2  1  2  2  2  2  1  1  1  1  2\n[17017]  1  2  2  2  3  2  1  1  1  2  2  2  2  2  1  2  2  2  1  2  2  2  1  1\n[17041]  1  1  1  2  1  2  1  2  1  2  2  2  2  2  2  2  2  2  2  2  2  1  2  2\n[17065]  2  2  2  1  2  2  1 NA  2  2  2  2  2  1  2  2  2  2  2  2  2  1  1  2\n[17089]  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  1  2  2  2  1  1  1  1\n[17113]  2  2  2  2  1  2  2  2  1  2  2  2  1  2  2  2  2  2  1  2  1  2  2  2\n[17137]  1  1  2  1  2  2  2  2  2  2  2  1  2  2  2  2  1  1  1  1  2  1  2  1\n[17161]  1  2  1  2  1  1  2  1  1  1  1  2  2  2  2  2  2  2  1  1  2  1  2  1\n[17185]  1  2  2  1  1  1  2  2  2  2  2  2  1  2  2  2  1  2  2  2  2  2  1  2\n[17209]  2  2  2  1  1  2  2  1  2  1  1  2  2  2  2  2  2  1  2  1  1  1  2  1\n[17233]  1  1  2  1  2  1  3  2  2  2  2  2  2  2  1  2  2  2  2  1  1  2  2  2\n[17257]  2  2  2  2  1  2  1  2  1  2  2  2  1  2  1  1  1 NA  1  2  1  1  2  1\n[17281]  1  1  2  1  2  2  2  2  2  2  1  1  1  2  2  2  2  2  1  1  2  2  1  2\n[17305]  2  2  1  2  2  2  2  2  1  2  2  2  1  2  2  2  2  2  2  1  2  1  2  1\n[17329]  2  2  2  2  2  1  2  2  2  2  1  2  1  2  2  2  2  1  1  1  2  1  2  2\n[17353]  1  1  2  2  2  2  2  1  2  1  2  1  1  1  2  2  2  1  2  1  2  2  2  1\n[17377]  2  1  2  1  1  1  1  2  2  1  1  2  1  1  2  1  1  1  1  1  1  1  2  1\n[17401]  1  2  2  2  1  2  1  1  2  1  2  2  1  1  1  1  2  1  2  2  1  1  2  2\n[17425]  1  2  2  2  2  2  2  2  2  1  2  1  2  1  1  2  2  2  2  1  2  2  1  2\n[17449]  1  2  1  1  2  1  2  2  2  2  2  2  2  2  1  2  2  1  1  1  1  1  2  2\n[17473]  2  2  2  2  1  1  1  1  2  1  2  1  1  2  1  2  2  2  1  1  1  2  2  2\n[17497]  2  2  1  2  2  1  2  2  2  1  1  1  2  1  1  1  2  2  1  2  1  1  1  2\n[17521]  2  2  2  1  2  2  1  1  2  2  1  1  2  2  2  1  2  1  1  2  2  1  1  1\n[17545]  2  1  2  1  2  2  1  1  2  1  2  1  2  1  1  1  1  2  1  1  2  1  1  1\n[17569]  2  2  2  2  1  2  2  2  2  2  2  2  2  2  2  2  2  2  2  1  1  1  2  2\n[17593]  2  1 NA  1  1  1  2  1  2  2  2  1  1  1  2  2  2  2  2  1  2  2  2  2\n[17617]  3  1  2  2  2  2  1  2  1  2  1  2  2  2  2  1  1  1  2  2  2  2  2  2\n[17641]  1  2  2  2  1  2  2  2  2  1  2  2  2  2  2  1  1  1  2  2  2  1  2  2\n[17665]  2  2  2  2  2  1  1  2  1  2  2  1  1  1  1  2  2  2  2  2  1  2  2  2\n[17689]  2  1  2  1  1  2  2  2  2  2  1  1  2  1  2  1  2  1  2  1  2  2  1  1\n[17713]  2  2  1  2  2  2  2  2  2  1  2  1  2  1  1  2  2  2  2  2  2  1  2  1\n[17737]  1  1  2  1  2  2  2  2  2  2  2  1  2  2  1  1  1  2  2  2  2  2  1  1\n[17761]  2  1  2  1  2  2  1  1  1  1  1  2  1  2  2  1  2  1  1  2  1  2  1  1\n[17785]  1  2  2  1  2  1  2  1  1  2  2  2  1  1  2  2  2  2  2  1  2  1  1  2\n[17809]  2  2  2  2  1  1  2  2  2  1  2  2  2  2  2  1  2  2  2  2  1  1  1  2\n[17833]  2  1  2  2  2  2  2  2  1  1  2  2  2  2  1  1  1  1  1  1  1  1  2  2\n[17857]  1  1  2  2  1  2  1  1  1  2  1  2  2  1  2  2  2  1  1  2  2  2  2  2\n[17881]  2  2  1  2  2  2 NA  2  2  2  2  2  2  2 NA  2  1  2  1  1  1  2  2  2\n[17905]  1  2  1  1  2  2  2  1  2  1  1  1  2  1  2  2  1  1  1  2  2  2  1  1\n[17929]  2  1  1  2  1  2  2  2  2  2  1  2  2  1  1  1  2  1  1  1  1  1  1  2\n[17953]  2  1  2  2  2  2  1  1  1  2  1  1  1  2  1  2  1  2  2  2  1  2  1  2\n[17977]  1  1  1  2  2  2  2  1  2  1  1  2  2  1  2  2  1  1  1  1  1  2  2  2\n[18001]  2  1  1  2  1  2  2  1  2  2  2  1  2  2  2  1  2  2  2  1  2  2  2  2\n[18025]  2  2  2  2  1  2  2  1  1  1  1  1  1  2  1  2  2  1  2  1  2  2  1  1\n[18049]  1  1  1  2  2  2  2  2  1  2  2  1  1  1  2  3  1  2  2  2  2  1  1  2\n[18073]  2  2  2  2  2  2  1  2  2  1  2  1  2  2  1  1  2  1  2  2  2  2  1  2\n[18097]  2  2  2  1  2  2  2  1  2  2  2  1  1  2  2  1  2  1  1  2  2  1  1  1\n[18121]  2  1  1  2  1  2  1  1  1  1  2  2  1  1  2  1  2  2  1  1  2  2  2  2\n[18145]  2  2  2  1  1  2  1  2  2  2  2  1  1  2  2  2  2  2  1  2  2  2  1  1\n[18169]  1  1  2  2  2  1  2  2  1  2  2  2  2  2  1  2  1  2  2  1  1  1  2  2\n[18193]  2  2  2  2  1  2  2  2  2  2  1  2  2  2  1  2  1  1  2  2  2  2  2  2\n[18217] NA  2  1  2  2  2  1  2  2  2  2  1  2  1  2  2  2  2  1  1  2  2  2  3\n[18241]  2  1  1  1  1  1  1  1  2  1  1  1  1  2  2  2  1  2  1  2  2  1  1  2\n[18265]  1  2  2  1  2  2  2  2 NA  2  2  2  2  2  2  2  1  1  1  2  1  1  1  2\n[18289]  1  2  1  2  1  2  2  2  2  2  2  1  2  2  1  2  2  2  2  1  1  2  2  2\n[18313]  2  2  2  2  2  1  1  1  1  1  2  2  1  1  2  1  1  1  2  2  2  1  2  2\n[18337]  2  2  2  2  2  1  2  2  2  1  2  2  2  2  1  2  1  2  2  2  2  2  2  2\n[18361]  1  2  2  1  2  2  2  1  1  1  1  1  1  1  2  2  2  2  1  1  2  2  2  2\n[18385]  1  1  1  2  1  2  2  1  2  2  2  1  1  1  1  2  2  2  1  1  2  1  2  2\n[18409]  1  2  1  1  1  1  1  1  1  2  2  2  2  2  1  1  2  1  2  2  1  2  1  2\n[18433]  2  2  1  2  2  2  2  2  2  2  2  2  2  1  2  2  1  1  1  1  1  1  1  2\n[18457]  1  2  2  1  1  2  2  2  2  2  1  2  1  1  2  1  1  2  1  1  2  1  2  1\n[18481]  2  1  2  1  1  1  1  2  3  2  1  1  2  1  2  2  1  2  2  1  2  2  2  2\n[18505]  1  2  1  1  2  1  1  2  1  2  1  1  2  1  1  1  2  2  1  2  2  1  2  2\n[18529]  2  1  2  1  1  2  2  1 NA  2  2  2  2  2  1  2  1  2  1  2  1  2  1  2\n[18553]  2  1  1  2  1  1  2  1  2  2  2  2  2  2  2  2  2  2  2  2  2  1  1  1\n[18577]  2  2  1  1  2  2  2  1  1  2  2  2  1  1  2  2  2  2  2  1  1  2  2  2\n[18601]  2  2  2  2  1  2  1  2  2  2  2  2  1  2  1  1  2  2  2  1  1  2  1  2\n[18625]  2  2  2 NA  2  1  1  2  2  1  2  2  1  2  2  2  1  1  2  2  2  1  1  2\n[18649]  1  1  2  2  2  2  1  2  1  2  2  1  2  1  2  2  1  2  1  1  1  1  1  1\n[18673]  1  1  2  2  2  1  1  1  2  3  1  2  3  1  1  2  2  1  1  2  2  3  2  2\n[18697]  2  2  2  2  2  1  2  2  2  1  2  2  1  2  2  2  2  2  2  1  2  2  2  1\n[18721]  1  2  2  2  1  2  1  2  2  2  2  2  1  1  2  2  1  2  2  1  1  2  1  1\n[18745]  1  1  2  1  2  1  1  2  1  2  1  1  1  2  2  2  2  1  2  1  1  2  1  2\n[18769]  2  2  1  1  2  2  2  2  2  1  1  2  2  2  2  2  1  1  1  2  2  1  2  1\n[18793]  1  1  2  2  2  2  1  2  2  2 NA  2  1  1  1  2  1  2  1  1  2  1  2  2\n[18817]  1  2  2  2  2  1  2  2  1  1  1  1  2  2  2  2  2  1  1  1  2  1  1  1\n[18841]  2  1  1  2  2  1  2  2  2  2  2  2  1  2  1  2  2  2  2  2  2  1  2  1\n[18865]  1  1  1  2  1  1  1  1  1  2  1  2  2  1  2  2  1  2  2  2  1  2  2  2\n[18889]  1  2  2  1  1  2  2  2  2  3  1  2  2  1  1  1 NA  2  1  1  1  1  2  1\n[18913]  2  2  2  1  2  1  2  2  2  1  1  1  1  2  2  2  1  1  2  2  1  1  2  2\n[18937]  1  2  2  2  2  2  1  2  2  1  1  2  2  2  2  2  1  1  2  1  2  2  1  2\n[18961]  2  2  1  1  2  1  1  2  2  2  1  2  2  1  1  2  2  1  1  1  2  1  2  2\n[18985]  1  2  1  2  2  2  2  2  2  1  2  2  2  1  1  1  1  1  2  2  2  2  2  2\n[19009]  2  2  2  2  1  1  1  1  2  1  2  2  1  2  2  2  2  1  2  2  1  2  1  2\n[19033]  1  2  1  2  1  2  1  2  2  2  2  2  1  2  2  1  2  1  1  2  1  1  2  1\n[19057]  2  2  2  2  2  1  1  2  2  1  2  2  2  1  2  2  2  1  2  1  2  2  2  1\n[19081]  1  1  2  1  2  2  1  2  2  2  2  2  2  1  2  2  2  1  2  1  1  3  2  2\n[19105]  2  2  2  2 NA  2  2  2  1  2  2  2  2  2  2  2  2  2  2  1  2  2  1  2\n[19129]  1  2  1  2  1  2  2  1  2  2  2  2  1  1  2  2  1  2  2  1  1  1  1  2\n[19153]  2  1  1  2  2  1  2  2  1  2  2  2  2  2  1  1  1  1  1  2  1  2  2  2\n[19177]  1  2  2  1  2  1  2  2  1  2  2  1  2  2  2  2  2  1  2  2  1  2  1  2\n[19201]  2  1  1  2  3  2  1  2  2  1  2  2  1  2  2  1  2  1  1  3  2  2  2  2\n[19225]  2  1  1  1  2  2  2  2  2  2  2  2  3  1  1  2  2  2  1  1  2  2  2  1\n[19249]  2  2  2  2  2  2  1  2  1  2  2  2  1  1  2  2  2  1  2  1  1  1  1  2\n[19273]  1  2  1  2  1  2  2  2  1  1  2  2  2  1  2  2  2  1  2  2  2  2  1  2\n[19297]  1  2  2  2  2  1  1  1  1  1  1  2  1  2  1  2  1  2  1  2  1  1  2  2\n[19321]  3  2  2 NA  1  1  1  2  2  1  1  1  2  1  1  2  2  2  1  2  2  2  2  1\n[19345]  2  2  2  2  2  2  2  2  2  2  2  1  1  2  2  1  2  2  2  2  2  2  2  2\n[19369]  2  2  2  2  2  1  2  2  2  2  2  2  2  2  2  2  1  2  2  2  1  1  1  2\n[19393]  2  1  2  2  3  2  2  2  2  2  2  2  2  2  1  1  1  2  2  2  2  2  1  1\n[19417]  1  2  1  2  2  2  1  2  1  2  1  2  2  1  1  1  2  2  2  2  1  2  2  2\n[19441]  1  1  1  2  1  2  2  2  2  2  1  2  2  2  1  2  2  1  2  2  2  1  2  2\n[19465]  2  1  2  2  1  2  1  2  1  1  2  1  1  1  2  2  2  2  2  1  2  1  1  2\n[19489]  2  2  2  2  2  2  2  2  1  2  1  2  1  2  3  1  2  1  2  1  2  2  2  2\n[19513]  2  2  2  2  1  2  2  2  2  1  2  2  2  2  2  2  2  2  2  3  2  2  2  2\n[19537]  2  2  2  1  2  2  1  2  2  2  2  2  2  1  1  2  2  2  2  2  1  2  2  2\n[19561]  2  2  3  2  3  2  2  3  1  1  2  2  2  1  2  1  1  2  2  2  2  2  2  2\n[19585]  2  2  2  2  2  2  2  2  2  2  1  2  2  2  2  1  3  2  2  2  2  2  2  3\n[19609]  2  2  1  2  1  2  1  2  2  2  2  2  2  2  2  1  1  2  2  2 NA  1  2  1\n[19633]  2  2  3  1  1  2  2  2  1  1  1  1  2  3  2  2  2  2  2  2  2  1  2  2\n[19657]  1  1  2  1  1  1  1  2  1  1  1  2  2  2  2  2  2  1  2  2  2  2  2  2\n[19681]  3  2  2  2  2  2  1  2  2  2  1  2  1  2  1  1  2  2  2  2  2  2  2  2\n[19705]  1  2  2  2  2  1  2  1  1  1  2  1  2  2  2  1  1  2  2  1  2  2  2  1\n[19729]  1  2  2  2  2  2  3  2  1  2  2  1  2  1  2  1  2  2  2  2  2  2  2  1\n[19753]  1  1  2  2  2  2  1  2  2  2  1  2  2  2  1  2  2  1  1  1  1  1  2  2\n[19777]  1  2  1  2  2  2  2  2  1  2  2  2  2  1  1  2  1  2  2  2  1  2  2  1\n[19801]  1  2  2  2  2  2  2  2  2  2  2  2  1  2  2  2  1  1  1  1  2  2  1  2\n[19825]  3  2  1  1  2  1  1  1  1  2  2  2  2  2  2  2  2  1  2  2  1  1  2  2\n[19849]  1  2  1  1  2  2  1  1  1  1  2  2  2  2  2  2  1  2  1  2  2  1  1  1\n[19873]  2  1  2  2  2  2  2  2  1  2  2  1  1  1  2  2  2  1  1  1  2  2  1  2\n[19897]  1  2  2  2  1  1  2  1  2  2  1  2  2  2  1  1  2  1  2  1  1  2  1  2\n[19921]  2  2  2  2  2  2  2  2  2  2  2  2  1  1  1  2  2  2  2  2  2  2  2  2\n[19945]  2  2  2  2  2  1  2  2  1  1  2  1  2  2  1  2  1  2  2  2  1  2  2  1\n[19969]  2  2  2  2  2  2  2  2  1  1  2  2  1  2  2  2  1  2  1  1  1  1  2  2\n[19993]  1  1  1  2  2  2  1  2  2  2  1  1  1  1  2  1  2  1  2  1  2  1  2  1\n[20017]  1  1  2  2  2  1  2  2  2  2  2  1  2  2  2  2  2  2  1  2  1  1  2  2\n[20041]  2  1  1  2  1  2  2  2  2  1  1  1  2  2  1  2  1  1  1  1  2  2  2  2\n[20065]  1  2  2  1  1  1  2  1  1  1  1  2  1  2  2  2  2  2  1  1  1  2  2  1\n[20089]  2  1  1  2  2  1  2  2  2  2  2  1  1  1  2  2  2  2  2  3  1  2  1  2\n[20113]  2  1  2  2  2  2  2  2  1  2  2  2  2  1  2  2  1  1  1  2  1  2  2  1\n[20137]  1  2  1  2  1  1  1  2  2  1  1  2  2  1  2  2  1  2  2  2  2  2  1  1\n[20161]  2  1  1  2  2  1  1  1  2  2  2  1  1  2  1  2  1  3  1  1  1  2  2  2\n[20185]  1  2  2  2  2  1  1  2  1  1  2  2  2  2  1  2  1  2  2  2  1  2  2  2\n[20209]  2  1  1  1  2  2  2  2  2  1  2  1  1  1  1  1  2  2  2  2  2  2  1  2\n[20233]  2  2  1  1  2  1  2  2  2  1  1  2  2  2  2  2  1  2  2  2  2  1  2  2\n[20257]  1  2  2  2  2  1  1  1  3  2  1  2  1  1  2  2  2  2  1  2  2  2  2  2\n[20281]  1  2 NA  2  2  2  2  2  1  1  2  2  2  1  2  1  1  2  2  2  2  1  1  2\n[20305]  1  2  1  2  3  2  2  2  3  2  1  2  2  1  1  2 NA  1  1  2  2  2  2  2\n[20329]  2  1  1  1  2  2  2  1  2  2  1  1  2  2  2  2  1  1  1  2  1  2  2  2\n[20353]  2  1  2  1  1  2  1  2  1  2  2  1  2  2  2  2  2  2  2  2 NA  2  3  2\n[20377]  1  1  2  1  2  1  2  2  2  2  2  2  1  2  2  2  1  2  1  2  2  2  2  1\n[20401]  2  1  1  2  2  1  2  2  1  2  3  2  1  2  1  1  1  2  2  2  2  1  2  1\n[20425]  2  2  2  1  2  1  2  1  2  2  1  2  2  2  2  1  1  1  2  1  1  1  3  2\n[20449]  2  2  1  2  2  2  2  2  2  2  2  2  2  1  2  2  2  1  2  2  2  2  2  1\n[20473]  1  2  2  2  2  2  2  2  1  2  2  1  3  1  2  2  1  2  2  2  2  2  1  2\n[20497]  1  1  1  2  1  1  2  2  1  1  2  1  2  2  2  1  2  2  2  1  1  2  2  2\n[20521]  2  2  2  2  2  1  2  1  2  2  1  2  1  2  2  1  2  1  1  2  1  1  2  2\n[20545]  2  2  2  1  1  2  2  2  2  2  1  2  1  2  1  2  2  1  2  2  2  2  1  1\n[20569]  1  2  1  2  1  2  1  2  2  1  1  2  1  2  1  2  2  1  1  2  2  2  1  1\n[20593]  1  1  1  2  1  2  2  2  1  2  1  2  2  1  2  2  2  2  1  2 NA  2  1  2\n[20617]  1  2  2  2  2  1  1  2  1  2  1  2  1  1  2  1  1  1  2  1  2  2  1  2\n[20641]  1  1  2  2  2  2  2  1  2  2  1  1  2  2  2  1  2  2  2  1  2  2  1  1\n[20665]  1  1  1  2  1  2  2  1  2  2  2  1  2  2  1  2  2  2  2  2  2  2  1  1\n[20689]  2  1  1  1  2  1  2  2  2  2  2  2  2  2  1  2  1  2  2  2  2 NA  2  2\n[20713]  2  1  1  2  1  1  2  1  2  2  2  2  2  2  1  2  2  2  1  1  2  1  2  1\n[20737]  1  2  1  2  1  2  1 NA  1  1  2  2  2  2  2  2  2  1  2  1  2  1  1  2\n[20761]  1  1  2  2  1  2  2  1  2  2  2  1  2 NA  1  2  1  2  1  2  1  2  2  2\n[20785]  2  2  1  1  1  1  2  2  1  2  1  2  2  1  2  2  2  1  1  2  2  2  2  2\n[20809]  1  1  2  1  1  1  2  2  1  3  2  1  1  1  1  1  2  1  1  2  1  2  1  2\n[20833]  1  1  1  1  1  2  2  1  1  2  2  1  2  1  2  1  2  2  2  2  2  2  1  1\n[20857]  2  2  2  2  2  2  1  1  2  2  2  2  2  2  2  2  1  1  1  2  2  2  2  2\n[20881]  2  2  1  2  1  2  1  2  1  1  2  2  2  2  2  2  2  2  1  2  1  2  2  1\n[20905]  2  2  2  2  2  2  1  2  1  2  1  2  1  2  2  2  2  1  1  2  1  1  2  1\n[20929]  2  2  2  1  1  2  2  2  2  1  2  2  1  2  2  1 NA  1  1  1  2  1  1  2\n[20953]  1  2  2  1  2  1  1  1  1  2  1  2  2  2  1  1  2  1  1  2  2  2  2  1\n[20977]  2  2  2  1  1  2  2  1  2  2  1  2  2  2  2  3  1  2  2  1  2  2  2  2\n[21001]  2  2  1  2  2  1  1  2  2  2  1  2  2  1  2  2  1  1  2  2  2  1  3  2\n[21025]  2  1  1  2  1  1  2  1  2  1  1  2  1  2  1  2  2  1  1  2  2  2  2  2\n[21049]  2  2  1  2  2  2  1  2  2  2  2  2  1  1  2  1  2  2  1  2  1  1  2  2\n[21073]  1  1  3  2  1  1  1  1  2  1  2  1  2  2  2  2  1  2  2  2  2  2  2  1\n[21097]  1  2  1  2  2  2  1  1  1  1  1  2  2  2  2  2  1  2  2  2  1  1  1  1\n[21121]  2  1  2  1  3  2  2  2  2  2  2  2  2  2  2  1  1  1  1  2  2  1  2  2\n[21145]  2  2  1  1  1  1  2  2  2  2  2  1  1  1  2  2  2  2  1  2  2  2  1  1\n[21169]  2  1  2  2  2  1  3  1  1  2  1  2  1  1  1  2  2  1  2  2  2  2  2  1\n[21193]  1  2  2  2  2  1  2  2  2  2  1  2  2  1  2  2  2  2  2  2  2  2  2  2\n[21217]  2  1  2  2  2  2  2  2  1  1  2  2  2  2  2  1  1  1  2  1  2  1  2  1\n[21241]  2  1  1  2  1  2  1  1  1  1  2  2  1  2  2  2  2  1  2  3  2  2  2  2\n[21265]  2  2  2  2  2  2  2  2  1  2  1  2  1  2  2  2  1  1  2  1  2  2  2  2\n[21289]  1  1  2  2  1  2  1  2  1  3  2  2  1 NA  1  2  2  2  3  2  1  1  2  1\n[21313]  2  1  2  2  2  2  2  2  1  2  2  1  2  1  1  2  2  2  2  1  2  2  1  2\n[21337]  2  2  2  2  2  1  2  2  2  1  2  2  2  2  1  2  2  1  2  2  1  2  2  2\n[21361]  1  1  2  2  2  2  1  2  2  1  1  2  1  2  1  2  2  2  2  2  1  2  2  1\n[21385]  2  2  2  2  1  2  1  2  2  1  1  1  2  1  2  2  2  1  2  1  1  1  2  2\n[21409]  2  2  2  1  2  2  2  2  2  1  2  2  1  1  2  2  1  1  1  1  2  2  1  2\n[21433]  1  1  1  2  2  2  2  1  2  1  1  2  2  2  2  1  1  2  2  2  1  2  2  1\n[21457]  3  2  2  2  2  2  2  2  2  2  3  1  1  1  1  2  2  2  1  2  2  3  2  2\n[21481]  2  2  2  2  1  2  1  2  2  2  2  2  2  2  1  1  2  1  1  1  2  1  2  2\n[21505]  1  1 NA  1  2  2  2  2  1  1  2  2  1  1  1  1  2  2  2  2  2  2  2  2\n[21529]  2  2  2  2  2  2  1  2  2  1  2  2  2  2  2  2  3  1  1  1  2  2  1  1\n[21553]  1  2  2  1  1  2  2  1  2  1  2  2  1  2  2  2  2  2  1  1  2  2  1  2\n[21577]  2  2  2  1  2  2  2  2  2  2  2  2  1  2  1  2  2  2  2  1  1  1  2  1\n[21601]  1  1  2  1  2  2  2  1  1  1  1  2  2  2 NA  2  2  2  2  1  1  1  1  2\n[21625]  1  1  1  2  3  2  2  2  1  1  2  1  2  1  2  1  2  2  2  2  2  2  2  2\n[21649]  2  1  2  2  2  2  2  1  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2\n[21673]  2  1  2  2  2  1  1  2  2  2  2  1  1  1  2  1  1  2  1  1  2  2  2  2\n[21697]  1  2  2  2  2  2  2  2  2  2  1  2  1  2  2  2  2  2  2  2  1  1  2  2\n[21721]  2  2  2  1  1  3  2  2  2  2  1  1  1  1  2  2  1  2  1  2  2  1  1  2\n[21745]  2  2  2  2  1  1  2  2  2  2  1  2  2  2  2  2  2  2  1  2  2  1  2  2\n[21769]  1  2  1  1  2  2  2  2  2  1  2  2  2  1  2  2  1  1  2  1  2  1  2  2\n[21793]  1  2  2  2  2  2  2  1  2  2  1  1  1  1  2  1  2  2  2  2  2  2  2  2\n[21817]  2  1  2  2  2  2  2  2  1  2  1  1  1  2  1  1  1  1  2  1 NA  2  2  2\n[21841]  2  1  2  2  2  2  1  2  2  2  2  1  2  2  2  1  3  2  1  1  2  2  2  1\n[21865]  2  1  1  1  1  1  1  2  1  2  1  1  2  2  2  1  2  1  2  2  2  2  1  2\n[21889]  2  2  2  1  1 NA  2  2  2  1  2  2  2  2  2  2  2  2  2  1  2  2  2  2\n[21913]  2  1  2  1  2  2  2  2  2  2  2  2  2  2  1  1  1  2  1  1  2  2  2  1\n[21937]  1  1  2  1  1  2  2  2  2  1  2  2  2  2  1  1  2  1  1  2  2  1  1  1\n[21961]  1  1  1  2  2  2  2  2  2  2  1  1  1  1  1  1  1  2  2  1  2  1  2  1\n[21985]  1  1  2  2  2  1  1  2  2  2  2  2  1  1  2  2  1  2  1  2  1  1  2  1\n[22009]  1  1  1  3  3  2  2  1  2  2  1  1  1  1  2  1  2  1  2  1  1  2  2  2\n[22033]  2  2  1  1  2  1  2  1  1  2  1  1  2  2  2  2  2  2  2  2  2  2  1  2\n[22057]  2  2  1  2  2  1  2  1  2  1  2  2  1  2  1  2  1  1  2  2  1  1  2  2\n[22081]  1  3  2  2  3  2  2  2  1  2  1  2  2  1  2  2  2  1  2  2  2  2  2  2\n[22105]  2  2  1  2  2  2  2  2  1  1  2  2  1  1  1  2  1  1  2  2  2  1  2  1\n[22129]  2  1  2  1  2  2  2  2  1  2  1  1  1  2  2  1  1  2 NA  2  2  2  2  1\n[22153]  1  2  2  2 NA  2  2  1 NA  2  2  2  3  2  2  1  2  1  2  1  3  1  2  2\n[22177]  2  2  2  2  1  2  2  2  2  2  2  2  2  2  1  1  2  1  2  3  2  1  1  2\n[22201]  1  1  1  1  1  2  1  1  2  1  2  2  1  2  1  1  1  2  2  2  2  2  2  2\n[22225]  2  2  1  2  2  2  2  2 NA  2  2  1  2  1  1  2  2  1  2  1  1  1  2  1\n[22249]  2  2  2  2  2  1  2  2  2  1  1  1  2  2  2  2  2  2  2  2  2  2  2  2\n[22273]  2  3  2  2  1  2  2  1  2  1  2  2  2  2  1  1  1  2  2  2  1  2  2  1\n[22297]  2  2  1  2  1  2  1  2  1  1  2  2  2  1  2  2  2  2  2  1  1  1  2  2\n[22321]  1  2  2  2  2  1  2  1  2  2  2  2  2  1  2  1  1  1  1  1  1  1  2  2\n[22345]  2  2  3  1  1  1  2  2  2  1  2  2  2  2  1  1  1  2  1  2  2  1  2  1\n[22369]  2  1  2  2  1  2  2  2  2  2  1  1  1  1  1  1  2  1  2  2  1  2  2  2\n[22393]  1  1  1 NA  2  1  1  2  2  1  1  2  2  2  2  2  1 NA  1  2  1  2  2  1\n[22417]  2  1  1  2  2  1  2  2  2  1  1  1  1  2  1  1  2  2  2  2  2  2  2  1\n[22441]  2  2  2  2  2  2  2  2  1  2  1  2  1  2  2  2  2  2  2  2  2  2  2  2\n[22465]  2  1  2  2  1  2  2  1  2  1  2  2  2  1  1  2  2  1  1  2  2  2  2  2\n[22489]  2  1  2  2  2  2  2  1  1  1  2  2  2  1  2  2  2  2  1  3  2  2  2  2\n[22513]  1  2  2  1  2  2  2  2  1  2  1  2  1  2  1  1  2  2  1  2  2  2  1  2\n[22537]  1  1  1  2  2  1  1  1  1  2  1  1  2  1  2  1  2  2  1  1  1  1  2  1\n[22561]  1  2  2  2  2  2  1  1  2  2  2  2  2  2  1  1  2  1  2  1  2  2  2  2\n[22585]  2  1  1  2  1  2  1  2  2  2  2  2  2  1  1  1 NA  2  2  2  2  2  1  1\n[22609]  2  2  2  2  1  2  2  2  2  1  2  2  2  2  1  1  2  1  2  1  2  2  2  1\n[22633]  2  1  1  2  2  1  1  1  2  1  1  2  1  1  2  2  2  2  2  2  1  1  2  2\n[22657]  2  2  2  2  1  2  1  3  2  2  2  2  2  2  1  1  2  2  1  1  2  1  2  2\n[22681]  1  2 NA  2  2  2  1  2  2  1  2  2  2  2  2  1  2  1  2  1  1  1  2  2\n[22705]  1  2  2  2  2  2  2  1  1  2 NA  1  1  2  1  2  1  2  2  1  2  2  2  1\n[22729]  1  1  1  2  2  2  2  1  1  1  1  2  1  1  2  2  2  2  1  1  2  1  1  1\n[22753]  2  2  1  2  2  2  2  2  2  1  1  1  1  1  2  2  2  1  2  1  1  2  1  2\n[22777]  2  2  1  2  2  2  2  2  1  2  1  1  1  1  1  1  2  2  2  2  2  1  2  2\n[22801]  1  1  1  2  2  1  1  1  2  2  2  1  2  2  2  1  1  2  2  1  2  2  1  1\n[22825]  1  1  1  1  2  2  2 NA  2  2  2  1  2  1  2  2  2  1  1  2  2  2  2  1\n[22849]  2  2  1  2  1  2  1  2  1  2  1  2  2  2  1  2  2  2  2  2  2  2  2  1\n[22873]  2  2  2  2  2  2  1  2  2  1  2  2  1  2  1  2  2  2  2  2  1  2  2  1\n[22897]  2  1  1  1  1  2  1  1  1  2  2  2  2  2  2  2  2  2  2  1  1  2  2  2\n[22921]  1  1  2  1 NA  2  2  2  2  2  2  2  2  2  1  2  1  1  2  1  2  1  1  2\n[22945]  2  1  2  1  2  1  2  2  2  2  2  1  1  2  2  2  2  1  1  2  2  1  1  2\n[22969]  1  2  2  2  1  2  2  2  2  2  2  2  2  2  2  1  2  1  2  2  1  2  1  1\n[22993]  1  2  1  1  2  2  1  2  2  2  2  2  2  2  2  2  2  1  1  1  2  1  2  1\n[23017]  2  1 NA  1  2 NA  2  2  2  1  2  2  2  2  2  1  2  1  1  2  1  1  2  1\n[23041]  2  1  2  1  2  2  2  2  2  2  1  1  2  1  1  3  2  2  1  2  1  1  2  2\n[23065]  2  2  1  2  2  2  2  2  2  2  1  2  1  2  2  1  1  2  1  2  2  2  2  2\n[23089]  1  1  2  1  2  2  2  1  2  2  1  2  1  2  2  1  2  2  2  2  1  2  2  1\n[23113]  1  2  2  2  2  1  1  2  2  3  1  2  2  2  2  2  2  2  1  2  2  2  2  2\n[23137]  1  2  2  1  2  2  2  2  2  1  2  2  2  2  2  1  2  2  1  1  2  2 NA  2\n[23161]  2  2  2  1  1  2  1  1 NA  1  1  1  2  1  2  3  1  2  2  2  2  1  2  2\n[23185]  1  1  1  2  2  2  2  1  1  2  1  2  2  2  3  1  1  2  3  1  2  2  2  2\n[23209]  1  1  1  2  2  2  2  1  2  2  1  2  2  2  2  1  2  1  1  2  1 NA  2  1\n[23233]  1  1  1  2  1  2  2  1  1  2  2  1  2  2  2  1  2  2  2  2  1  2  1  1\n[23257]  1  1  1  1  2  2  2  2  1  1  1  2  2  2  1  1  2  2  2  2  1  2  2  2\n[23281]  2  2  2  1  1  1  2  2  2  2  2  2  2  2  2  1  2  2  2  2  2  2  1  2\n[23305]  2  1  2  2  2  1  1  1  1  2  1  2  2  2  2  1  2  2  2  2  2  2  1  1\n[23329]  2  1  2  2  1  1  2  2  2  1  2  2  2  2  3  1  2  2  2  1  2  1  2  1\n[23353]  2  1  2  2  1  2  1  1  1  2  1  1  2  2  2  1  2  2  1  2  2  2  2  1\n[23377]  2  1  1  2  1  2  2  2  1  1  2  2  1  2  1  1  1  2  1  1  2  1  1  1\n[23401]  2  1  1  2  2  2  2  1  1  2  1  1  1  3  2  2  1  1  1  2  1  1  2  2\n[23425]  2  2 NA  1  2  2  1  2  2  2  2  1  2  2  2  1  2  2  1  2  2  1  1  2\n[23449]  1  2  2  1  2  2  1  1  1  1  2  2  1  2 NA  1  2  1  2  2  1  2  2  2\n[23473]  1  1  1  2  2  2  1  2  2  2  2  1  2  2  2  2  1  1  2  1  1  1  2  2\n[23497]  2  1  2  2  2  2  3  1  2  2  1  2  1  1  1  1  2  1  2  2  1  2  2  2\n[23521]  2  2 NA  2  1  2  2  1  2  2  2  2  1  2  2  2  1  1  1  2  2  1  2  1\n[23545]  2  1  2  2  2  1  1  2  1  2  1  1  1  2  2  2  2  2  2  1  1  2  1  1\n[23569]  2  1  2  2  2  2  2  1  2  2  2  1  2  2  1  2  1  2  1  2  2  2  2 NA\n[23593]  2 NA  1  2  2  2  2  2  2  1  1  2  2  1  1  2  1  2  2  2  2  2  1  2\n[23617]  2  2  2  1  2  1  2  2  1  2  2  2  2  1  2  2  2  2  2  1  2  1  1  2\n[23641]  1  2  2  2  2  1  2  2  2  2  2  1  1  1  2  2  2  2  2  2 NA  1  2  1\n[23665]  2  2  1  2  2  2  2  1  2  1  2  2  2  2  2  2  1 NA  2  2  2  2  2  1\n[23689]  2  2  1  1  1  1  2  2  2  2  2  1  1  2  2  2  2  2  1  2  2  2  1  1\n[23713]  2  2  1  1  2  2  1  2  1  2  2  2  2  1  2  2  2  2  1  1  2  2  1  2\n[23737]  1  1  1  2  2  2  1  2  2  1  1  1  1  1  2  2  2  2  2  1  2  1  2  3\n[23761]  1  1  2  2  1  2  2  2  2  2  2  2  2  1  2  1  2  2  2  2  1  2  1  1\n[23785]  2  1  2  1  2  2  1  2  2  2  1  1  1  2  1  2  2  1  2  2  2  2  2  2\n[23809]  2  2  1  1  1  2  2  1  3  1  2  1  2  2  1  1  1  1  2  2  1  2  1  2\n[23833]  2  2  2  1  2  2  1  1  1  2  2  1  1  2  2  1  1  2  1  2  1  2  2  2\n[23857]  2  3  2  2  1  2  2  2  2  2  1  2  1  1  2  2  2  1  2  1  1  2  3 NA\n[23881]  2  1  1  2  2  2  2  2  1  2  2  1  2  1  1  2  1  2  1  2  2  2  2  2\n[23905]  1  2  1  1  1  2  1  1  2  1  1  2  2  1  2  2  2  2  2  2  2  2  2  1\n[23929]  2  2  1  3  2  2  2  1  1  1  1  1  1  2  2  2  1  2  2  2  2  2  1  2\n[23953]  1  2  2  2  2  2  1  2  1  2  2  1  2  1  1  2  1  1  1  1  1  3  2  2\n[23977]  1  1  1  2  2  1  2  1  2  1  1  1  1  2  2  1  1  1  2  2  1  1  1  2\n[24001]  2  1  2  2  1  1  2  1  2  2  2  3  2  1  2  2  2  1  2  2  2  1  1  1\n[24025]  2  1  2  1  2  2  2  2  2  2  2  2  2  2  2  2  2  1  2  1  2  2  2  2\n[24049]  2  1  2  2  2  1  2  2  2  2  1  2  1  1  2  2  2  1  2  2  2  1  2  2\n[24073]  2  2  1  2  1  1  2  2  2  2  2  2  1  1 NA  2  1  1  2  2  2  2  2  1\n[24097]  1  1  1  1  2  1  1  2  2  1  2  1  2  2  2  1  1  2  3  1  1  1  2  2\n[24121]  2  1  2  2  2  1  2  1  2  2  2  2  1  1  1  1  1  2  1  2  1  2  2  1\n[24145]  1  1  2  1  2  1  1  1  2  1  2  1  2  2  2  1  2  2  1  2  2  2  2  1\n[24169]  2  1  1  1  2  2 NA  1  2  1  2  2  2  1  1  2  2  1  2  1  3  2  1  2\n[24193]  1  2  3  2  2  2  1  2  2  2  2  2  2  2  2  1  1  2  2  1  1  2  2  1\n[24217]  1  1  2  2  2  2  2  1  2  1 NA  2  1  2  1  2  1  1  1  2  2  2  2  1\n[24241]  2  2  2  1  2  1  1  1  2  1  1  1  1  2  2  1  2  2  2  2  2  2  2  1\n[24265]  2  1  1  1  2  2  2  2  1  2  2  2  1  1  2  2  2  2  1  2  1  1  1  1\n[24289]  1  2  1  2  1  2  1  2  2  2  2  1  1  1  2 NA  2  1  1  1  2  1  1  1\n[24313]  1  1  2  1  1  1  2  2  2 NA  1  1  1  1  2  1  2  2  1  1  2  2  2  1\n[24337]  2  2  2  1  2  2  2  2  2  2  1  2  2  2  1  2  1  2  2  2  1  1  2  2\n[24361]  1  2  1  2  1  1  1  1  1  3  1  1  2  1  2  2  2  1  1  2  1  1  2  1\n[24385]  1  2  1  2  1 NA  1  2  2  2  2  3  2  1  2  2  2  1  1  1  1  2  2  1\n[24409]  2  2  2  2  2  1  2  2  1  2  2  1  2  2  1  1  1  1  2  2  2  1  1  1\n[24433]  2  1  2  3  2  1  1  2  1  2  2  2  2  2  1  1  2  1  1  1  2  1  1  1\n[24457]  2  1  2  1  1  2  2  2 NA  1  1  1  2  1  1  2  2  2  1  1  2  2  1  2\n[24481]  2  1  1  1  2  2  2  1  2  1  2  2  1  2  2  2  2  2  1  2  1  2  1  2\n[24505]  1  2  2  1  1  1  1  1  2  1  2  2  1  1  1  2 NA  2  2  2  1  2  1  2\n[24529]  2  2  1  1  2  1  1  1  2  2  1  1  1  1  1  2  1  1  1  1  2  2  2  2\n[24553]  1  2  1  1  2  1  2  2  2  2  1  2  1  2  2  2  1  1  1  1  2  2  2  2\n[24577]  2  1  1  1  1  2  2  2  2  1  2  2  2  1  2  2  1  1  1  1  1  2  1  2\n[24601]  2  2  2  2  2  1  1  2  2  1  2  2  1  2  2  1  2  1  2  2  1  1  2  2\n[24625]  2  1  1  1  1  1  2  2  2  1  1  1  2  2  1  1  2  2  1  1  2  1  1 NA\n[24649]  1  2  2  2  1  2  2  2 NA  2  1  1  2  2  1  2  2  2  1 NA  3  2  2  2\n[24673]  2  1  2  2  2  2  2  3  2  2  2  1  2  2  1  2  1  2  1  1  2  2  2  1\n[24697]  1  1  1  2  2  2  2  2  2  1  2  3  2  2  2  2  2  1  2  1  2  1  2  2\n[24721]  2  2  1  1  1  2  2  1  2  2  2  2  1  1  1  1  2  2  1  1  2  1  1  1\n[24745]  2  1  1  2  2  1  1  1  1  1  2  2  2 NA  2  1  1  2  1  1  2  1  2  1\n[24769]  2  2  2  2  2  2  1  1  1  2  1  2  1  2  1  1  2  2  2  2  2  1  1  1\n[24793]  1  2  1  2  1  2  2  2  1  2  1  2  1  1  2  2  2  2  2  2  1  1  2  1\n[24817]  2  2  1  2  2  2  2  2  1  1  2  1  2  2  2  2  1  1  1  2  2  2  2  2\n[24841]  1  2  1  2  1  2  2  2  1  2  2  2  1  2  1  2  1  2  1  2  2  2  2  1\n[24865]  1  2  1  1  2  2  2  1  1  1  2  1  2  1  1  2  1  2  1  1  2  1  2  1\n[24889]  1  2  1  2  1  1  2  1  1  1  1  1  1  2  2  2  1  1  2  1  1  2  1  1\n[24913]  1  1  1  1  2  2  1  1  1  1  2  1  1  1  2  2  2  2  2  1  2  1  1  1\n[24937]  1 NA  2  1  2  2  1  1  2  2  2  1  1  1  1  1  1  1  2  1  2  1  2  2\n[24961]  2  2  1  2  2  2  2  1  2  1  2  1  1  2  1  2 NA  2  1  2  2  1  1  1\n[24985]  1  2  2  2  2  2  1  2  1  1  2  2  2  2  2  2  1  2  2  2  2  1 NA  2\n[25009]  1  2  2  1  2  1  2  2  2  2  2  2  2  2  2  2  1  2  1  2  2  2  1  2\n[25033]  1  1  2  2  2  2  2  1  2  2  2  2  1  2  2  2  1  2  2  2  2  2  2  2\n[25057]  1  2  2  1  2  1  1  1  2  2  2  2  2  2  2  1  2  1  2  1  2  1  1  1\n[25081]  1  1  2  1  1  1  2  1  2  2  2  2  2  2  2  2  2  1  2  2  1  2  2  1\n[25105]  1  2  2  2  2  1  2  1  2  2  1  2  1  2  2  2  1  1  2  1  1  2  2  1\n[25129]  2  1  1  2  2  2  1  2  2  2  2  2  2  2  2  2  2  1  2  2  1  1  2  2\n[25153]  2  2  2 NA  1  1  2  2  1  1  2  2  1  1  2  2  1  2  2  2  1  2  1  2\n[25177]  1  1  1  1  1  2  2  1  1  2  2  2  1  1  1  1  2  1  1  2  2  1  2  2\n[25201]  2  1  2  2  2  2  2  2  2  1  2  2  2  1  2  1  2  2  1  1  2  2  2  2\n[25225]  2  2  1  2  1  1  2  2  2  2  1  2  2  2  2  1  2  2  1  2  2  2  2  1\n[25249]  2  2  2  1  1  2  2  2  2  2  2  1  2  1  1  1  2  2  1  1  1  1  2  2\n[25273]  2  1  1  2  2  2  2  2  2  1  1  1  1  1  1  1  1  2  1  2  2  1  1  2\n[25297]  1  2  1  2  2  1  2  1  1  2  1  2  2  2  2  2  1  1  1  1  2  1  1  1\n[25321]  1  2  1  2  1  2  2  2  2  2  2  2  1  2  1  1  2  2  2  1  1  3  2  2\n[25345]  2  2  2  2  1  1  2  1  1  2  1  1  2  2  1  2  1  2  2  2  1  1  1  2\n[25369]  2  1  2  2  2  2  2  2  2  1  1  1  1  2  1  1  2  2  2  2  2  1  2  2\n[25393]  1  2  2  2  2  1  2  1  2  2  2  2  2  2  2  1  2  1  1  2  2  2  2  1\n[25417]  2 NA  2  2  2  1  2  2  2  2  2  2  2  2  2  2  2  2  1  1  2  2  2  2\n[25441]  2  1  1  2  1  1  1  1  1  2  2  2  1  2  2  2  1  2  2  2  2  2  2  2\n[25465]  2  1  2  1  2  2  1  2  1  1  1  1  2  2  1  2  2  2  2  2  1  2  2  2\n[25489] NA  2  1  2  1  2  2  2  1  2  2  2  1  1  1  2  2  2  1  2  2  2  2  2\n[25513]  1  2  2  2  1  2  2  2  2  2  2  1  1  1  2  2  1  2 NA  2  1  1  2  1\n[25537]  1 NA  1  2  2  2  1  2  2  2  1  1  1  2  2  2  3  1  2  2  1  2  2  2\n[25561]  1  2  2  2  2  1  2  2  2  2  2  1  2  1  2  2  2  2  2  2  1  2  1  1\n[25585]  1  2  1  1  2  1  1  1  1  1  1  2  2  2  2  1  2  1  1  2  2  1  1  1\n[25609]  2  2  2  3  2  1  2  2  2  1  2  2  1  2  2  2  1  2  2  2  2  2  1  2\n[25633]  1  2  2  2  2  2  2  2  1  1  2  2  1  2  2  2  2  2 NA  1  1  2  2  2\n[25657]  1  2  1  2  2  2  2  1  1  2  2  2  2  1  1  2  1  1  3  1  2  1  2  1\n[25681]  2  1  2  1  1  2  2  2  2  1  1  1  1  1  1  1  1  2  2  2  1  1  1  1\n[25705]  1  2  2  1  1  1  1  2  1  2  2  2  2  2  2  2  1  1  1  1  1  1  2  2\n[25729]  2  1  2  1  1  2  1  2  2  1  2  2  2  2  2  2  2  2  2  2  2  2  1  1\n[25753]  2  1  2  2  2  1  1  2  2  1  1  1  2  2  3  1  2  2  2  2  2  2  1  1\n[25777]  2  1  2  1  1  2  1  2  2  2  1  1  1  1  1  2  2  1  2  2  1  2  1  2\n[25801]  2  2  2  2  2  2  2  1  2  2  2  2  2  2  1  1  2  2  2  2  2  1  1  2\n[25825]  1  2  1  2  1  2  2  1  2  2  1  2  1  1  1  2  2  2  1  1  1  1  1  2\n[25849]  1  2  1  1  1  2  2  2  2  2  2  2  2  2  2  2  1  1  2  1  1  2  2  1\n[25873]  1  1  2  1  2  2  2  2  2  2  2  2  2  2  2  1  2  1  2  1  1  2  2  2\n[25897]  2  2  1  1  1  1  2  1  2  2 NA  1  2  1  1  2  1  2  2  1  2  2  2  2\n[25921]  2  2  2  1  2  1  1  2  1  1  2  2  2  2  1  1  2  1  2  2  1  2  2  2\n[25945]  2  1  2  1  1  2  1  2  1  2  2  1  1  1  2  2  1  2  2  1  1  1  2  3\n[25969]  1  2  2  2  2  2  1  1  2  1  2  2  1  1  1  2  1  2  1  1  1  1  1  1\n[25993]  1  1  2  1  2  1  1  2  1  2  2  2  2  2  2  1  2  1  2  1  1  1  1  1\n[26017]  2  2  2  2  2  2  2  2  2  2  1  2  1  2  1  2  1  2  2  2  1  2  1  2\n[26041]  1  2  2  2  2  1  2  1  2  2  1  1  2  1  2  2  2  1  1  2  1  2  1  2\n[26065]  2  2  1  2  2  2  2  2  2  2  1  1  1  2  2  2  2  2  1  2  1  1  2  2\n[26089]  2  2 NA  1  1  1  2  1  1  2 NA  2  1  1  2  2  2  3  2  2  1  1  2  1\n[26113]  2 NA  2  1  1  2  2  1  1  2  1  2  2  2  1  1  2  1  1  1  2  2  1  2\n[26137]  1  2  2  1  2  2  2  2  1  2  2  2  2  1  2  2  2  2  2  2  2  1  2  1\n[26161]  2  1  1  2  2  2  1  1  2  2  1  1  2  2  2  1  2  1  2  2  1  2  2  2\n[26185]  2  2  2  2  1  1  2  2  1  1  1  2  2  1  1  2  2  2  1  2  1  2  1  2\n[26209]  1  2  2  1  1  1  1  2  2  1  2  2  1  2  2  2  2  1  2  2  1  2  2  2\n[26233]  2  2  2  2  2  1  1  1  2  2  1  2  1  1  2  2  1  2  1  2  1  2  2  2\n[26257]  1  1  1  2  1  2  2  2  1  2  2  2  1  1  1  1  1  2  1  1  2  2  1  2\n[26281]  1  2  2  2  2  2  2  1 NA  2  1  2  1  2  2  2  1  2  2  2  1  2  2  2\n[26305]  2  2  2  2  2  2  1  1  2  2  2  1  2  2 NA  2  1  2  1  2  2  2  1  2\n[26329]  2  1  2  2  1  2  1  1  2  2  2  2  1  1  2  1  2  1  1  2  1  2  1  2\n[26353]  1  2  2  1  2  2  1  2  2  1  1  2  2  1  1  2  2  1  1  2  2  1  2  1\n[26377]  1  1  2  2  2  2  1  1  1  2  1  2  2  1  2  2  2  2  1  2  2  2  2  2\n[26401]  2  1  1  2  2  2  2  2  2  2  1  2  2  1  2  1  1  1  2  2  2  1  2  1\n[26425]  2  2  1  1  2  2  2  1  2  2  1  2  1  2  2  2  1  1  1  2  1  2  2  2\n[26449]  1  2  1  2  1  2  2  1  1  1  1  1  1  2  1  2  2  2  2  2  1  2  1  1\n[26473]  2  2  2  2  2  2  1  1  2  2  2  2  2  2  1  2  2 NA  1  1  2  2  2  2\n[26497]  2  1  2  2  1  2  2  1  2  1  2  2  1  2  2  2  2  1  1  2  2  1  2  2\n[26521]  1  1  1  2  2  1  2  1  2  1  1  2  2  1  1  2  1  2  2  2  1  2  2  2\n[26545]  2  1  2  1  2  1  2  2  2  1  1  2  2  2  2  2  1  1  1  2  1  2  2  1\n[26569]  2  2  1  2  2  1  2  2  2  2  2  2  1  2  2  2  1  1  1  2  2  2  2  1\n[26593]  2  1  2  1  1  1  2  1  2  3  2  1  2  1  2  1  2  1  2  1  2  2  1  2\n[26617]  2  2 NA  2  2  2  1  2  1  2  2  1  2  2  2  2  1  2  1  2  2  1  1  1\n[26641]  2  2  2  2  1  2  2  2  2  2  2  1  2  2  2  2  1  1  2  2  2  1  2  2\n[26665]  2  2  2  2  1  2  2  2  2  1  2 NA  1  1  2  2  2  1  2  2  2  2 NA  2\n[26689]  2  1  2  2  2  1  2  2  1  1  2  2  1  1  2  2  2  2  1  2  1  2  2  1\n[26713]  1  2  1  2  2  2  2  1  2  2  1  2  2  2  2  1  2  1  2  2  2  2  2  2\n[26737]  2  2  2  2  2  2  2  1  2  1  2  1  1  2  2  1  2  1  2  1  2  1  1  1\n[26761]  1  1  2  2  1  1  2  1 NA  1  1  1  2  1  2  1  1  2  2  2  2  2  2  2\n[26785]  2 NA  1  1  2  2  2  1  1  2  2  1  1  2  2  1  2  2  1  1  2  2  2  2\n[26809]  2  2  2  2  2  2  2  1  2  1  2  2  2  1  2  1  1  2  2  1  1  1  2  2\n[26833]  2  2 NA  2  1  1  1  2  2  2  1  2  2  1  1  2  2  2  2  1  2  2  1  1\n[26857]  1  2  1  1  2  2  1  2  1  2  2  1  2  2  1  2  1  2  2  2  1  2  2  1\n[26881]  2  2  2  1  1  2  1  2  2  2  2  1  2  1  1  2  2  2  2  2  2  1  2  1\n[26905]  2  2  1  2  2  1  1  2 NA  1  2  1  1  2  2  2  2  2  2  2  2  2  1  2\n[26929]  2  1  2  2  1  2  1  2  2  2  2  1  1  2  2  2  2  2  1  2  1  2  1  1\n[26953]  1  1  1  2  2  1  1  1  2  2  1  1  1  1  1  2  1  1  1  2  1  2  2  2\n[26977] NA  2  2  2  2  1  2  2  2  2  2  1  1  1  2  1  1  1  1  1  2  2  2  2\n[27001]  1  2  2  2  1  1  2  2  2  2  2  2  1  2  2  2  2  1  1  2  2  1  2  2\n[27025]  1  1  1  2  2 NA  2  1  2  1  1  2  1  2  1  2  2  2  1  2  2  1  2  1\n[27049]  1  1  1  2  1  1  1  2  2  1  1  1  2  1  2  1  2  1  1  2  2  2  2  2\n[27073]  2  1  1  2  1  1  2  2  2  2  1  2  2  2  2  1  2  2  1  1  1  2  1  2\n[27097]  2  2  2  2  1  2  1  1  1  2  1  2  2  2  1  1  2  2  1  2  2  3  1  1\n[27121]  1  2  1  2  2  2  1  1  1  2  1  2  1  2  1  1  2  1  1  2  1  1  1  2\n[27145]  3  2  2  2  1  2  1  1  1  1  1  2  1  1  1  1  1  1  2  1  2  1  2  1\n[27169]  1  2  1  1  2  1  2  1  2  2  2  1  1  2  2  2  2  2  1  2  1  1  1  2\n[27193]  1  2  1  2  1  2  1  2  2  1  2  2  2  2  2  2  1  2  2  1  2  2  1  2\n[27217]  2  2  2  1  1  2  2  2  2  2  1  1  1  2  1  1  2  1  1  1  2  2  2  2\n[27241]  1  2  2  1  2  2  1  1  2  1  1  1  1  1  2  1  2  2  1  2  2  1  1  1\n[27265]  1  2  1  2  2  2  2  1  1  2  2  2  1  2  2  3  2  2  2  2  2  2  1  2\n[27289]  1  2  1  2  2  1  2  1  2  1  2  1  2  1  1  2  2  2  2  2  1  1  1  2\n[27313]  2  1  2  2  1  1  2  1  1  2  2  2  2  2  2  1  1  1  1  2  1  2  1  1\n[27337]  1  2  1  1  2  2  2  1  1  1  1  2  2  2  2  2  2  2  2  1  2  2  2  1\n[27361]  1  1  1  2  1  2  2  2  1  1  2  2  2  2  1  1  2  2  1  1  2  2  2  2\n[27385]  1  1  2  2  2  1  1  1  2  2  2  1  2  2  2  2  2  2  2  2  1  2 NA  2\n[27409]  1  1  1  1  2  1  1  2  1  2  1  2  2  1  2  2  1  1  1 NA  1  1  1  2\n[27433]  2  1  2  1  1  2  2  2  2  2  2  1  2  2  1  2  2  1  1  1  1  1 NA  2\n[27457]  2  1  2  2  2  1  2  2  1  2  2  1  2  2  1  2  1  1  2  1  2  2  2  1\n[27481]  2  1 NA  2  2  2  2  2  2  1  2  2  1  1  1  2  2  2  2  1  2  1  2  3\n[27505]  1  2  2  2  1  2  1  2  2  1  2  1  2  1  2  1  2  1  2  2  2  2  2  2\n[27529]  1  2  2  2  2  2  2  2  2  1  2  2  1  1  1  2  2  1  1  2  1  2  1  2\n[27553]  1  2 NA  2  1  2  1  1  2  1  2  1  1  1  2  1  1  1  2  1  1  1  2  1\n[27577]  2  1  1  2  1  2  2  1  2  1  2  2  2  1  2  2  2  2  2  1  1  2  2  1\n[27601]  2  1  2  2  1  2  2  3  2  2  1  1  2  1  2  1  2  1  1  1  1  1  2  2\n[27625]  2  2  2  2  2  2  2  2  1  2  1  2  1  1  3  1  2  2  2  2  2  1  2 NA\n[27649]  1  1  2  1  2 NA  2  1  1  2  2  2  2  1  2  2  1  2  2  2  2  2  2  1\n[27673]  1  2  2  1  2  1  2  2  2  1  2  1  2  2  2  2  1  2  2  2  2  2  2  1\n[27697]  1  1  1  2  2  1  2  1  1  2  2  2  2  2  1  1  1  1  2  1  2  2  2  1\n[27721]  2  1  2  1  1  1  1  1  2  2  1  2  2  2  1  1  2  2  1  3  2  2  2  2\n[27745]  2  2  2  2  2  1  2  2  1  1  2  1  2  2  1  1  1  2  1  2  1  1  1  1\n[27769]  1  2  2  2  1  3  2  1  1  2  1  2  2  2  1  1  1  2  1  2  2  2  2  2\n[27793]  2  2  2  1  2  2  2  2  2  1  1  1  1  1  1  2  2  2  1  2  1  1  2  1\n[27817]  1  2  2  2  1  2  2  1  1  1  2  2  2  1  2  1  2  2  1  2  1  2  2  2\n[27841]  2  2  2  2  2  1  2  2  2  1  2  1  2  1  1  2  2  2  1  2  1  1  2  1\n[27865]  1  2  2  1  2  1  2  2  2  2  2  1  2  2  1  2  2  1  2  2  2  2  2  1\n[27889]  2  2  1  1  2  1  1  1  2  1  2  2  1  1  2  1  2  2  2  1  2  2  1  2\n[27913]  1  1  1  1  2  2  2  2  2  1  2  2  1  1  2  2  2  2  1  2  1  2  2  1\n[27937]  2  2  2  2  2  2  2  2  2  1  1  2  2  1  2  1  2  2  2  1  2  2  2  2\n[27961]  2  1  1  2  2  2  2  2  2  2  1  2  1  2  1  1  1  1  1  2  2  1  2  2\n[27985]  2  1  3  2  1  2  2  1 NA  2  2  2  2  2  1  2  1  2  2  1  1  2  2  1\n[28009]  2  2  2  2  1  1  1  2  2  2  1  2  2  2  2  2  1  2  1  1  2  2  2  2\n[28033]  2  2  2  1  2  2  2  2  2  2  2  1  2  1  1  2  2  2  1  2  3  2  1  2\n[28057]  2  1  2  1  2  2  2  2  1  1  2  2  2  2  1  2 NA  2  2  1  2  1  1  1\n[28081]  2  2  1  2  1  2  2  1  1  2  1  1  1  2  1  2  1  2  1  1  2  2  1  1\n[28105]  2  2  2  1  1  2  1  2  2  1  2  2  1  1  2  2  1  2  2  2  2  1  2  2\n[28129]  1  2  1  2  1  1  1  1  2  1  1  1  2  1  1  2  1  2  2  1  1  2  1  1\n[28153]  1  2  2  2  2  1  2  2  2  2  2  1  2  1  1  1  1  3  1  2  1  2  2  2\n[28177]  1  1  1  2  1  1  2  1  1  2  2  1  2  1  2  2  1  1  1  1  1  1  2  2\n[28201]  2  1  2  2  2  2  2  2  1  1  2  2  2  1  2  1  2  2  2  1  1  1  2  2\n[28225] NA  2  1  2  2  2  2  2  2  2  2  1  1  2  2  1  2  1  2  2  1  2  2  2\n[28249]  1  1  1  1  1  2  2  1  2  1  2  1  1  2  1  1  1  2  1  1  2  2  1  2\n[28273]  1  2  1  2  2  1  2  2  2  1  2  2  1  2  2  1  1  3  2  1  2  1  1  2\n[28297]  1  2  2  1  2 NA  2  1  2  1  2  2  1  2  2  2  2  2  2  1  2  2  3  2\n[28321]  2  2  1  1  1  1  2  2  2  2  2  2 NA  2  2  1  1  1  2  1  1  2  1  1\n[28345]  1  1  1  1  2 NA  1  1  2  2  2  2  2  2  1  2  2  2  2  1  1  1  2  2\n[28369]  1  2  2  2  2  2  1  2  1  2  2  1  2  2  2  2  2  1  2  1  1  2  1  2\n[28393]  2  2  2  2  1  2  2  2  2  1  1  2  2  2  2  1  2  2  1  2  1  2  2  2\n[28417]  1  1  2  2  1  2  2  2  2  2  2 NA  2  2  2  2  2  2  2  2  2  1  1  2\n[28441]  2  1  1  2  2  1  1  2  2  2  1  1  2  2  2  1  2  2  2  1  2  2  2  2\n[28465]  2  2  2  2  1  2  1  2  2  2  2  2  2 NA  1  2  1  2  2  1  1  2  1  2\n[28489]  2  1  2  2  1  2  2  2  2  1  1  2  2  2  2  2  2  3  2 NA  2  2  2  1\n[28513]  1  2  2  2  1  2  2  2  1 NA  2  2  1  1  1  2  1  2  2  1  1  1  1  2\n[28537]  1  1  2  2  2  1  2  2  2  1  2  1  3  1  2  2  1  2  1  1  2  2  2  2\n[28561]  1  1  1  2  1  1  2  1  2  2  1  1  2  1  2  2  2  1  1  2  1  2  1  1\n[28585]  2  1  2  1  1  1  2  2  2  1  1  2  1  2  1  2  2  1  1  1  1  2 NA  1\n[28609]  1  2  1  1  2  1  1  2  1  1  1  2  2  1  1  1  2  2  2  2  2  1  2  2\n[28633]  2  2  2  1  1  2  2  2  2  1  2  1  2  3  2  2  2  2  2  2  1  2  2  2\n[28657]  1  2  2  2  1  1  1  1  2  1  2  1  1  2  2  1  2  2  1  2  2  1  2  1\n[28681]  2  2  1  1  2  2  2  1  1  2  2  2  1  2  2  2  1  2  2  1  2  1  1  2\n[28705]  2  2  2  2  2  1  2  2  2  2  1  1  1  2  2  1  2  2  1  2  1  2  2  2\n[28729]  2  2  2  2  2  2  1  1  2  2  2  2  1  2  1  2  2  2  2  2  1  1  2  2\n[28753]  1  1  2  2  1  1  1  2  1  1  1  1  2  1  2  2  1  1  2  1  2  2  2  2\n[28777]  1  1  2  2  2  2  2  2  2  2  2  1  2  2  1  2  2  2  1  2  1  2  3  1\n[28801]  2  1  2  2  1  2  2  2  1  2  1  2  2  1  1  2  1  2  2  2  2  2  1  2\n[28825]  2  1  2  2  2  2  2  2  2  1  2  1  2  2  2  2  2  1  2  2  2  2  1  2\n[28849]  1  2  2  2  2  2  1  1  2  2  2  2  1  2  2  2  1  1  2  2  1  2  2 NA\n[28873]  1  2  2  2  2  1  2  2  2  2  1  2  1  2  2  2  2  2  2  1  2  1  1  1\n[28897]  1  2  1  2  2  1  3  2  1  2  2  1  1  1  1  2  2  2  1  2  2  1  2  1\n[28921]  1  2  2  2  2  2  2 NA  2  2  1  2  2  2  1  2  2 NA  2  1  1  2  1  2\n[28945]  1  2  2  1  1  1  2  1  2  1  2  2  1  2  1  2  1  2  1  2  2  2  2  2\n[28969]  2  2  1  2  2  2  2  2  1  2  2  2  2  2  2  1  2  1  2  2  1  2  2  2\n[28993]  2  2  2  1  1  1  2  2  1  2  2  1  2  1  2  2  2  1  1  2  2  1  2  2\n[29017]  1  1  1  1  1  2  2  2  2  1  1  2  2  2  1  2  1 NA  1  2  2  2  1  2\n[29041]  2  1  1  2  1  1  2  1  2  1  2  2  2  2  2  2  2  2  2  2  1  1  2  2\n[29065]  2  2  2  1  2  2  2  2  2  1  2  2  1  2  2  1  1  2  2  2  1  2  1  1\n[29089]  2  2  2  2  1  2  2  2  2  2  2  1  1  2  2  2  2  2  1  1  2  2  2  2\n[29113]  2  1  1  1  2 NA  1  2  1 NA  1  2  2  2  1  2  2  2  1  1  1  2  2  2\n[29137]  2  2  2  1  1  2  2  1  2  2  2  2  2  2  2  1  2  2  2  2  1  1  1  1\n[29161]  2  2  2  2  2  1  2  1  2  2  2  1  1  1  2  1  2  2  2  1  1  2  1  2\n[29185]  2  1  1  2  2  2  2  2  1  1  1  2  1  2  2  2  2  2  1  2  1  2  2  2\n[29209]  2  1  2  2 NA  2  1  1  2  2  2  2  2  2  1  2  1  2  2  2  2  2  2  2\n[29233]  2  2  1  2  1  2  1  1  2  2  2  1  1  1  2  2  1  2  2  2  1  1  2  2\n[29257]  2  1  2  2  2  2  2  2  1  2  2  1  2  2  2  2  2  1  1  2  1  2  2  2\n[29281]  2  1  2  1  1  1  2  1  2  2  2  1  2  2  2  2  2  1  1  1  1  2  1  3\n[29305]  2  2  2  1  2  2  2  1  2  2  1  2  1  1  1  2  1  2  2  2  1  2  1  1\n[29329]  2  1  2  2  2  1  2  1  2  1  1  2  2  2  2  1  3  2  1  1  2  2  2  2\n[29353]  2  1  1  2  2  1  1  2  1  2  2  1  2  2  2  2  2  2  2  3  1  1  2  2\n[29377]  2  2  2  2  2  2  2  2  2  2  1  2  2  1  2  2  2  2  2  2  2  2  2  2\n[29401]  2  2  2  2  2  2  2  2  2  2  2  2  3  2  2  2  1  2  2  2  2  2  2  2\n[29425]  2  2  2  2  2  2  2  2  2  1  1  2  2  2  2  1  2  1  2  1  2  2  2  1\n[29449]  2  2  2  2  2  2  2  2  1  1  1  2  1  2  2  1  1  1  2  2  3  3  2  2\n[29473]  2  2  2  2  2  2  1  2  2  1  1  2  3  2  2  2  2  2  2  2  2  2  2  2\n[29497]  2  2  2  2  2  2  3  2  2  2  2  2  2  2  2  2  2  2  2  2  2  1  1  2\n[29521]  2  2  2  3  1  2  2  2  1  2  2  2  2  2  2  1  2  2  2  2  2  2  2  1\n[29545]  2  2  2  2  2  2  2  2  2  2  2  1  2  1  2  2  2  1  2  2  2  2  2  2\n[29569]  1  2  2  2  1  2  2  2  2  2  2  2  1  2  2  1  1  2  3  2  2  2  1  1\n[29593]  2  2  2  2  1  3  2  2  2  2  2  2  2  2  2  2  1  2  2  3  2  1  2  2\n[29617]  1  2  1  1  2  2  2  1  2  2  2  2  3  2  1  2  2  1  2  2  2  2  2  2\n[29641]  2  2  2  2  1  2  2  1  2  2  1  1  2  2  1  2  1  2  1  1  1  1  1  1\n[29665]  1  2  1  1  1  2  1  2  2  1  2  1  2  1  2  2  2  2  1  2  1  2  2  1\n[29689]  1  1  2  2  1  2  1  2  2  2  2  2  2  1  2  2  2  1  1  2  2  2  2  2\n[29713]  2  2  1  2  2  2  1  2  2  2  2  2  1  1  1  2  2  2  2  2  1  2  2  1\n[29737]  1  2  2  2  2  3  1  1  1  2 NA  1  2  2  2  2  3  1  2  2  2  1  1  2\n[29761]  1  1  2  1  2  1  2  2  1  2  2  2  2  1  1  2  2  1  1  2  2  1  1  2\n[29785]  1  2  2  1  2  2  1  2  2  2  1  1  2  2  1  3  2  1  1  1  2  2  2  1\n[29809]  2  2  1  1  1  2  2  1  1  2  1  2  1  1  2  2  2  1  1  2  2  2  1  2\n[29833]  2  2  2  2  2  2  2  2  2  2  1  1  1  2  2  2  1  1  1  2  2  2  2  3\n[29857]  1  1  2  1  2  2  2  2  2  1  2  2  2  1  1  2  1  2  2  2  1  2  2  2\n[29881]  2  2  1  2  2  2  2  2  1  2  2  2  2  1  1  2  2  2  1  2  1  2  1  1\n[29905]  1  2  1  1  2  2  2  1  2  2  2  2  2  1  2  2  2  2  1  2  2  2  2  2\n[29929]  2  2  2  1  2  2  2  1  2  2  2  1  2  2  1  1  2  2  1  1  1  2  2  2\n[29953]  2  2  1  2  1  1  2  2  2  2  2  2  2  2  2  1  2  2  1  1  2  2  2  2\n[29977]  1  2  2  1  2  2  2  2  1  2  2  1  2  1  2  2  2  2  1  2  2  2  2  2\n[30001]  2  2  1  1  1  3  1  2  1  1  2  1  2  2  2  2  1  1  2  2  1  2  2  2\n[30025]  2  2  2  2  2  1  1  1  2  1  1  2  2  1  1  1  1  2  2  1  2  2  2  2\n[30049]  2  1  2  2  1  1  2  2  2  1  2  2  2  2  2  2  1  1  2  2  2  2  1  2\n[30073]  2  2  2  1  2  2  2  1  1  1  2  2  2  2  2  2 NA  2  2  2  1  2  2  1\n[30097]  2  1  1  1  2  2  1 NA  1  2  2  1  2  1  2  2  2  2  2  2  1  2  1  3\n[30121]  2  2  2  2  1  1  1  2  2  2  2  2  2  1  1  1  2  1  2  2  2  2  2  2\n[30145]  1  2  1  2  1  2  2  1  1  2  2  2  2  1  2  2  2  2  2  2  2  2  2  2\n[30169]  1  2  1  2  2  2  2  1  2  1  2  2  2  2  1  2  2  2  2  1  2  1  1  1\n[30193]  1  2  2  1  2  1  1  1  1  2  2  2  1  2  1  2  1  2  1  1  1  1  2  2\n[30217]  1  2  1  2  2  2  1  2  1  2  1  2  2  1  2  2  2  1  2  2  2  2  1  1\n[30241]  2  2  1  1  2  1  1  2  2  2  2  2  2  2  2  2  2  2  2  2  1  1  2  2\n[30265]  2  1  2  1  2  2  1  2  2  2  2  1  1  2  1  2  2  1  1  2  2  2  1  1\n[30289]  1  1  2  2  2  2  1  1  2  2  2  1  2  2  1  1  2  1  1  1  2  2  2  2\n[30313]  2  2  1  1  3  1  1  2  2  1  1  1  2  1  2  1  2  2  2  1  2  1  2  2\n[30337]  1  1  1  1  1  1  2  2  2  2  2  2  2  2  2  2  2  1  2  2  2  1  2  1\n[30361]  2  2  2  2  2  2  2  1  2  2  2  1  1  1  2  2  2  1  1  1  2  1  2  2\n[30385]  3  2  2  2  1  1  2  2  2  1  2  2  2  2  1  1  2  2  1  2  1  1  1  1\n[30409]  2  2  2  1  1  2  1  2  2  1  1  2  1  2  2  2  2  2  2  2  2  2  2  2\n[30433]  1  1  1  1  1  2  1  1  2  1  1  2  2  1  1  1 NA  1  2  2  2  1  2  2\n[30457]  1  2  2  2  2  1  2  2  2  1  2  1  1  2  2  2  2  1  1  2  2  1  1  2\n[30481]  1  2  2  1  2  2  1  2  1  2  2  1  2  2  1  2  2  2  2  2  2  1  2  2\n[30505]  2  2  1  3  3  1  2  2  2  2  2  2  2  2  2  1  2  2  2  1  2  2  1  2\n[30529]  2  2  1  2  2  1  2  2  1  2  3  1  1  1  2  2  2  2  2  2  1  2  2  2\n[30553]  2  1  1  2  2  1  1  2  1  2  1  2  2  2  1  2  2  2  1  2  1  1  2  2\n[30577]  2  2  2  2  2  1  2  2  2  2  1  1  1  1  2  1  2  1 NA  2  2  1  2  2\n[30601]  2  2  2  1  2  2  2  1  2  2  2  1  2  2  1  1  2  1  2  2  1  2  2  2\n[30625]  2  2  2  2  2  1  1  1  2  1  2  2  2  2  2  2  1  2  1  2  2  2  1  2\n[30649]  2  2  1  2  1  1  1  3  2  2  1  1  2  2  1  2  2  2  2  1  2  2  1  1\n[30673]  2  2  2  1  2  1  1  2  2  2  2  2  2  2  1  2  3  1  1  1  1  2  2  2\n[30697]  2  2  2  2  1  1  2  1  2  2  2  2  2  1  2  2  1  2  1  2  2  2  1  1\n[30721]  2  2  2  2  2  2  2  2  2  1  2  2  2  2  2  2  2  1  1  2  1  2  1  1\n[30745]  2  1  2  1  2  2  2  1  2  1  1  2  2  2  1  2  1  2  2  2  1  2  2  2\n[30769]  2  1  2  2  2  2  1  2  1  1  1  2  2  2  2  1  2  2  1  2  2  1  2  2\n[30793]  2  2  1  2  2  2  1  2  2  2  2  1  2  2  1  2  2  2  2  2  2  2  2  1\n[30817]  2  1  1  1  1  2  1  2  1  2  2  1  1  2  1  2  1  2  2  2  1  2  1  2\n[30841]  1  2  2  1  2  1  2  1  2  1  2  2  2  2  2  2  1  1  2  2  2  1  1  2\n[30865]  2  1  1  2  1  2  2  1  2  2  2  1  2  2  1  1  2  2  2  2  1  1  1  2\n[30889]  2  2  2  2  1  2  2  2  1  2  2  2  1  1  2  1  1  1  1  1  2  2  1  1\n[30913]  2  1  1  2  1  2  2  2  2  2  1  1  2  2  2  2  1  1  2  1  2  1  2  1\n[30937]  2  2  2  1  2  2  2  2  2  2  2  2  2  1  2  2  1  2  1  1  2  2  2  2\n[30961]  2  2  1  1  2  1  1  1  2  2  2  1  2 NA  1  2  2  2  1  2  2 NA  2  1\n[30985]  2  1  2  2  1  1  2  2  2  1  2  1  2  1  2  2  2  2  2  1  1  1  1  2\n[31009]  1  1  2  2  2  2  1  2  1  1  1  2  2  1  2  2  1  2  2  1  1  2  2  1\n[31033]  2  2 NA  2  2  2  2  1  2  1  2  2  1  2  2  1  2  2  1  1  1  2  1  1\n[31057]  2  1  2  1  1  2  3  2  1  2  2  1  2  1  1  2  2  2  1  2  2  2  1  2\n[31081]  2  2  2  2  1  2  2  2  2  2  1  2  2  2  2  2  1  2  1  2  2  2  1  2\n[31105]  1  2  1  1  2  1  2  2  1  2  2  1  1  2  2  1  2  2  2  2  1  2  1  2\n[31129]  2  2  1  1  2  2  2  1  2  2  2  1  1  2  2  2  2  1  1  2  2  2  2  1\n[31153]  1  2  2  1  2  2  2  2  2  2  1  1  2  1  2  2  2  1  1  2  2  1  1  2\n[31177]  2  1  2  2  1  1  2  2  1  2  1  2  2  1  1  1  1  1  2  2  1  1  1  2\n[31201]  2  2  2  2  2  2  2  2  2  2  2  2  1  2  2  1  1  2  1  2  2  2  2  1\n[31225]  2  2  2  2  2  1  2  2  3  2  2  2  2  1  2  1  2  1  2  2  2  2  2  2\n[31249]  1  1  1  2  2  2  2  1  1  1  2 NA  1  2  2  2  1  2  2  2  2  1  1  2\n[31273]  1  1  2  1  2  2  2  2  2  1  2  2  2  1  1  2  2  2  1  2  1  2  1  2\n[31297]  2  2  2  1  2  2  1  2  1  1  2  2  2  2  3  2  2  1  2  2  1  2  2  2\n[31321]  1  2  1  1  2  1  2  1  2  1  2  2  1  2  1  1  2  2  1  2 NA NA  2  2\n[31345]  1  1  1  1  2  2  1  2  1  1  2  1  2  2  2  3  1  2  2  1  1  2  2  3\n[31369]  1  2  2  1  1  1  2  1  1  2  1  2  2  2  1  2  1  2  2  2  1  2  2  1\n[31393]  2  2  2  2  1  1  2  1  2  2  2  2  2  2  2  2  2  1  2  2  2  2  3  2\n[31417]  2  2  2  2  2  2  1  2  2  2  2  2  2  1  1  1  2  2  1  2  2  2  2  1\n[31441]  2  1  2  2  2  2  2  1  1  1  2  3  1  1  1  2  1  2  2  2  2  1  2  2\n[31465]  2  2  2  2  1  2  2  1  2  2  1  2  2  1  2  2  2  1  2  2  1  2  1  1\n[31489]  2  2  2  2  2  2  1  2  1  2  2  1  1  2  1  1  2  1  1  2  2  2  1  2\n[31513]  2  2  1  2  1  2  2  2  2  2  2  1  2  2  2  2  1  2  2  1  1  2  2  2\n[31537]  2  1  2  1  2  2  2  1  1  1  1  2  2  2  1  2  2  2  2  1  2  2  2  2\n[31561]  2  2  1  2  2  1  1  1  2  1 NA  2  2  2  1  2  2  2  2  2  2  2  2  2\n[31585]  1  1  2  1  2  2  2 NA  1  2  2  2  1  2  1  2 NA  2  1  2  2 NA  2  2\n[31609]  2  2  2  1  2  2  1  1  1  2  2  1  2  2  1  1  1  2  2  2  2  1  1  2\n[31633]  1 NA  2  1  1  2  2  2  2  1  2  2  2  1  1  2  2  1  2  2  2  2  1  1\n[31657]  2  2  2  1  2  2  1  2  2  2  1  2  2  2  2  2  2  2  1  2  1  2  2  2\n[31681]  1  1  2  2  2  1  1  2  1  1  1  1  2  1  1  1  2  2  2  2  1  1  2  2\n[31705]  2  2  1  2  2  2  2  2  1  1  1  2  2  2  2  2  2  2  1  1  2  2  1  2\n[31729]  2  1  2  1  1  1  2  2  2  1  1  2  2  1  2  2  2  2  2  2  2  2  1  2\n[31753]  2  2  2  1  2  1  2  2  2  2  1  2  1  1  1  2  1  1  1  2  2  2  2  2\n[31777]  1  2  2  1  2  2  2  2  1  1  2  2  2  2  2  1  2  2  1  1  1  2  2  2\n[31801]  1  1  2  2  1  2  2  1  1  2  1  1  1  2  2  1  2  2  2  2  1  2  2  2\n[31825]  2  2  1  2  2  1  1  3  2  2  2  2  2  2  2  1  2  1  2  2  2  2  2  2\n[31849]  1  2  1  2  2  1  2  2  1  1  2  1  2  2  2  1  2  1  1  2  2  2  2  2\n[31873]  1  2  1  2  2  1  2  2  2  1  1  1  2  2  2  1  1  2  1  2  2  2  1  2\n[31897]  2  2  1  1  2  1  2  2  2  1  2  2  2  1  2  1  2  2  2  2  2  2  2  1\n[31921]  2  2  2  2  1  2  2  1  1  2  2  1  2  1  1  1  1  1  2  2  1  2  2  1\n[31945] NA  2  2  2  2  2  2  2  1  2  2  1  2  1  2  2  1  1  1  2  1  1  2  2\n[31969]  2  2  2  2  2  1  2  1  2  1  1  2  1  2  2  1  1  2  1  2  2  2  2  2\n[31993]  2  2  1  2  1  1  3  2  2  1  1  2  2  2  2  1  1  2  2  2  1  2  2  1\n[32017]  2  2  2  2  2  2  1  2  1  2  2  2  1  2  2  2  1  2  1  2  1  1  1  2\n[32041]  1  2  2  2  2  2  2  1  2  2  2  3  2  2  1  1  2  2  1  2  1  1  2  1\n[32065]  1  2  1  1  2  2  1  2  2  2  2  1  2  2  2  1  1  2  2  1  1  1  1  2\n[32089]  1  2  2  2  2  2  1  2  2  2  2  2  1  1  1  2  2  1  2  1  1  2  2  2\n[32113]  2  1  1  2  1  2  2  2  2  2  2  2  1  1  1  1  2  2  1  2  1  2  2  2\n[32137]  2  1  2  2  2  2  1  2  2  2  1  1  1  2  2  2  1  2  1  2  2  2  2  2\n[32161]  1  1  2  2  1  2  2  2  3  1  1  2  2  2  2  2  2  2 NA  1  1  2  2  1\n[32185]  1  1  1  2  1  1  2  2  1  2  2  2  2  2  3  2  2  1  2  2  2  1  1  1\n[32209]  2  1  2  1  1  2  1  1  2  1  2  1  2  2  2  1  1  1  2  2  1  2  2  2\n[32233]  1  2  2  2  1  1  1  2  2  1  2  2  1  2  1  1  2  2  2  2  1  1  2  2\n[32257]  1  2  2  2  2  2  1  1  1  2  2  2  1  1  2  2  1  1  2  1  1  1  2  2\n[32281]  2  2  1  1  2  2  2  2  2  1  2  2  2  2  2  2  2  1  1  1  1  2  1  1\n[32305]  2  1  2  1  1  1  2  2  2  1  1  1  1  1  2  1  1  1  2  1  2  1  1  2\n[32329]  2  1  2  2  1  1  2  2  2  1  2  2  2  2  2  1  2  1  1  2  2  1  2  2\n[32353]  2  2  1  2  1  2  2  2  3  2  2  2  1  1  2  2  1  1  2  2  2  2  1  2\n[32377]  2  2  2  2  2  2  1  2  2  2  2  2  2  2  1  2  1  2  2  1  2  2  2  1\n[32401]  1  1  2  2  2  2  2  1  1  1  2  2  2  2  2  2  2  2  3  2  1  1  2  2\n[32425]  2  2  2  1  1  2  2 NA  2  1  2  2  2  1  2  2  1  2  2  1  2  2  2  2\n[32449]  2  2  2  2  1  1  2  1  1  2  2  2  2  2  1  2  1  1  1  1  1  2  1  2\n[32473]  1  2  2  2  2  2  1  2  2  1  2  2  2  3  2  2  2  2  2  1  2  2  2  2\n[32497]  1  2  2  2  2  2  2  1  2  1  2  2  2  2  2  2  1  1  2  1  1  2  2  2\n[32521]  2  1  2  1  2  2  2  2  2  1  1  2  1  2  2  1  1  2  2  2  2  2  1  2\n[32545]  2  1  1  2  2  1  1  2  2  1  1  2  2  2  1  2  1  2  2  2  1  2  2  1\n[32569]  1  2  2  1  1  2  2  2  2  1  1  2  1  2  2  1  2  2  1  2  2  2  2  2\n[32593]  2  2  2  2  1  1  1  1  2  2  2  2  2  2  2  1  1  1  2  2  2  2  1  2\n[32617]  2  2  2  1  2  2  2  2  2  2  2  2  2  2  2  1  2  1  1  2  2  2  2  1\n[32641]  2  2  1  2  1  2  2  2  2  2  2  1  2  2  2  2  2  2 NA  2  1  2  2  2\n[32665]  2  2  2  2  2  2  2  3  1  2  2  2  2  2  2  2  2  1  2  2  2  2  2  2\n[32689]  2  2  2  1  2  2  2  2  2  2  2  2  2  2  2  2  1  2  2  2  2 NA  1  2\n[32713]  2  2  2  1  2  2  2  2  2  2  2  2  2  2  1  2  2  2  2  1  2  2  2  2\n[32737]  2  2  2  2  2  2  1  2  1  2  2  2  1  1  2  2  2  2  2  2  1  2  2  2\n[32761]  2  2  2  2  2  2  1  1  2  2  2 NA  2  2  1  2  2  1  2  2  2  2  2  2\n[32785]  2  2  3  2  3  1  1  1  2  1  2  2  2  2  2  2  1  2  2  1  3  2  2  2\n[32809]  2  2  2  1 NA  3  1  2  2  2  2  2  2  1  2  2  2  2  2  2  2  2  2  2\n[32833]  2  1  1  2  2 NA  2  1  2  2  1  2  2  2  2  2  2  2  2  2  1  2  2  1\n[32857]  2  1  1  2  1  2  2  1  2  2  2  1  2  2  2  1  2  2  2  2  2  2  2  2\n[32881]  2  2  2  1  2  2  2  2  2  2  2  1  2  2  1  2  2  1  1  2  2  2  2  1\n[32905]  2  1  2  2  1  2  1  1  1  1  2  2  2  2  1  2  2  1  1  1  1  1  1  1\n[32929]  1  1  1  2  2  2  2  1  2  1  2  2  2  1 NA  2  2  2  1  2  2  1  2  2\n[32953]  1  1  2  2  2  2  1  2  2  2  1  2  2  1  2  2  2  2  1  1  1  2  2  2\n[32977]  2  1  2  2  2  1  2  2  1  2  2  2  2  2  2  2  2  2  2  2  2  2  2  1\n[33001]  1  2  2  2  2  2  2  2  2  2  1  2  2  2  2  1  2  2  2  1  2  2  2  2\n[33025]  2  2  2  2  2  1  2  2  1  2  1  1  2  1  2  2  2  2  2  1  2 NA  1  2\n[33049]  2  2  2  1  2  2  2  2  2  1  2  2  2  2  2  2  1  3  2  1  1  1  1  1\n[33073]  1  2  2  2  1  1  2  1  2  2  2  1  1  2 NA  1  2  2  1  2  2  2  2  2\n[33097]  2  1  1  2  2  2  2  2  2  2  2  2  2  2  1  2  2  2  2  2  2  2  2  2\n[33121]  2  2  1  1  1  2  1  2  2  2  1  2  1  2  1  1  2  2  2  2  2  2  2  2\n[33145]  2  2  1  1  1  1  2  2  2  2  2  2  2  2  2  2  2  1  1 NA  2  1  1  2\n[33169]  1  2  1  2  2  1  2  2  2  2  1  2  2  2  2  1  2  2  1  2  2  2  1  2\n[33193]  2  2  2  2  2  2  1  1  2  2  2  1  2  2  2  2  2 NA  2  2  2  2  2  2\n[33217]  1  2  1  2  2  2  2  2 NA NA  2  2  2  2  2  1  2  1  2  1  1  2  1  2\n[33241]  1  2  2  2  2  2 NA  2  2  2  2  1  2  2  1  2  1  1  2  2  1  2  2  2\n[33265]  1  2  2  2  2  2  2  2  1  2  2  1  2  2  1  2  2  2  2  2  2  1  2  2\n[33289]  2  2  2  2  2  1  2  1  1  2  2  1  2  2  1  1  2  1  1  1  2  1  2  1\n[33313]  2  1  2  2  2  2  1  2  2  1  2  2  1  2  2  2  1  2  1  2  2  2  2  2\n[33337]  2  1  1  2  2  1  2  2  2  2 NA  1  2  2  1  2  2  2  2  2  2  2  2  1\n[33361]  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  1  2  1  2  2  2  2  2  2\n[33385]  2  1  1  2  2  2  1  2  2  2  1  2  1 NA  2  2  1  2  2  2  3  2  2  2\n[33409]  1  2  1  2  2  1  2  2  2  2  2  1  1  2  1  1  1  2  2  2  2  1  2  2\n[33433]  2  1  2  2  1  2  2  2  2  2  2  2  1  1  2  2  3  2  2  2  2  2  2  2\n[33457]  2  2  1  1  2  2  2  1  2  1  2  1  2  2  2  2  2  2  1  2  2  1  1  1\n[33481]  2  2  2  2  1  2  2  2 NA  1  2  1  1  1  1  2  1  2  1  2  2  1  2  1\n[33505]  2  2  1  2  1  2  2  1  1  2  2  2  1  2  1  2  1  1  1  2  2  1  2  2\n[33529]  1  2  2  2  2  2  2  2  2  2  2  1  1  1  2  1  1  1  2  1  1  1  1  1\n[33553]  1  2  1  1  2  2  1  2  1  2  2  2  1  2  2  2  1  2  1  2  2  2  2  1\n[33577]  1  2  1  1  2  1  1  2  2  2  1  1  1  2  2  2  2  2  1  2  2  2  2  2\n[33601]  2  2  1  2  1  2  2  2  2  2  2  2  2  2  2  1  1  2  2  2  2  2  1  2\n[33625]  2  2  1  1  1  1  1  1  1  1  1  2  1  1  2  2  2  2  2  2  2  2  1  2\n[33649]  2  2  1  1  1  2  2  1  2  2  2  2  2  2  2  2  2  1  1  2  2  1  2  1\n[33673]  2  1  1  2  2  2  2  2  2  2  2  2  2  3  1  1  1  2  1  2  2  1  1  1\n[33697]  1  1  2  1  1  1  1  2  2  1  1  2  2  1  1  2  1  2  2  2  1  2  2  2\n[33721]  1  1  2  2  1  2  2  2  2  2  2  2  2  1  2  2  2  2  2  2  1  1  1  2\n[33745]  2  2  2  1  1  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  1  2\n[33769]  2  2 NA  2  1  2  2  2  2  2  2  2  2  2  2  2  2  2  2  1  1  2  2  2\n[33793]  2  2  2  1  1  2  1  1  2  1  1  2  2  1  2  2  1  2  2  2  2  1  2  1\n[33817]  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  1  2  2  2  2  1\n[33841]  2  2  1  1  2  1  2  2  2  2  1  1  2  2  2  2  2  2  1  1  1  2  2 NA\n[33865]  1  2  1  2  2  2  2  2  1  2  2  2  1  1  2  2  2  2  1  2  1  1  1  2\n[33889]  2  2  2  2  1  2  2  2  1  2  1  2  2  2  1  2  2  2  2  2  2  2  2  2\n[33913]  1  1  2  1  2 NA  2  2  1  1  1  2  1  1  1  2  2  1  1  2  1  2  1  2\n[33937]  2  1  2  2  1  1  1  1  2  2  2  2  1  2  2  2  2  1  1  2  2  2  1  1\n[33961]  2  1  1  2  2  2  1  2  2  2  1  2  2  1  2  2  1  1  2  2  2  2  1  2\n[33985]  1  1  1  2  2  1  2  1  1  1  2  2  2  2  2  2  2  2  2  1  2  2  2  2\n[34009]  1  2  2  1  1  2  2  2  2  2  2  2  2  1  1  1  2  2  1  1  1  2  1  2\n[34033]  2  2  1  1  2  2  1  2  1  2  2  1  2  2  2 NA  1  2  1  1  2  2  2  2\n[34057]  2  2  2  1  2  2  1  1  1  1  1  2 NA  1  2  2  1  2  1  2  2  1  2  2\n[34081]  2  1  1  2  1  2  2  1  2  2  1  2  2  2  2  2  1  1  1  1  2  2  2  2\n[34105]  3  1  2  2  1  2  1  1  1  1  1  2  2  2  2  2  2  2  2  2  2  1  2  1\n[34129]  1  2  2  2  2  2  2  1  2  1  2  1  1  2  2  2  2  2  2  2  1  1  1  1\n[34153]  2  1  2  2  1  2  2  1  2  1  1  2  2  2  2  2  2  2  2  2  2  1  2  1\n[34177]  2  2  2  1  1  2  2  2  2  2  2  2  2  2  1  1  2  1  2  1  2  3  2  2\n[34201]  1  1  2  2  1  2  2  1  2  2  1  2  1  2  1  2  2  2  1  2  1  2  2  2\n[34225]  2  2  2  1  2  2  2  1  2  1  2  2  2  2  2  2  2  1  1  2  2  1  2  1\n[34249]  1 NA  2  2  1  2  1  2  2  1  2  2  2  1  2  2  1  1  2  2  2  1  1  2\n[34273]  2  2  2  2  2  1  1  2  1  2  2  1  1  2  2  1  1  1  1  2  2  1  2  2\n[34297]  2  1  1  2  2  1  2  1  2  2  1  2  1  1  2  2 NA  1  1  1  2  2  2  1\n[34321]  2  1  2  1  2  2  2  1  2  2  1  1  2  1  1  1  2  2  1  2  2  2  1  2\n[34345]  2  1  2  1  2  2  1 NA  1  1  2  1  2  1  1  1  2  2  2  2  1  2  1  1\n[34369]  2  1  1  1  2  2  2  1  1  2  2  2  1  2  2  2  2  1  2  2  2  2  2  2\n[34393]  2  2  2  2  2  1  2  2  2  2  2  1  1  2  2  2  2  1  2  2  1  1  2  2\n[34417]  2  1  2  2  1  1  1  2  2  1  2  2  2  2  2  1  1  2  2  2  2  2  2  2\n[34441]  2  1  2  1  2  2  2  1  2  2  2  1  2  1  1  2  2  1  2  2  2  1  2  2\n[34465]  1  2  1  2  2  2  2  2  2  1  2  2  2  2  2  2  1  2  2 NA  1  2  1  1\n[34489]  2  2  2  1  2  2  2  2  1  2  2  1  1  2  2  2  2  2  2  2  2  1  2  2\n[34513]  2  1  1  3  2  2 NA  2  2  2  2  2  1  1  2  1 NA  2  2  2  1  1  1  2\n[34537]  2  2  2  1  2  2  2  2  1  2  2  2  2  2  1  1  2  1  1  2  2  2  2  2\n[34561]  2  2  2  1  2  1  1  2  2  1  1  2  1  1  1  1  2  2  1  2  2  2  1  1\n[34585]  1  2  2  2  2  2  1  2  2  1  1  1  2  2  2  1  2  1  1  1  2  2  1  2\n[34609]  1  1  1  2  2  2  2  1  1  2  2  1  2  2  1  2  1  2  2  1  1  2  1  2\n[34633]  2  2  2  2  2  2  2  2  2  1  1  2  2  1  1  1  1  1  2  2  2  2  1  2\n[34657]  2  2  2  1  2  2  2  1  1  1  2  2 NA NA  2  1  2  1  2  2  2  2  1  1\n[34681]  1  1  2  1  1  1  2  1  1  1  1  1  1  2  1  1  1  2  1  1  1  2  2  2\n[34705]  2  2  1  1  1  2  1  2  2  2  2  1  2  2  2  2  2  2  2  2  2  2  2  2\n[34729]  1  2  1  1  1  2  1  2  2  1  2  2  2  1  2  2  2  1  1  2  2  2  2  1\n[34753]  2  1  2  1  2  1  1  1  2  1  2  1  1  2  2  1  1  1  2 NA  1  2  1  2\n[34777]  2  2  2  1  1  1  1  2  2  2  2  2  1  1  1  1  2  1  1  1  2  1  2  2\n[34801]  2  1  1  1  1  1  2  1  1  1  2  1  1  1  1  1  1 NA  1  1  1  2  1  1\n[34825]  1  1  1  2  2  2  2  1  1  1  2  1  1  1  1  2  1  1  2  1  1  2  2  1\n[34849]  2  2  1  2  1  1  1  1  2  2  1  1  1  2  1  2 NA  1  2  2  2  2  2  1\n[34873]  1  2  1  1  2  1  2  1  1  2  2 NA  2  2  2  3  2  2  1  2  2  2  2  1\n[34897]  1  1  2  2  2  2  2  2  2  2  1  1  1  1  2  2  1  2  2  2  1  2  2  2\n[34921]  2  2  2  1  2  1  1  2  2  2  2  1  2  2  1  2  1  1  2  2  1  2  2  1\n[34945]  1  1  2  1  1  2  1  2  2  2  1  2  1  2  1  2  1  2  1  2  2  1  1  1\n[34969]  2  1  2  1  2  2  1  1  1  2  1  1  1  1  1  2  1  2  1  1  1  2  2  2\n[34993]  1  2  2  1  1  1  2  1  2  2  2  1  2  1  2  1  2  2  2  2  2  1  2  2\n[35017]  1  2  1  2  1  2  2  2  2  1  1  2  1  2  2  1  2  1  1  2  1  2  2  1\n[35041]  2  1  2  2  1  2  2  2  2  2  1  2  2  1  1  1  2  2  1  2  2  2  2  1\n[35065] NA  2  2  1  2  2  1  2  2  2  1  2  2  2  2  2  2  2  2  1  1  1  2  1\n[35089]  1  3  1  2  2  1 NA NA  2  2  2  1  2  1  2  1  2  2  2  1  2  2  1  2\n[35113]  2  2  2  2  2  2  2  1  2  2  2  1  2  2  2  2  2  1  2  1  2  2  2  2\n[35137]  2  1  2  1  2  1  2  2  2  1  2  2  2  1  1  2  2  1  1  2  2  2  1  1\n[35161]  2  2  2  2  1  1  2  2  1  1  1  2  2  2  1  2  2  1  2  2  1  2  2  2\n[35185]  2  2  2  2  2  2  2  2  1  2  1  1  3  1  2  1  2  2  1  2  2  1  1  1\n[35209]  2  2  2  2  1  1  2  2  2  2  1  2  1  2  2  1  2  2  2  2  2  2  1  2\n[35233]  2  3  2  1  2  1  2  2  2  2  1  2  2  1  2  2  3  2  2  1  2  1  1  2\n[35257]  1  2 NA  1  1  1  2  2  1  2  2  2  2  2  2  1  2  2  2  1  2  2  2  2\n[35281]  2  1  2  2  1  1  2  2  2  2  1  2  2  2  2  1  2  1  2  2  1  2  1  2\n[35305]  2  2  2  1  1  2  2  2  1  2  1  1  2  2  2  2  1  2  1  2  2  2  1  2\n[35329]  2  1  2  2  1  2  2  1  1  1  2  1  2  1  2  1  2  2  2 NA  2  2 NA  2\n[35353]  1  2  2 NA  2  2  2  2  1  1  2  1  2  1  2  2  2  3  2  1  2  2  1  2\n[35377]  2  2  2  2  2  2  1  2  2  1  2  2  2  2  1  1  2  2  2  1  1  2  2  2\n[35401]  2  2  1  1  2  1  2  2  2  2  1  2  1  1  1  2  2  2  2  1  1  2  1  2\n[35425]  2  2  2  2  2  2  2  1  1  1  2  2  2  1  1  1  2  2  1  2  2  2  2  2\n[35449]  2  2  1  1  2  1  2 NA  2  2  1  2  2  2  2 NA  2  2  1  3  1  2  2  1\n[35473]  2  1  2  2  2  2  2  2  2  2  2  1  2  2  1  2  2  2  2  1  1  2  2  1\n[35497]  1  2  2  2  1  1  2  2  2  2  2  2  1  1  1  1  2  2  2  1  2  1  2  1\n[35521]  2  2  1  2  1  2  1  2  1  2  1  2  1  2  1  2  2  2  2  2  2  2  2  1\n[35545]  2  2  2  2  1  2  2  1  2  1  2  1  2  2  2  2  2  2  1  2  2  2  2  2\n[35569]  2  1  1  1  2  1  2  1  1  2  2  2  1  1  2  1  2  2  1  2  1  3  2  2\n[35593]  1  2  2  1  2  2  2  2  1  1  2  2  1 NA  2  2  2  2  2  2  1  2  2  1\n[35617]  2  2  2  2  1  2  2  1  2  1  2  2  2  2  2  1  1  1  1  1  2  1  2  2\n[35641]  1  2  1  1  2  2  1  2  1  2  2  2  1  2  2  1  2  2  2  2  1  1  1  2\n[35665]  2  2  2  1  1  2  2  2  2  1  1  1  1  1  2  2  1  2  1  2  1  1  1  2\n[35689]  2  1  1  2  1  1  1  2  1  2  1  2  2  2  2  1  2  2  2  2  2  2  2  2\n[35713]  2  1  2  2  2  1  2  2  1  2  1  2  2  2  2  1  2  2  2  2  1  2  2  1\n[35737]  1  1  2  1  1  2  1  2  2  2  2  1  1  1  2  2  2  1  2  2  1  2  1  2\n[35761]  2  2  1  1  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2\n[35785]  2  2  2  2  2  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n[35809]  1  1  1  1  1  1  2  1  2  2  2  2  1  2  2  2  2  1  2  2  2  2  2  1\n[35833] NA  2  1  1  2  1  2  1  1  2  1  1  1  2  2  2  2  1  2  2  2  2  2  1\n[35857]  2  2  1  1  2  2  1  1  1  1  1  2  2  1  1  2  2  2  1  1  2  2  2  1\n[35881]  2  2  1  2  1  2  2  2  2  2  1  1  1  2  2  2  2  2  1  2  1  2  2  2\n[35905]  1  2  2  2  2  2  2  2  2  2  1  2  2  1  1  2  2  1  1  2  2  2  1  2\n[35929]  2  1  1  1  2  2  2  2  2  2  1  1  1  2  2  2  2  1  2  2  2  1  2  2\n[35953]  2  2  2  1  2  2  2  1  2  2  1  2  2  2  2  2  2  2  1  2  2  2  2  2\n[35977]  2  2  2  2  2  2  2  2  1  2  1  1  2  2  1  2  2  2  2  2  2  2  2  2\n[36001]  1  1  1  2  2  1  2  2  2  2  2  1 NA  2  3  1  1  2  1  2  2  2  1  1\n[36025]  2  2  2  2  1  2  2  2  2  2  1  2  2  2  1  1  2  1  1  2  2  2  1  2\n[36049]  2  2  2  2  2 NA  2  2  1  2  2  1  1  2  2  2  1  2  2  1  2  1  2 NA\n[36073]  2  2  2  1  1  2  2  1  2  2  2  1  2  1  2  1  2  2  2  2  2  2  2  2\n[36097]  1  2  2  2  1  1  1  1  1  2  1  1  2  2  1  2  2  1  2  1  1  1  2  1\n[36121]  2  1  2  2  1  1  2  2  2  2  1  2  2  2  2  2  2  2  1  2  2  1  1  1\n[36145]  2  1  2  2  2  2  2  2  2  1  2  1  1  1  2  2  2  1  2  1  2  1  2  1\n[36169]  1  1  1  2  2  1  2  2  1  2  2 NA  1  1  1  2  2  1  2  2  2  2  2  2\n[36193]  2  2  2  2  1  1  2  1  2  2  2  2  2  2  2  2  2  2  1  2  1  2  2  2\n[36217]  1  2  2  2  2  1  2  1  1  2  2  2  2  2  1  1  2  2  2  2  2  2  2  2\n[36241]  2  2  2  2  2  2  2  1  2  2  2  2  2  2  2  2  2  2  1 NA  2  2  2  2\n[36265]  2  2  1  2  1  2  2  1  2  2  1  1  1  2  2  2  2  2  2  2  2  1  1  2\n[36289]  1  1  2  2  2  2  2  1  2  2  2  2  2  1  2  1  2  1  2  1  2  2  2  1\n[36313]  2  2  2  2  2  2  1  2  2  1  2  2  1  1  2  1  2  1  2  1  1  2  2  2\n[36337]  1  2  1  2  2  2  2  2  2  1  2  2  2  2  2  1  1  1  1  1  2  1  1  2\n[36361]  2  1 NA  2  2  2  2  1  2  1  2  2  2  2  2  2  1 NA  2 NA  2  2  2  2\n[36385]  2  2  2  2  2  2  2  2  2  1  2  2  2  2  1  1  2  1  2  2  2  2  1  2\n[36409]  1  1  1  1  2  2  1  1  1  2  2  2  1  2  2  1  2  2  2  1  2  2  1  1\n[36433]  1  1 NA  2  2  2  1 NA  2  1  2  2  2  1  1  2  2  2  2  2  1  2 NA  2\n[36457]  2  2  2  2  2  2  2  2  2  2  2  1  2  2  2  1  2  1  2  1  1  1  2  1\n[36481]  2  1  2  2  1  2  2  2  1  2  2  2  2  2  2  1  2  2  2  2  2  1  2  2\n[36505]  2  2  2  1  1  1  1  2  2  2  2  1  2  2  1  1  2  2  2  1  2  1  1  2\n[36529]  2  1  1  1  3  2  2  1  1  2  2  2  2  2  1  2  2  2  2  1  1  1  1  2\n[36553]  1  1  1  2  2  1  2  1  1  1  2  2  2  1  2  2  1  1  2  2  1  2  2  2\n[36577]  1  1  2  2  1  2  2  1  2  1  2  1  2  1  2  1  2  2  1  1  2  2  2  1\n[36601]  2  1  2  2  2  3  2  2  2  1  1  1  2  2  2  1  2  1  1  1  2  2  2  2\n[36625]  1  2  2  2  2  2  2  2  2  1  2  2  1  1  2  2  2  2  2  2  1  2  2  1\n[36649]  2  2  1  1  1  2  2  1  1  1  2  1  2  2  2  1  1  1  2  1  2  1  1  2\n[36673]  2  1  1  2  1  2  2  2  2  2  2  2  2  2  1  2  2  2  1  1  2 NA  2  2\n[36697]  1  1  2  2  2  2  1  1  1  2  2  2  1  1  2  2  2  1  2  1  1  1  2  2\n[36721]  2  2  1  2  2  2  2  1  1  1  1  2  1  1  2  1  1  2  2  1  1  2  1  2\n[36745]  2  2  2  1  2  2  1  1  1  1  2  2  2  1  2  2  1  3  1  2  1  2  1  2\n[36769]  2  2  2  2  2  1  2  1  2  1  2  2  2  2  2  2  2  2  1  2  2  1  1  2\n[36793]  2  1  1  1  2  2  2  2  2  2  1  2  2  2  1  2  2  2  2  2  2  2  2  2\n[36817]  2  2  2  2 NA  2  1  1  2  2  2  2  2  2  1  1  1  1  1  2  2  1  1  2\n[36841]  1  2  1  2  2  1  2  2  2  2  1  2  2  2  2  2  2  1  2 NA  2  2  1  2\n[36865]  2  2  1  1  1  1  1  1  2  2  1  1  1  2  1  1  1  1  1  1  2  2  2  1\n[36889] NA  2  2  1  2  2  2  2  2  1  2  2  2  2  1  2  1 NA  2  2  2  2  2  1\n[36913]  2  2  1  1  2  1  2  1  2  1  2  1  2  1  1  2  1  2  1  2  2  1  1  2\n[36937]  2  1  2  2  2 NA  1  1  1  2  2  1  3  1  2  1  1  2  2  2  2  2  2  2\n[36961]  2  1  1  2  1  2  1  2  2  1  1  2  1  2  1  1  1  2  2  2  2  2  1  1\n[36985]  2  2  2  1  1  2  2  2  2  2  2  2  1  2  2  2  1  2  2  2  2  2  1  1\n[37009]  2  2  2  2  1  2  2  2  2  2  1  2  1  2  1  1  2  2  2  1  1  2  2  2\n[37033]  2  2  2  2  2  1  2  1  1  2  2  2  2  1  1  2  1  2  2  2  2  2  1  2\n[37057]  2  1  2  1  2  2  2  2  2  2  2  1  2  1  2  2  1  1  2  2  1  1  2  1\n[37081]  2  1  1  2  2  1  1  2  1  2  2  2  2  2  1  2  1  2  2  1  2  2  2  2\n[37105]  2  1  2  2  2  2  2  2  1  1  2  2  1  2  2  2  1  2  1  2  1  2  1  2\n[37129]  2  2  1  2  1  2  2  1 NA  2  1  1  2  2  2  2  1  1  2  2  1  2  1  2\n[37153]  2  1  2  2  2  1  1  2  2  2  1  2  1  1  1  1  1  2  1  2  1  2  2  1\n[37177]  2  1  1  1  1  1  2  2  2  2  1  2  2  2  2  2  1  1  1  2  1  2  2  2\n[37201]  2  2  2  1  3  2  2  2  2  2  1 NA  2  1  1  1  2  1  2  1  2  2  2  1\n[37225]  1  2  2  2  1  2  2  2  2  1  2  2  1  2  2  2  2  2  1  2  2  2  2  2\n[37249]  1  2  2  2  2  2  2  2  1  2  2  2  1  1  2  2  2  2  2  2  2  2  2  1\n[37273]  2  2  2  1  2  2  2  2  2  2  3  2  2  2  2  1  1  3  1  2  1  1  2  1\n[37297]  2  2  2  2  1  1  2  2  2  1  2  1  2  2  2  1  2  2  2  2  2  1  2  2\n[37321]  2  1 NA  2  2  2  2  2  2  1  2  1  2  1  2  1  2  2  1  1  2  2  2  2\n[37345]  1  1  2  2  2  1  2  1  2  2  2  1  1  2  2  3  2  2  2  2  1  1  1  2\n[37369]  2  2  1  2  1  1  1  2  2  2  2  2  2  2  2  2  2  1  1  2  1  2  2  2\n[37393]  2  1  1  2  2 NA  1 NA  2  1  1  1  2  1 NA  2  2  1  2  2  2  1  2  2\n[37417]  2  2  1  1  1  1  2  1  1  1  2  1  2  1  1  2  1  2  2  1  1  2  1  1\n[37441]  2  2  2  1  2  1  1  1  2  1  2  2  1  1  2  2  2  2  2  1  2  2  1  2\n[37465]  2  2  1  2  2  2  1  3  2  2  1  2  1  2  2  2  2  2  1  2  2  2  2  2\n[37489]  1  2  2  1  1  2  2  1  2  2  2  2  1  2  1  2  2  2  2  1  1  2  2  1\n[37513]  2  3  2  2  2  2  2  1  2  2 NA  2  1  2  2  1  2  2  2  2  2  1  1  2\n[37537]  2  2  2  1  2  2  1  2  2  1  1  1  1  2  1  1  2  1  2  2  2  1  2  2\n[37561]  1  1  1  2  2  2  2  2  1  2  2  1  1  1  2  2  1  2  1  2  1  2  1  2\n[37585]  1  1  2  2  1  2  2  2  1  2  1  1  1  2  1  1  1  2  2  2  1  2  2  2\n[37609]  1  2  2  1  2  1  2  2  2  2  2  1 NA  1  2  2  2  2  2  2  1  2  2  2\n[37633] NA  1  1  2  1  1  2  2  1  2  2  2  1  2  1  1  2  2  1  2  1  2  1  1\n[37657]  2  2  2  2  2  2  2  2  1  2  1  2  1  1  2  2  2  2  1  1  2  2  3  2\n[37681]  1  2  1  2  2  2  1  1  1  1  3  1  2  1  2  2  2  3  1  1  2  2  1  1\n[37705]  1  2  1  2  2  2  1  2  1  2  2  2  1  1  2  2  1  1  1  2  2  2  2  2\n[37729]  2  1  2  2  1  2  2  2  2  1  2  2  2  2  2  2  2  2  1  1  1 NA  2  3\n[37753]  1  2  2  2  1  2  2  2  3  1  2  2  2  2  2  2  1  2  2  1  1  2  1  3\n[37777]  1  2  1  1  2  1  1  2  1  1  2  2  2  2  1  2  1  2  2  1  1  2  1  2\n[37801]  1  1  2  1  2  1  1  1  2  1  1  2  2  2  2  2  2  2  1  2  1  2  2  2\n[37825]  1  2  2  1  2  1  2  1  1  2  2  2  1  1  2  1  1  2  1  2  1  1  1  1\n[37849]  1  2  2  1  2  2  2  2  2  1  2  2  2  2  2  1  1  2  2  2  2  2  1  2\n[37873]  2  2  1  1  2  1  2  1  2  1  1  2  2 NA  1  1  2  2  2  3  2  2  3  2\n[37897]  2  2  2  2  2  1  1  2  1  1  2  1  2  2  2  1  2  2  1 NA  2  1  1  2\n[37921]  1  1  2  2  2  2 NA  2  1  2  2  2  2  2  2  2  2  2  2  2 NA  1  2  2\n[37945]  2  1  1  2  1  3 NA  2  1  1  1  2  2  1  1  2  1  1  2  2  2  2  1  2\n[37969]  2  2  1  1  2  1  1  1  1  2  2  2  2  2  2  1  1  3  2  2  1  1  1  2\n[37993]  2  1  2  1  2  2  2  2  2  1  2  2  1  2  2  2  1  1 NA  1  1  2  2  1\n[38017]  2  1  2  1  2  2  2  2  1  2  1  2  2  1  1  2  1  2  1  1  1  1  2  2\n[38041]  2  2  2  2  2  2  2  2  1  2  2  2  1  2  1  1  2  1  2  2  2  1  2  2\n[38065]  1  1  1  1  2  1  2  2  2  1  2  1  1  2  1  2  1  2  1  2  2  1  2  1\n[38089]  1 NA  2  2  2  1  2  1  2  2  2  1  2  2  1  1  2  1  2  2  1  1  2  1\n[38113]  2  2  1  2  2  1  2  1  1  1  2  2  2  1  1  1  2  1  1  2  1  1  1  1\n[38137]  1  1  1  1  1  2  1  1  2  2  2  1  2  1  2  2 NA  2  2  1  2  3  2  2\n[38161]  2  2  2  1  2  2  1  1  2  1  2  2  2  1  1  2  2  2  2  2  2  2  1  1\n[38185]  1  2  2  2  2  2  1  2  1  1 NA  2  1  1  2  2  2  2  2  1  2  2  2  1\n[38209]  2  2  2  1  2  1  2  2  1  2  2  2  2  1  1  2  2  2  2  1  2  2  2  1\n[38233]  2  1  2  1  1  2  1  1  2  2  1  2 NA  2  2  2  1  2  1  2  1  2  1  1\n[38257]  2  2  2  2  2  1  2  1  2  2  2  2  1  2  2  2  2  1  1  2  2  2  1  1\n[38281]  2  2  2  1  2  1  1  1  2  1  2  1  2  2  1  2  1  2  2  2  2  2  2  1\n[38305]  2  2  2  2  2  2  2  2  2  2  2  1  1  2  2  1  1  1  2  1  1  2  1  2\n[38329]  1  2  2  1  2  2  2  2  1  2  1  1  2  2  2  1  2  2  1  2  2  1  2  1\n[38353]  2  2  1  1  1  2  2  2  1  1  1  2  1  1  2  2  1  1  1  2  2  1  2  2\n[38377]  1  2  2  2  1  2  1  1  1  1  1  2  1  2  1  2  1  2  1  1  2  2  1  2\n[38401]  2  2  2  2  2  2  1  1  2  1  1  2  1  2  2  1  1  2  2  1  2  2  2  2\n[38425]  1  1  2  1  2  1  1  2  2  1  2  2  2  2  1  2  1  2  2  2  2  2  1  2\n[38449]  2  2  2  2  1  2  1  2  2  1  1  1  1  1  2  1  2  2  1  1  1  1  2  2\n[38473]  2  2  1  1  1  1  2  1  2  2  2  3  1  1  2  2  2  2  1  1  1  2  2  1\n[38497]  2  1  1  2  2  2  2  1  1  2  2  1  1  2  2  1  2  1  2  1  1  1  1  2\n[38521]  1  1  1  1  1  1  1  2  2  2  2  1  2  2  2  1 NA  1  2  2  1  2  2  2\n[38545]  1  1  1  2  1  2  2  1  2  2  2  1  1  1  2  1  2  2  2  2  2  2  2  2\n[38569]  2  2  1  1  2  1  2  2  1  2  2  1  2  2  2  1  1  2  2  1  2  2  2  2\n[38593]  2  2  1  2  2  2  2  1  3  1  2  2  1  2  2  2  2  2 NA  1  2  1  2  2\n[38617]  1  2  2  2  2  2  2  1  1  2  1  2  1  2  2  2  2  2  2  1  2  1  1  2\n[38641]  2  2  3  1  2  1  1  2  1  1  1  2  2  1  1  1  1  1  1  1  1  2  1  1\n[38665]  2  2  1  1  2  2  2  1  2  2  1  2  1  2  1  2  2  2  2  1  2  1  2  1\n[38689]  1  1  1  2  2  2  2  1  1  1  1  1  2  2  1  2  1  1  2  2  1  2  1  2\n[38713]  2  1  1  2  2  1  1  1  2  2  1  2  2  1  1  1  2  2  1  2  2  1  1  1\n[38737]  1  1  2  2  2  2  1  2  2  1  1  2  1  1  2  2  2  1  1  2  1  2  2  1\n[38761]  2  2  2  1  1  1  2  1  2  1  3  2  1  2  2  2  2  2  2  1  2  1  2  1\n[38785]  1  1  2  1  2  1  1  2  1  1  2  2  2  1  1  2  1  2  2  1  1  1  2  1\n[38809]  1  1  2  1  1  1  1  1  1  2  2  1  2  1  2  2  1  2  1  3  2  1  2  1\n[38833]  2  2  2  1  1  2  2  2  2  1  2  2  1  1  1  1  2  3  2  1  2  2  2  2\n[38857]  1  1  2  3  1  2  2  1  1  1  2  1  1  1  2  1  1  2  2  1  2  2  2  2\n[38881]  2  2  1  1  2  1  2  1  1  2  2  1  2  2  2  1  2  2  2  1  2  1  1  1\n[38905]  2  1  2  2  1  2  2  2  2  2  2  2  2  2  2 NA  1  1  2  1  2  2  1  2\n[38929]  2  1  2  2 NA  2  1  2 NA  2  1  2  2  2  2  1  2  2  2  2  1  2  2  2\n[38953]  1  2  1  1  2  1  2  2  2  1  2  2  2  2  1  1  2  1  1  1  1  2  1  1\n[38977]  2  2  1  1  2  2  1  1  1  1  1  2  1  1  2  2  3  2  1  2  2  1  2  2\n[39001]  2  1  2  2  2  1  2  2  1  2  1  2  1  2  2  2  2  1  1  1  1  1  2  1\n[39025]  2  1  2  1  2  2  2  1  1  2  2  2  2  2  1  2  2  2  2  2  1  2  2  2\n[39049]  2  2  2  2  2  1  2  1  2  2  2  2  1  2  2  2  2  2  1  1  1  2  2  1\n[39073]  1  2  1  2  1  1  1  1  2  1  1  1  1  1  1  1  2  2  2  2  2  2  1  2\n[39097]  2  1  2  2  1  2  1  2  2  2 NA  1  2  1  2  1  1  2  2  2  2  2  1  2\n[39121]  1  1  2  1  2  2  1  2  2  2  2  1  2  1  1  1  2  2  1  2  1  1  1  1\n[39145]  2  2  2  1  1  2  2  2  2  2  2  2  2  2  1  1  1  2  1  1  1  2  1  2\n[39169]  2  2  1  1  1  2  1  2  1  1  3  1  3  1  1  2  2  2  2  2  1  2  2  2\n[39193]  2  2  1  1  2  1  1  2  1  2  2  1  1  2  2  1  2  2  2  1  1 NA  2  1\n[39217]  2  2  2  2  2  2  2  1  1  1  2  1  2  2  2  1  1  1  2  2  1  2  1  3\n[39241]  2  2  2  2  2  1  1  2  1  2  2  2  2  1  2  3  1  2  2  1  2  2  1  1\n[39265]  1  1  2  1  2  2  2  2  1  2  1  1  1  1  2  2  2  2  1  1  2  1  2  2\n[39289]  2  2  2  1  2  2  1  1  2  1  2  2  2  2  2  2  1  2  2  2  2  2  1  1\n[39313]  2  2  2  1  1  2  2  2  2  2  1  1  2  2  1  1  1  1  2  1  2  2  2  2\n[39337]  1  1  2  2  1  1  1  2  2  2  2  2  2  2  2  1  2  2  1  1  1  1  2  2\n[39361]  1  2  2  1  1  1  1  2  2  2  1  2  1  2  2  2  2  1  1  2  2  1  2  1\n[39385]  2  2  1  1  1  2  2  2  2  2  2  1  1  2  1  1  2  2  1  1  2  1  1  1\n[39409]  2  1  2  2  1  1  1  2  2  2  1  2  2  2  2  2  1  2  2  2  2  2  1  1\n[39433]  2  2  1  2  1  2 NA  3  2  2  2  2  1  1  1  2  1  2  2  2  2  1  1  2\n[39457]  2  1  1  2  1  2  2  1  2  2  1  1  2  2  1  1  2  1  2  1  2  1  1  1\n[39481]  2  2  1  1  2  2  2  1  1  2  1  1  1  1  2  1  2  2  1  3  2  2  2  2\n[39505]  1  1  1  2  2  1  2  2  2  2  2  2  2  2  2  1  2  2  1  2  2  1  2  2\n[39529]  2  2  2  1  2  1  1  1  2  2  2  1  2  1  2  2  2  2  2  1 NA  2  1  1\n[39553]  1  1  2  2  2  2  1  1  1  1  1  1  1  2  2  1  2  2  2  1  2  1  2  1\n[39577]  1  2  1  2  2  2  2 NA  2  2  2  2  1  1  1  2  2  2  2  2  1  2  1  2\n[39601]  2  1  1  2  2  2  1  2  1  1  2  2  2 NA  2  2  2  2  1  2  1  2  1  1\n[39625]  2  1  2  1  1  2  2  2  2  2  1  1 NA  1  1  2  2  2  1  1  1  1  2  1\n[39649]  1  1  1  2  1  2  2  2  2  1  2  2  2  2  2  2  2  2  1  2  3  2  2  2\n[39673]  2  2  2  1  2  2  1  1  1  2  2  2  1  2  1  2  1  2  1  1  1  2  1  2\n[39697]  2  2  2  2  2  2  1  2  1  1  1  2  1  3  2  2  1  2  1  2  2  1  1  2\n[39721]  2  1  2  2  1  1  2  2  2  1  2  1  2  2  2  2  2  2  2  2  2  1  1  1\n[39745]  2  2  2  2  2  1  1  1  2  1  2  2  1  1  1  1  2  2  1  2  2  2  2  2\n[39769]  1  2  2  1  2  1  1 NA  2  3  2  2  1  2  2  2  1  2  2  1  2  2  1  2\n[39793]  1  1  2  1  2  2  2  2  2  2  1  1  2  2  2  1  2  2  2  1  2  2  2  2\n[39817]  2  1  1  2  2  2  1  1  1  1  1  2  2  1  1  1  2  2  2  2  2  1  1  2\n[39841]  2  2  2  2  1  2  2  2  2  1  1  2  2  1  1  2  2  2  2  2  2  2  1  2\n[39865]  2  1  1  2  2  2  2  1  2  2  2  2  2  1  2  2  2  1  2 NA  2  1  2  1\n[39889]  1  1  2  2  2  1  2  2  2  2  2  2  2  2  2  2  2  2  2  2  1  2  2  2\n[39913]  1  1  2  1  2  3  2  2  2  1  2  1  1  2  2  1  1  2  1  2  1  1  2  2\n[39937]  2  1  2 NA  2  2  1  2  2  2  1  2  1  2  2  2  1  1  1  2  1  2  1  1\n[39961]  2  2  2  2  2  1  1  2  2  2  2  1  2  1  1  2  1  1  1  2  2  1  3  2\n[39985]  2  2  2  2  3  2  2  2  2  2  1  2  2  2  1  1  1  2  2  2  2  2  1  1\n[40009]  2  1  2  2  3  2  2  1  2  1  1  2  2  1 NA  2  1  2  1  2  2  2  2  2\n[40033]  1  1  2  1  2  2  1  1  2  2  2  2  2  1  2  1  1  1  1  2  2  1  2  1\n[40057]  2  2  1  2  2  2  2  2  1  2  2  1  2  2  2  2  1  2  1  2  2  2  1  2\n[40081]  2  2  2  2  2  1  2  2  1  2  1  2  2  2 NA  2  2  2  2  1  2  2 NA  2\n[40105]  1  2  2 NA  2  2  1  2  2  2  1  1  2  2  2  2  2  1  1  1  2  2  1  2\n[40129]  2  1  2  1  2  2  2  1  2  2  2  2  2  1  1  2  2  2  1  2  2  2  1  2\n[40153]  2  1  2  2  2  2  2  2  1  2  2  1  1  2  2  2  1  2  2  1  1  1  2  1\n[40177]  1  2  2  1  2  2  1  1  2  1  1  1  1  2  1  1  2  2  1  1  2  2  1  2\n[40201]  2  2  2  2  2  1  2  1  2  1  2 NA  2  2  1  1  2  3 NA  2  1  1  1  1\n[40225]  1  2  1  1  2  1  1 NA  2  1  1  1  3  2  2  2  1  2  1  1  2  2  1  1\n[40249]  1  2  2  1  1  2  1  2  1  2  2  2  1  2  2  2  2  2  2  1  2  1  2  2\n[40273]  2  1  2  1  2  1  2  2  2  1 NA  2  1  1  2  2  1  1  2  1  1  1  2  2\n[40297]  2  3  2  2  1  1  2  1  1  1  1  1  1  2  2  1  1  2  3  2  1  2  1  2\n[40321]  1  2  2  2  2  1  1  2  2  2  1  2  2  2  2  1  2  1  2  1  1  1  2  2\n[40345]  2  2  2  2  1  1  2  1  1  1  2  1  2  1  1  1  2  2  1  2  2  2  2  2\n[40369]  2  2  2  2  1  2  1  1  2  2  1  2  2  2  2  2  2  1  2  2  2  2  2  2\n[40393]  2  1  1  2  2  2  2  2  1  2  2  2  2  1  1  2  1  2  2  2  1  1  2  1\n[40417]  1  1  1  2  1  1  1  1  2  2  1  1  1  1  1  1  2  1  1  2  1  2  2  2\n[40441]  2  2  1  2  1  2  1  2  1  1  2  1  2  1  2  2  2  1  1  2  2  1  2  1\n[40465]  2  1  2  2  2  2  2  2  2  1  1  1  2  2  2  2  2  2  2  2  1  2  2  1\n[40489]  2  2  2  1  2  2  1  1  2  2  2  2  1  2  2  1  2 NA  2  1  2  1  1  2\n[40513]  2  2  2  1  2  2  1  1  1  1  2  1  1  2  2  1  1  2  1  1  1  2  1  2\n[40537]  1  1  1  2  2  2  2  2  2  1  1  1  2  2  1  2  2  2  2  1  1  2  1  1\n[40561]  1  2  1  2  1  2  2  2  2  1  1  2  1  1  1  1  2  2  1  1  1  1  2  1\n[40585]  2  1  1  2  2  2  3  1  2  2  2  1  2  1  1  2  2  1  1  1  2  1  2  2\n[40609]  2 NA  2  2  1  1  1  2  1  1  2  2  2  1  2  2  1  2  1  2  1  2  2  1\n[40633]  1  1  2  2  1  2  1  2  1  2  1  1  1  2  1  1  2  1  1  2  2  2  1  2\n[40657]  1  2  2  3  2  1  2  1  1  2  2  2  2  1  2  2  1  2  1  2  2  2 NA  2\n[40681]  2 NA  2  3  2  1  1  2  2  2  2  2  2  2  2  2  1  1  2  2  2  1  2  2\n[40705]  2  2  2  1  1  2  2  2  2  2  1  2  2  2  2  1  1  1  2  2  2  2  2  1\n[40729]  2  2  1  2  2  1  2  1  2  2  1  2  1  2  2  2  2  1  2  2  2  3  2  2\n[40753]  2  2  1  2  2  2  1  2  2  2  2  1  1  2  2  1  2  2  1  2  2  1  2  2\n[40777]  2  1 NA  2  2  1  1  1  2  1  1  1  1  1  2  2  2  1 NA  2  2  2  2  1\n[40801]  1  2  1  1  2  1  2  2  2  2  2  2  2  3  2  2  1  1  2  1  2  2  2  2\n[40825]  1  1  1  1  2  2  2  1  2  2  2  2  2  2  2  2  2  2  1  2  1  1  2  2\n[40849]  1  2  2  1  2  2  2  2 NA  2  1  2  2  2  1  2  2  2  2 NA  2  2  2  1\n[40873]  2  2  2  2  1  1  2  2  3  3  1  2  2  1  2  1  2  2  2  2  2  2  1  2\n[40897]  2  1  2  1  2  2  2  2  2  2  1  1  2  1  2  2  2  2  1  1  2  2  1  1\n[40921]  2  2  1  2  2  2 NA  1  2  2  1  2  2  1  2  2 NA  2  1  2  2  2  2  2\n[40945]  1  2  2  2  2  1  2  2  2  1  2  2  2  2  2  2  2  1  2  2  2  2  1  2\n[40969]  2  1  1  1  1  2  2  2  2  2  1  2  2  2  1  1  1  2  2  2 NA  2  1  2\n[40993]  2  1  2  1  2  1  2  3  2  1  1  2  1  2  2  2  1  1  2  1  2  1  2  2\n[41017]  2  1  2  1  1  1  1  1  1  1  2  2  2  2  2  2  2  2  2  1  1  1  1  2\n[41041]  2  2  2  2  1  2  2  1  2  2  1  2  1  2  1  2  1  2  1  1  1  1  1  1\n[41065]  1  1  1  2  2  2  2  1  2  2  1  2  1  2  1  1  2  2  1  2  1  2  1  2\n[41089]  2  2  1  1  1  1  2  1  2  1  1  2  1  1  1  2 NA  2  2 NA  2  2  3  2\n[41113]  2  2  2  2  2  2  2  2  1  2  1  2  1  1 NA  2  1  2  2  1  1  2  1  2\n[41137]  1  2  1  2  2  1  2  2  2  2  1  2  1  2  1  2  1  1  2  1  1  2  2  1\n[41161]  2  2  2  2  2  1  1  2  1  1  2  1  1  2  2  2  1  1  1  2  2  2  1  2\n[41185]  2  1  2  1  1  2  2  1  2  2  2  3  2  2  1  2  3 NA  1  2  2  1  2  2\n[41209]  2  1  2  1  1  1  2  2 NA  2  1  1  1  2  2 NA  2  2  1  2  2  2  1  2\n[41233]  2  1  2  2  2  2  2  1  1  2  2  2  2  2  1  1  1  1  1  2  2  2  1  2\n[41257]  1  1  2  2  2  2  1  1  1  2  1  1  1  2  1  1  2  1  2  2  2  2  1  2\n[41281]  1  2  1  2  2  2  2  1  2  1  2  1  1  2  1  1  2  1  1  2  2  2  1  1\n[41305]  2  2  2  1  1  1  2  1  1  2  2  1  1  1  2  1  2  2  2  1  1  1  2  2\n[41329]  2  2  2  1  2  1  1  2  2  1  1  2  1  2  2  2  2  2  1  1  2  1  2  2\n[41353]  1  2  2  1  2  2  2  2  1  2  2  2  1  1  1  3  1  2  1  2  1  2  1  2\n[41377]  2  2  1  2  2  2  2  2  2  1  2  1  2  1  2  2  1  1  2  2  2  2  1  1\n[41401]  2  2  2  2  1  2  2  2  2  2  2  1  2  1  1  2  1  2  2  2  2  2  1  2\n[41425]  1  1  1  1  1  1  2  2  2  1  2  1  1  2  1  2  1  2  2  2  2  2  1  1\n[41449]  1  2  2  2  1  2  2  1  2  1  1  2  1  1  1  2  1  1  1  2  1  2  1  2\n[41473]  2  2  2 NA  2  1  1  1  2  2  2  1  2  2  2  2  2  2  1  2  1  2  1 NA\n[41497]  2  2  1  2  1  1  1  2  2  2  2  2  2  2  1  2  1  2  2  2  1  2  2  1\n[41521]  2  1  2  1  2  1  2  1  2  2  2  1  3  1  2  2  1  2  1  2  1  2  2  2\n[41545]  2  1  1  2  2  2  1  2  2  2  1  1  2  1  2  2  2  2  2 NA  1  1  2  2\n[41569]  2  1  1  1  1  1  1  2  2  2  2  2  1  1  2  1  1  1  1  2  1  2  1  1\n[41593]  1  2  1  1  1  1  1  1  2  1  2  2  2  1  2  2  1  2  1  1  1  2  2  1\n[41617]  1  1  1  1  1  1  3  2  2  2  1  2  2  2  2  2  2  2  2  2  1  2  2  2\n[41641]  1  2  2  2  2  2  1  2  1  2  2  2  1  2  1  1  2  2  1  1  2  1  2  2\n[41665]  1  2  2  1  2  2  1  2  1  1  1  2  1  1  2  1  1  2  1  2  1  2  1  2\n[41689]  1  1  1  1  1  2  1  2  2  1  1  2  1  2  2  2  1  2  1  2  1  2  2  2\n[41713]  2  2  2  1  1  2  2  1  2  2  2  1  1  1  1  1  2  1  2  1  1  1  2  2\n[41737]  1  2  2 NA  2  2  2  1  2  1  1  2  1  2  2  2  2  1  2  2  2  2  1  2\n[41761]  2  2  2  2  1  1  2  1 NA  1  1  2  1  2  1  1  2  2  1 NA  2  2  1  1\n[41785]  2  2  2  2  1  1  2  2  1  2  1  2  1  2  1  2  2  2  2  2  2  1  2  2\n[41809]  1  1  2  1  2  1  1  1  1  1 NA  2  1  1  2  2  2  2  2  1  1  2 NA  1\n[41833]  1  1  2  1  2  2  2  2  2  2  2  2  3  2  2  2  1  2  1  2  1  1  2  1\n[41857]  1  2  2  2  2  1  1  2  1  2  2  2  2  1  2  2  2  2  2  2  1  2  2  1\n[41881]  2  1  2  1  1  1  2  1  2  2  1  2  2  2  2  2  2  2  2  2  1  2  1  1\n[41905]  2  2  2  2  1  2  2  2  1  1  1  2  2  2  1  2  1  2  2  2  2  2  2  1\n[41929]  2  1  2  1  2  2  2  2  1  2  2  1  1  2  2  2  1  1  2  1  2  2  1  1\n[41953]  2  1  2  2  1  2  2  2  1  2  1  2  1  2  1  2  1  2  1  1  2  2  1  2\n[41977]  1  2  2  2  2  3  2  2  2  2  2  1  2  1  2  1  1  1  1  2  1  2  2  1\n[42001]  2  1  2  2  2  2  1  2  2  2  2  2  1  2  2  2  2  1  2  2  2  2  2  2\n[42025]  2  1  2  2  1  1  2  1  2  2  2  1  2  2  1  2  2  2  2  2  2  2  2  2\n[42049]  2  2  2  2  2  2  1  2  2  2  1  2  2  2  2  1  2  2  1  2  2  2  2  2\n[42073]  2  2  1  2  1  1  2  2  1  1  2  2  1  2  2  1  2  2  2  2  1  2  2  1\n[42097]  1  2  2  2  1  2  2  2  2 NA  1  2  2  1  1  2  2  2  1  1  2  2  1  2\n[42121]  1  2  1  1  1  2  2  2  2  2  2  2  2  2  2  2  2  1  1  2  2  2  2  2\n[42145]  1  1  2  1  1  2  2  2 NA  2  1  2  1  2  1  2  2  2  2  2  2  2  2  1\n[42169]  1  1  1  1  2  2  1  2  1  1  1  2  2  1  2  1  2  2  2  1  2  2  2  2\n[42193]  2  2  2  2  1  2  2  2  2  3  2  1  1  1  2  2  1 NA  2  2  2  2  2  2\n[42217]  1  1  2  2  2  2  2  2  2  2  2  2  1  1  2  3  2  2  1  1  3  1  2  2\n[42241]  2  1  2  1  1  2  2  2  1  2  2  2  2  1  2  1  2  2  2  2  1  1  1  1\n[42265]  2  2  2  1  1  2  2  1  2  2  2  2  2  2  1  1  2  1 NA  2  1  2  1  2\n[42289]  1  2  1  1  1  1  1  1  2  2  2  2  1  2  2  2  2  2 NA  1  2  2  1  2\n[42313]  2  2  1  1  2  1  2  1  2  2  2  2  1  2 NA  1  2  2  2  1  2  1  2  2\n[42337]  2  2  1  2  2  2  1  1  2  2  2  1  3  2  1  2  2  2  1  2  2  1  1  2\n[42361]  1  2  2  2  2  1  2  2  1  2  1  2  1  2  2  2  1  2  1  1  2  2  2  3\n[42385]  1  1  2  2  1  2  2  1  2  2  1  1  1  1  2  1  1  2  2  2  2  2  1  2\n[42409]  2  1  2  2  2  2  1  1  1  2  2  1  2  2  2  1  2  2  1  2  2  2  2  1\n[42433]  1  2  2  2  2  2  1  2  1  2  2  1  2  1  2  1  2  1  2  1  1  2  2  1\n[42457]  2  2  1  1  2  2  1  2  1  2  2  2  1  1  1  2  1  1  2  2  1  1  2  2\n[42481]  1  1  2  1  2  1  2  2  2  1  2  2  2  2  1  1  1  1  2  2  1  2  1  2\n[42505]  2  1  2  2  2  1  2  2  2  2  1  1  2  2  2  2  2  1  2  1  2  1  1  2\n[42529]  2  1  1  1  2  2  2  2  1  2  2  1  2  2  2  1  1  1  2  2  2  2  2  1\n[42553]  1  1  1  1  2  1  1  1  1  1  2  1  2  3  1  2  2  1  2  1  2  2  2  2\n[42577]  1  1  1  2  2  2  2  2  2  2  2  1  2  2  2  2  2  2  1  2  2  2  1  1\n[42601]  2  2  2  2  1  2  1  1  2  1  2  2  2  1  1  2  2  2  2  1  2  2  2  1\n[42625]  1  2  2  2  2  1  2  1  1  1  2 NA  1  1  2  1  1  1  1  2  2  1  1  2\n[42649]  1  2  1  2  1  1  2  2  2  2  2  1  1  1  1  1  2  2  2  2  2  2  2  2\n[42673]  2  1  2  2  2  2  2  2  2  2  2  2  1  1  2  2  2  2  1  1  1  2  2  2\n[42697]  2  1  3 NA  2  1  2  2  1  2  1  2  1  2  2  2  2  1  2  2  2  2  1  2\n[42721]  1  1  2  1  2  1  2  2  2  2  2  1  2  1  1  1  2  2  2  2  1  2  2  2\n[42745]  2  2  1  2  1  2  1  2  3  2  2  2  2  2  2  2  2  2  2  1  2  2  2  2\n[42769]  2  2  2  1  2  2  1  2  1  1  2  1  1  1  1  1  2  1  1  2  2  2  2  2\n[42793]  2  2  1 NA  3  1  2  2  1  1  1  1  1  2  1  2  2  1  2  2  2  2  2  1\n[42817]  2  1  2  2  2  2  1  2  2  2  1  3  2  2  1  1  2  2  2  1  2  1  2  2\n[42841]  2  1  1  2  2  2  2  1  2  2  2  2  1  2  2  1  2  2  2  2  1  2  2  2\n[42865]  1  1  1  2  2  2  2  2  1  2  1  2  2  1  2 NA  1  1  1  2  2  1  1  2\n[42889]  1  1  1  1  1  2  1  2  2  1  2  2  1  3  2  1  1  1  2  2  2  1  1  2\n[42913]  2  1  2  2  2  2  1  2  2  1  1  2  2  2  1  2  2  2  1  1  2  2  2  1\n[42937]  1  1  1  2  2  2  1  1  1  1  2  1  1  2  2  1  1  1  2  2  1  2  2  1\n[42961]  2  2  2  2  2  2  2  2  2  2  1  1  1 NA  2  2  1  1  2  2  2  2  2  2\n[42985]  2  2  2  2  2  1  2  1  2  2  2  2  2  1  2  2 NA  2  2  1  2  1  2  2\n[43009]  2  2  2  1  2  2  1  2  2  1  2  2 NA  2  2  2  2  1  1  1  3  2  2  1\n[43033]  2  2  1  1  1  1  2  2  2  1  2  1  1  2  1  1  2  2  2  2  1  1  1  2\n[43057]  2  2  2  1  1  2  2  2  2  2  2  1  2  2  2  1  1  2  2  1  3  1  2  2\n[43081]  1  2  3  2  2  1  2  2  2  1  2  2  1  2  2  2  1  2  2  1  2  2  2  2\n[43105]  1  1  1  2  2  1  2  1  2  2  1  2  2  2  1  2  2  2  2  2  2  2  2  2\n[43129]  2  1  1  1  2  2  2  2  1  2  1  1  1  2  2  1  2  1  1  1  2  1  1  1\n[43153]  1  2  2  1  2  2  2  1  1  2  2  2  2  2  2  2  1  2  2  2  2  1  1  1\n[43177]  1  1  2  1  1  1  2  1  1  2  1  2  1  2  1  2  1  2  1  2  2  2 NA  1\n[43201]  2  2  1  2  2  2  2  3  2  2  1  1  1  2  2  2  2  1  1  2  2  2  1  2\n[43225]  2  1  2  2  2  2  1  2  2  1  1  1  2  1  2  1  2  2  1  1  1  1  2  2\n[43249]  2  2  2  2  1  2  3  2  2  2  2  1  2  1  2  2  1  2  2  1  2  2  1  2\n[43273]  1  2  2  1  2  2  2  2  1  2  2  2  2  2  1  1  2  1  2  1  2  2  1  1\n[43297]  2  2  2  1  2  2  2  2  2  1  2  1  1  2  1  2  2  2  2  1  2  1  2  2\n[43321]  3  2  2  1  1  2  2  1  2  1  2  1  1  2  1  1  2  2  2  1  1  1  1  1\n[43345]  2  2  1  2  2  1  2  2  1  2 NA  1  2  2  1  2  2  2  2  2  1  1  1  1\n[43369]  1  2  1  1  1  2  2  1  2 NA  1  1  1  2  2  2  2  2  2  2  1  2  1  1\n[43393]  2  2  2  2  1  2  2  2  2  2  2  2  1  1  1  2  2  1  2  2  1  2  2  2\n[43417]  2  1  2  2  1  1  2  1  1  2  1  1  1  2  2  2  2  1  2  1  2  2  3  1\n[43441]  1  2  2  2  1  1  1  1  2  1  2  2  2  2  2  2  2  1  2  1  1  1  1  2\n[43465]  2  1  2  2  2  2  1  2  2  2  1  2  1  1  2  1  2  2  2  2  2  2  2  2\n[43489]  1  1  2  2  2  2  2  2  1  2  1  2  2  1  2  2  2  1  2  2  1  1  1  1\n[43513]  2  1  1  1  1  2  2  2  1  2  2  1  2  2  2  2  2  2  2  1  2  2  2  1\n[43537]  2  2  2  2  2  2  2  1  2  2  2  1  2  1  2  2  2  1  1  1  2  2  2  2\n[43561]  1  2  2  2  2  1  2  2  2  2  2  1  1  1  2  2  2  2  3  3  2  1  1  2\n[43585]  2  2  1  2  1  1  2  1  1  1  1  2  2  2  1  1  2  1  2  2  2  2  1  1\n[43609]  2  2  2  1  2  2  2  1  1  2  1  2  2  2  1  1  1  1  2  1  1  1  1  1\n[43633]  2  2  2  1  2  1  2  1  1  2  2  1  1  2  2  2  2  2  2  2  1  2  2  2\n[43657]  2  1  1  2  2  2  1  2  1  1  1  1  1  1  2  2  2  2  2  2  2  2  2  2\n[43681]  2  2  2  2  2  1  2  2  2  1  2  2  1  2  2  2  2  1  2  2  1  2  1  1\n[43705]  2  2  2  2  1  2  2  1  2  2  1  1  1  2  1  1  2  1  1  2 NA  2  1  1\n[43729]  2  2  2  1  2  1  2  1  2  2  2  1  1  2  2  2  2  3  2  1  2  2  2  2\n[43753]  1  1  2  2  2  2  2  1  2  2  2  2  2  1  2  1  1  2  2  2  2  1  1  1\n[43777]  1  1  2  1  2  1  1  1  1 NA  2  2  1  2  2  2  1  2  1  2  2  1  2  2\n[43801]  1  2  1  1  2  1  2  2  2  2  1  2  1  2  1  1  2  1  2  2  2  1  1  1\n[43825]  2  2  2  2  2  2  1  2  1  1  2  1  1  1  2  2  2  2  2  1  2  2  1  2\n[43849]  2  1  2  1  2  2  2  1  1  2  2  2  2  2  1  1  1  2  1  2  2  1  2  1\n[43873]  2  2  1  1  1  2  2  2  2  2  2  2  2  2  1  2  1  2  2  1  2  2  2  1\n[43897]  2  1  2  1  2  1  2  1  1  2  1  2  2 NA  1  1  2  2  2  1  2  2  1  1\n[43921]  2  2  2  2  2  1  1  2  2  2  2  1  1  1  2  2  2  2  2  1  2  2  1  2\n[43945] NA  2  2  2  2  1  1  2  1  2  1  1  1  2  2  2  2  2  2  2  1  2  2  1\n[43969]  2  2  1  1  1  1  1  2  2  2  2  2  2  2  2  2  1  2  2  1  1  2  1  1\n[43993]  2  2  1  1  1  2  2  2  1  2  2  2  2  1  1  1  2  1  1  1  1  2  1  2\n[44017]  1  1  2  2  1  1  2  1  1  1  1  2  2  1  2  2  2  1  1  2  1  1  1  2\n[44041]  2  2  1  1  2  2  1  2  2  1  1  2  2  2  1  1  1  2  1  2  1  1  2  1\n[44065]  1  1  1  2  1  1  2  2  1  1  1  2  2  2  3  2  1  1  1  2  2  1  1  1\n[44089]  2  1  2  2  1  2  2  1  2  2  1  2  2  2  2  1  2  2  2  2  2  1  1  2\n[44113]  2  1  1  2  2  2  1  2  2  2  2  2  2  1  1  1  2  1  1  2  1  2  1  1\n[44137]  2  1  2  1  2  1  2  1  2  2  2  2  1  1  1  2  1  1  1  2  1  2  1  2\n[44161]  2  2  2  2  2  2  1  2  2  2  2  1  1  2  1  1  2  1  2  1  2  1  1  2\n[44185]  2  2  2  2  2  1  2  1  1  1  2  2  2  2  2  2  2  2  2  2  2  2  1  2\n[44209]  1  2  1  2  2  2  1  1  2  2  2  2  2  2  1  1  2  1  2  2  2  3  1  2\n[44233]  1  2  1  1  2  2  1  2  1  2  2  2  2  2  2  2  2  2  2  2  2  1  1  2\n[44257]  2  2  2  1  1  2  2  2  1  2  2  1  2  1  1  2  2  2  1  2  2  2  2  2\n[44281]  1  2  1  2 NA  2  2  2  1  1  1  1  1  1  2  1  1  2  2  2  2  2  1  1\n[44305]  2  1  2  2  2  2  2  2  2  2  1  1  1  2  2  2  1  2  1  2  2  1  2  1\n[44329]  1  1  1  2  2  2  2  1  1  2  2  2  1  1  2  1  3  1  1 NA  2  2  2  2\n[44353]  1  2  2  1  1  2 NA  1  2  1  1  2  1  1 NA  2  2  2  2  2  2  1  2  2\n[44377]  2  2  1  2  1  1  2  2  1  2  1  2  2  1  2  2  2  2  2  2  1  2  2  2\n[44401]  1  1  2  2  1  2  2  1  1  2  2 NA  2  2  1  2  2  2  2  2  1  2  1  2\n[44425]  2  1  2  2  1  2  2  2  1  1  2  2  2  1  2  1  2  2  1  2  2  1  1  1\n[44449]  2  2  2  2  2  1  2  2  1  2  2  2  1  1  1  1  1  1  2  1  2  2  2  2\n[44473]  2  2  1  2  2 NA  2  1  1  1  2  3  2  1  2  2  2  1  2  2  2  1  2  2\n[44497]  2  2  2  1  1  2  2  2  2  1  2  2  2  2  2  1  2  1  2  1  1  1  2  1\n[44521]  2  2  2  2  2  1  1  2  2  2  2  2  1  1  2  1  2  2  2  1  1  1  2  2\n[44545]  2  1  2  2  1  2  1  2  1  2  1  2  2  1  2  2  1  2  2  1  2  2  1  2\n[44569]  1  1  2  1  1  1  1  1  2  2  2  1  1  2  2  1  2  2  2  2  2  2  2  1\n[44593]  1  2  1  2  2  2  1  2  1  1  2  2  2  1  2  1  3  3  2  2  2  2  1  1\n[44617]  1  2  2  1  2  1  1  1  1  1  2  2  2  2  2  2  2  1  1  2  2  2  2  1\n[44641]  2  1  1  2  1  2  1  3  2  2  2  1  2  2  2  2  2  1  1  2  2  2  2  2\n[44665]  2  2  2  2  1  2  1  2  1  1  2  2  2  1  1  1  2  2  2  1  2  1  2  2\n[44689]  1  1  2  3  1  1  2  2  2  1  2  2  2  2  1  2  2  1  2  1  2  2  1  2\n[44713]  2  2  1  1  2  1  1  2 NA  2  1  1  2  2  2  1  2  2  1  2  2  1  2  1\n[44737]  1  1  1  2  1  2  2  1  2  1  1  2  2  2  1  2  2  2  2  2  1  2  2  1\n[44761]  2  2  2  1  2  2  2  2  2  2  3  2  1  2  2  2  2  2  2  2  1  1  2  2\n[44785]  2  2  2  1  2  1  1  2  1  1  1  2  1  2  2  2  1  1  2  1  1  1 NA  2\n[44809]  2  1  2  2  2  2  1  2  1  2  1  1  2  2  1  2  2  2  1  1  1  1  2  2\n[44833]  2  2  2  2  2  1  1  2  2  2  1  1  1  3  1  2  2  2  1  2  2  2  1  2\n[44857]  2  2  2  1  2  2  2  1  2  2  1  2  2  2  2  2  2  1  2  1  1  3  1  2\n[44881]  2  1  2  1  1  1  1  2  2  1  2  1  1  2  1  2  2  2  1  1  1  2 NA  2\n[44905]  2  1  1  2  2  1  1  2  1  2  2  1  2  2  2  1  2  2  2  2 NA  1  2  1\n[44929]  2  2  1  2  2  1  2  1  2  1  2  1  2  2  2  2  2  2  1  2  2  2  3  2\n[44953]  1  2  2  2  2  2  2  2  1  1  1  1  2  2  2  1  2  2  1  2  1  2  2  2\n[44977]  2  2  1  1  2  2  2  2  1  2  2  3  2  1  2  1  1  1  2  2  2  1  1  1\n[45001]  2  2  2  2  2  1  1  2  1  1  1  2  1  1  1  2  1  2  1  2  1  2  2  2\n[45025]  1  2 NA  2  2  2  1  1  1  1  2  2  1  3  2  1  2  1  1  2  2  2  2  2\n[45049]  1  2  1  2  2  2  2  1  1  2  1  2  1  1  2  2  2  2  1  2  1  2  2  1\n[45073]  2  2  2  1  2  2  2  2  2  2  2  1  2  1  2  2  2  2  1  1  2  2  1  2\n[45097]  2  2  2  2  2  2  1  1  1  2  2  1  2  2  2  2  1  1  1  2  1  2  1  2\n[45121]  2  2  1  2  2  2  1  2  2  2  1  2  2  2  2  2  2  2  2  2  2  1 NA  2\n[45145]  2  2  2  1  2  2  2  1  1  2  2  2  2  1  2  2  1  2  2  2  1  1  1  1\n[45169]  2  1  1  2  2  2  2  2  1  1  1  2  2  2  2  1  2  2  2  2  1  2  2  1\n[45193]  2  2  2  2  2  1  2  1  1  2  1  1  1  1  1  1  1  2  2  1  2  1  1  2\n[45217]  2  1  1  1  2  2  2  2  2  2  2  2  2  2  2  1  2  2  2  2  1  1  2  1\n[45241]  2  2  1  1  2  2  1  2  2  2  2  1  1  1  2  2  2  2  2  2  1  2  1  2\n[45265]  2  2  2  1  2  1  1  1  2  2 NA  2  2  2  1  1  2  2  2  1  1  2  1  1\n[45289]  1  1  2  1  1  2  2  2  2  2  1  1  2  2  2 NA  2  1  1  2  2  2  2  2\n[45313]  1  1  2  2  2  1  2  2  2  2  1  2  2  2  2  2  1  2  2  2  2  2  2  2\n[45337]  2  2  2  2  2  1  1  2  1  1  2  2  2  2  1  1  1  2  2  1  2  2  2  2\n[45361]  1  1  1  1  2  1  1  1  1  2  1  1  1  1  1  2  2  2  1  2  2  2  2  2\n[45385]  2  2  2  2  2  2  2  2  1  1  1  1  2  2  1  1  1  1  2  1  1  2  2  1\n[45409]  2  1  2  2  2  1  2  2  1  1  2  1  1  2  2  2  2  1  1  2  2  2  2  2\n[45433]  2  2  1  2  2  1  1  1  2  1  2  2  1  2  1  1  2  2  2  1  2  1  1  3\n[45457]  1  2  2  2  2  1  2  1  2  2  1  2  2  1  2  1  2  1  1  2  2  1  2  2\n[45481]  1  2  2  1  2 NA  1  2  2  2  1  2  2  2  2  2  2  1  2  2  1  1  1  1\n[45505]  2  1  2  1  1  1  2  1  2  2  1  1  1  2  2  1  2  2  2  1  2  2  2  1\n[45529]  2  1  2  2  2  2  1  2  2  2  2  2  2  1  2  2  1  2  2  2  2  2  2  2\n[45553]  2  2  2  2  2  1  2  1  1  2  2  2  1  2  1  1  2  2  1  2  2  2  1  2\n[45577]  1  1  1  2  2  1  1  1  1  2  2  2  1  2  2  1  2  2  1  1  2  2 NA  1\n[45601] NA  2  2  1  2  1  2  2  1  2  2  1  1  2  2  2  2  2  2  1  2  1  2  2\n[45625]  1  1  2  2  2 NA  2  1  2  2  2  2  2  2  2  1  1  2  2  2  1  1  1  1\n[45649]  1  1  1  1  1  2  1  2  1  1  1  2  1  1  1  2  1  2  2  1  2  2  1  2\n[45673]  2  2  2  2  1  2  2  1  2  1  2  1  2  2  1  1  2  2  1  2  2  2  2  2\n[45697]  2  1  2  2  1  2  2  1  2  2  2  1  2  2  2  2  1  2  2  1  1  1  1  1\n[45721]  2  2  2  2  1  1  2  1  2  1  1  2  1  2  2  2  2  2  2  2  2  2  2  2\n[45745]  2  2  1  1  2  2  2  2  1 NA  1  2  2  2  2  1  2  2  1  1  1  1  2  2\n[45769]  2  2  2  2  2  2  2  1  1  2  1  1  2  2  2  1  2  2  2  1  1  1  2  2\n[45793]  1  2  1  2  2  1  1  2  1  2  1  1  2  1  2  1  1  1  1  2  2  2  2  2\n[45817]  1  2  1  2  1  2  2  2  2  2  1 NA  2  2  2  2  1  2 NA  2  1  1  2  1\n[45841]  2  2  2  2  1  2  1  1  2  1  2  2  2  2  2  1  3  2  1  1  1  1  2  2\n[45865]  1  2  2  2  1  2  2  1  2  2  1  2  2  2  2  2  1  1  2  2  1  1  1  2\n[45889]  2  2  2  3  2  2  1  1  1  2  1  2  1  1  2  2  2  2  2  1  2  2  2 NA\n[45913]  1  1  2  1  1  1  2  2  2  2  2  1  1  2  1  2  2  2  2  2  1  1  1  2\n[45937]  2  2  2  3  1  2  2  2  1  1  2  2  1  2  2  2  1  2  2  2  1  2  2  2\n[45961]  2  2  1  1  2  2  2  2  1  2  2  2  2  1  2  1  1  2  1  2  2  2  2  1\n[45985]  2  1  2  2  2  1  1  1  2  1  2  2  1  2  1  2  2  1  2  1  1  2  1  2\n[46009]  1  2  2  2  2  1  2  2  2  2  2  2  2  2  2  1  2  2  1  2  2  2  1  1\n[46033]  2  1  1  2  2  1  1  1  2  1  2  2  2  1  2  1 NA  2  2  2  2  2  1  2\n[46057] NA  2  2  1  2  2  2  1  2  1  2  2  2  1  1  1  2  2  2  1  2  1  2  2\n[46081]  2  2  2  3  2  2  1  2  1  2  1  1  2  2  2  2  2  2  2  2  1  1  2  2\n[46105] NA  2  1  2  1  2  2  1  2  2  1  2  2  1  2  2  2  2  2  2  2  2  2  2\n[46129]  2  1  1  1  2  2  1  2  1  2  2  1  2  1  1  2  2  2  2  1  2  2  2  2\n[46153]  1  2  2  2  1  2  1  2  2  2  1  2  1  1  2  1  2  2  2  3  2  1  1  1\n[46177]  1  1  1  2  2  1  2  1  2  1  1  2  1  2  1  1  2 NA  2  1  2  1  1  2\n[46201]  2  1  2  2  2  2  2  2  2  1  2  1  2  1  1  2  2  1  3  1  2  1  1  2\n[46225]  2  2  1  2  2  2  1  2  1  1  2  1  2  1  2  2  2  2  1  2  2  1  2  2\n[46249]  1  1  2  2  2  2  1  2  2  2  1  1  1  1  1  1  2  2  2  2  1  2  1  2\n[46273]  2  1  2  2  1  2  1  2  2  1  2  2  2  1  2  1  2  2  1  1  2  2  1  1\n[46297]  2  1  1  2  2  2  2  1  2  2  2  1  2  2  1  2  2  2  1  2  2  2  2  2\n[46321]  2  2  1  2  2  1  2  2  1  2  2  2  2  1  1  2  1  2  1  2  2  2  1  1\n[46345]  2  3  2  2  2  2 NA  2  2  2  2  2  1  1  2  1  3  2  2  2  2  2  2  2\n[46369]  1  2  1  2  1  1  2  2  2  2  2  1  1  1  2  2  2  1  2  2  2  1  2  1\n[46393]  1  2  1  2  2  2  2  2  2  3  2  2  1  2  2  1  2  2  2  2  2  2  1  1\n[46417]  1  1  1  2  1  1  1  1  2  2  2  1  1  1  2  1  2  1  1  2  2  2  2  1\n[46441]  2  2  2  1  1  2  2  2  2  1  2  2  1  1  1  2  2  2  2  2  2  2  2  2\n[46465]  1  2  2  1  2  1  1  2  1  2  3  2  2  1  2  2  2  2  2  1  2  2  2  2\n[46489]  1  1  2  2  1  2  1  2  2  2  2  2  2  2  1  1  2  2  2  1  2  2  2  3\n[46513]  1  2  1  1  1  1  1  2  2  2  2  2  1  2  2  2  1  2  1  2  1  2 NA  2\n[46537]  2  1  1  1  1  1  2  2  2  2  2  2  2  2  1  2  1  1  1  2  2  1  1  2\n[46561]  1  2  2  3  2  1  2  2  1  1  2  2  1  2  2  2  2  2  2  2  1  1  1  2\n[46585]  2  1  2  2  1  1  2  1  2  1  1  1  2  1 NA  2  2  1  2  1  1  1  2  2\n[46609]  2  2  1  2  1  2  2  1  2  1  2  2  2  2  1  2  1  2  1  2  1  1 NA  2\n[46633]  2  2  2  2 NA  2  2  2  2  2  1  2  2  1  2  2  2  1  2  1  2  1  2  1\n[46657]  2  2  2  2  2  2  2  2  2  1  2  2  2  1  2  1  1  2  1  2 NA  1  2  2\n[46681]  2  2  2  2  2  2  1  2  2  1  1  2  1  2  1  1  1  2  2  2  2  2  1  2\n[46705]  2  2  1  1  2  1  2  1  2  1  2  1 NA  1  1  1  1  2  1  2  2  2  2  2\n[46729]  2  2  2  2  1  2  1  1  2  2  2  2  1  2  2  2  1  2  2  2  2  2  2  2\n[46753]  2  1  2  2  1  1  2  2  1  1  2  1  1  1  1  1  2  2  2  2  2  2  1  2\n[46777]  1  2  2  2  2  1  1  2  1  1  1  2  1  2  2  2  2  2  2  2  2  2  2  2\n[46801]  2  1  2  1  1  1  2  2  2  3  2  2  2  1  3  2  2  1  1  2  2  2  1  2\n[46825]  2  2  2  2  1  2  2  1  2  2  2  2  2  1  2  2  2  1  2  2  1  1  2  1\n[46849]  2  1  2  1  2  1  2  2  1  2  2  2  1  2  2  2  1  1  2  2  2  2  2  2\n[46873]  2  1  1  1  2  2  2  1  2  1  1  2  1  2  1  2  2  2  1  2  2  2  2  2\n[46897]  2  1  2  1  2  1  2  2  2  1  1  1  2  2  2  1  1  1  2  2  2  1  1  1\n[46921]  1  2  1  1  2  2  1  2  2 NA  2  2  2  1  1  1  2  2  1  2  2  2  2  1\n[46945]  2  2  1  2  2  2  1  2  1  2  2  2  1  2  1  1  2  2  2  1  2  2  1  2\n[46969]  2  1  2  2  2  2  2  2  2  1  2  1  2  1  2  2  2  2  1  2  2  2  2  1\n[46993]  1  1  1  2  2  1  1  1  2  2  2  2  2  1  2  1  1  2  2  2  1  1  2  2\n[47017]  2  2  2  2  2  1  2  1  2  1  2  2  2  2  1  2  2  2  1  1  1  2  2  2\n[47041]  2  1  2  2  2  2  2  2  2  1  1  1  2  2  2  1  1  2  2  2  2  2  1  1\n[47065]  2  2  1  2  2  2  2  2  1  2  1  2  2  2  2  2  2  2  2  2  2  1  1  2\n[47089]  2  2  2  2  2  2  2  2  2  2  1  2  1  2  2  1  2  2  1  2  1  2  2  2\n[47113]  1  2  1  2  1  2  2  2  1  2  2  1  2  2  2  2  2  2  2  2  2  1  2  2\n[47137]  1  1  2  1  2  2  2  1  2  2  2  2  2  1  2  1  2  1  2  1  2  2  2  2\n[47161]  2  2  2  2  1  2  1  2  2  2  2  2  2  2  2  2  2  1  2  2  2  2 NA  1\n[47185]  2  2  2  1  2  1  2  2  2  2  2  2  3  2  2  2  1  2  1  2  2  2  1  1\n[47209]  2  1  1  2  2  2  2  2  1  1  2  1  2  2  1  2  2  1  1  2  2  2  2  2\n[47233]  1  1  1  1  2  1  2  1  1  2  2  2  2  1  1  1  1  2  2  2  2  2 NA  2\n[47257]  1  2  1  2  2  2  2  2  2  1  2  1  1  2  2  1  2  1  2  1  2  2  2  1\n[47281]  2  2  1  1  2  1 NA  1  2  1  3  2  2  2  1  2  1  2  1  3  1  2  2  1\n[47305]  3  1  2  1 NA  1  2 NA  2  2  1  2  2  2  2  2  1  2  2  2  2  2  2  2\n[47329]  2  2  2  2  1  2  2  2  1  2  1  2  1  2  2  1  2  2  2  2  1  1  2  2\n[47353]  2  1  2  1  1  2  2  2  2  1  1  1  2  2  2  1  2  2  2  2  1  1  2  1\n[47377]  1  2  1  2  1  1  1  1  2  1  1  2  1  1  2  2  1  2  2  2  1  2  2  1\n[47401]  1  1  2  2  2  2 NA  2  1  2  1  1  2  2  1  2  1  2  1  1  2  1  2  2\n[47425]  2  1  2  2  2  1 NA  1  2  1  2  2  1  1  2  2  1  2  2  1  1  2  2  1\n[47449]  2  2  1  2  2  2  2  2  1  2  2  2  2  1  1  2  2  1  1  1  2  1  2  2\n[47473]  2  1  1  2  2  1  2  2  2  1  1  2  1  1  2  1  2  2  2  2  2  2  2  2\n[47497]  2  2  2  1  2  2  1  1  2  2  1  1  2  2  2  2  2  2  2  1  2  1  2  2\n[47521]  2  1  2  2  2  1  2  1  2  1  2  2  1  1  2  2  2  2  1  1  1  1  2  2\n[47545]  1  2  2  1  2  2  2  2  2 NA  2  2  2  2  2  2  2  1  1  2  2  2  2  2\n[47569]  1  2  1  2  2  2  2  1  1 NA  3  1  1  2  1  2  2  2  2  2  1  2  1  2\n[47593]  1  1  1  2  1  3  2  2  1  2  2  1  2  1  2  2  1  2  2  2  2  2  2  2\n[47617]  2  2  2  1  2  2  1  2  3  2  2  2  2  2  1  2  1  2  2  1  2  1  2  1\n[47641]  2  1  2  2  2  2  2  1  2  2  2  2  1  1  2  2  2  2  2  2  2  1  1  2\n[47665]  1  1  2  2  2  1  2  1  1  1  1  1  2  2  2  2  1  1  1  2  2  2  2  1\n[47689]  2  1  2  2  2  2  1  2  2  1  2 NA  2  2  2  2  2  1  2  2  2  2  1  2\n[47713]  2  1  2  1  1  2  2  2  2  1  2  2  2  2  2  1  1  2  1  2  1  2  1  2\n[47737]  2  2  2  2  2  2  2  2  1  2  2  2  2  2  2  1  1  2  1  2  2  2  1  2\n[47761]  1  2  2  1  2  2  1  1  1  2  2  2  2  2  1  1  2  2  1  1  2  1  2  1\n[47785]  1  2  2 NA  2  2  2  1  2  2  1  2  1  2  2  1  2  2  1 NA  2  1  2  3\n[47809]  2  2  1  2  1  2  1  1  2  1  2  2  2  1  2  2  3  2  2  2  2  1  2  1\n[47833]  2  2  2  2  2  1  1  2  1  2  2  1  2  1  2  1  1  2  2  1  2 NA  2  2\n[47857]  2  2  2  2  2  2  2  2  2  1  1  1  2  2  2  1  2  2  2  2  2  2  2  1\n[47881]  1  2  2  1  2  2  1  2  2  1  1  2  2  2  2  2  1  2  1  1  2  1  2  2\n[47905]  2  2  2  2  2  2  2  2  1  2  2  1  2  2  2  2  1  1  2  1  2  1  2  2\n[47929]  1  1  2  2  2  1  2  1  2  2  2  1  2  2  2  2  1  2  1  1  2  1  1  2\n[47953]  2  1  2  2  1  2  1  2  2  2  1  2  1  2  1  3  1  1  1  2  1  2\n\nselfes$genF &lt;- as.factor(selfes$gender)\nlevels(selfes$genF) &lt;- c(\"Male\", \"Female\", \"Other\")\nplot(selfes$genF, col = 'black', xlab = \"Gender (Self-Reported)\")\n\n\n\n\n\n\n\nsummary(selfes$genF)\n\n  Male Female  Other   NA's \n 17801  29182    529    462 \n\n\nChallenge Problem (Optional, but Encouraged!) Report the mean and standard deviation of self-esteem for people who are identified as female, male, and ‚Äúother‚Äù in the dataset.\nNote : there are MANY ways to do this in R; you can use the subset function or indexing to divide the dataset into three groups - ‚Äúfemales‚Äù, ‚Äúmales‚Äù, and people who reported ‚Äúother‚Äù. Or other fancier methods we will talk about later. See how many different ways you can do it.\n\nnames(selfes)\n\n [1] \"Q1\"      \"Q2\"      \"Q3\"      \"Q4\"      \"Q5\"      \"Q6\"      \"Q7\"     \n [8] \"Q8\"      \"Q9\"      \"Q10\"     \"gender\"  \"age\"     \"source\"  \"country\"\n[15] \"SELFES\"  \"genF\"   \n\n## The indexing approach, broken up into clear steps.\nselfesF &lt;- selfes[selfes$genF == \"Female\",] # creating new datasets for each group\nselfesM &lt;- selfes[selfes$genF == \"Male\",]\nselfesO &lt;- selfes[selfes$genF == \"Other\",]\n\npar(mfrow = c(1,3)) # checking my work; should see only one bar per graph\nplot(selfesF$genF)\nplot(selfesM$genF)\nplot(selfesO$genF) # yep!\n\n\n\n\n\n\n\nmean(selfesF$SELFES, na.rm = T)\n\n[1] 2.574468\n\nmean(selfesM$SELFES, na.rm = T)\n\n[1] 2.731565\n\nmean(selfesO$SELFES, na.rm = T)\n\n[1] 2.258706\n\nsd(selfesF$SELFES, na.rm = T)\n\n[1] 0.6921987\n\nsd(selfesM$SELFES, na.rm = T)\n\n[1] 0.6956196\n\nsd(selfesO$SELFES, na.rm = T)\n\n[1] 0.7222484\n\n## indexing another way\nmean(selfes[selfes$genF == \"Female\", ]$SELFES, na.rm = T) # as one line of code.\n\n[1] 2.574468\n\n## the subset function\nsF &lt;- subset(selfes, genF == \"Female\")\nmean(sF$SELFES, na.rm = T) # another way.\n\n[1] 2.574468\n\n## the tapply function.\ntapply(selfes$SELFES, selfes$genF == \"Female\", mean, na.rm = T) # another way\n\n   FALSE     TRUE \n2.717905 2.574468 \n\n## and then there's tidyverse...\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nsF2 &lt;- group_by(selfes, selfes$genF)\nsF2 %&gt;% summarise(mean(SELFES, na.rm = T))\n\n# A tibble: 4 √ó 2\n  `selfes$genF` `mean(SELFES, na.rm = T)`\n  &lt;fct&gt;                             &lt;dbl&gt;\n1 Male                               2.73\n2 Female                             2.57\n3 Other                              2.26\n4 &lt;NA&gt;                               2.61\n\n## feel like I'm forgetting an obvious one? Point is there are a LOT of ways to do this in R, and it matters less how you do it than getting the correct answer."
  },
  {
    "objectID": "gradstats/gradlabs/2Lab_DataDescription_KEY.html#footnotes",
    "href": "gradstats/gradlabs/2Lab_DataDescription_KEY.html#footnotes",
    "title": "Psych 205 - Lab 2 - Prof.¬†Key",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOptional, but potentially fun and useful if you have the time and interest.‚Ü©Ô∏é"
  },
  {
    "objectID": "gradstats/gradlabs/2_Description.html#part-1-describing-data-with-r-statistics-and-human-language",
    "href": "gradstats/gradlabs/2_Description.html#part-1-describing-data-with-r-statistics-and-human-language",
    "title": "Lecture 2 : Describing Data with Pictures and Numbers",
    "section": "",
    "text": "summary() or table(): to count frequencies of categorical data\nas.factor() and as.numeric() : to translate data\nlevels() or factor() : to relevel or change factor names\nplot : graphing categorical data\n\n\n\n\n\nsummary() or psych::describe() # this comes from the psych package which you must install : to summarize data\n\nmean()\nmedian()\nsd()\nrange()\nnote : if there are missing data (often!) you must manually tell R to remove the missing data : mean(d$variable, na.rm = T)\n\nhist() or boxplot() : graphing numeric data\n\n\n\n\nQuarto is a version of R Markdown, which is a version of Markdown, which is a powerful way to author code that is meant for both humans and computers to read.\n\nAdvantages :\n\ncan create a document that works for R code, can create a presentation, or a website\nmuch faster to get your code from R to something that humans can read\n\nno more copy-paste graphs or output.\ncan update graphs and output as your needs / datset changes\n\nlots of features - open-source heritage and culture, but supported financially my Micro$oft.\n\nability to format your code; render it as a website, pdf, book, etc.\ninteractive documents (Shiny; html-live; etc.)\n\n\nDisadvantages :\n\ncode must be ‚Äúperfect‚Äù in order to correctly render.\ncan go down formatting and feature rabbit holes that are not necessarily condusive to good science.\nanother dialect of the language you are trying to learn\n\nin R : code is the default; human comments added with #s\nin Quarto : human text is the default; you insert a code block when you want R to do something (and can then comment in that code)\n\n\n\n1+1 # like this\n\n[1] 2\n\n\nIn this class, we will work with both .R scripts and .qmd Quarto Markdown Files\n\n.R Scripts for tinkering with data (in-class tutorials; initial analyses)\n.qmd files for ‚Äúfinal‚Äù products (Lecture notes, lab documents, your project)\n\nThere are many thorough guides on how to use Quarto, but honestly the official Quarto reference book is almost constantly open on my computer (in multiple tabs‚Ä¶.sigh.) But let me know if you find another cool resource!\nThings to do in Quarto :\n\nwrite in human text\ninsert a code block\ninsert inline code\nrender everything with no pain and drama as a .pdf or .html file and share this with others."
  },
  {
    "objectID": "gradstats/gradlabs/2Lab_DataDescription_KEY.html#in-section",
    "href": "gradstats/gradlabs/2Lab_DataDescription_KEY.html#in-section",
    "title": "Psych 205 - Lab 2 - Prof.¬†Key",
    "section": "",
    "text": "Choose one of the datasets from the class folder (avoid the one(s) labeled [repeated measures] as we will get to these later. I‚Äôll be focusing on the ‚ÄúPerceptions of the Wealthy‚Äù dataset if you want to follow along with my key when it‚Äôs posted). Look over the accompanying article for a guide to the variables, and identify two numeric variables from the dataset that seem interesting to you. Graph these variables, report the relevant descriptive statistics for each variable. Then explain what these statistics and graphs tell you about the individuals in the dataset, and what other questions you might ask about these individuals (e.g., what do the data NOT tell you?)\n\n\nI‚Äôm going to work with the ‚ÄúPerceptions of the Wealthy‚Äù study again. First I‚Äôll load my dataset, and check to make sure it loaded correctly.\n\n\nd &lt;- read.csv(\"~/Dropbox/!GRADSTATS/Datasets/Perceptions of the Wealthy/Dawtry 2015 Study 1a.csv\", # my file path\n              stringsAsFactors = T, # to convert strings into factors\n              na.strings = \"\") # empty data --&gt; NA\nhead(d) # did it work?\n\n   PS PD_15 PD_30 PD_45 PD_60 PD_75 PD_90 PD_105 PD_120 PD_135 PD_150\n1 233    27    48    21     0     0     0      0      0      0      0\n2 157    39     0     0     0     0     0      0      0      0      0\n3 275     0     0    50     0     0    50      0      0      0      0\n4 111     9    14    17    17    17     8      7      5      2      2\n5  52    68    32     0     0     0     0      0      0      0      0\n6  11    10    31     6    14     8     8      6      5      5      7\n  PD_150plus fairness satisfaction SC_15 SC_30 SC_45 SC_60 SC_75 SC_90 SC_105\n1          4        1            1    50    24    26     0     0     0      0\n2         61        5            2    19     0     0     0    53     0     28\n3          0        5            5     0     0    11     0     0     0     35\n4          2        7            7     2     7     7    11    11    13     14\n5          0        4            5     0     0    57     0     0    43      0\n6          0        1            4     8    25     8    16     7    11     11\n  SC_120 SC_135 SC_150 SC_150plus redist1 redist2 redist3 redist4\n1      0      0      0          0       6       3       6       1\n2      0      0      0          0       2       2       3       4\n3      0     54      0          0       5       4       5       5\n4     14     14      5          2       1       3       3       4\n5      0      0      0          0       4       5       4       5\n6      9      3      1          1       6       5       6       6\n  Household_Income Political_Preference age gender\n1               NA                    5  40      2\n2               20                    5  59      2\n3              100                    5  41      2\n4              150                    8  59      2\n5              500                    5  35      1\n6              600                    3  34      2\n  Population_Inequality_Gini_Index Population_Mean_Income\n1                         38.78294                  29715\n2                         37.21451                 123630\n3                         20.75000                  60000\n4                         35.37958                  59355\n5                         16.87500                  15360\n6                         40.92448                  57600\n  Social_Circle_Inequality_Gini_Index Social_Circle_Mean_Income\n1                            28.05674                     21150\n2                            24.32339                     65355\n3                            14.44258                    107100\n4                            26.92590                     86640\n5                            21.40106                     56850\n6                            37.08824                     59835\n\nnames(d) # \n\n [1] \"PS\"                                  \"PD_15\"                              \n [3] \"PD_30\"                               \"PD_45\"                              \n [5] \"PD_60\"                               \"PD_75\"                              \n [7] \"PD_90\"                               \"PD_105\"                             \n [9] \"PD_120\"                              \"PD_135\"                             \n[11] \"PD_150\"                              \"PD_150plus\"                         \n[13] \"fairness\"                            \"satisfaction\"                       \n[15] \"SC_15\"                               \"SC_30\"                              \n[17] \"SC_45\"                               \"SC_60\"                              \n[19] \"SC_75\"                               \"SC_90\"                              \n[21] \"SC_105\"                              \"SC_120\"                             \n[23] \"SC_135\"                              \"SC_150\"                             \n[25] \"SC_150plus\"                          \"redist1\"                            \n[27] \"redist2\"                             \"redist3\"                            \n[29] \"redist4\"                             \"Household_Income\"                   \n[31] \"Political_Preference\"                \"age\"                                \n[33] \"gender\"                              \"Population_Inequality_Gini_Index\"   \n[35] \"Population_Mean_Income\"              \"Social_Circle_Inequality_Gini_Index\"\n[37] \"Social_Circle_Mean_Income\"          \n\nnrow(d)\n\n[1] 305\n\n\n\nOkay, this looks good. Next step will be to focus on the variable Household Income.\n\n\nhist(d$Household_Income, breaks = 20, \n     main = \"Household Income\", \n     xlab = \"Household Income (USD)\")\n\n\n\n\n\n\n\nboxplot(d$Household_Income)\n\n\n\n\n\n\n\nsummary(d$Household_Income, na.rm = T)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n     20   25000   42000   54732   75000  350000       4 \n\nsd(d$Household_Income, na.rm = T)\n\n[1] 47238.63\n\n\nI See the Following :\n\nmean = NA) =\nmedian = NA =\nstandard deviation = NA)\nrange = NA)\n\nThings I Learned and Questions I Had :\n\n\n\nFor the categorical variable, I‚Äôll focus on\n\nhist(d$fairness)"
  },
  {
    "objectID": "gradstats/gradlabs/2_Description.html#part-2-mean-thoughts-r-demo",
    "href": "gradstats/gradlabs/2_Description.html#part-2-mean-thoughts-r-demo",
    "title": "Lecture 2 : Describing Data with Pictures and Numbers",
    "section": "",
    "text": "We will see how far we get in exploring these ideas!\n\n\n\nIllustrating the mean as a ‚Äúline of best fit‚Äù [our first linear model!]\nIllustrating the standard deviation as the ‚Äúaverage‚Äù amount of residual error.\n\n\n\n\n\nIllustrating that the mean is the value that reduces the sum of our residual error."
  },
  {
    "objectID": "calstats/Lectures/1L_Welcome.html",
    "href": "calstats/Lectures/1L_Welcome.html",
    "title": "Lecture 1 | Welcome to Psych 101",
    "section": "",
    "text": "for (x in 1:5) {\n  print(x ** 2)\n}\n\n\n\n\n\nSection Swap : post on bCourses ASAP.\nNext Week :\n\nAttend Discussion Section\nComplete Lab 1 (will start in lecture!)\nRead Chapter 2\nComplete Quiz 2\n\nWaitlisted Students : Thanks for your patience! Nothing I can do :(\nJoin the Class Discord\n\n\n\n\n\n2:10 - 2:30 | Check-In & Announcements\n2:30 - 3:00 | RECAP : Science as Prediction"
  },
  {
    "objectID": "calstats/Lectures/1L_Welcome.html#check-in-agenda-and-announcements",
    "href": "calstats/Lectures/1L_Welcome.html#check-in-agenda-and-announcements",
    "title": "Lecture 1 | Welcome to Psych 101",
    "section": "",
    "text": "for (x in 1:5) {\n  print(x ** 2)\n}\n\n\n\n\n\nSection Swap : post on bCourses ASAP.\nNext Week :\n\nAttend Discussion Section\nComplete Lab 1 (will start in lecture!)\nRead Chapter 2\nComplete Quiz 2\n\nWaitlisted Students : Thanks for your patience! Nothing I can do :(\nJoin the Class Discord\n\n\n\n\n\n2:10 - 2:30 | Check-In & Announcements\n2:30 - 3:00 | RECAP : Science as Prediction"
  },
  {
    "objectID": "calstats/Lectures/1L_Welcome.html#r-practice-defining-variables",
    "href": "calstats/Lectures/1L_Welcome.html#r-practice-defining-variables",
    "title": "Lecture 1 | Welcome to Psych 101",
    "section": "R Practice : Defining Variables",
    "text": "R Practice : Defining Variables\n\nThinking about Programming (Free Association Activity)\n\nClose your eyes\nTake a deep breath (inhale / exhale)\nVisualize an image based on the word that you hear me say.\nWhat do you observe?"
  },
  {
    "objectID": "calstats/Lectures/1L_Welcome.html#recap-science-as-prediction",
    "href": "calstats/Lectures/1L_Welcome.html#recap-science-as-prediction",
    "title": "Lecture 1 | Welcome to Psych 101",
    "section": "Recap : Science as Prediction",
    "text": "Recap : Science as Prediction\n\nVariables and Variation\n\n\nIn R : Variables\n\n\n\n\n\n\n\nCode\nDescription\n\n\n\n\ntest &lt;- c(#,#,#,#)\n\n\n\n\n\n\n\n\n\nCreating Numeric Variables\n\ncounting &lt;- c(1,2,3,4,5) # the numbers one through five\nprint(counting) # one way to \"print\" the variable\n\n[1] 1 2 3 4 5\n\ncounting # another way to \"print\" the variable\n\n[1] 1 2 3 4 5\n\nhist(counting) # a way to graph the variable (a histogram)\n\n\n\n\n\n\n\n\n\n\nCreating Non-Numeric Variables\n\nlaundryhang &lt;- c(\"shirt\", \"shirt\", \"leggings\", \"leggings\", \"shirt\", \n             \"shirt\", \"leggings\", \"pants\", \"sweater\", \"sweater\") # defining a string variable\nprint(laundryhang)\n\n [1] \"shirt\"    \"shirt\"    \"leggings\" \"leggings\" \"shirt\"    \"shirt\"   \n [7] \"leggings\" \"pants\"    \"sweater\"  \"sweater\" \n\nlaundryhang # another way to \"print\" the variable\n\n [1] \"shirt\"    \"shirt\"    \"leggings\" \"leggings\" \"shirt\"    \"shirt\"   \n [7] \"leggings\" \"pants\"    \"sweater\"  \"sweater\" \n\nlaundryhang &lt;- as.factor(laundryhang) # changing the format of the sting variable into a categorical factor\nplot(laundryhang) # a way to graph the non-numeric variable\n\n\n\n\n\n\n\n\n\n\n\nPrediction & Error\n\n\nEpistemic Beliefs"
  },
  {
    "objectID": "calstats/Lectures/1L_Welcome.html#the-linear-model",
    "href": "calstats/Lectures/1L_Welcome.html#the-linear-model",
    "title": "Lecture 1 | Welcome to Psych 101",
    "section": "The Linear Model",
    "text": "The Linear Model\n\nDefinition\n\n\nExamples"
  },
  {
    "objectID": "calstats/calstatsSP25.html",
    "href": "calstats/calstatsSP25.html",
    "title": "Psych 101 | Research and Data Analysis in Psychology (SP25)",
    "section": "",
    "text": "Hi class! This page will contain deadlines and links to course readings and lecture notes.\nPlease use bCourses to submit Quiz, Milestone, and Lab Assignments, and access Lecture Recordings, Datasets, and the link for our class discord page.\n\n\nNote : Chapters are from our course textbook : Why Statistics?\n\n\n\nDate\nReading & Quiz\nLecture Notes and R Script\nHomework Assigned\n\n\n1/24\nChapter 1\n1 | Introductions + Prof.¬†R Script\nLab 1 + Key\n\n\n1/31\nChapter 2\n2 | Data [pdf] + Prof.¬†R Script\nLab 2 + Key\n\n\n2/7\nChapter 3\n3 | Description + Prof.¬†R Script\nLab 3 + Key\n\n\n2/14\nChapter 4\n4 | Scales + Prof.¬†R Script\nLab 4 + Key\n\n\n2/21\nChapter 5\n5 | Scientific Methods\nLab 5\n\n\n2/28\n\nMini R Exam [85 minutes]\nProject Milestone: Intro Outline and Study Proposal\n\n\n3/7\n\n6A | Introduction to Linear Models\nMilestone 2 : Study Draft\n\n\n3/14\nChapter 6\n6B | Linear Model (Continuous IV)\nLab 6 + KEY\n\n\n3/21\nChapter 7\n7 | Linear Models (Categorical IV)\nLab 7\n\n\n3/28\n\nSPRING BREAK!\n\n\n\n4/4\nChapter 8\n8 | Experiments and Sample Bias\nMilestone 3 : Participants & Descriptive Stats\n\n\n4/11\nChapter 9\n9 | Inferential Statistics\nMilestone 4 : Linear Models\n\n\n4/18\nChapter 10\n10 | Multiple Regression & Review\nLab 8 [Practice Exam]\n\n\n4/25\n\nMega R Exam [85 minutes]\nProject Milestone : Draft\n\n\n5/2\nChapter 11 (Optional; on Interaction Effects)\n11 | Conclusion\n\n\n\n5/9\n\nRRR Week | Project Workshop\nFinal Project is Due 5/12 at 11:59 PM\n\n\n\n\n\n\n\nLecture | Fridays 2:00 - 5:00 PM in 100 Lewis\nProfessor : Arman Daniel Catterson, PhD [he / his / ÿßŸà ] üê±(catterson@berkeley.edu)\nOffice Hours | Wednesdays 4:00 - 5:00 PM on Zoom. E-mail / Discord to find another time to meet virtually, or say hi after class.\nTextbook : We will use Why Statistics? to learn the course material. I‚Äôll provide free required readings and videos each week. For additional support, I recommend Danielle Navarro‚Äôs FREE Learning Statistics with R, or copy of Field, Miles, & Field (2012). Discovering Statistics Using R. SAGE Publications (I‚Äôve uploaded sample chapters to bCourses).\n\n\n\n\nYour grade in the class will be based on the following components.\n\n\n\n5% Check-Ins (Lecture and Readings)\n10% Reading Quizzes\n\n\n20% Lab and Project Milestone Assignments\n29% R Exam (Mini = 5 points;¬† Mega (Chill) = 24 points)\n\n\n5% Discussion Section Attendance\n31% Final Project (Research Paper)\n\n\n\nLetter grades will be based on the following cutoffs :\n\n\n\n\n\n\n\n\n\nA+ is &gt; 96.5\nA is &gt; 92.5\nA- is &gt; 89.5\nB+ is &gt; 86.5\nB is &gt; 82.5\nB- is &gt; 79.5\nC + is &gt; 76.5\nC is &gt; 72.5\nC- is &gt; 69.5\nD is &gt; 59.5\nF is &lt; 59.5\nnote : an 89.499 is a B+\n\n\n\n\n\n\nYou must attend the section for which you are registered, led by your awesome GSI. Go GSIs!!\n\n\n\nName (& Pronouns)\nSection Times & Location\nE-mail\nOffice Hour Time & Location\n\n\nNathalie Fernandez\n\nnfe@berkeley.edu\n\n\n\nSarah Gao (she/her)\nSection 108: Wed 12-2, Evans 7\nSection 109: Wed 2-4, Evans 7\nsarah.gao@berkeley.edu\nTues 10:30-11:30 AM at BWW 3rd floor booths or book an appt here\n\n\nAkshat Gupta\n\nakshat.g@berkeley.edu\n\n\n\nAratrik Paul\nSection 101: Wednesday 12-2 (VLS 2062)\nSection 105: Monday 12-2 (Lewis 9)\naratrikpaul@berkeley.ed u\nWednesdays 10 AM - 12 PM ( Zoom)\n\n\nSahana Sridhar\nSection 107: Wed 10-2, Evans 7\nSection 110: Wed 4-6, Giannini 201\nsahanasri@berkeley.edu\nThursdays 11 AM -12 PM at BWW 3rd floor atrium or via Zoom\n\n\n\n\n\n\nReadings and Reading Quizzes¬† : Each week before lecture, you will read and watch videos from a textbook chapter designed to teach you the basic skills. At the end of each chapter, there will be a short quiz based on the content of this chapter that is due before lecture. This way, we can use time in lecture to review and discuss course concepts, and practice working on homework assignments together. The quizzes are multiple choice, untimed, open-note, and allow for multiple attempts.¬†\nCheck-Ins : I will ask you to complete short assessments in lecture and lab documents that consist of multiple choice and/or short-answer questions. These check-ins are graded for completion. It is your responsibility to make sure all check-ins are complete, and that you are logged in with your own UC Berkeley account (e.g., yourname@berkeley.edu). Check-ins must be completed by the beginning of the next lecture (e.g., when assignments are due) in order to be guaranteed a grade.¬†\nDiscussion Section : Discussion sections are a way to review course material, work on lab assignments in a space where you can get immediate help from other students and your GSI. Please attend the discussion section for which you are registered, though you may ask a GSI for permission to attend another section once in order to make-up for an absence or because of some other issue. GSIs will also be in attendance during lecture in order to help troubleshoot R problems you may have during live demonstrations.¬†\nLab Assignments : Every week, you will work through a ‚Äúlab‚Äù document that contains text and videos designed to help you understand key concepts in statistics, research methods, and R. You‚Äôll work on these at home, in lecture, and in your discussion sections. Each lab document will have a set of questions to answer - you should submit answers to these questions as a separate document to bCourses. Lab assignments will be graded for completion, accuracy, and effort and are due the week after they are assigned.\nFinal Project (Research Paper) : Students will conduct a study of their own, and will write up the results as if for publication. There are several milestones for this assignment due throughout the semester. For more details, see the final project description and rubric.\nR Exam : This exam will be scheduled ‚Äúlive‚Äù during our scheduled time. You should plan to take the exam during this time if possible. ¬†DSP students will receive extra time accommodations in accordance with their letters, and students with conflicts for the scheduled R Exam should submit a make-up request here at least TWO WEEKS before the exam date in order to be guaranteed a make-up time. For the exam, I‚Äôll give you a novel dataset and you‚Äôll use R to answer questions based on the dataset. The exam will be open-book and open note - you must work alone, but may use any resource available to you (including the Internet). A practice exam will be posted at least one week before the exam. Late submissions for the R exam will be penalized.¬†\nExtra Credit : There will be no extra credit offered in this course, even for students who ask kindly.¬†\nFinal Exam : There is no final exam in this class. Your final project will serve as the final assessment of your skills and training. Apologies to my former students who had a final exam in this class :|\n\n\n\nLate Work Policy. Assignments (quizzes, labs, and project milestones) are due before lecture. Late lab and project milestone assignments will be penalized 25% for any reason (even if they are 1-minute late), and 50% if turned in more than a week late (after the key has been posted). Students with extra time DSP accommodations should work out a plan to submit work within a week of the original deadline. Students may drop their lowest submitted lab and quiz assignment. Note : in the past, I‚Äôve tried a variety of more lenient late-work policies, and found that these policies often disincentivized students turning in work on time to their overall detriment in the class; so let‚Äôs try this and see how it goes.\nComputer Access. Access to a personal computer is strongly recommended. We will spend time each lecture working in R - a free and powerful statistics program that works with all operating systems and computers. Laptops are available for rent from the library (see Berkeley‚Äôs Technology Loan Program), or you can purchase one that will work for this class for as little as $150 (less than the price of most textbooks, which is not required for this class). Talk to me or a GSI if access to a laptop or computer is interfering with your ability to do well in the course.\nDiscussion Section Policy : In-person attendance is expected and required for discussion sections. Working with other students and your GSIs to review concepts, practice problems, and get feedback on your project is a critical part of the course (and the research process). Of course, the health and safety of you and your classmates and GSIs is most important - please do not attend your discussion section if you are worried you are sick, have tested positive for COVID-19 (or another contagious illness), or have closely interacted with someone who you believe is sick / tested positive for COVID-19 (or another contagious illness). Students may miss up to two discussion sections with no penalty. Please talk to your GSI / the professor if there is a longer-term illness or issue that will prevent you from regularly attending your discussion section and we will work out another arrangement.\nRegrades. Students may ask for a re-grade on exams and papers if they believe they lost points for something they should have earned. To request a regrade, talk to your GSI in office hours or set up a meeting. In the case that you and your GSI cannot resolve the regrade issue, the professor will step in and regrade the exam. For final project regrades, just e-mail the professor. When requesting a regrade, you consent to have your score increased or decreased if we find that you earned points you should not have earned.\nStudent Contact. It is your responsibility to regularly check e-mail for announcements. Please post questions about the class or R to our class discord page, so other students can see the question (and answer) and benefit from your question! If you have specific questions to e-mail me, please make sure to (a) search the syllabus and / or Google for the answer to your question and (b) contact your GSI about your question. I am always happy to answer any questions in office hours (either during the scheduled time or online by appointment), or before, during, or after lecture. I should respond to e-mail within 24 hours on weekdays (I do not check e-mail on weekends); please send a gentle reminder if I do not respond in this timeframe. Thanks!\nIncomplete Policy. I will only grant an incomplete to students who (1) have a significant life event that prevents them from completing the final project at the end of the semester and (2) are passing the class at the point they request the incomplete.\nAcademic Integrity. Do NOT cheat. Do NOT plagiarize. To copy text or ideas from another source (including your own submitted coursework) without appropriate reference is plagiarism. You may work on lab assignments with other students, but should be able to understand what you are doing. You may not work with other students on the R exam. Anyone caught cheating or plagiarizing will receive a zero on the assignment or test, and will be reported to the Office of Student Conduct. It is your responsibility to read the official Student Conduct Policy for more information about campus standards and policies regarding Academic Integrity.\nChat-GPT / AI Policy. If you use ChatGPT or other generative-AI software for any part of this class (writing, coding, troubleshooting, etc.), you must make this clear at the top of your assignment by writing : ‚ÄúI USED CHAT GPT TO SUPPORT THIS ASSIGNMENT‚Äù (in all-caps and red text), and include an appendix where you include screenshots of your specific queries and answers. Failure to do this will result in a zero of the assignment, and the professor submitting a report to the Academic Integrity office. Ask your GSI / professor if you have questions about this policy.\nRespect for Others. We all belong in our classroom, and I hope that everyone feels free and supported to express our identities when relevant. To achieve this goal, make sure that you are treating others with respect in this course. This means considering others‚Äô opinions and perspectives, making sure not to generalize to groups (or ask students to be representatives of their group), and being mindful of the words that you use. (Also remember that nothing is ever really deleted from the internet, so please be extra mindful of the words you use.) While I will be monitoring class posts and communication, feel free to reach out to me if you see another student engaging in disrespectful or threatening behavior. Of course, this extends to me as well - if you feel like there are parts of the course instruction, subject matter, or class environment that create barriers to your success or inclusion, please contact me via e-mail and trust that your voice will be heard and I will not punish you for offering constructive criticisms of my teaching. I‚Äôm very open to learning how to be a better teacher, and always learn a lot from my students.¬†\nSensitive Subjects. Our class will touch on important and potentially sensitive topics that are related to psychology and psychological processes, such as racism, depression, sex, and poverty. These topics can generate strong emotional responses in students for a variety of reasons - such as their own past histories with these topics or their reactions to the ways the information is presented or described. I will try to make sure to give you a heads up when we start to discuss these topics so you can be mindful of your reactions.\nStrategies for success. Do your readings, complete assignments on time, understand how the topics and information relate to what you‚Äôve learned (and are learning), and if you don‚Äôt understand something, please ask the question!¬†\nMost important. Please let me know as soon as there is anything going on in your life that you think may affect your ability to do as well as you would like in this class (e.g.¬†sickness, work, small children at home, threats to immigration status, etc.), and I will do my best to work with you on a plan to succeed in the class. If you‚Äôre not comfortable talking to me directly, you can talk to your GSI who can talk to me about your issue.\n\n\n\nUC Berkeley offers a variety of services to help support students facing difficulties in these historic times.\nStudents with Disabilities : Students who require support or adaptive equipment because of a specific disability ‚Äì or would like to be tested to see if they qualify for a learning, auditory, or visual disability ‚Äì can request these services through the Disabled Students Program (DSP) office. Please note that I can only provide accommodations authorized by the DSP office. If you have DSP status, make sure you submit your letter and feel free to talk to me or a GSI about any specific accommodations you need, or other ways we might be able to make the class more accessible. [260 C√©sar E. Ch√°vez Student Center| 510-642-0518 | https://dsp.berkeley.edu/]\nStudents with Mental Health Issues : If you feel a mental health issue (e.g., depression, anxiety) is negatively impacting your ability to succeed, I‚Äôd highly encourage you to seek out experts who can help. Berkeley has several free, confidential, and science-based programs designed to help students. [https://uhs.berkeley.edu/caps | Tang Center 3rd Floor | 510-642-9494 | After hours support line : 855-817-5667]\nUndocumented Students : I believe that all people deserve equal access to education. The UC Berkeley Undocumented Student Program has organized resources to support undocumented students, protect their rights, and offer potential financial aid | https://undocu.berkeley.edu/\nStudents from Low-Income Backgrounds : The Extended Opportunity Program (EOP) provides college support services for low-income students. Services include additional counseling, financial assistance (study-time parent grants, work-study assignments, and book vouchers), child-care opportunities, and assistance transferring to four-year colleges. [119 C√©sar E. Chavez Student Center | 510-642-7224 |¬† https://eop.berkeley.edu/]\nAdditional Course Resources. Some students feel that they need additional support in this class. If you find yourself struggling, please seek these resources as soon as possible.\nPrivate Tutoring : I‚Äôve put together a list of students who excelled in previous semesters of this class - you can contact these people directly to coordinate time, place, and payment (some tutors can work with groups, which will help reduce the cost for each student).\n\n\n\n\nTheData Peer Consulting program in the Division of Computing, Data Science, and Society. This program strives to help make data science accessible across the broader campus community, by aiming to help undergraduate students, graduate students, staff, and faculty with research project infrastructure or other projects and modules that incorporate data.¬†\nThe UC Berkeley D-Lab. Drop-in consulting where people may be able to help you with R specific problems.\nLibrarians are great. They can help you find research articles for your final project, think about how to organize or structure your literature review, and probably have other powers from a life of being surrounded by books."
  },
  {
    "objectID": "calstats/calstatsSP25.html#course-outline",
    "href": "calstats/calstatsSP25.html#course-outline",
    "title": "Psych 101 | Research and Data Analysis in Psychology (SP25)",
    "section": "",
    "text": "Note : Chapters are from our course textbook : Why Statistics?\n\n\n\nDate\nReading & Quiz\nLecture Notes and R Script\nHomework Assigned\n\n\n1/24\nChapter 1\n1 | Introductions + Prof.¬†R Script\nLab 1 + Key\n\n\n1/31\nChapter 2\n2 | Data [pdf] + Prof.¬†R Script\nLab 2 + Key\n\n\n2/7\nChapter 3\n3 | Description + Prof.¬†R Script\nLab 3 + Key\n\n\n2/14\nChapter 4\n4 | Scales + Prof.¬†R Script\nLab 4 + Key\n\n\n2/21\nChapter 5\n5 | Scientific Methods\nLab 5\n\n\n2/28\n\nMini R Exam [85 minutes]\nProject Milestone: Intro Outline and Study Proposal\n\n\n3/7\n\n6A | Introduction to Linear Models\nMilestone 2 : Study Draft\n\n\n3/14\nChapter 6\n6B | Linear Model (Continuous IV)\nLab 6 + KEY\n\n\n3/21\nChapter 7\n7 | Linear Models (Categorical IV)\nLab 7\n\n\n3/28\n\nSPRING BREAK!\n\n\n\n4/4\nChapter 8\n8 | Experiments and Sample Bias\nMilestone 3 : Participants & Descriptive Stats\n\n\n4/11\nChapter 9\n9 | Inferential Statistics\nMilestone 4 : Linear Models\n\n\n4/18\nChapter 10\n10 | Multiple Regression & Review\nLab 8 [Practice Exam]\n\n\n4/25\n\nMega R Exam [85 minutes]\nProject Milestone : Draft\n\n\n5/2\nChapter 11 (Optional; on Interaction Effects)\n11 | Conclusion\n\n\n\n5/9\n\nRRR Week | Project Workshop\nFinal Project is Due 5/12 at 11:59 PM"
  },
  {
    "objectID": "calstats/Lectures/2L_WorkingWithData.html",
    "href": "calstats/Lectures/2L_WorkingWithData.html",
    "title": "Class 2 | Working With Data",
    "section": "",
    "text": "Welcome Back! Access this Document Here : https://catterson.github.io/calstats/calstatsSP25.html\nPLEASE COMPLETE THIS CHECK-IN : tinyurl.com/againmodels"
  },
  {
    "objectID": "calstats/Lectures/2L_WorkingWithData.html#a-picture-is-worth-1000-words",
    "href": "calstats/Lectures/2L_WorkingWithData.html#a-picture-is-worth-1000-words",
    "title": "Class 2 | Working With Data",
    "section": "A Picture Is Worth 1000 Words",
    "text": "A Picture Is Worth 1000 Words\n\nFlorence Nightingale :\n\nDid you learn about Florence Nightingale in other classes?\nWhat did you remember learning about Florence Nightingale in other classes?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiscussion.\nLook at the graph below, and use it to answer the following questions.\n\nIce Breaker : What‚Äôs the worst time you‚Äôve been sick? What‚Äôs your best way of trying to feel better / self-care when sick?\nLook at the graph below : What‚Äôs going on in this graph / who cares / how can we use this knowledge??\n\nshowing death from diseases, other causes, wounds.\ntime is split into twelve months (but also maybe there‚Äôs something going on with season??)\nhard to accurately represent deaths by triangles.\n\nIt‚Äôs all linear models : What are the variables in this graph? How would you organize these as a linear model?\n\nmortality (among soldiers) ~ cause of death + time + ERROR\n\n\n\n\n\n\n\n\n\nKEY IDEA : Visuals Matter\n\nWhat is better about the way the barchart1 visualizes these data?\nWhat is worse (or still confusing)?"
  },
  {
    "objectID": "calstats/Lectures/2L_WorkingWithData.html#learning-from-histograms-no-statistics-terms",
    "href": "calstats/Lectures/2L_WorkingWithData.html#learning-from-histograms-no-statistics-terms",
    "title": "Class 2 | Working With Data",
    "section": "Learning from Histograms (No Statistics Terms!)",
    "text": "Learning from Histograms (No Statistics Terms!)\nBelow are some data that I graphed2. Take 1-2 minutes and SILENTLY (on your own) think about what you learn about the variable from this graph. Avoid FANCY STATS LANGUAGE - just explain the main ideas without those labels for now.\n\nhist(d$SELFES, col = 'black', bor = 'white', \n     main = \"Histogram of Self-Esteem\", \n     xlab = \"Self-Esteem Score\", breaks = 15)\n\n\n\n\n\n\n\n\nThings We Learned From the Graph\n\nself-esteem scores are between 1 and 4.\nmore people have really high self-esteem than really low self-esteem\nthere‚Äôs a lot of people in the data\nthe variable is self-esteem.\npeople have lots of different self-esteems\na lot of people are in the middle [2.5???]; it looks like a bell!\n\n\nThings We Cannot Learn From the Graph\n\n‚Äúmore people have really high self-esteem than really low self-esteem‚Äù\n\nself-esteem ~ age + error (and this dataset is maybe mostly older people?)\n‚Äúself-presentation‚Äù (people are presenting a desirable version of the self)\n\nother variables that might be relevant to self-esteem!\n\npersonality types\nattitudes\nself-perceptions of ego :P\n\nwho are all these people in this study???"
  },
  {
    "objectID": "calstats/Lectures/2L_WorkingWithData.html#code-book-the-covid-19-behavior-dataset",
    "href": "calstats/Lectures/2L_WorkingWithData.html#code-book-the-covid-19-behavior-dataset",
    "title": "Class 2 | Working With Data",
    "section": "CODE BOOK : The Covid-19 Behavior Dataset",
    "text": "CODE BOOK : The Covid-19 Behavior Dataset\n\nLook over the codebook (below).\n\nWhat is one variable from the dataset that is interesting to you (if any)?\nIs this categorical or numeric data?\nWhat predictions do you have about this variable?\nHow might you use this variable in a linear model (as a DV or as a IV?)\n\nLoading Data Issues :\n\nrename this to something short!\nposit.cloud : clicking on the name to load (vs.¬†the ‚ÄúImport Dataset‚Äù)\n\n\nLink to Data (also on bCourses)"
  },
  {
    "objectID": "calstats/Lectures/2L_WorkingWithData.html#in-r-the-covid-19-behavior-dataset",
    "href": "calstats/Lectures/2L_WorkingWithData.html#in-r-the-covid-19-behavior-dataset",
    "title": "Class 2 | Working With Data",
    "section": "IN R : The Covid-19 Behavior Dataset",
    "text": "IN R : The Covid-19 Behavior Dataset\nThings we will do.\n\nOpen up Lab 2\nCreate an RScript\nLoad the Covid-19 Behavior Dataset (.csv file) and the CODE BOOK (.pdf)\n\nthe CODEBOOK explains what the variables measured\nthe .csv data file contains the data.\nMake sure the data loaded correctly into R\n\nGraph some variables and learn about the individuals from this graph\n\nnumeric data\ncategorical data\n\nSave your work for Lab 2, Questions 1 and 2 and 3. Yeah!"
  },
  {
    "objectID": "calstats/Lectures/2L_WorkingWithData.html#creating-a-mini-class-dataset",
    "href": "calstats/Lectures/2L_WorkingWithData.html#creating-a-mini-class-dataset",
    "title": "Class 2 | Working With Data",
    "section": "Creating a Mini Class Dataset",
    "text": "Creating a Mini Class Dataset\n\nHere‚Äôs a list of the variables that y‚Äôall thought would be interesting to measure in the class (and I approved) from the check-in.\n\nheight [numeric]\n\nas categorical : group people in ‚Äúbuckets‚Äù\n\n# of languages you speak (polyglots) [numeric : discreet values\n\nfluency in language ‚Äî&gt; more continuous\nare you multilingual : Y / N = categorical.\n\nr u an intended psych major\nwhere are you from?\nhat / no hat\n\nas categorical : Y / N\nas numeric : # of hats\nas continuous : # of seconds you‚Äôve worn a hat on your head.\n\nage\nSPORT PLAYER\nbirth month / date\nglasses or not\nalcohol consumption\nscreen time\nhappiness\n\ncategorical : R U HAPPY Y/N\nscale : On a scale from 0 to 100‚Ä¶.\n\ninner monologue or not\nself-consciousness / self-awareness\n\nDISCUSS : which of these variables would be best measured with numbers (e.g., a scale from 0 to 10)? Which variables would best be measured with categories? (what would the factor and levels be?)\n\nNumeric Data Variables :\nCategorical Data :"
  },
  {
    "objectID": "calstats/Lectures/2L_WorkingWithData.html#operationalization-construct-and-measurement-error",
    "href": "calstats/Lectures/2L_WorkingWithData.html#operationalization-construct-and-measurement-error",
    "title": "Class 2 | Working With Data",
    "section": "Operationalization, Construct, and Measurement Error",
    "text": "Operationalization, Construct, and Measurement Error\n\noperationalization : how researchers define the variable(s) they will study; this is a process; often the focus of a researcher‚Äôs question in the scientific method.\nconstruct : some operationalized psychological phenomenon of interest. some examples below :\n\nvoxel : three dimensional area of brain activation\nself-esteem : how a person feels about themselves\nsecure attachment style : how much a person seeks out and trusts a relationship partner.\n\nmeasurement error : when there is a lack of validity in our measures. The more error in our measures, the more error there will be in our predictions (‚Äúgarbage in ‚Üí garbage out‚Äù).\n\nKEY IDEA : the way a variable is measured is CRITICAL.\n\nThe News Article : What comes to mind when you think of a ‚ÄúCognitive Test‚Äù?\n\n\n\n\n\n\n\nThe Scientific Operationalization of this ‚ÄúCognitive Test‚Äù"
  },
  {
    "objectID": "calstats/Lectures/2L_WorkingWithData.html#activity-counting-interruptions",
    "href": "calstats/Lectures/2L_WorkingWithData.html#activity-counting-interruptions",
    "title": "Class 2 | Working With Data",
    "section": "ACTIVITY : Counting Interruptions",
    "text": "ACTIVITY : Counting Interruptions\nCheck-in :tinyurl.com/dudesinterrupting\n\nCount the number of interruptions in the video (which professor will play below).¬†\nSubmit your answer, then wait for the letter of the day.\n\nVideo\nDISCUSSION TOPICS :\n\nHow do we OPERATIONALIZE an INTERRUPTION?\n\nwhen the guest on the right STOPS his sentence and has to restart.\nthe number of times the guest on the right is INTERRUPTED by the host on the left.\n\nWhat PREDICTIONS can we make about counting interruptions a second time?\n\nthe number of interruptions at T2 will go down, since we narrowed it to a) stopping one person and b) just counting one person.\nthe number of interruptions at T2 will be more similar to each other."
  },
  {
    "objectID": "calstats/Lectures/2L_WorkingWithData.html#footnotes",
    "href": "calstats/Lectures/2L_WorkingWithData.html#footnotes",
    "title": "Class 2 | Working With Data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://www.r-bloggers.com/2013/02/extracting-the-epidemic-model-going-beyond-florence-nightingale-part-ii/‚Ü©Ô∏é\nYou will work with these later in the semester once we review how to create a likert scale (that combines 10 questions into one variable).‚Ü©Ô∏é"
  },
  {
    "objectID": "calstats/lectures/1L_WhyStats.html",
    "href": "calstats/lectures/1L_WhyStats.html",
    "title": "Lecture 1 | Welcome to Psych 101",
    "section": "",
    "text": "access these lecture notes on bCourses!\nclick on this link to check-in (or visit : tinyurl.com/first101class)\n\n\n\n\n\n\n\n\n\nSection Swap : post on bCourses to find someone to swap with.\nWaitlisted Students : Thanks for your patience! Nothing I can do :(\nJoin the Class Discord : link on bCourses\nNext Week :\n\nAttend Discussion Section\nComplete Lab 1 (will start in lecture!)\nRead Chapter 2\nComplete Quiz 2\n\n\n\n\n\n\n2:10 - 2:20 | Check-In & Announcements\n2:20 - 3:10 | RECAP : Science as Prediction\n3:10 - 3:20 | Break #1\n3:20 - 3:50 | Positivism and Linear Models\n3:50 - 3:55 | Break #2\n3:55 - 4:30 | In R : Defining Variables\n4:30 - 5:00 | So you‚Äôre interested in being a researcher / going to graduate school?"
  },
  {
    "objectID": "calstats/lectures/1L_WhyStats.html#welcome-and-check-in",
    "href": "calstats/lectures/1L_WhyStats.html#welcome-and-check-in",
    "title": "Lecture 1 | Welcome to Psych 101",
    "section": "",
    "text": "access these lecture notes on bCourses!\nclick on this link to check-in (or visit : tinyurl.com/first101class)"
  },
  {
    "objectID": "calstats/lectures/1L_WhyStats.html#course-announcements",
    "href": "calstats/lectures/1L_WhyStats.html#course-announcements",
    "title": "Lecture 1 | Welcome to Psych 101",
    "section": "",
    "text": "Section Swap : post on bCourses to find someone to swap with.\nWaitlisted Students : Thanks for your patience! Nothing I can do :(\nJoin the Class Discord : link on bCourses\nNext Week :\n\nAttend Discussion Section\nComplete Lab 1 (will start in lecture!)\nRead Chapter 2\nComplete Quiz 2"
  },
  {
    "objectID": "calstats/lectures/1L_WhyStats.html#agenda",
    "href": "calstats/lectures/1L_WhyStats.html#agenda",
    "title": "Lecture 1 | Welcome to Psych 101",
    "section": "",
    "text": "2:10 - 2:20 | Check-In & Announcements\n2:20 - 3:10 | RECAP : Science as Prediction\n3:10 - 3:20 | Break #1\n3:20 - 3:50 | Positivism and Linear Models\n3:50 - 3:55 | Break #2\n3:55 - 4:30 | In R : Defining Variables\n4:30 - 5:00 | So you‚Äôre interested in being a researcher / going to graduate school?"
  },
  {
    "objectID": "calstats/lectures/1L_WhyStats.html#activity-variables-and-variation-in-the-room",
    "href": "calstats/lectures/1L_WhyStats.html#activity-variables-and-variation-in-the-room",
    "title": "Lecture 1 | Welcome to Psych 101",
    "section": "Activity : Variables and Variation in the Room",
    "text": "Activity : Variables and Variation in the Room\nClass Activity. Let‚Äôs create a list of variables that we observe in this classroom.\n\nhair style\nhair color\nethnicity\neye color\nbody size\ndevices\ngender\nidentity\n\naffect : feeling of affinity for fitting in w/ your culture.\nbehavior : how you express it\ncognition : self-perception (do you think about this identity / perceive it in others?)\n\nglasses\nhats vs.¬†no hats\n\nKey Terms. From the readings.\n\nAffect, Behavior, Cognition\nBetween vs.¬†Within-Person Variation\n\n[7 Minutes] Answer the following questions with your buddy.\nFind a buddy in the class! (There‚Äôs a discord thread if you prefer to communicate with someone online.)\n\nIf you could have dinner with anyone in the world (living or dead) who would it be?\nWhy are you a psychology major? What interests you about people (or non-human animals)?\nHow would you label this interest as a variable?\n\nAre you interested in the between-person or within-person version of this variable?\nAre you interested in the Affective, Behavioral, or Cognitive aspect of this variable?\n\n\nStudent Examples\n\nHamza : motivation for goal changes over time [within person - starts as affect ‚Äì&gt; behavior]\nMax : emotional effect of languages (how much the same word in a western language influences someone compared to that word in an eastern language) [between person - comparing people who speak one language to people who speak another.]\nCamille : Moral regret (you did something bad and feel bad about it) and how that changes over time."
  },
  {
    "objectID": "calstats/lectures/1L_WhyStats.html#thinking-about-programming-free-association-activity",
    "href": "calstats/lectures/1L_WhyStats.html#thinking-about-programming-free-association-activity",
    "title": "Lecture 1 | Welcome to Psych 101",
    "section": "Thinking about Programming (Free Association Activity)",
    "text": "Thinking about Programming (Free Association Activity)\n\nClose your eyes\nTake a deep breath (inhale / exhale)\nVisualize an image based on the word that you hear me say.\nWhat do you observe?"
  },
  {
    "objectID": "calstats/lectures/1L_WhyStats.html#r-is-your-friend",
    "href": "calstats/lectures/1L_WhyStats.html#r-is-your-friend",
    "title": "Lecture 1 | Welcome to Psych 101",
    "section": "R is Your Friend",
    "text": "R is Your Friend\n\nThe Console\nThe console is where R does its work.\n\nACTIVITY : Look at the image below. What do you see? What makes sense / what seems confusing?\n\n\n\n\n\n\nSome Additional Notes on R Studio and the Source File (.R)\nIn this class, we‚Äôll be using RStudio. RStudio is an IDE (Integrated Development Environment) that includes the console along with other useful windows and tools.\n\nThe Console is at the bottom left of the IDE. Hi console!\nThe R script is at the top left of the IDE, and is a document that you use to write (and organize) code. You will want to do most of your work in the R script, and feel an appropriate level of anxiety when you notice that your Rscript is unsaved (as indicated by the red text and *).\nThe Environment is at the top right of the IDE, and shows you all of the ‚Äúobjects‚Äù that you have defined in R.\nThe File Window is at the bottom right of the IDE, and shows you the files. Note that there are tabs here for Plots (where graphs will pop up), Packages (things you can download to give R extra features), a Help viewer (sometimes very useful!).\n\n\n\n\nACTIVTY : open up RStudio\n\nType some math into an Rscript, and send it to the console. Yeah, you are programming!\nDefine two variables in R - one numeric and one string variable (these can be unrelated to your project topic!) Make sure to collect at least ten data points for each variable, and show that you successfully defined the variable in R by ‚Äúprinting‚Äù it in R. Yeah, you‚Äôre programming!\nCopy/paste (or screenshot) your code and output from the question above to a document to answer Lab 1, Question 1."
  },
  {
    "objectID": "calstats/lectures/1L_WhyStats.html#in-r-variables-and-variation",
    "href": "calstats/lectures/1L_WhyStats.html#in-r-variables-and-variation",
    "title": "Lecture 1 | Welcome to Psych 101",
    "section": "In R : Variables and Variation",
    "text": "In R : Variables and Variation\n\nNumeric Variables in R\n\n\n\n\n\n\n\nCode\nDescription\n\n\n\n\nvariable &lt;- c(#, #, #, #, etc.)\n\n\n\ntired &lt;- c(1,2,3,4)\nvariable = an object that you will define in R\n&lt;- = ‚Äúassign‚Äù; tells R to save whatever comes on the right to whatever object is on the left.\nc = combine : tells R to combine whatever happens in the parentheses\n() = parentheses to group related terms\n# = what you store in the variable; each item should be separated by a comma and space.\n\n\nhist(dat$variable)\nFor continuous variables : draws a histogram.\n\n\n\n\n\nExample : Creating Numeric Variables\n\ncounting &lt;- c(1,2,3,4,5) # the numbers one through five\nprint(counting) # one way to \"print\" the variable\n\n[1] 1 2 3 4 5\n\ncounting # another way to \"print\" the variable\n\n[1] 1 2 3 4 5\n\nhist(counting) # a way to graph the variable (a histogram)\n\n\n\n\n\n\n\n\n\n\nString Variables\n\n\n\n\n\n\n\nvariable &lt;- c(‚Äúname1‚Äù, ‚Äúname2‚Äù, ‚Äúname1‚Äù, etc.)\n\nemotion &lt;- c(‚Äúsad‚Äù, ‚Äúhappy‚Äù, ‚Äúsad‚Äù)\nvariable = an object that you will define in R\n&lt;- = ‚Äúassign‚Äù; tells R to save whatever comes on the right to whatever object is on the left.\nc = combine : tells R to combine whatever happens in the parentheses\n() = parentheses to group related terms\n# = what you store in the variable; each item should be separated by a comma and space.\n\n\nas.factor(variable)\n\nas.factor(emotion)\nas.factor() # converts a string variable into a categorical factor\n\n\nvariable &lt;- as.factor(variable)\n# ‚Äúsaves‚Äù this conversion as the original variable\n\n\nplot(dat$variable)\nFor categorical variables : draws a barplot. For continuous variables :¬† illustrates values of the variable (y-axis) as a function of their index (x-axis).\n\n\n\n\n\nExample : Creating Non-Numeric Variables\nThe data below describe the categories of family laundry that was hanging in my apartment to dry.\n\nlaundryhang &lt;- c(\"shirt\", \"shirt\", \"leggings\", \"leggings\", \"shirt\", \n             \"shirt\", \"leggings\", \"pants\", \"sweater\", \"sweater\") # defining a string variable\nprint(laundryhang)\n\n [1] \"shirt\"    \"shirt\"    \"leggings\" \"leggings\" \"shirt\"    \"shirt\"   \n [7] \"leggings\" \"pants\"    \"sweater\"  \"sweater\" \n\nlaundryhang # another way to \"print\" the variable\n\n [1] \"shirt\"    \"shirt\"    \"leggings\" \"leggings\" \"shirt\"    \"shirt\"   \n [7] \"leggings\" \"pants\"    \"sweater\"  \"sweater\" \n\nlaundryhang &lt;- as.factor(laundryhang) # changing the format of the sting variable into a categorical factor\nplot(laundryhang) # a way to graph the non-numeric variable"
  },
  {
    "objectID": "calstats/lectures/1L_WhyStats.html#break-time-meet-back-at-325",
    "href": "calstats/lectures/1L_WhyStats.html#break-time-meet-back-at-325",
    "title": "Lecture 1 | Welcome to Psych 101",
    "section": "Break Time: Meet Back at 3:25",
    "text": "Break Time: Meet Back at 3:25"
  },
  {
    "objectID": "calstats/lectures/1L_WhyStats.html#prediction-power",
    "href": "calstats/lectures/1L_WhyStats.html#prediction-power",
    "title": "Lecture 1 | Welcome to Psych 101",
    "section": "Prediction & Power",
    "text": "Prediction & Power\n\nPredictions in Real Life\n\nWill it rain tonight?\n\nKnowledge (what information did you use to make the prediction)?\nPower (ways your predictions influence future behaviors)\n\nProfessor predicted that attendance would be HIGH today, but will dip later in the semester :(\n\nKnowledge (what information did you use to make the prediction)?\n\nfirst day of the semester; students are STOKED and MINIMALLY STRESSED.\npattern seen in almost every past semester.\nFriday afternoons are basiscally the weekend.\n\nPower (ways your predictions influence future behaviors)\n\nprepared; made sure to arrive on time; tucked in shirt.\nremind students to attend with weekly announcements.\ncreate a positive classroom environment where students feel supported and like attendance is helpful.\ncreated an example that serves as meta-commentary on the importance of attendance.\n\nWas Professor Valid? TBD! &lt;3\n\nWork on Lab 1, Question 1. [In Lecture] What‚Äôs a prediction about people that you made today? What information did you use to make this prediction? How did (or could) you use this prediction to influence outcomes? Were you valid in your predictions?\n\nJESSICA :\n\nprediction : this professor is awesome!\nknowledge : other people told me the class was managable and friendly.\npower : felt excited to enter the class\nvalid : maybe? let‚Äôs see.\n\nTRISHA :\n\nprediction : thought people would come out of apartments when fire alarm went off in building; there was an emergency.\nknowledge : we are taught to do this\npower : left my apartment (without thinking too much about grabbing my keys)\nvalid : no, I was the only one outside.\nmodel : behavior in response to fire alarm ~ training + motivation not to die + social norms + error\n\n\n\n\n\nScientific Predictions\nPsychological scientists seek to better understand variation, in order to help make valid predictions in ways that help exert power over our environments.\n\n\n\nTopic\nOther Questions We Might Ask?\n\n\n\n\nVideo | |\n\n\n\n1 | Are there differences between how long people actually use social media and how long people want to use social media? | | | What dopamine reactions do people get after using social media? | | How do reactions to visual cues in the environemnt differ when people are on their phones? | | What factors keep people off social media long-term? | | Do the engineers who create infinite scrolling feel guilty.\n\n\n\n2 | |"
  },
  {
    "objectID": "calstats/lectures/1L_WhyStats.html#the-linear-model",
    "href": "calstats/lectures/1L_WhyStats.html#the-linear-model",
    "title": "Lecture 1 | Welcome to Psych 101",
    "section": "The Linear Model",
    "text": "The Linear Model\n\nDefinition\nStatistical Model : DV ~ IV1 + IV2 + ‚Ä¶ + IVk + error\n\nDV = dependent variable = the variable you want to predict\nIV = independent variable(s) = the variable(s) you think will predict the DV\n\nk = any number = there can be MANY IVs\nany variable can be an IV or DV - it‚Äôs up to the researcher to choose\n\n~ = a squiggly line / tilde = our model is uncertain (not equal)\nerror = other factors that are not part of your model that would also explain the DV\nWe say: ‚Äúthe DV is a function of‚Ä¶‚Äù; ‚Äúthe DV depends on the IV(s)‚Äù\n\n\n\nExamples\n\nrain ~ number of umbrellas other people have + personal experience + weather app + clouds + sun + smell + pressure + error\nclass quality ~ communication + study habits + lecture attendance + quality of lecturing + mood / vibes + professor and student adaptability to the class + workload + participation + time management + hunger + sleep + life shit + 3 hours lectures on a friday afternoon + respect + right amount of material that is relevant and interetsting + study buddy + LEARNING somethign useful and relevant + ERROR\n\n\n\nAnother Example\nKEY IDEA : Linear Models Help Make and Quantify Prediction\n\nwhat information (IV) is related to the DV (predict)\nwhich IV allows us to make the best predictions (effect size)\nthe amount of error in your prediction (error can come from our measures, our models, and maybe is just inherent to science?)"
  },
  {
    "objectID": "calstats/lectures/1L_WhyStats.html#work-on-lab-1-predictions-research-questions.",
    "href": "calstats/lectures/1L_WhyStats.html#work-on-lab-1-predictions-research-questions.",
    "title": "Lecture 1 | Welcome to Psych 101",
    "section": "Work on Lab 1 : Predictions & Research Questions.",
    "text": "Work on Lab 1 : Predictions & Research Questions.\n\nOn Your Own\nDuring our break, think a little bit about what research questions you might want to address for the project in this class. Below are some ideas to help you get started thinking of a research question if you are feeling stuck!\n\nIs there a real-world issue that you care about? What variables make up this questions?\nWhat is something about people (your friends, parents, classmates) that you think is interesting or confusing? What variables are the focus of this interest or question?\nWhat‚Äôs a future career you might want to pursue with your psychology degree? What‚Äôs a variable that is related to this career? What questions might you ask abut this variable?\n\nQuestion 3 (In Lecture / On Your Own). Get started on the final project by thinking through a research question you might be interested in studying as a psychology researcher. (Totally fine to change this, but great to start focusing on a question.)\n\nWhat is your question? Why do you care about this question (and / or why does this question matter to others)? How interested in this question are you on a scale from 0 (just doing to get credit for this question) to 10 (this is what motivates you to wake up each day and you will answer this question with the energy and passion of 1000 suns)?\nHow do your past experiences and background inform this question?\nWhat is the variable that is the focus of this question? How does this variable relate to affect, behavior, and cognition? Which aspect of this variable are you most interested in focusing on for your project?\nWhat is the between-person form of variation for this variable? What is the within-person form of variation for this variable? Note: for the final project, I strongly recommend focusing on a between-person variation version of the variable for the final project.\nDo you have ideas about what might predict or explain this variable (the answer to your question)? How would you write this out as a linear model?\n\n\n\nStudent Examples"
  },
  {
    "objectID": "calstats/lectures/1L_WhyStats.html#getting-research-experience-as-an-ra",
    "href": "calstats/lectures/1L_WhyStats.html#getting-research-experience-as-an-ra",
    "title": "Lecture 1 | Welcome to Psych 101",
    "section": "Getting Research Experience as an RA",
    "text": "Getting Research Experience as an RA\n\nRA = Research Assistant\n\nMostly Unpaid Experiences\nSome paid experiences exist!\n\nFrom the berkeley website‚Ä¶\nBusine$$ $chool\nStanford [maybe paid]\nA list a student sent me that they found.\n‚ÄúCold calling‚Äù labs who are doing work you think is cool.\nChat with your TAs / Professors\n\n\nAs an RA :\n\nwork with data : transcribing data; behavioral coding data; recruiting and participants to collect data; setting up psychophysiological recordings; cleaning data; etc.\nother opportunities to gain skills you can demonstrate :\n\nreading & discussing papers\nworking with IRB (institutional review board - an ethics thing)\nanalyzing data ‚Üí presenting research at a conference (poster) or submitting a paper for publication [your golden ticket]\ngeneral mentorship (how to apply to grad school; where to apply; who to talk to & e-mail; etc.)\nNOTE : this work and these skills apply to other work outside of research applications [time management; coordinating schedules; juggling responsibilities; etc.]\n\nget a sense of whether this [work or lab] is for you?\n\ndo you enjoy the work? are you going to look forward to showing up and doing the work / fulfilling the commitment?¬†\nare you working with a horrible monster?\n\nnot responsive\ninconsistent work / no plan for your work\nkind of a bully (emotionally abusive ‚Üí stealing your work)¬†\n\nor are you working with someone who is super cool and a positive influence on mentoring young minds!?!?! [YES!!!!]"
  },
  {
    "objectID": "calstats/lectures/1L_WhyStats.html#applying-to-graduate-school",
    "href": "calstats/lectures/1L_WhyStats.html#applying-to-graduate-school",
    "title": "Lecture 1 | Welcome to Psych 101",
    "section": "Applying to Graduate School",
    "text": "Applying to Graduate School\n\nYou are applying to work on research with a specific professor(s) at a school.\n\nShould have a sense of the topic you want to pursue.\nGood to have a narrative about how your past work and studies have prepared you for this topic / demonstrate an enduring interest in the topic.\n\nIndependent Thesis / Research Project :\n\nan official honors‚Äô thesis\nundergraduate research project (e.g., SURF; Psych 101!)\nyour own independent study / advanced work you did as an RA\n\nPersonal Statement : Experiences with Research You Can Write About\n\nI‚Äôm fascinated by people‚Ä¶Over the last year, I worked on an independent research study to better understand‚Ä¶.\nWorking as an RA; your research project; attending / presenting at a conference; etc.\n\n3-4 Letters of Recommendation : folks who can speak personally to your ability to do research.\nClinical Students : some kind of clinical internship / experience üòü\nTalk to people who are doing the thing you want to be doing about their journey\n\n\nThe Academic Job Market\nSome Data [Source]\n\n\n\nPhDs get jobs?\n\n\n\nbut not in academia‚Ä¶\n\n\n\n$$$$$$$$"
  },
  {
    "objectID": "calstats/lectures/1L_WhyStats.html#footnotes",
    "href": "calstats/lectures/1L_WhyStats.html#footnotes",
    "title": "Lecture 1 | Welcome to Psych 101",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHere‚Äôs a link to the article where this headline comes from. These data are a little dated, and I couldn‚Äôt immediately find more recent data - my guess is Meta does not really want to advertise that people are using the product more and more. However, in reports to investors reports consistent growth in metrics like ‚Äúad impressions‚Äù and ‚Äúdaily active users‚Äù. Let me know if you find other sources to show how technology companies are capturing more and more of our attention!‚Ü©Ô∏é\nOscar Grant, Trayvon Martin, Philando Castile, Eric Garner, George Floyd, Tamir Rice, Breonna Taylor, Ahmaud Aubrey, Jacob Blake. Here‚Äôs a more comprehensive list, and here‚Äôs a summary article on policing and race.\n‚Ü©Ô∏é"
  },
  {
    "objectID": "calstats/lectures/2L_WorkingWithData.html",
    "href": "calstats/lectures/2L_WorkingWithData.html",
    "title": "Class 2 | Working With Data",
    "section": "",
    "text": "Welcome Back! Access this Document Here : https://catterson.github.io/calstats/calstatsSP25.html\nPLEASE COMPLETE THIS CHECK-IN : tinyurl.com/againmodels"
  },
  {
    "objectID": "calstats/lectures/2L_WorkingWithData.html#a-picture-is-worth-1000-words",
    "href": "calstats/lectures/2L_WorkingWithData.html#a-picture-is-worth-1000-words",
    "title": "Class 2 | Working With Data",
    "section": "A Picture Is Worth 1000 Words",
    "text": "A Picture Is Worth 1000 Words\n\nFlorence Nightingale :\n\nDid you learn about Florence Nightingale in other classes?\nWhat did you remember learning about Florence Nightingale in other classes?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiscussion.\nLook at the graph below, and use it to answer the following questions.\n\nIce Breaker : What‚Äôs the worst time you‚Äôve been sick? What‚Äôs your best way of trying to feel better / self-care when sick?\nLook at the graph below : What‚Äôs going on in this graph / who cares / how can we use this knowledge??\nIt‚Äôs all linear models : What are the variables in this graph? How would you organize these as a linear model?\n\n\n\n\n\n\n\n\nKEY IDEA : Visuals Matter\n\nWhat is better about the way the barchart1 visualizes these data?\nWhat is worse (or still confusing)?"
  },
  {
    "objectID": "calstats/lectures/2L_WorkingWithData.html#learning-from-histograms-no-statistics-terms",
    "href": "calstats/lectures/2L_WorkingWithData.html#learning-from-histograms-no-statistics-terms",
    "title": "Class 2 | Working With Data",
    "section": "Learning from Histograms (No Statistics Terms!)",
    "text": "Learning from Histograms (No Statistics Terms!)\nBelow are some data that I graphed2. Take 1-2 minutes and SILENTLY (on your own) think about what you learn about the variable from this graph. Avoid FANCY STATS LANGUAGE - just explain the main ideas without those labels for now.\n\nhist(d$SELFES, col = 'black', bor = 'white', \n     main = \"Histogram of Self-Esteem\", \n     xlab = \"Self-Esteem Score\", breaks = 15)\n\n\n\n\n\n\n\n\nThings We Learned From the Graph\n\ngo here.\n\nThings We Cannot Learn From the Graph\n\ngo here."
  },
  {
    "objectID": "calstats/lectures/2L_WorkingWithData.html#code-book-the-covid-19-behavior-dataset",
    "href": "calstats/lectures/2L_WorkingWithData.html#code-book-the-covid-19-behavior-dataset",
    "title": "Class 2 | Working With Data",
    "section": "CODE BOOK : The Covid-19 Behavior Dataset",
    "text": "CODE BOOK : The Covid-19 Behavior Dataset\n\nLook over the codebook (below).\n\nWhat is one variable from the dataset that is interesting to you (if any)?\nIs this categorical or numeric data?\nWhat predictions do you have about this variable?\nHow might you use this variable in a linear model (as a DV or as a IV?)\n\nLoading Data Issues :\n\nrename this to something short!\nposit.cloud : clicking on the name to load (vs.¬†the ‚ÄúImport Dataset‚Äù)\n\n\nLink to Data (also on bCourses)"
  },
  {
    "objectID": "calstats/lectures/2L_WorkingWithData.html#in-r-the-covid-19-behavior-dataset",
    "href": "calstats/lectures/2L_WorkingWithData.html#in-r-the-covid-19-behavior-dataset",
    "title": "Class 2 | Working With Data",
    "section": "IN R : The Covid-19 Behavior Dataset",
    "text": "IN R : The Covid-19 Behavior Dataset\nThings we will do.\n\nOpen up Lab 2\nCreate an RScript\nLoad the Covid-19 Behavior Dataset (.csv file) and the CODE BOOK (.pdf)\n\nthe CODEBOOK explains what the variables measured\nthe .csv data file contains the data.\nMake sure the data loaded correctly into R\n\nGraph some variables and learn about the individuals from this graph\n\nnumeric data\ncategorical data\n\nSave your work for Lab 2, Questions 1 and 2 and 3. Yeah!"
  },
  {
    "objectID": "calstats/lectures/2L_WorkingWithData.html#creating-a-mini-class-dataset",
    "href": "calstats/lectures/2L_WorkingWithData.html#creating-a-mini-class-dataset",
    "title": "Class 2 | Working With Data",
    "section": "Creating a Mini Class Dataset",
    "text": "Creating a Mini Class Dataset\n\nHere‚Äôs a list of the variables that y‚Äôall thought would be interesting to measure in the class (and I approved) from the check-in.\n\nList\nGoes\nHere\n\nDISCUSS : which of these variables would be best measured with numbers (e.g., a scale from 0 to 10)? Which variables would best be measured with categories? (what would the factor and levels be?)\n\nNumeric Data Variables :\nCategorical Data :"
  },
  {
    "objectID": "calstats/lectures/2L_WorkingWithData.html#operationalization-construct-and-measurement-error",
    "href": "calstats/lectures/2L_WorkingWithData.html#operationalization-construct-and-measurement-error",
    "title": "Class 2 | Working With Data",
    "section": "Operationalization, Construct, and Measurement Error",
    "text": "Operationalization, Construct, and Measurement Error\n\noperationalization : how researchers define the variable(s) they will study; this is a process; often the focus of a researcher‚Äôs question in the scientific method.\nconstruct : some operationalized psychological phenomenon of interest. some examples below :\n\nvoxel : three dimensional area of brain activation\nself-esteem : how a person feels about themselves\nsecure attachment style : how much a person seeks out and trusts a relationship partner.\n\nmeasurement error : when there is a lack of validity in our measures. The more error in our measures, the more error there will be in our predictions (‚Äúgarbage in ‚Üí garbage out‚Äù).\n\nKEY IDEA : the way a variable is measured is CRITICAL.\n\nThe News Article : What comes to mind when you think of a ‚ÄúCognitive Test‚Äù?\n\n\n\n\n\n\n\nThe Scientific Operationalization of this ‚ÄúCognitive Test‚Äù"
  },
  {
    "objectID": "calstats/lectures/2L_WorkingWithData.html#activity-counting-interruptions",
    "href": "calstats/lectures/2L_WorkingWithData.html#activity-counting-interruptions",
    "title": "Class 2 | Working With Data",
    "section": "ACTIVITY : Counting Interruptions",
    "text": "ACTIVITY : Counting Interruptions\nCheck-in :tinyurl.com/dudesinterrupting\n\nCount the number of interruptions in the video (which professor will play below).¬†\nSubmit your answer, then wait for the letter of the day.\n\nVideo\nDISCUSSION TOPICS :\n\nHow do we OPERATIONALIZE an INTERRUPTION?\nWhat PREDICTIONS can we make about counting interruptions a second time?"
  },
  {
    "objectID": "calstats/lectures/2L_WorkingWithData.html#footnotes",
    "href": "calstats/lectures/2L_WorkingWithData.html#footnotes",
    "title": "Class 2 | Working With Data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://www.r-bloggers.com/2013/02/extracting-the-epidemic-model-going-beyond-florence-nightingale-part-ii/‚Ü©Ô∏é\nYou will work with these later in the semester once we review how to create a likert scale (that combines 10 questions into one variable).‚Ü©Ô∏é"
  },
  {
    "objectID": "gradstats/gradlabs/2Lab_DataDescription.html#in-section",
    "href": "gradstats/gradlabs/2Lab_DataDescription.html#in-section",
    "title": "Psych 205 - Lab 2 - Prof.¬†Key",
    "section": "",
    "text": "Choose one of the datasets from the class folder (avoid the one(s) labeled [repeated measures] as we will get to these later. I‚Äôll be focusing on the ‚ÄúPerceptions of the Wealthy‚Äù dataset if you want to follow along with my key when it‚Äôs posted). Look over the accompanying article for a guide to the variables, and identify two numeric variables from the dataset that seem interesting to you. Graph these variables, report the relevant descriptive statistics for each variable. Then explain what these statistics and graphs tell you about the individuals in the dataset, and what other questions you might ask about these individuals (e.g., what do the data NOT tell you?)"
  },
  {
    "objectID": "gradstats/gradlabs/2Lab_DataDescription.html#on-your-own",
    "href": "gradstats/gradlabs/2Lab_DataDescription.html#on-your-own",
    "title": "Psych 205 - Lab 2 - Prof.¬†Key",
    "section": "",
    "text": "Use these notes to answer the following questions. Let me know if anything is unclear!\nPsychologists often want to combine information from multiple questions (that measure the same construct) into one variable. There are lots of different ways to do this, but the first that we will talk about is just the humble average. For example, the Rosenberg (1965) self-esteem scale has 10 questions all related to self-esteem; rather than work with 10 different variables, it might be nice to just average these 10 and report one average number. These are called ‚Äúlikert‚Äù scales. It‚Äôs technically pronounced ‚Äòlick-ert‚Äô, but no one says that because it sounds kind of gross.\nHere‚Äôs an overview (from my 101 class notes) of how to work with such likert scales conceptually and computationally (in R). You‚Äôll need to load the psych library into R; to do this, run the following code in your console.\ninstall.packages(\"psych\") # installs the \"psych\" package. you only need to do this once.\nlibrary(psych) # loads the library. you need to do this every R session.\n\nSelf-Esteem Problems. Use the self-esteem dataset (from the class datasets folder). You should find a codebook that describes these data in the same folder.\n\nCheck to make sure the data loaded correctly. (Note that 0s in this dataset mean the person was missing data.) Report the sample size of the dataset.\nCreate a self-esteem scale from the 10-items. Make sure to reverse-score the negatively-keyed items so that for every question, higher numbers measure higher self-esteem. Graph this variable as a histogram, and make the graph look nice (ready for publication). Report the alpha reliability, mean, and standard deviation of this variable. Below your graph, describe what these statistics tell you about the self-esteem of the participants. What other questions do you have about this variable, or the output?\nThen, graph the variable gender as a categorical factor, and report the number of people who identified as ‚Äúfemale‚Äù, ‚Äúmale‚Äù, and ‚Äúother‚Äù. You will need to do some data cleaning here.\n\nSelf-Esteem is Conditional. Report the mean and standard deviation of self-esteem for people who are identified as female, male, and ‚Äúother‚Äù in the dataset. What differences do you observe? How might we use this knowledge / what other questions do you have? Note : there are MANY ways to do this in R; you can use the subset function or indexing to divide the dataset into three groups - ‚Äúfemales‚Äù, ‚Äúmales‚Äù, and people who reported ‚Äúother‚Äù. Or other fancier methods we will talk about later. See how many different ways you can do it."
  },
  {
    "objectID": "calstats/labs/Lab1_KEY.html",
    "href": "calstats/labs/Lab1_KEY.html",
    "title": "Lab 1 - KEY",
    "section": "",
    "text": "Lab 1 - KEY\n\n\n\n\n\n\nProfessor Answers below in these green boxes. Note that normally the key will post 1-week after the Lab deadline. I‚Äôve posted this one so you can use it as a guide as we start the semester.\n\n\n\nCreate a Google Doc to answer these questions. Please number your responses to help your GSI grade. There‚Äôs no word count, and we won‚Äôt penalize students for grammar or spelling. Do your best to explain your ideas in your own words, and if you get stuck or confused, post on Discord / ask your GSI / explain why you got stuck and what you tried to do to figure things out.¬†\n\n[In Lecture] What‚Äôs a prediction about people that you made today? What information did you use to make this prediction? How did (or could) you use this prediction to influence outcomes? Were you valid in your predictions?\n\n\n\n\n\n\n\nI predicted that my child would not remember that I told him we could play ‚ÄúHoot Hoot Owl‚Äù when he woke up in the morning (after he threw a temper tantrum because we ran out of time to play last night.) I used the information that he is three years old and often has many interests to make this prediction. After making this prediction, I hit snooze on my alarm clock and got some extra sleep since I didn‚Äôt think I needed to get breakfast and lunch ready to make time for a board game in the morning. I was not valid, and he immediately asked to play the game. (We rushed breakfast and we found time to play.)\n\n\n\n\n[In Lecture] Get started on the final project by thinking through a research question you might be interested in studying as a psychology researcher. (Totally fine to change this, but great to start focusing on a question.)\n\nWhat is your question? Why do you care about this question (and / or why does this question matter to others)? How interested in this question are you on a scale from 0 (just doing to get credit for this question) to 10 (this is what motivates you to wake up each day and you will answer this question with the energy and passion of 1000 suns)?\n\n\n\n\n\n\n\nMy question is what‚Äôs the consequence of seeing bombed out images of war-torn countries on people‚Äôs perceptions of the inhabitants of the countries that were bombed / war-torn?\nI think this matters because we live in a very violent society, and are exposed to lots of images of destruction. I think on one hand, these images can motivate people to feel empathy and compassion toward the victims of bombing. On the other hand, I wonder if it desensitizes people, and dehumanizes people in some ways - if all my exposure to a group of people is seeing them as victims, then I may not see them as full humans. This is my question though!\nI‚Äôm very interested in this question; been thinking about it the past few years - just want and need to find the time? Let me know if you are reading this and also interested and maybe we can start an informal reading group?\n\n\n\n\nHow do your past experiences and background inform this question?\n\n\n\n\n\n\n\nHmm, I‚Äôm half-Iranian, and have family / travelled to Iran, so have some emotional and experiential connection to these places. I also have been very anti-war - think I was influenced by my parents here; my mom runs a ‚ÄúPeace Studies‚Äù program and my dad was suspended from school for leading an anti-Vietnam war protest. I also have a buddy in grad school who studies dehumanization, so was likely influenced to think about these issues from that perspective.\n\n\n\n\nWhat is the variable that is the focus of this question? How does this variable relate to affect, behvaior, and cognition? Which aspect of this variable are you most interested in focusing on for your project?\n\n\n\n\n\n\n\nThere are two variables here - exposure to violent images in the news and people‚Äôs perceptions of the inhabitants of countries that are bombed. The focus of the question is really about people‚Äôs perceptions of the inhabitants of the countries that are bombed.\n\naffect : emotional feelings toward the victims : compassion, interest,\nbehavior : helping behaviors toward victims; desire to seek out more information about victims / war.\ncognition : perceptions of humanization / dehumanization; prejudice for / against the victims.\n\nI think all 3 are interesting, though I‚Äôm more interested in the affect and cognition pieces of this question.\n\n\n\n\nWhat is the between-person form of variation for this variable? What is the within-person form of variation for this variable? Note : for the final project, I strongly recommend focusing on a between-person variation version of the variable for the final project.\n\n\n\n\n\n\n\nBetween person variation : how much someone who sees war images differs in their perception of dehumanization compared to someone who sees non-war images.\nWithin-person variation : how much someone‚Äôs perceptions of dehumanization change after seeing war images.\n\n\n\n[In Lecture / Discussion Section / Chapter 2] Define two variables in R - one numeric and one string variable (these can be unrelated to your project topic!) Make sure to collect at least ten data points for each variable, and show that you successfully defined the variable in R by ‚Äúprinting‚Äù it in R. Yeah, you‚Äôre programming!\n\n\n\n\n\n\n\n\nhats &lt;- c(\"butterfly\", \"union\", \"hummingbird\", \"grogu\", \"texas4000\",\n          \"luckyduck\", \"padre island\", \"A's\", \"dad's hat\", \"floppystraw\")\n\nbook.shelf &lt;- c(42, 52, 11, 45, 33, 59, 40, 34, 71, 33)\n\nhats\n\n [1] \"butterfly\"    \"union\"        \"hummingbird\"  \"grogu\"        \"texas4000\"   \n [6] \"luckyduck\"    \"padre island\" \"A's\"          \"dad's hat\"    \"floppystraw\" \n\nbook.shelf\n\n [1] 42 52 11 45 33 59 40 34 71 33\n\n\n\n\n\n\n[In Discussion Section] With your discussion section,define each of the six biases described in the Goldacre (2010) reading on cognitive biases, and come up with an example from real-life.\n\n\n\n\n\n\nI‚Äôm going to let you work on this with your TA in discussion section! Curious to see y‚Äôalls answers :)\n\n\n\n\n\n\n\nBias\nDefinition\nReal-Life Example\n\n\nPositive Evidence\n\n\n\n\nPrevious Belief\n\n\n\n\nPatterns in Randomness\n\n\n\n\nAvailability Bias\n\n\n\n\nSocial Influence\n\n\n\n\nRegression to the Mean"
  },
  {
    "objectID": "gradstats/gradlabs/2_Description.html#announcements",
    "href": "gradstats/gradlabs/2_Description.html#announcements",
    "title": "Lecture 2 : Describing Data with Pictures and Numbers",
    "section": "",
    "text": "Presentations:\n\nBegin today (professor)\nSign-Up Sheet Updated w/ another presentation :)\n\nUpdates / Questions from Week 1.\n\nShifting Mini Exam to 2/28\n\nMore time to practice\nCan cover (and review) Linear Models [foundational!]\nNo class ‚Äúafter‚Äù the exam. Just the exam. That‚Äôs enough? Yeah.\n2/28 presentation moved to 2/21\nLet me know if this causes issues / stress!\n\nDo we want a class discord (community / troubleshooting / another notification?)"
  },
  {
    "objectID": "calstats/calstatsSP25.html#course-information",
    "href": "calstats/calstatsSP25.html#course-information",
    "title": "Psych 101 | Research and Data Analysis in Psychology (SP25)",
    "section": "",
    "text": "Lecture | Fridays 2:00 - 5:00 PM in 100 Lewis\nProfessor : Arman Daniel Catterson, PhD [he / his / ÿßŸà ] üê±(catterson@berkeley.edu)\nOffice Hours | Wednesdays 4:00 - 5:00 PM on Zoom. E-mail / Discord to find another time to meet virtually, or say hi after class.\nTextbook : We will use Why Statistics? to learn the course material. I‚Äôll provide free required readings and videos each week. For additional support, I recommend Danielle Navarro‚Äôs FREE Learning Statistics with R, or copy of Field, Miles, & Field (2012). Discovering Statistics Using R. SAGE Publications (I‚Äôve uploaded sample chapters to bCourses)."
  },
  {
    "objectID": "calstats/labs/Lab2.html",
    "href": "calstats/labs/Lab2.html",
    "title": "Lab 2",
    "section": "",
    "text": "Datasets are Fun To Load : Load the ‚Äúcovid_behavior_dataset.csv‚Äù dataset. Check to make sure the data loaded correctly with the head() function. Report the sample size and names of the variables from this dataset, and look over the code book. What are some variables that seem interesting to you? How might you use these variables in a linear model (either as a DV or as IVs?)\n\n\n\nNumber Problems : Graph two numeric variables from this dataset. Change the arguments on the graph to make them look ‚Äúnice‚Äù. Describe what you learn about the individuals for each variable (no stats terminology yet!) and what additional questions you have about the variables.\nCategory Problems : Graph two categorical variables from this dataset. Describe what you learn from each variable, and report the frequency of the number of individuals in each group. Do you think it‚Äôs possible (or preferred) to measure this categorical variable in a numeric / continuous way? Why / why not?\n(In Discussion Section) Mini Problems : Repeat the steps above for variables from another dataset - the ‚Äúclass_mini‚Äù dataset that we discussed in lecture today. These data will be posted to bCourses as a .csv file after class, along with a CODEBOOK that explains what the variables measure.(In Discussion Section) Sharing Problems : Share your R code for Problem 4 with another student in the class who you‚Äôve never talked to before. Paste their code into your R script and generate the graph that they created below. Hooray for open science!!!"
  },
  {
    "objectID": "gradstats/gradlabs/2_Description.html#break-time-article-presentation",
    "href": "gradstats/gradlabs/2_Description.html#break-time-article-presentation",
    "title": "Lecture 2 : Describing Data with Pictures and Numbers",
    "section": "",
    "text": "See the article presentation slideshow [warning : everyone has edit access. be careful!]"
  },
  {
    "objectID": "calstats/calstatsSP25.html#grade-details",
    "href": "calstats/calstatsSP25.html#grade-details",
    "title": "Psych 101 | Research and Data Analysis in Psychology (SP25)",
    "section": "",
    "text": "Your grade in the class will be based on the following components.\n\n\n\n5% Check-Ins (Lecture and Readings)\n10% Reading Quizzes\n\n\n20% Lab and Project Milestone Assignments\n29% R Exam (Mini = 5 points;¬† Mega (Chill) = 24 points)\n\n\n5% Discussion Section Attendance\n31% Final Project (Research Paper)\n\n\n\nLetter grades will be based on the following cutoffs :\n\n\n\n\n\n\n\n\n\nA+ is &gt; 96.5\nA is &gt; 92.5\nA- is &gt; 89.5\nB+ is &gt; 86.5\nB is &gt; 82.5\nB- is &gt; 79.5\nC + is &gt; 76.5\nC is &gt; 72.5\nC- is &gt; 69.5\nD is &gt; 59.5\nF is &lt; 59.5\nnote : an 89.499 is a B+"
  },
  {
    "objectID": "calstats/calstatsSP25.html#teaching-assistants-graduate-student-instructors",
    "href": "calstats/calstatsSP25.html#teaching-assistants-graduate-student-instructors",
    "title": "Psych 101 | Research and Data Analysis in Psychology (SP25)",
    "section": "",
    "text": "You must attend the section for which you are registered, led by your awesome GSI. Go GSIs!!\n\n\n\nName (& Pronouns)\nSection Times & Location\nE-mail\nOffice Hour Time & Location\n\n\nNathalie Fernandez\n\nnfe@berkeley.edu\n\n\n\nSarah Gao (she/her)\nSection 108: Wed 12-2, Evans 7\nSection 109: Wed 2-4, Evans 7\nsarah.gao@berkeley.edu\nTues 10:30-11:30 AM at BWW 3rd floor booths or book an appt here\n\n\nAkshat Gupta\n\nakshat.g@berkeley.edu\n\n\n\nAratrik Paul\nSection 101: Wednesday 12-2 (VLS 2062)\nSection 105: Monday 12-2 (Lewis 9)\naratrikpaul@berkeley.ed u\nWednesdays 10 AM - 12 PM ( Zoom)\n\n\nSahana Sridhar\nSection 107: Wed 10-2, Evans 7\nSection 110: Wed 4-6, Giannini 201\nsahanasri@berkeley.edu\nThursdays 11 AM -12 PM at BWW 3rd floor atrium or via Zoom"
  },
  {
    "objectID": "calstats/calstatsSP25.html#summary-of-course-components",
    "href": "calstats/calstatsSP25.html#summary-of-course-components",
    "title": "Psych 101 | Research and Data Analysis in Psychology (SP25)",
    "section": "",
    "text": "Readings and Reading Quizzes¬† : Each week before lecture, you will read and watch videos from a textbook chapter designed to teach you the basic skills. At the end of each chapter, there will be a short quiz based on the content of this chapter that is due before lecture. This way, we can use time in lecture to review and discuss course concepts, and practice working on homework assignments together. The quizzes are multiple choice, untimed, open-note, and allow for multiple attempts.¬†\nCheck-Ins : I will ask you to complete short assessments in lecture and lab documents that consist of multiple choice and/or short-answer questions. These check-ins are graded for completion. It is your responsibility to make sure all check-ins are complete, and that you are logged in with your own UC Berkeley account (e.g., yourname@berkeley.edu). Check-ins must be completed by the beginning of the next lecture (e.g., when assignments are due) in order to be guaranteed a grade.¬†\nDiscussion Section : Discussion sections are a way to review course material, work on lab assignments in a space where you can get immediate help from other students and your GSI. Please attend the discussion section for which you are registered, though you may ask a GSI for permission to attend another section once in order to make-up for an absence or because of some other issue. GSIs will also be in attendance during lecture in order to help troubleshoot R problems you may have during live demonstrations.¬†\nLab Assignments : Every week, you will work through a ‚Äúlab‚Äù document that contains text and videos designed to help you understand key concepts in statistics, research methods, and R. You‚Äôll work on these at home, in lecture, and in your discussion sections. Each lab document will have a set of questions to answer - you should submit answers to these questions as a separate document to bCourses. Lab assignments will be graded for completion, accuracy, and effort and are due the week after they are assigned.\nFinal Project (Research Paper) : Students will conduct a study of their own, and will write up the results as if for publication. There are several milestones for this assignment due throughout the semester. For more details, see the final project description and rubric.\nR Exam : This exam will be scheduled ‚Äúlive‚Äù during our scheduled time. You should plan to take the exam during this time if possible. ¬†DSP students will receive extra time accommodations in accordance with their letters, and students with conflicts for the scheduled R Exam should submit a make-up request here at least TWO WEEKS before the exam date in order to be guaranteed a make-up time. For the exam, I‚Äôll give you a novel dataset and you‚Äôll use R to answer questions based on the dataset. The exam will be open-book and open note - you must work alone, but may use any resource available to you (including the Internet). A practice exam will be posted at least one week before the exam. Late submissions for the R exam will be penalized.¬†\nExtra Credit : There will be no extra credit offered in this course, even for students who ask kindly.¬†\nFinal Exam : There is no final exam in this class. Your final project will serve as the final assessment of your skills and training. Apologies to my former students who had a final exam in this class :|"
  },
  {
    "objectID": "calstats/calstatsSP25.html#course-policies",
    "href": "calstats/calstatsSP25.html#course-policies",
    "title": "Psych 101 | Research and Data Analysis in Psychology (SP25)",
    "section": "",
    "text": "Late Work Policy. Assignments (quizzes, labs, and project milestones) are due before lecture. Late lab and project milestone assignments will be penalized 25% for any reason (even if they are 1-minute late), and 50% if turned in more than a week late (after the key has been posted). Students with extra time DSP accommodations should work out a plan to submit work within a week of the original deadline. Students may drop their lowest submitted lab and quiz assignment. Note : in the past, I‚Äôve tried a variety of more lenient late-work policies, and found that these policies often disincentivized students turning in work on time to their overall detriment in the class; so let‚Äôs try this and see how it goes.\nComputer Access. Access to a personal computer is strongly recommended. We will spend time each lecture working in R - a free and powerful statistics program that works with all operating systems and computers. Laptops are available for rent from the library (see Berkeley‚Äôs Technology Loan Program), or you can purchase one that will work for this class for as little as $150 (less than the price of most textbooks, which is not required for this class). Talk to me or a GSI if access to a laptop or computer is interfering with your ability to do well in the course.\nDiscussion Section Policy : In-person attendance is expected and required for discussion sections. Working with other students and your GSIs to review concepts, practice problems, and get feedback on your project is a critical part of the course (and the research process). Of course, the health and safety of you and your classmates and GSIs is most important - please do not attend your discussion section if you are worried you are sick, have tested positive for COVID-19 (or another contagious illness), or have closely interacted with someone who you believe is sick / tested positive for COVID-19 (or another contagious illness). Students may miss up to two discussion sections with no penalty. Please talk to your GSI / the professor if there is a longer-term illness or issue that will prevent you from regularly attending your discussion section and we will work out another arrangement.\nRegrades. Students may ask for a re-grade on exams and papers if they believe they lost points for something they should have earned. To request a regrade, talk to your GSI in office hours or set up a meeting. In the case that you and your GSI cannot resolve the regrade issue, the professor will step in and regrade the exam. For final project regrades, just e-mail the professor. When requesting a regrade, you consent to have your score increased or decreased if we find that you earned points you should not have earned.\nStudent Contact. It is your responsibility to regularly check e-mail for announcements. Please post questions about the class or R to our class discord page, so other students can see the question (and answer) and benefit from your question! If you have specific questions to e-mail me, please make sure to (a) search the syllabus and / or Google for the answer to your question and (b) contact your GSI about your question. I am always happy to answer any questions in office hours (either during the scheduled time or online by appointment), or before, during, or after lecture. I should respond to e-mail within 24 hours on weekdays (I do not check e-mail on weekends); please send a gentle reminder if I do not respond in this timeframe. Thanks!\nIncomplete Policy. I will only grant an incomplete to students who (1) have a significant life event that prevents them from completing the final project at the end of the semester and (2) are passing the class at the point they request the incomplete.\nAcademic Integrity. Do NOT cheat. Do NOT plagiarize. To copy text or ideas from another source (including your own submitted coursework) without appropriate reference is plagiarism. You may work on lab assignments with other students, but should be able to understand what you are doing. You may not work with other students on the R exam. Anyone caught cheating or plagiarizing will receive a zero on the assignment or test, and will be reported to the Office of Student Conduct. It is your responsibility to read the official Student Conduct Policy for more information about campus standards and policies regarding Academic Integrity.\nChat-GPT / AI Policy. If you use ChatGPT or other generative-AI software for any part of this class (writing, coding, troubleshooting, etc.), you must make this clear at the top of your assignment by writing : ‚ÄúI USED CHAT GPT TO SUPPORT THIS ASSIGNMENT‚Äù (in all-caps and red text), and include an appendix where you include screenshots of your specific queries and answers. Failure to do this will result in a zero of the assignment, and the professor submitting a report to the Academic Integrity office. Ask your GSI / professor if you have questions about this policy.\nRespect for Others. We all belong in our classroom, and I hope that everyone feels free and supported to express our identities when relevant. To achieve this goal, make sure that you are treating others with respect in this course. This means considering others‚Äô opinions and perspectives, making sure not to generalize to groups (or ask students to be representatives of their group), and being mindful of the words that you use. (Also remember that nothing is ever really deleted from the internet, so please be extra mindful of the words you use.) While I will be monitoring class posts and communication, feel free to reach out to me if you see another student engaging in disrespectful or threatening behavior. Of course, this extends to me as well - if you feel like there are parts of the course instruction, subject matter, or class environment that create barriers to your success or inclusion, please contact me via e-mail and trust that your voice will be heard and I will not punish you for offering constructive criticisms of my teaching. I‚Äôm very open to learning how to be a better teacher, and always learn a lot from my students.¬†\nSensitive Subjects. Our class will touch on important and potentially sensitive topics that are related to psychology and psychological processes, such as racism, depression, sex, and poverty. These topics can generate strong emotional responses in students for a variety of reasons - such as their own past histories with these topics or their reactions to the ways the information is presented or described. I will try to make sure to give you a heads up when we start to discuss these topics so you can be mindful of your reactions.\nStrategies for success. Do your readings, complete assignments on time, understand how the topics and information relate to what you‚Äôve learned (and are learning), and if you don‚Äôt understand something, please ask the question!¬†\nMost important. Please let me know as soon as there is anything going on in your life that you think may affect your ability to do as well as you would like in this class (e.g.¬†sickness, work, small children at home, threats to immigration status, etc.), and I will do my best to work with you on a plan to succeed in the class. If you‚Äôre not comfortable talking to me directly, you can talk to your GSI who can talk to me about your issue."
  },
  {
    "objectID": "calstats/calstatsSP25.html#student-support-services.",
    "href": "calstats/calstatsSP25.html#student-support-services.",
    "title": "Psych 101 | Research and Data Analysis in Psychology (SP25)",
    "section": "",
    "text": "UC Berkeley offers a variety of services to help support students facing difficulties in these historic times.\nStudents with Disabilities : Students who require support or adaptive equipment because of a specific disability ‚Äì or would like to be tested to see if they qualify for a learning, auditory, or visual disability ‚Äì can request these services through the Disabled Students Program (DSP) office. Please note that I can only provide accommodations authorized by the DSP office. If you have DSP status, make sure you submit your letter and feel free to talk to me or a GSI about any specific accommodations you need, or other ways we might be able to make the class more accessible. [260 C√©sar E. Ch√°vez Student Center| 510-642-0518 | https://dsp.berkeley.edu/]\nStudents with Mental Health Issues : If you feel a mental health issue (e.g., depression, anxiety) is negatively impacting your ability to succeed, I‚Äôd highly encourage you to seek out experts who can help. Berkeley has several free, confidential, and science-based programs designed to help students. [https://uhs.berkeley.edu/caps | Tang Center 3rd Floor | 510-642-9494 | After hours support line : 855-817-5667]\nUndocumented Students : I believe that all people deserve equal access to education. The UC Berkeley Undocumented Student Program has organized resources to support undocumented students, protect their rights, and offer potential financial aid | https://undocu.berkeley.edu/\nStudents from Low-Income Backgrounds : The Extended Opportunity Program (EOP) provides college support services for low-income students. Services include additional counseling, financial assistance (study-time parent grants, work-study assignments, and book vouchers), child-care opportunities, and assistance transferring to four-year colleges. [119 C√©sar E. Chavez Student Center | 510-642-7224 |¬† https://eop.berkeley.edu/]\nAdditional Course Resources. Some students feel that they need additional support in this class. If you find yourself struggling, please seek these resources as soon as possible.\nPrivate Tutoring : I‚Äôve put together a list of students who excelled in previous semesters of this class - you can contact these people directly to coordinate time, place, and payment (some tutors can work with groups, which will help reduce the cost for each student)."
  },
  {
    "objectID": "calstats/calstatsSP25.html#free-on-campus-tutoring",
    "href": "calstats/calstatsSP25.html#free-on-campus-tutoring",
    "title": "Psych 101 | Research and Data Analysis in Psychology (SP25)",
    "section": "",
    "text": "TheData Peer Consulting program in the Division of Computing, Data Science, and Society. This program strives to help make data science accessible across the broader campus community, by aiming to help undergraduate students, graduate students, staff, and faculty with research project infrastructure or other projects and modules that incorporate data.¬†\nThe UC Berkeley D-Lab. Drop-in consulting where people may be able to help you with R specific problems.\nLibrarians are great. They can help you find research articles for your final project, think about how to organize or structure your literature review, and probably have other powers from a life of being surrounded by books."
  },
  {
    "objectID": "calstats/Lectures/3L_Description.html",
    "href": "calstats/Lectures/3L_Description.html",
    "title": "Class 3 | Description",
    "section": "",
    "text": "link to RScript [also on course page]\ndata are here [also on the data dropbox folder on bCourses]\nname the dataset d to follow along w/ professor\n\n\n\nProfessor Check-In Code Goes Here\n\n## CHECK-IN : the interruption dataset\n##  Download the \"interruption\" dataset and import the data to RStudio.\n\n# Q1. How many individuals are in this dataset?\n\n# Q2. How many variables are in this dataset?\n\n# Q3. What is the value of the second row, third column of this dataset?\n\n# Q4. How many individuals said \"yes, I can relax my feet\"?\n\n# Q5. How many individuals said \"no, my feet were already totally relaxed\"?\n\n# BONUS QUESTION (AND DEMONSTRATION): I'm going to randomly call on 10 students.\n## I'll ask them to report the number of instagram accounts they follow. \n## Predict these 10 numbers by filling in your guesses below.\n\n## my guesses : \n\nAgenda and Announcements\n\n2:10 - 2:30 | Check-In & Announcements\n2:30 - 3:30 | Describing Data in R (and Removing Outliers)\n3:45 - 4:00 | BREAK TIME\n4:00 - 4:20 | Student Show and Tell\n4:20 - 5:00 | Final Project Workshop"
  },
  {
    "objectID": "calstats/Lectures/3L_Description.html#check-in-and-announcements",
    "href": "calstats/Lectures/3L_Description.html#check-in-and-announcements",
    "title": "Class 3 | Description",
    "section": "",
    "text": "Professor Check-In Code Goes Here\nAgenda and Announcements\n\n2:10 - 2:30 | Check-In & Announcements\n2:30 - 3:30 | Describing Data in R (and Removing Outliers)\n3:45 - 4:00 | BREAK TIME\n4:00 - 4:20 | Student Show and Tell\n4:20 - 5:00 | Final Project Workshop"
  },
  {
    "objectID": "calstats/Lectures/3L_Description.html#part-1-describing-data",
    "href": "calstats/Lectures/3L_Description.html#part-1-describing-data",
    "title": "Class 3 | Description",
    "section": "Part 1 : Describing Data",
    "text": "Part 1 : Describing Data\n\nLab 3, Problem 1. Interruption Problems\nDownload the ‚Äúinterruption‚Äù data from bCourses, and import this data into R. This dataset has two variables of the number of interruptions counted before (int1) and after (int2) our operationalization.\n\nLoad the data (I‚Äôll call it d if you want to follow along with my code), check to make sure it loaded correctly, and report the sample size and names of the variables.\nGraph these variables as a histogram (use the par() function to graph them side by side). Change the arguments so the graphs have the same x-axis and y-axis ranges, and nice labels.\nReport the mean, median, range, and standard deviation for both variables. Then, calculate the standard deviation ‚Äúby hand‚Äù for int1 (you should get a similar, but not exact, number, as what R gives you.)\nDescribe how these statistics changed after operationalizing an interruption, and why these changes make sense given the nature of our operationalizations. Then, decide whether our operationalization would be good enough if we were researchers trying to scientifically study interruptions.\nGraph a categorical variable from the dataset and report the frequency of each group.\n\n\n\nKEY IDEA : The Mean as Prediction\n\nHow would you feel if I told you that the average last semester was a 50%? A 90%? Why would you feel this way?\n\n\n\n\n\n\n\nDid professor guess better than you about the # of instagram followers? Let‚Äôs find out.\nWhere would a vertical line best fit through these data?\n\n\nWhere the Line?There the Line!\n\n\n\nmini &lt;- read.csv(\"~/Dropbox/!WHY STATS/Class Datasets/cal_mini_SP25.csv\")\nplot(mini$hrs.sleep)\n\n\n\n\n\n\n\n\n\n\n\nplot(mini$hrs.sleep)\nabline(h = mean(mini$hrs.sleep, na.rm = T), col = 'red', lwd = 5)\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Mean As Prediction (and Error)\nThe following equation formalizes this idea, that an individual‚Äôs score is related to the mean and error.\n\\[\n\\Huge y_i = \\bar{Y} + \\epsilon_i\n\\]\nWe can break this up into the following components :\n\n\\(y_i\\) = the individual‚Äôs (\\(_i\\)) actual score of a variable (\\(y\\)) we are trying to predict.\n\non the graph : each individual dot; the value on the y-axis is the individual‚Äôs score; value of the x-axis is the person‚Äôs index (when people submitted the survey).\n\n\\(\\bar{Y}\\) = our prediction (the mean, in this case)\n\non the graph : the solid line is drawn at the mean for the variable.\n\n\\(\\epsilon_i\\) = residual error\n\non the graph : the vertical distance between the predicted values of y (the line) and the individual‚Äôs actual value of y (the dot)\n\n\nI like to remember that an individual‚Äôs actual score comes first in the equation, because as researchers we (supposedly) care about REAL PEOPLE and not just data or predictions."
  },
  {
    "objectID": "calstats/Lectures/3L_Description.html#break-time-meet-back-at-400",
    "href": "calstats/Lectures/3L_Description.html#break-time-meet-back-at-400",
    "title": "Class 3 | Description",
    "section": "BREAK TIME : MEET BACK AT 4:00",
    "text": "BREAK TIME : MEET BACK AT 4:00"
  },
  {
    "objectID": "calstats/Lectures/3L_Description.html#part-2-more-r-practice",
    "href": "calstats/Lectures/3L_Description.html#part-2-more-r-practice",
    "title": "Class 3 | Description",
    "section": "Part 2 : More R Practice",
    "text": "Part 2 : More R Practice\nWork on Lab 3, Problem 2.2. Choose another numeric variable from the dataset (see the codebook for a guide). Graph this variable as a histogram, and report the mean, median, standard deviation, and range. Make sure to do any necessary data cleaning (e.g., outlier removal), make the graph look nice, and describe what each descriptive statistic teaches you about the people in the dataset (you do not need to draw them again on the graph, unless you want to / find this helpful!)\nShare Your Work With the Class! Post a screenshot of your graph on the class Discord; and then come up to the front to talk about your variable! Yeah?!?"
  },
  {
    "objectID": "calstats/Lectures/3L_Description.html#part-3-final-project-workshop",
    "href": "calstats/Lectures/3L_Description.html#part-3-final-project-workshop",
    "title": "Class 3 | Description",
    "section": "Part 3 : Final Project Workshop",
    "text": "Part 3 : Final Project Workshop\nThis Week : Decide on a Dependent Variable\n\nStudent ideas go here.\n\nNext Week : Decide on How to Measure the Dependent Variable (Probably a Self-Report Survey!)\n\nProfessor ideas go here."
  },
  {
    "objectID": "calstats/Lectures/3L_Description.html#check-in-here-tinyurl.cominterruptlifewithr",
    "href": "calstats/Lectures/3L_Description.html#check-in-here-tinyurl.cominterruptlifewithr",
    "title": "Class 3 | Description",
    "section": "",
    "text": "link to RScript [also on course page]\ndata are here [also on our dropbox folder on bCourses]\nname the dataset d to follow along w/ professor\n\n\n\nProfessor Check-In Code Goes Here\n\n## CHECK-IN : the interruption dataset\n##  Download the \"interruption\" dataset and import the data to RStudio.\n\n# Q1. How many individuals are in this dataset?\n\n# Q2. How many variables are in this dataset?\n\n# Q3. What is the value of the second row, third column of this dataset?\n\n# Q4. How many individuals said \"yes, I can relax my feet\"?\n\n# Q5. How many individuals said \"no, my feet were already totally relaxed\"?\n\n# BONUS QUESTION (AND DEMONSTRATION): I'm going to randomly call on 10 students.\n## I'll ask them to report the number of instagram accounts they follow. \n## Predict these 10 numbers by filling in your guesses below.\n\n## my guesses : \n\nAgenda and Announcements\n\n2:10 - 2:30 | Check-In & Announcements\n2:30 - 3:30 | Describing Data in R (and Removing Outliers)\n3:45 - 4:00 | BREAK TIME\n4:00 - 4:20 | Student Show and Tell\n4:20 - 5:00 | Final Project Workshop"
  },
  {
    "objectID": "gradstats/gradlabs/3_NormalDistributions.html",
    "href": "gradstats/gradlabs/3_NormalDistributions.html",
    "title": "Lecture 3 : Normal Distributions",
    "section": "",
    "text": "Look at the distribution below.\n\nWhat‚Äôs the shape of this distribution? What does this shape tell you?\nWhat would you consider to be outliers in this distribution? Why?\n\n\nd &lt;- read.csv(\"../datasets/Protestant Work Ethic/data.csv\", sep = \"\\t\")\n\nhist(d$Q1E, breaks = 100, \n     main = \"RT for Answering Q1\",\n     xlab = \"Response Time (RT) in ms\",\n     col = 'black', bor = 'white')\nabline(v = mean(d$Q1E, na.rm = T), col = 'red', lwd = 5)\n\n\n\n\n\n\n\n\nQuestion: What would you want to know about whether someone is an outlier on this question??\n\nhow many other people took this long on other question?\nhow and where was the survey administered?\nwhat was the question?\nhow long did the person take on other questions?\n\n\nd$Q1E[d$Q1E &gt; 40000] # finding the outliers\n\n [1]  69152  78174  42701  63399  59044 181246  61932  75233  70669  53020\n[11] 130234  64001  65153  40022  61663  41649  41206  44816  70506  73577\n[21] 137802\n\n(d$Q1E[d$Q1E &gt; 40000] - mean(d$Q1E, na.rm = T)) # residual\n\n [1]  61800.38  70822.38  35349.38  56047.38  51692.38 173894.38  54580.38\n [8]  67881.38  63317.38  45668.38 122882.38  56649.38  57801.38  32670.38\n[15]  54311.38  34297.38  33854.38  37464.38  63154.38  66225.38 130450.38\n\nsd(d$Q1E, na.rm = T) # on average, people were 10,135 ms different from the mean in their RT\n\n[1] 10135.58\n\n(d$Q1E[d$Q1E &gt; 40000] - mean(d$Q1E, na.rm = T)) / sd(d$Q1E, na.rm = T) # z-score!\n\n [1]  6.097373  6.987505  3.487654  5.529768  5.100094 17.156834  5.385030\n [8]  6.697339  6.247044  4.505751 12.123869  5.589163  5.702822  3.223338\n[15]  5.358490  3.383861  3.340154  3.696325  6.230962  6.533954 12.870545\n\n## how far an individual is from the mean in units of standard deviation.\n\nd$Q1E[d$Q1E &gt; 40000] &lt;- NA # removes outliers JUST for the variable\n# d[d$Q1E &gt; 40000, ] &lt;- NA # removes outliers for the entire dataset (based on the rule at that variable)\n\nhist(d$Q1E, breaks = 100, \n     main = \"RT for Answering Q1\",\n     xlab = \"Response Time (RT) in ms\",\n     col = 'black', bor = 'white')"
  },
  {
    "objectID": "gradstats/gradlabs/3_NormalDistributions.html#check-in-here",
    "href": "gradstats/gradlabs/3_NormalDistributions.html#check-in-here",
    "title": "Lecture 3 : Normal Distributions",
    "section": "",
    "text": "Look at the distribution below.\n\nWhat‚Äôs the shape of this distribution? What does this shape tell you?\nWhat would you consider to be outliers in this distribution? Why?\n\n\nd &lt;- read.csv(\"../datasets/Protestant Work Ethic/data.csv\", sep = \"\\t\")\n\nhist(d$Q1E, breaks = 100, \n     main = \"RT for Answering Q1\",\n     xlab = \"Response Time (RT) in ms\",\n     col = 'black', bor = 'white')\nabline(v = mean(d$Q1E, na.rm = T), col = 'red', lwd = 5)\n\n\n\n\n\n\n\n\nQuestion: What would you want to know about whether someone is an outlier on this question??\n\nhow many other people took this long on other question?\nhow and where was the survey administered?\nwhat was the question?\nhow long did the person take on other questions?\n\n\nd$Q1E[d$Q1E &gt; 40000] # finding the outliers\n\n [1]  69152  78174  42701  63399  59044 181246  61932  75233  70669  53020\n[11] 130234  64001  65153  40022  61663  41649  41206  44816  70506  73577\n[21] 137802\n\n(d$Q1E[d$Q1E &gt; 40000] - mean(d$Q1E, na.rm = T)) # residual\n\n [1]  61800.38  70822.38  35349.38  56047.38  51692.38 173894.38  54580.38\n [8]  67881.38  63317.38  45668.38 122882.38  56649.38  57801.38  32670.38\n[15]  54311.38  34297.38  33854.38  37464.38  63154.38  66225.38 130450.38\n\nsd(d$Q1E, na.rm = T) # on average, people were 10,135 ms different from the mean in their RT\n\n[1] 10135.58\n\n(d$Q1E[d$Q1E &gt; 40000] - mean(d$Q1E, na.rm = T)) / sd(d$Q1E, na.rm = T) # z-score!\n\n [1]  6.097373  6.987505  3.487654  5.529768  5.100094 17.156834  5.385030\n [8]  6.697339  6.247044  4.505751 12.123869  5.589163  5.702822  3.223338\n[15]  5.358490  3.383861  3.340154  3.696325  6.230962  6.533954 12.870545\n\n## how far an individual is from the mean in units of standard deviation.\n\nd$Q1E[d$Q1E &gt; 40000] &lt;- NA # removes outliers JUST for the variable\n# d[d$Q1E &gt; 40000, ] &lt;- NA # removes outliers for the entire dataset (based on the rule at that variable)\n\nhist(d$Q1E, breaks = 100, \n     main = \"RT for Answering Q1\",\n     xlab = \"Response Time (RT) in ms\",\n     col = 'black', bor = 'white')"
  },
  {
    "objectID": "gradstats/gradlabs/3_NormalDistributions.html#announcements-agenda",
    "href": "gradstats/gradlabs/3_NormalDistributions.html#announcements-agenda",
    "title": "Lecture 3 : Normal Distributions",
    "section": "Announcements & Agenda",
    "text": "Announcements & Agenda\nGoal : understand why and how the normal distribution is ‚Äúnormal‚Äù; focus more on understanding the mean as a prediction of this distribution vs.¬†a prediction of the population, and learn one method of estimating how well the mean describes the population.\n\n9:10 - 9:30 | Check-In and Week 2 Recap\n9:30 - 10:00 | ‚ÄúNormal‚Äù Distributions\n10:00 - 10:30 | Sampling Error [Conceptual]\n10:30 - 10:40 | BREAK TIME\n10:40 - 11:00 | Presentation\n11:00 - 12:00 | Sampling Error"
  },
  {
    "objectID": "gradstats/gradlabs/3_NormalDistributions.html#recap-week-2",
    "href": "gradstats/gradlabs/3_NormalDistributions.html#recap-week-2",
    "title": "Lecture 3 : Normal Distributions",
    "section": "RECAP : Week 2",
    "text": "RECAP : Week 2\n\nCreating a Scale\n\nOrganize your items; reverse-score; evaluate reliability.\n\n\n## Loading Dataset\nselfes &lt;- read.csv(\"../datasets/Self-Esteem Dataset/data.csv\",\n                   stringsAsFactors = T,\n                   na.strings = \"0\", sep = \"\\t\")\n\n## Creating the Scale\nposkey.df &lt;- selfes[,c(1:2,4,6,7)] # pos-keyed items (from the codebook)\nnegkey.df &lt;- selfes[,c(3,5,8:10)] # neg-keyed items (from the codebook)\nnegkeyR.df &lt;- 5-negkey.df # reverse scoring the neg-keyed items\nSELFES.DF &lt;- data.frame(poskey.df, negkeyR.df) # bringing it all 2gether.\n\nlibrary(psych) # loading the library\nalpha(SELFES.DF) # alpha reliability.\n\n\nReliability analysis   \nCall: alpha(x = SELFES.DF)\n\n  raw_alpha std.alpha G6(smc) average_r S/N     ase mean  sd median_r\n      0.91      0.91    0.92      0.52  11 0.00058  2.6 0.7     0.52\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.91  0.91  0.91\nDuhachek  0.91  0.91  0.91\n\n Reliability if an item is dropped:\n    raw_alpha std.alpha G6(smc) average_r  S/N alpha se  var.r med.r\nQ1       0.90      0.90    0.91      0.51  9.5  0.00064 0.0089  0.51\nQ2       0.91      0.91    0.91      0.52  9.7  0.00063 0.0085  0.52\nQ4       0.91      0.91    0.91      0.53 10.3  0.00061 0.0081  0.53\nQ6       0.90      0.90    0.90      0.50  9.2  0.00067 0.0087  0.51\nQ7       0.90      0.90    0.91      0.51  9.3  0.00066 0.0089  0.51\nQ3       0.90      0.90    0.91      0.51  9.3  0.00066 0.0094  0.51\nQ5       0.90      0.91    0.91      0.52  9.6  0.00065 0.0098  0.51\nQ8       0.91      0.91    0.92      0.54 10.7  0.00059 0.0064  0.54\nQ9       0.90      0.91    0.91      0.52  9.6  0.00065 0.0085  0.52\nQ10      0.90      0.90    0.90      0.51  9.3  0.00067 0.0086  0.51\n\n Item statistics \n        n raw.r std.r r.cor r.drop mean   sd\nQ1  47876  0.76  0.77  0.75   0.70  3.0 0.87\nQ2  47658  0.73  0.74  0.71   0.66  3.1 0.79\nQ4  47751  0.66  0.68  0.62   0.59  2.9 0.81\nQ6  47809  0.81  0.81  0.79   0.75  2.6 0.92\nQ7  47758  0.79  0.79  0.77   0.74  2.4 0.93\nQ3  47751  0.79  0.79  0.76   0.73  2.7 0.95\nQ5  47781  0.76  0.76  0.72   0.69  2.6 0.98\nQ8  47797  0.64  0.63  0.56   0.54  2.3 0.96\nQ9  47728  0.76  0.75  0.73   0.69  2.2 0.99\nQ10 47772  0.81  0.80  0.78   0.74  2.4 1.07\n\nNon missing response frequency for each item\n       1    2    3    4 miss\nQ1  0.06 0.18 0.44 0.32 0.00\nQ2  0.04 0.13 0.50 0.33 0.01\nQ4  0.05 0.21 0.50 0.24 0.00\nQ6  0.14 0.33 0.37 0.17 0.00\nQ7  0.18 0.34 0.35 0.14 0.00\nQ3  0.13 0.28 0.37 0.22 0.00\nQ5  0.14 0.32 0.32 0.22 0.00\nQ8  0.21 0.41 0.24 0.14 0.00\nQ9  0.27 0.40 0.20 0.14 0.01\nQ10 0.24 0.33 0.22 0.22 0.00\n\n\n\nAverage the items into one variable; graph & describe.\n\n\nselfes$SELFES &lt;- rowMeans(SELFES.DF, na.rm = T) # creating the scale\nhist(selfes$SELFES, col = 'black', bor = 'white', # the graph\n     main = \"Histogram of Self-Esteem\", \n     xlab = \"Self-Esteem Score\", breaks = 15)\n\n\n\n\n\n\n\n\n\n\nMean is a Prediction (of the Sample)\n\nWhere is the Mean?There is the Mean!Quantifying Errors (Residuals)\n\n\n\nlils &lt;- selfes[sample(1:nrow(selfes), 100),]# 100 random data points\nplot(lils$SELFES, \n     ylab = \"Self-Esteem (100 Points)\",\n     xlab = \"Index\") \n\n\n\n\n\n\n\n\n\n\n\nplot(lils$SELFES,\n     ylab = \"Self-Esteem (100 Points)\",\n     xlab = \"Index\") \nabline(h = mean(lils$SELFES, na.rm = T), lwd = 5)\n\n\n\n\n\n\n\n\n\n\n\n## quantifying errors (residuals)\nresiduals &lt;- selfes$SELFES - mean(selfes$SELFES, na.rm = T)\nSST &lt;- sum(residuals^2, na.rm = T)\nSST\n\n[1] 23401.4\n\nSST/length(residuals) # average of squared residuals (variance)\n\n[1] 0.4877935\n\nsqrt(SST/length(residuals)) # average of residuals, unsquared (standard deviation)\n\n[1] 0.6984221\n\nsd(selfes$SELFES, na.rm = T) # \n\n[1] 0.6987572"
  },
  {
    "objectID": "gradstats/gradlabs/3_NormalDistributions.html#the-normal-distribution",
    "href": "gradstats/gradlabs/3_NormalDistributions.html#the-normal-distribution",
    "title": "Lecture 3 : Normal Distributions",
    "section": "The ‚ÄúNormal‚Äù Distribution",
    "text": "The ‚ÄúNormal‚Äù Distribution\n\nWhen do we see a ‚Äúnormal‚Äù distribution?\n\nWhen ‚Äúlife is complex‚Äù (multiple influences on an outcome.)\nThat complexity is independent.\n\n\n\n\nDiscussion : Why is this variable almost Normal?\n\nMultiple & independent influences on people‚Äôs attitudes about psychology as a science?\nNon-independent influences on people‚Äôs attitudes about psychology as a science?\n\n\nhist(selfes$SELFES, col = 'black', bor = 'white')\n\n\n\n\n\n\n\n\n\n\nActivity : Let‚Äôs simulate a normal distribution in R\nCode you might need.\n\ncoinflip &lt;- c(0,1) # defining a coin-flip.\nsample(x, n) # randomly sample from x n times\nreplicate(n, expr) # to repeat an expression n times\n\nProfessor Code Goes Here.\n\ncoinflip &lt;- c(0,1) # defining a coin-flip.\nplot(as.factor(coinflip))\n\n\n\n\n\n\n\n?sample\nsample(coinflip, 1)\n\n[1] 1\n\nreplicate(10, sample(coinflip, 1))\n\n [1] 1 1 0 1 0 1 0 1 1 1\n\nsum(replicate(10, sample(coinflip, 1)))\n\n[1] 6\n\nlife &lt;- array()\nfor(i in c(1:1000)){\n  life[i] &lt;- sum(replicate(10, sample(coinflip, 1)))\n}\nlife\n\n   [1] 5 5 6 6 4 3 7 4 2 2 3 8 5 7 5 5 5 6 4 4 4 7 6 7 7 1 6 4 4 6 4 6 7 4 5 7 4\n  [38] 6 2 5 6 5 3 6 4 2 4 5 4 4 3 5 7 4 4 5 3 4 6 5 4 5 6 8 2 8 4 7 9 6 5 6 6 5\n  [75] 4 2 5 7 4 7 4 3 7 6 4 4 6 6 4 4 3 4 4 5 4 4 8 4 5 6 2 4 6 6 7 6 4 5 6 5 5\n [112] 6 1 3 1 6 6 3 7 8 4 4 5 5 3 5 4 4 4 5 7 3 2 4 7 5 5 6 8 5 6 4 5 5 5 5 4 5\n [149] 4 6 6 7 4 5 3 4 6 4 6 7 4 4 7 3 5 2 5 7 7 3 4 4 7 4 2 2 5 2 6 4 5 1 2 6 6\n [186] 5 6 4 3 5 6 5 4 3 5 7 5 5 6 3 3 5 4 5 5 4 6 5 6 6 4 6 4 6 5 6 7 6 5 5 3 6\n [223] 7 5 6 5 3 2 7 5 8 8 5 4 5 3 6 5 5 4 6 5 3 5 4 5 4 4 6 7 4 7 3 2 3 6 4 3 4\n [260] 5 6 5 7 7 3 5 6 4 3 7 5 4 6 4 4 5 5 4 7 5 5 4 5 6 6 7 6 4 5 7 4 4 4 3 5 5\n [297] 4 7 4 6 4 4 4 6 6 3 7 6 7 7 4 3 4 6 4 3 6 6 7 4 7 5 5 5 5 2 8 5 7 4 1 6 6\n [334] 6 5 3 5 3 6 5 5 5 3 7 6 8 6 4 5 5 4 4 4 4 4 5 5 7 4 5 4 0 9 6 5 5 5 5 2 6\n [371] 9 5 7 3 3 5 7 2 5 5 3 7 4 6 8 4 7 2 6 7 4 4 2 5 4 5 6 4 5 6 3 6 8 5 6 3 6\n [408] 5 5 8 3 5 6 6 2 2 3 5 7 4 3 6 2 5 5 4 7 2 5 3 4 5 5 2 6 5 8 6 6 4 7 2 3 5\n [445] 7 4 6 5 3 3 6 3 6 9 6 3 5 3 6 3 5 5 8 9 7 6 4 4 4 6 4 5 6 5 4 6 7 6 6 5 4\n [482] 3 6 4 8 2 7 5 5 4 5 5 2 3 4 3 7 8 6 5 5 4 2 5 6 5 6 6 4 7 7 3 5 4 5 4 7 2\n [519] 6 5 6 5 5 4 5 3 7 4 3 5 5 7 6 0 7 6 7 3 8 3 5 5 4 4 3 5 4 5 6 7 5 5 6 5 3\n [556] 5 4 6 6 7 4 5 3 6 4 3 4 7 5 7 6 2 6 5 5 2 3 6 7 7 7 4 9 5 5 5 6 5 3 4 5 3\n [593] 4 4 7 7 4 4 5 8 8 8 4 5 6 4 8 6 2 7 7 4 8 3 1 5 1 7 4 5 5 5 5 3 7 6 6 6 3\n [630] 1 2 2 7 7 3 4 6 7 6 4 2 4 8 4 9 5 3 7 5 4 7 7 4 5 7 4 3 5 3 7 5 6 4 5 9 7\n [667] 9 2 3 5 5 5 9 3 3 7 6 7 6 6 6 3 4 6 5 5 8 4 5 7 8 5 7 4 3 5 3 5 3 4 7 5 5\n [704] 4 3 6 4 5 7 4 2 6 4 5 4 5 3 5 7 3 7 6 7 5 6 5 2 7 4 6 6 8 7 5 5 6 6 4 8 5\n [741] 3 4 5 5 8 4 3 6 4 5 3 5 8 7 3 4 5 8 5 6 7 5 5 3 7 8 4 8 3 7 4 4 4 7 6 2 8\n [778] 8 6 2 4 7 2 5 6 4 5 5 8 5 7 5 6 5 5 3 6 4 5 6 4 3 6 4 3 5 5 6 4 8 5 4 5 7\n [815] 4 3 7 3 3 7 6 5 6 6 7 7 7 4 0 5 3 5 3 6 4 6 5 2 7 4 3 3 3 6 6 7 7 7 2 6 6\n [852] 3 3 1 8 7 4 5 6 4 8 3 3 7 4 7 8 7 6 4 6 5 4 2 5 4 6 5 4 3 4 5 5 5 5 6 6 7\n [889] 6 7 6 6 6 5 7 5 6 4 7 3 6 5 6 6 6 4 6 4 5 9 5 5 3 9 4 7 5 5 4 7 4 4 4 6 7\n [926] 5 6 5 6 3 4 6 3 9 4 4 5 6 3 3 7 6 5 6 5 5 5 5 5 6 6 3 5 1 7 6 3 4 6 5 3 4\n [963] 5 5 7 6 5 8 5 5 6 4 6 5 5 7 4 4 4 6 6 9 5 6 5 7 4 3 6 1 7 3 8 5 5 3 4 6 6\n[1000] 7\n\nhist(life)\n\n\n\n\n\n\n\n\nWhat if the Coin is Biased?\nModify the code we worked on in lecture to simulate 1000 coin flips where there‚Äôs a 80% chance of flipping one option. What type of distribution do you expect to see? Why?? Note : the sample() function can take another argument that can adjust the probability.\n\ncoinflip &lt;- c(0,1) # defining a coin-flip.\nplot(as.factor(coinflip))\n\n\n\n\n\n\n\n?sample\nsample(coinflip, 1, prob = c(.3, .7))\n\n[1] 1\n\nunfairlife &lt;- array()\nfor(i in c(1:1000)){\n  unfairlife[i] &lt;- sum(replicate(10, sample(coinflip, 1,  prob = c(.3, .7))))\n}\nunfairlife\n\n   [1]  5  6  8  5  6  9  8  9  5  7  7  6  6  6  4  9  6  9  7  4  7  4  6  7\n  [25] 10  6 10  9  7  9  5  7  7  8  7  9  6  3  8  6  7 10  7  7  9  4  7  6\n  [49]  9  9  5  9  5  6  8  7  7  9  5  6  8  6  9  5  5  7  6  9  9  7  7  7\n  [73]  7  7  7  9  7  8  5  8  5  3  8  9  6  4  6  7  4  9  7  8  7  5  7 10\n  [97]  7  6  3  8  9  7  8  6  8  8  7  8  7  5  5  7  8  5  8  7  9  7  7  3\n [121]  8  9  4  7  8  8  8  6  6  8  9  5  9  8  8  6  7  6  8  6  6  6  6  8\n [145]  7  6  9  9  8  7  9  7  7  8  6  7  6  8  6  7  8 10  6  6  9  6  6  6\n [169]  7  6  7  6  5  6  7  7  8  8  6  8  9  7  5  7  6  5  9  5  8  7  7  7\n [193]  8  8 10  7  6  7  7  7 10  6  5  8  8  8  5  9  9  5 10  7  7  7  7 10\n [217]  5  9  8  5  7  8 10  6  8  8  7  8  9  7  7  6  9  8  6  8 10  6  5  8\n [241]  5  5  6  9  7  8  9  6  9  7 10  8  4  8  6  6  7  6  9  5  8  5  9  7\n [265]  7  7  6  8  9  5  6  7  5  6  8  8  8  9  3  7  7  8  6  7  9  4  5  8\n [289]  8  9  6  8  9  7  7  8  8  9  8  8  5  7  9  5  8  5  9  7 10  7  8  8\n [313]  5  6  7  9  6  8  8  8  6  6  5  9  5  5  7  7  9  7  9  5  8  8  5  4\n [337]  7  7  3  5  7  5  7  5  7  9  5  7  8  8  7  8  7  7  5  6  6  6  5  7\n [361]  8  6  7  6  9  7  6  8  7  7  9  8  8  5  7  9  7  7  9  4  6  7  5  7\n [385]  9  7  9  7  8  6  4  6  7  6  7  8  6  6  4  6  9  7  9  7  7  5  7  7\n [409]  7  7  7  7  8  8  7 10  5  6  5  9  6  7  7  7  6  6  7  7  7  7  6  7\n [433]  6  6  9  9  5  7  7  6  5  6  7  7  9  6  7  9  9  6  8 10  9  7  7  5\n [457]  8  7  7  8  5  7  6  5  7  8  6  7  8  7  8  6  6  6  8  9  7 10  5  8\n [481]  6  6  8  4  7  6  8  5  9  9  6  7  7  9  6  7  9  8  7  8  8  8  6  6\n [505]  8  7  8  6  7  7  5  8  5  8  9  7  6  9  8  7  9  9  6  6  7  6  9  6\n [529]  7  8  8  6  7  7  7  7  6  5 10  8  8  5  8  6  9  9  8  8  8  4  8  7\n [553]  9  7 10  7  7  4  5  5  7  5  7  8  6  5  7  7  6  8  3  5  5  9  6  9\n [577]  7  5  6  8  6  5  7  7  7  7  7  6  6  7  6  6  5  6  5  9  6  6  5  9\n [601]  8  7  8  8  4  9  6  3  6  4  5  6  9  9  7  7  9  7  7  6  4  8  9  6\n [625]  7  7  9  7  7  6  6  8  7  7  8  9  9  6  7  8  7  8  7  8  8  8  5  7\n [649]  4  8  8 10  8  9  8  5  9  9  5  5  5  8  7  7  8  9  6  6  5  8  7  6\n [673]  8  7  4  7  6  5  7  6  4  9  5  8  7  8  7  9  6  6  4  8  9  7 10  8\n [697]  7  5  6 10  5  5  8  7  7  7  4  8  8  8  6  7  9  9  4  9  8  5  8  7\n [721]  7  7  7  6  8  9  5  6  7  8 10  7 10  6  6  6  9  7  8  7  7  7  6  8\n [745]  5  6  9  7  7  7  8  7  8  8  7  5  7  5  3  6  8  6  7  9  8  7  6  6\n [769]  7  6  6  7  7  6  7  8  4  7  7  6  6  5  8  9  7  3  9  7  4  5  6  9\n [793]  6  7  6  7  7  6  8  8  9  7  9  8  8  9  6  8  7  6  6  9  7  7  9  7\n [817]  3  6  6  5  7  8  6  7  5  6  4  4  4  8  5  9  7  7  9  6  4  8  8  6\n [841]  7  7  8  9  6  8  5  7  7  7  7  8  9  5  7  8  7  6  7  7  9  5  8  6\n [865]  7  9  7  7  8  7  8  7  4  8  6  5  6  6  7  5 10  5  7  7  5  7  8  8\n [889]  8 10  7  8  5  6  7  6 10  9  7  6  4  5  6  9  8  8  8  8  7  7  5  7\n [913]  7  8  8  9  7  5  8  8  9  7  7  6  7  6  7  5  5  5  6  6  6  7  6  6\n [937]  5  8  9  8  5  8  6 10  9  8 10  7  7  6  6 10  6  9  7  8  7  8  6  7\n [961]  8  7  6  9  8  7  7  7  9  7  8 10  8  6  9  8  8  6  9  7  7  7  8  6\n [985]  8  5  8  8  9  8  6  7  6  7  6  6  6  7  7  7\n\nhist(unfairlife, xlim = c(0,10))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCritical Race Theory DEI Alert!\n\n\n\nFrancis Galton was a super racist and inventor of eugenics, and influenced (or invented) many statistics that we use today. For example, he defined the ‚Äúcentral limit theorem‚Äù with the Galton Board (see image on the right). Whereas before, scholars considered the ‚Äúaverage‚Äù to be the ideal state of humanity (it is closest to all the people; the Platonic Ideal!), Galton considered the goal of humanity to achieve to be better than average - something we have internalized today.\n\nIndeed, Galton had a motivated agenda to use statistics to demonstrate there was a hierarchy to individual ‚Äúeminence.‚Äù In his own words:\nTo conclude, the range of mental power between‚ÄîI will not say the highest Caucasian and the lowest savage‚Äîbut between the greatest and least of English intellects, is enormous. ‚Ä¶ I propose in this chapter to range men according to their natural abilities, putting them into classes separated by equal degrees of merit, and to show the relative number of individuals included in the several classes‚Ä¶..The method I shall employ for discovering all this, is an application of the very curious theoretical law of ‚Äúdeviation from an average.‚Äù First, I will explain the law, and then I will show that the production of natural intellectual gifts comes justly within its scope. - Galton, Hereditary Genius (1869). Linked here.\nWhy does it matter that a super racist invented statistics? I have a few ideas, but would like to hear your thoughts first :)\n\nreasons relevant :\n\nlots of statistics are based on convention; convention based on history; good to know that history to think about whether we want to accept that convention or not.\ncould value certain metrics\n\ne.g., outliers are ‚Äúbad‚Äù\nwhole idea of ranking people\n\ngood to think about who is using these tools; what tools can do; what does it mean to ‚Äúimprove‚Äù things and make things better.\n\nreasons not :"
  },
  {
    "objectID": "gradstats/gradlabs/3_NormalDistributions.html#sampling-error-conceptual",
    "href": "gradstats/gradlabs/3_NormalDistributions.html#sampling-error-conceptual",
    "title": "Lecture 3 : Normal Distributions",
    "section": "Sampling Error (Conceptual)",
    "text": "Sampling Error (Conceptual)\n\nScientific Method Stuff\n\nSample v. Population\n\nPopulation : All the people relevant to your research question.\nSample : The people in your study.\nKEY IDEA : Our sample will never equal the population!\n\nSampling Bias : our sample differs from the population in predictable ways.\nSampling Error : our sample differs from the population in random ways.\nRandom : Each individual in the population has an equal probability of being in our sample.\n\n\nFor Lab 3 : Find an article; is the sample representative (probably not)? How might bias influence the results?!\n\n\n\nSampling Error in R\n\nBe an omnipotent higher power who can create an entire world of individuals.\n\n\n# rnorm(10000000, mean = 100, sd = 20)\nfakey &lt;- rnorm(10000000, mean = 100, sd = 10)\nlength(fakey)\n\n[1] 10000000\n\nhead(fakey)\n\n[1] 100.64226  83.06206 100.52210  82.61450 106.02953 113.13004\n\n\n\nRun some stats on these data as we do.\n\n\nmean(fakey, na.rm = T)\n\n[1] 100.0023\n\nhist(fakey)\nabline(v = mean(fakey), lwd = 5)\n\n\n\n\n\n\n\n\n\nTake a random sample from this population.\n\n\n?sample # our friend, the sample function\nsample(1:10, 1) # are you vibing with R?\n\n[1] 3\n\nsample(1:length(fakey), 1) # are you REALLY vibing?\n\n[1] 8531181\n\nsample(1:length(fakey), 10) # a small sample\n\n [1] 9061831 2415579 3574285 2685989 5924137 8621906 4400953 5761836 4350454\n[10] 3410270\n\nfakey[sample(1:length(fakey), 10)] # ten random individuals from fakey.\n\n [1]  89.54050  86.78826 106.07174  83.43126  89.59729  96.60157 108.30774\n [8] 110.78022  97.64407 108.48739\n\nfakey[10000001]\n\n[1] NA\n\nlilfakey &lt;- fakey[sample(1:length(fakey), 10)] # ten random individuals from fakey.\nlilfakey\n\n [1]  98.16598 106.64116  94.44091  95.73702 106.73363  95.02213 127.12970\n [8]  94.32900 106.25280  79.58748\n\n\n\nRun some stats on this sample.\n\n\nmean(lilfakey)\n\n[1] 100.404\n\nhist(lilfakey)\nabline(v = mean(lilfakey), lwd = 5)\n\n\n\n\n\n\n\n\n\nRepeat These Steps Until You Get ‚ÄúTHE TRUTH‚Äù\n\n\nlilfakey &lt;- fakey[sample(1:length(fakey), 10)] # ten random individuals from fakey.\nhist(lilfakey, xlim = c(0,200), ylim = c(0,10),\n     breaks = 5,\n     main = paste(c(\"mean=\", round(mean(lilfakey), 4)), sep = \"\"))\nabline(v = mean(lilfakey), lwd = 5)\n\n\n\n\n\n\n\n\n\nDoing this 1000 times\n\ntruthbucket &lt;- array()\nfor(i in c(1:1000)){\n  lilfakey &lt;- fakey[sample(1:length(fakey), 10)] # ten random individuals from fakey.\n  truthbucket[i] &lt;- mean(lilfakey)\n}\nlength(truthbucket)\n\n[1] 1000\n\nhist(truthbucket)\n\n\n\n\n\n\n\nmean(truthbucket)\n\n[1] 99.94711\n\n\n\nWhat‚Äôs the Point, Professor??? (Sampling Error Edition)\n\nThere is a difference between our sample mean and population mean.\n\nthis is bad : if you want to draw a conclusion about a population from your sample, your conclusion based on the sample does not match the population and you are making an error ‚Ä¶.a‚Ä¶..sampling error.\nthis is good :\n\ncool to see the beauty of diversity of human life in action!\na method of finding those ‚Äúoutliers‚Äù we love and care about (and want to study ‚Äì&gt; TED TALK!!!)\nif we just replicate our study 1000 times that are perfectly randomly drawn from the population, we can expect to converge on THE TRUTH!!!!\n\n\nOTHER IDEAS :\n\nThe larger the sample size the less difference between the sample mean and population mean."
  },
  {
    "objectID": "gradstats/gradlabs/3_NormalDistributions.html#break-time-presentations",
    "href": "gradstats/gradlabs/3_NormalDistributions.html#break-time-presentations",
    "title": "Lecture 3 : Normal Distributions",
    "section": "BREAK TIME & PRESENTATIONS",
    "text": "BREAK TIME & PRESENTATIONS\n\nlink to article presentation"
  },
  {
    "objectID": "gradstats/gradlabs/3_NormalDistributions.html#bootstrapping-to-estimate-sampling-error",
    "href": "gradstats/gradlabs/3_NormalDistributions.html#bootstrapping-to-estimate-sampling-error",
    "title": "Lecture 3 : Normal Distributions",
    "section": "Bootstrapping to Estimate Sampling Error",
    "text": "Bootstrapping to Estimate Sampling Error\nOkay, let‚Äôs work through a real example of using bootstrapping to estimate sampling error, and why this might be useful.\nRemember that in the grad onboarding survey, we saw people rated their own skills as lower than their classmates‚Äô skills.\n\nd &lt;- read.csv(\"../datasets/Grad Onboard 2025/grad_onboard_SP25.csv\", stringsAsFactors = T, na.strings = \"\")\npar(mfrow = c(1,2))\n\nhist(d$self.skills, breaks = c(0:5), \n     col = 'black', bor = 'white', main = \"Computer Skills\\n(Self-Perceptions)\")\n\nhist(d$class.skills, breaks = c(0:5),\n     col = 'black', bor = 'white', main = \"Computer Skills\\n(Perceptions of Classmates)\")\n\n\n\n\n\n\n\n\nBut would we expect to observe this same difference in a different sample of graduate students???\nLet‚Äôs use bootstrapping to test this.\n\nmean(d$self.skills)\n\n[1] 3.424242\n\nsample(1:10, 10, replace = T)\n\n [1]  7  3  7  3  2  6  9  8  6 10\n\nd[,] # two dimensional dataset \n\n                         time has.laptop write.code know.prog has.data know.r\n1   2025/01/17 3:43:14 PM PST        Yes        Yes       Yes       No      3\n2   2025/01/17 3:53:37 PM PST        Yes        Yes       Yes      Yes      3\n3   2025/01/17 4:04:11 PM PST        Yes        Yes       Yes      Yes      2\n4   2025/01/17 4:18:32 PM PST        Yes        Yes       Yes      Yes      2\n5   2025/01/17 5:15:26 PM PST        Yes        Yes       Yes      Yes      3\n6   2025/01/17 5:25:38 PM PST        Yes        Yes       Yes       No      3\n7   2025/01/17 5:44:44 PM PST        Yes        Yes       Yes       No      3\n8   2025/01/17 6:08:27 PM PST        Yes        Yes       Yes      Yes      3\n9   2025/01/17 7:00:45 PM PST        Yes        Yes       Yes      Yes      3\n10  2025/01/17 7:02:36 PM PST        Yes        Yes       Yes      Yes      4\n11 2025/01/18 10:28:15 PM PST        Yes        Yes       Yes      Yes      3\n12  2025/01/19 5:49:05 AM PST        Yes        Yes       Yes      Yes      3\n13  2025/01/19 3:06:24 PM PST        Yes         No        No       No      2\n14  2025/01/19 3:53:43 PM PST        Yes        Yes       Yes       No      3\n15 2025/01/20 10:06:19 AM PST        Yes        Yes       Yes       No      2\n16 2025/01/20 10:33:04 AM PST        Yes        Yes       Yes      Yes      3\n17  2025/01/20 7:18:59 PM PST        Yes         No        No      Yes      2\n18  2025/01/21 9:40:26 AM PST         No         No        No       No      2\n19 2025/01/21 12:17:24 PM PST        Yes        Yes       Yes      Yes      4\n20  2025/01/21 4:16:53 PM PST        Yes        Yes       Yes      Yes      4\n21  2025/01/21 5:28:40 PM PST        Yes         No        No       No      1\n22 2025/01/22 10:15:09 AM PST        Yes         No       Yes       No      1\n23 2025/01/22 11:44:15 AM PST        Yes        Yes       Yes      Yes      1\n24  2025/01/23 2:50:15 PM PST        Yes        Yes       Yes      Yes      2\n25  2025/01/23 3:11:23 PM PST        Yes        Yes       Yes      Yes      3\n26  2025/01/23 7:50:10 PM PST        Yes        Yes       Yes      Yes      3\n27 2025/01/23 10:20:39 PM PST        Yes        Yes       Yes      Yes      2\n28 2025/01/24 12:31:17 AM PST        Yes        Yes       Yes      Yes      4\n29  2025/01/24 3:05:33 AM PST        Yes        Yes       Yes      Yes      2\n30  2025/01/24 7:57:01 AM PST        Yes         No        No       No      1\n31  2025/01/24 8:02:47 AM PST        Yes        Yes       Yes      Yes      3\n32  2025/01/24 8:23:39 AM PST        Yes        Yes       Yes      Yes      3\n33  2025/01/28 2:53:16 PM PST        Yes        Yes       Yes      Yes      4\n   can.import can.clean can.graph can.render can.lm can.interp can.pvalue\n1         Yes       Yes       Yes      Maybe     No      Maybe        Yes\n2         Yes       Yes       Yes        Yes    Yes        Yes      Maybe\n3         Yes       Yes       Yes        Yes    Yes        Yes        Yes\n4         Yes     Maybe       Yes      Maybe    Yes        Yes      Maybe\n5         Yes       Yes       Yes        Yes    Yes        Yes        Yes\n6         Yes       Yes       Yes      Maybe  Maybe        Yes        Yes\n7         Yes       Yes       Yes      Maybe  Maybe        Yes      Maybe\n8         Yes       Yes     Maybe      Maybe  Maybe        Yes      Maybe\n9         Yes     Maybe       Yes         No    Yes        Yes        Yes\n10        Yes       Yes       Yes        Yes    Yes        Yes        Yes\n11        Yes       Yes       Yes        Yes  Maybe      Maybe        Yes\n12        Yes     Maybe     Maybe         No  Maybe      Maybe      Maybe\n13      Maybe        No        No         No     No         No        Yes\n14        Yes       Yes       Yes      Maybe    Yes        Yes        Yes\n15        Yes     Maybe     Maybe      Maybe  Maybe         No      Maybe\n16        Yes       Yes       Yes        Yes  Maybe        Yes        Yes\n17      Maybe     Maybe     Maybe         No     No        Yes        Yes\n18        Yes        No     Maybe         No     No      Maybe      Maybe\n19        Yes       Yes     Maybe      Maybe    Yes        Yes        Yes\n20        Yes       Yes       Yes        Yes    Yes        Yes        Yes\n21         No        No        No         No     No         No         No\n22         No        No        No         No     No         No         No\n23      Maybe     Maybe        No         No     No        Yes      Maybe\n24      Maybe     Maybe        No         No     No        Yes        Yes\n25        Yes       Yes       Yes        Yes    Yes        Yes        Yes\n26        Yes     Maybe     Maybe         No  Maybe      Maybe        Yes\n27        Yes       Yes       Yes        Yes   &lt;NA&gt;        Yes        Yes\n28        Yes       Yes       Yes      Maybe    Yes        Yes        Yes\n29      Maybe        No     Maybe         No  Maybe         No        Yes\n30        Yes       Yes       Yes      Maybe  Maybe      Maybe      Maybe\n31        Yes     Maybe     Maybe        Yes  Maybe      Maybe        Yes\n32      Maybe     Maybe     Maybe      Maybe  Maybe      Maybe      Maybe\n33        Yes       Yes       Yes        Yes    Yes        Yes        Yes\n   can.sevsd can.95ci can.forloop self.skills class.skills self.excite\n1         No    Maybe       Maybe           4            4           5\n2        Yes    Maybe          No           4            4           5\n3        Yes      Yes         Yes           5            3           3\n4      Maybe    Maybe       Maybe           3            4           4\n5        Yes      Yes         Yes           5            3           3\n6        Yes      Yes         Yes           5            4           4\n7      Maybe    Maybe          No           4            5           5\n8      Maybe    Maybe         Yes           3            4           3\n9         No    Maybe       Maybe           4            4           5\n10     Maybe      Yes         Yes           5            5           4\n11       Yes    Maybe          No           4            4           4\n12     Maybe    Maybe       Maybe           2            4           5\n13       Yes      Yes          No           3            5           5\n14       Yes      Yes       Maybe           3            3           5\n15        No    Maybe       Maybe           3            5           4\n16       Yes      Yes       Maybe           3            3           4\n17     Maybe      Yes          No           4            4           5\n18        No    Maybe          No           3            4           4\n19     Maybe    Maybe       Maybe           5            5           5\n20       Yes    Maybe          No           3            4           5\n21        No       No          No           1            3           3\n22        No       No          No           1            5           5\n23     Maybe      Yes       Maybe           5            5           5\n24       Yes      Yes         Yes           2            2           5\n25     Maybe      Yes       Maybe           4            4           5\n26     Maybe      Yes          No           3            4           4\n27     Maybe    Maybe       Maybe           3            4           5\n28       Yes      Yes         Yes           5            4           5\n29     Maybe    Maybe       Maybe           3            4           5\n30     Maybe    Maybe       Maybe           2            4           3\n31     Maybe      Yes       Maybe           3            3           4\n32        No       No       Maybe           2            1           1\n33       Yes      Yes         Yes           4            4           5\n   class.excite self.prep class.prep learn.concept learn.r can.science\n1             4         5          5             5       5           4\n2             5         3          4             4       5           4\n3             3         5          5             4       5           5\n4             4         3          4             3       3           4\n5             3         5          3             5       5           5\n6             4         4          4             4       5           4\n7             5         4          5             4       5           5\n8             3         4          4             4       4           5\n9             4         5          5             5       5           3\n10            3         5          5             4       5           5\n11            3         3          4             4       3           5\n12            5         4          5             4       4           4\n13            4         3          5             4       3           4\n14            5         4          4             5       4           5\n15            2         2          4             4       3           4\n16            4         4          4             4       4           4\n17            5         4          4             5       5           4\n18            4         3          4             4       4           5\n19            5         4          5             4       5           5\n20            4         4          4             4       4           4\n21            3         2          3             3       3           5\n22            5         2          5             3       2           5\n23            5         5          5             5       4           5\n24            4         4          4             4       3           4\n25            4         4          4             5       5           5\n26            4         4          4             4       3           5\n27            4         5          4             5       5           5\n28            4         5          5             4       5           5\n29            4         4          5             5       4           4\n30            3         3          4             4       4           4\n31            4         4          4             4       4           4\n32            1         2          1             1       2           3\n33            4         4          5             4       4           3\n   should.science truth.people not.psychsci perf.pred noperf.pred too.complex\n1               4            3            1         1           5           4\n2               5            3            2         3           4           3\n3               5            2            3         2           4           4\n4               3            4            1         2           3           3\n5               5            1            3         1           5           5\n6               4            4            2         3           4           5\n7               3            5            3         4           2           3\n8               5            3            1         2           4           4\n9               3            2            4         3           4           4\n10              5            3            1         4           4           5\n11             NA            2            1         1           4           3\n12              4            4            3         4           2           2\n13              5            4            1         1           5           5\n14              5            5            1         5           1           3\n15              4            3            1         3           3           4\n16              4            4            2         3           4           4\n17              4            2            1         1           5           4\n18              4            4            1         4           2           4\n19              5            5            1         5           1           1\n20              4            2            2         2           4           5\n21              4            2            1         1           5           5\n22              5            5            3         2           3           4\n23              5            5            1         2           4           1\n24              4            3            2         3           3           4\n25              4            4            1         1           5           2\n26              4            3            1         3           4           4\n27              5            3            1         3           3           2\n28              5            5            2         5           1           1\n29              3            4            1         3           4           2\n30              4            3            2         3           3           4\n31              4            3            2         2           4           4\n32              2            3            2         3           1           1\n33              3            1            3         1           5           5\n                                                                                                                                             epistemology\n1          Reality exists, but we may not be able to ever understand REAL TRUTH because our ability to study reality will always be influenced by people.\n2          Reality exists, but we may not be able to ever understand REAL TRUTH because our ability to study reality will always be influenced by people.\n3          Reality exists, but we may not be able to ever understand REAL TRUTH because our ability to study reality will always be influenced by people.\n4          Reality exists, but we may not be able to ever understand REAL TRUTH because our ability to study reality will always be influenced by people.\n5          Reality exists, but we may not be able to ever understand REAL TRUTH because our ability to study reality will always be influenced by people.\n6          Reality exists, but we may not be able to ever understand REAL TRUTH because our ability to study reality will always be influenced by people.\n7                                                           There is a REAL TRUTH about what makes people think, feel, and act that we can someday learn.\n8          Reality exists, but we may not be able to ever understand REAL TRUTH because our ability to study reality will always be influenced by people.\n9            Reality and ‚Äútruth‚Äù are made up by people (and scientists); scientific knowledge is just a reflection of our society‚Äôs pre-existing beliefs.\n10         Reality exists, but we may not be able to ever understand REAL TRUTH because our ability to study reality will always be influenced by people.\n11         Reality exists, but we may not be able to ever understand REAL TRUTH because our ability to study reality will always be influenced by people.\n12         Reality exists, but we may not be able to ever understand REAL TRUTH because our ability to study reality will always be influenced by people.\n13         Reality exists, but we may not be able to ever understand REAL TRUTH because our ability to study reality will always be influenced by people.\n14         Reality exists, but we may not be able to ever understand REAL TRUTH because our ability to study reality will always be influenced by people.\n15         Reality exists, but we may not be able to ever understand REAL TRUTH because our ability to study reality will always be influenced by people.\n16         Reality exists, but we may not be able to ever understand REAL TRUTH because our ability to study reality will always be influenced by people.\n17           Reality and ‚Äútruth‚Äù are made up by people (and scientists); scientific knowledge is just a reflection of our society‚Äôs pre-existing beliefs.\n18         Reality exists, but we may not be able to ever understand REAL TRUTH because our ability to study reality will always be influenced by people.\n19                                                          There is a REAL TRUTH about what makes people think, feel, and act that we can someday learn.\n20           Reality and ‚Äútruth‚Äù are made up by people (and scientists); scientific knowledge is just a reflection of our society‚Äôs pre-existing beliefs.\n21           Reality and ‚Äútruth‚Äù are made up by people (and scientists); scientific knowledge is just a reflection of our society‚Äôs pre-existing beliefs.\n22         Reality exists, but we may not be able to ever understand REAL TRUTH because our ability to study reality will always be influenced by people.\n23           Reality and ‚Äútruth‚Äù are made up by people (and scientists); scientific knowledge is just a reflection of our society‚Äôs pre-existing beliefs.\n24         Reality exists, but we may not be able to ever understand REAL TRUTH because our ability to study reality will always be influenced by people.\n25         Reality exists, but we may not be able to ever understand REAL TRUTH because our ability to study reality will always be influenced by people.\n26         Reality exists, but we may not be able to ever understand REAL TRUTH because our ability to study reality will always be influenced by people.\n27                                                          There is a REAL TRUTH about what makes people think, feel, and act that we can someday learn.\n28                                                          There is a REAL TRUTH about what makes people think, feel, and act that we can someday learn.\n29                                                          There is a REAL TRUTH about what makes people think, feel, and act that we can someday learn.\n30           Reality and ‚Äútruth‚Äù are made up by people (and scientists); scientific knowledge is just a reflection of our society‚Äôs pre-existing beliefs.\n31         Reality exists, but we may not be able to ever understand REAL TRUTH because our ability to study reality will always be influenced by people.\n32 There is no reality or ‚Äútruth‚Äù to what people are like, and trying to understand ‚Äútruth‚Äù removes autonomy from people (it turns people into machines).\n33           Reality and ‚Äútruth‚Äù are made up by people (and scientists); scientific knowledge is just a reflection of our society‚Äôs pre-existing beliefs.\n                                             consent\n1  yes, you can add my responses to a class dataset.\n2  yes, you can add my responses to a class dataset.\n3  yes, you can add my responses to a class dataset.\n4  yes, you can add my responses to a class dataset.\n5  yes, you can add my responses to a class dataset.\n6  yes, you can add my responses to a class dataset.\n7  yes, you can add my responses to a class dataset.\n8  yes, you can add my responses to a class dataset.\n9  yes, you can add my responses to a class dataset.\n10 yes, you can add my responses to a class dataset.\n11 yes, you can add my responses to a class dataset.\n12 yes, you can add my responses to a class dataset.\n13 yes, you can add my responses to a class dataset.\n14 yes, you can add my responses to a class dataset.\n15 yes, you can add my responses to a class dataset.\n16 yes, you can add my responses to a class dataset.\n17 yes, you can add my responses to a class dataset.\n18 yes, you can add my responses to a class dataset.\n19 yes, you can add my responses to a class dataset.\n20 yes, you can add my responses to a class dataset.\n21 yes, you can add my responses to a class dataset.\n22 yes, you can add my responses to a class dataset.\n23 yes, you can add my responses to a class dataset.\n24 yes, you can add my responses to a class dataset.\n25 yes, you can add my responses to a class dataset.\n26 yes, you can add my responses to a class dataset.\n27 yes, you can add my responses to a class dataset.\n28 yes, you can add my responses to a class dataset.\n29 yes, you can add my responses to a class dataset.\n30 yes, you can add my responses to a class dataset.\n31 yes, you can add my responses to a class dataset.\n32 yes, you can add my responses to a class dataset.\n33      I am the professor and you should remove me.\n\nnd &lt;- d[sample(1:nrow(d), nrow(d), replace = T),] # sampling w/ replacement (rows)\nmean(d$self.skills)\n\n[1] 3.424242\n\nmean(nd$self.skills)\n\n[1] 3.909091\n\nnewmean &lt;- array()\nfor(i in c(1:10000)){\n  nd &lt;- d[sample(1:nrow(d), nrow(d), replace = T),] # sampling w/ replacement (rows)\n  newmean[i] &lt;- mean(nd$self.skills)\n}\n\nhist(d$self.skills)\n\n\n\n\n\n\n\nhist(newmean)\n\n\n\n\n\n\n\nmean(d$self.skills)\n\n[1] 3.424242\n\nmean(newmean)\n\n[1] 3.423979\n\nsd(newmean)\n\n[1] 0.1953151\n\n\n\nFor Lab 3 :\n\nKeep getting practice working with datasets, Quarto, and interpreting variables in R.\nUse (and adapt) the bootstrapping for-loop to estimate how much sampling error we can expect in variables, and in the difference between variables."
  },
  {
    "objectID": "calstats/Lectures/3L_Description.html#check-in-tinyurl.cominterruptlifewithr",
    "href": "calstats/Lectures/3L_Description.html#check-in-tinyurl.cominterruptlifewithr",
    "title": "Class 3 | Description",
    "section": "",
    "text": "link to RScript [also on course page]\ndata are here [also on the data dropbox folder on bCourses]\nname the dataset d to follow along w/ professor\n\n\n\nProfessor Check-In Code Goes Here\n\n## CHECK-IN : the interruption dataset\n##  Download the \"interruption\" dataset and import the data to RStudio.\n\n# Q1. How many individuals are in this dataset?\n\n# Q2. How many variables are in this dataset?\n\n# Q3. What is the value of the second row, third column of this dataset?\n\n# Q4. How many individuals said \"yes, I can relax my feet\"?\n\n# Q5. How many individuals said \"no, my feet were already totally relaxed\"?\n\n# BONUS QUESTION (AND DEMONSTRATION): I'm going to randomly call on 10 students.\n## I'll ask them to report the number of instagram accounts they follow. \n## Predict these 10 numbers by filling in your guesses below.\n\n## my guesses : \n\nAgenda and Announcements\n\n2:10 - 2:30 | Check-In & Announcements\n2:30 - 3:30 | Describing Data in R (and Removing Outliers)\n3:45 - 4:00 | BREAK TIME\n4:00 - 4:20 | Student Show and Tell\n4:20 - 5:00 | Final Project Workshop"
  },
  {
    "objectID": "gradstats/gradlabs/3Lab_SampleErrorBias.html",
    "href": "gradstats/gradlabs/3Lab_SampleErrorBias.html",
    "title": "Lab 3: Sampling Error and Bias",
    "section": "",
    "text": "Answer the questions below as a .PDF (you can either render from Quarto as .PDF, or render as .html and then print this html file as a .PDF). Make sure to reference any external sources (e.g., stack exchange; ChatGPT; peer help) that you used at the top of your lab assignment.\n\nIn Lab 2, we worked with the self-esteem dataset. We will work with these data again in Lab 3. Feel free to re-use your code from Lab 2, or look at & learn from my key (posted to the course page) as needed.\n\nLoad the data again and check to make sure the data loaded correctly.\nGraph the variable age as a histogram, and report the mean, median, standard deviation, and range. What do these statistics tell you about the data?\nRemove these outliers in age from the dataset, and explain why you considered them to be outliers. Graph the newly-cleaned variable as a histogram, and report the mean, median, standard deviation, and range.\nDescribe what you learn about this variable (and the participants in the sample) from this distribution. Do these data appear representative of the population? Why / why not?\n\nNow, use bootstrapping to estimate the sampling error of the age variable above. (In other words, resample the selfesteem dataset, calculate the average age from this ‚Äúnew‚Äù sample, save the value somewhere, and then repeat this 1000 times). Graph these 1000 average ages, and report the mean of this distribution of sample estimates (it should be very close to the mean of age in the original sample) and the standard deviation of the distribution of sample estimates. Why is the standard deviation of the distribution of sample estimates so small? Why is the graph of these 1000 average ages normal when the original distribution of scores is skewed??? Make sure you‚Äôve removed the outliers.\nIn Lab 2, we created a self-esteem scale from the 10-item Rosenberg (1965) self-esteem scale. The mean of this variable was higher than the mid-point of the scale (2.5), suggesting that people (on average) tend to see them a little more positively than truly neutral. Another possibility is that this difference is just due to sampling error. Create the scale again (you can copy / paste your code from Lab 2, or use / learn from the key if you struggled with this.) Then, use bootstrapping to resample the original dataset 1000 times, and estimate the average sampling error of the mean. Report the estimate of sampling error, and the 95% Confidence Interval of the mean. Do these statistics give you more or less confidence that people tend to see themselves as higher in self-esteem (than the mid-point)? Why / why not?\nFind an empirical article relevant to your research interests. This should be an original study, where the authors collected their own data. Submit your answers to this question as part of this lab, and also share your ideas in the 205 Vision Board. (You‚Äôll have to log in to your @berkeley.edu google account to edit the Google Sheet.)\n\nExport the article citation in APA format.\nWhat is the reseach question & theory. Just explain in a few words what the authors are studying.\nWho is the Population for this research question?\nWho was in the sample of this study? Report whatever characteristics were reported in the article. If there were multiple studies in the article, just pick one.\nIs the sample representative of the population? Why / Why Not?\nHow might this bias influence the results? Be specific here! Okay not to know, but come up with a few ideas."
  },
  {
    "objectID": "gradstats/gradlabs/3_NormalDistributions.html#break-time-meet-back-at-1045-presentations",
    "href": "gradstats/gradlabs/3_NormalDistributions.html#break-time-meet-back-at-1045-presentations",
    "title": "Lecture 3 : Normal Distributions",
    "section": "BREAK TIME MEET BACK AT 10:45 & PRESENTATIONS",
    "text": "BREAK TIME MEET BACK AT 10:45 & PRESENTATIONS\n\nlink to article presentation"
  },
  {
    "objectID": "calstats/labs/Lab3.html",
    "href": "calstats/labs/Lab3.html",
    "title": "Lab 3",
    "section": "",
    "text": "Use R to answer the following questions. Make sure that you show your R code and the output (result or graphs.) You can do this by taking a screenshot and pasting into your Google / Word Document, or (for students wanting an extra challenge) working in a Quarto document."
  },
  {
    "objectID": "calstats/labs/Lab3.html#problem-1.-interrpution-problems.",
    "href": "calstats/labs/Lab3.html#problem-1.-interrpution-problems.",
    "title": "Lab 3",
    "section": "Problem 1. Interrpution Problems.",
    "text": "Problem 1. Interrpution Problems.\nDownload the ‚Äúinterruption‚Äù data from bCourses, and import this data into R. This dataset has two variables of the number of interruptions counted before (int1) and after (int2) our operationalization.\n\nLoad the data (I‚Äôll call it d if you want to follow along with my code), check to make sure it loaded correctly, and report the sample size and names of the variables.\nGraph these variables as a histogram (use the par() function to graph them side by side). Change the arguments so the graphs have the same x-axis and y-axis ranges, and nice labels.\nReport the mean, median, range, and standard deviation for both variables. Then, calculate the standard deviation ‚Äúby hand‚Äù for int1 (you should get a similar, but not exact, number, as what R gives you.)\nDescribe how these statistics changed after operationalizing an interruption, and why these changes make sense given the nature of our operationalizations. Then, decide whether our operationalization would be good enough if we were researchers trying to scientifically study interruptions.\nGraph a categorical variable from the dataset and report the frequency of each group."
  },
  {
    "objectID": "calstats/labs/Lab3.html#problem-2.-mini-problems",
    "href": "calstats/labs/Lab3.html#problem-2.-mini-problems",
    "title": "Lab 3",
    "section": "Problem 2. Mini Problems",
    "text": "Problem 2. Mini Problems\nLoad the mini dataset (that you used in Lab 2), check to make sure the data loaded correctly and use this dataset to answer the questions below.\n\nFocus on the variable insta.followers - this measures the number of followers a person says they have on instagram (a social networking website owned by Mark Zuckerberg). Graph this variable as a histogram, and report the mean, median, standard deviation, and range. Make sure to do any necessary data cleaning (e.g., outlier removal), make the graph look nice, and draw vertical lines to illustrate the mean, median, and standard deviation (above and below the mean). Below the graph, describe what each statistic teaches you about the people in the dataset.\nChoose another numeric variable from the dataset (see the codebook for a guide). Graph this variable as a histogram, and report the mean, median, standard deviation, and range. Make sure to do any necessary data cleaning (e.g., outlier removal), make the graph look nice, and describe what each descriptive statistic teaches you about the people in the dataset (you do not need to draw them again on the graph, unless you want to / find this helpful!)"
  },
  {
    "objectID": "calstats/labs/Lab3.html#problem-3-in-discussion-section.-more-description-problems.",
    "href": "calstats/labs/Lab3.html#problem-3-in-discussion-section.-more-description-problems.",
    "title": "Lab 3",
    "section": "Problem 3 (In Discussion Section). More Description Problems.",
    "text": "Problem 3 (In Discussion Section). More Description Problems.\nRepeat the steps for Problem 2, with another dataset (and variable) from the class datasets."
  },
  {
    "objectID": "calstats/labs/Lab3.html#problem-4-in-discussion-section.-mode-problems",
    "href": "calstats/labs/Lab3.html#problem-4-in-discussion-section.-mode-problems",
    "title": "Lab 3",
    "section": "Problem 4 (In Discussion Section). Mode Problems",
    "text": "Problem 4 (In Discussion Section). Mode Problems\nR does not have a built-in function to calculate the mode. Use your favorite search engine to find a method - make sure to cite your source, and describe what you learned from the code that you found. Then check that your method works by defining two variables in R - one with a set of numbers that has one mode, and one with a set of numbers that has two modes. [For example : variable1 &lt;- c(1, 1, 2) has two modes]. Use the mode function on each variable to confirm that the mode function works. Then, use the mode function on the variable you focused on in Problem 3."
  },
  {
    "objectID": "calstats/Lectures/3L_Description.html#break-time-meet-back-at-352",
    "href": "calstats/Lectures/3L_Description.html#break-time-meet-back-at-352",
    "title": "Class 3 | Description",
    "section": "BREAK TIME : MEET BACK AT 3:52",
    "text": "BREAK TIME : MEET BACK AT 3:52"
  },
  {
    "objectID": "calstats/Lectures/4L_Scales.html",
    "href": "calstats/Lectures/4L_Scales.html",
    "title": "Class 4 | Scales",
    "section": "",
    "text": "check-in uses mini-data from Lab 3 (in the Dropbox Dataset Folder)\nlink to professor R script\n\n ##\n\n\n\nMini Exam is in TWO weeks [2/28]\n\ntake home; open-note; open-book; DO ON YOUR OWN.\nheld during normal class; 80 minutes (DSP students get extra time accommodations)\npractice exam will post next week\nNo new R content after today\n\n2/17 is a Holiday\n\nMonday Sections : attend extra GSI office hour OR jump to another GSIs section [email them for heads up]; no penalty if you can‚Äôt attend / makeup.\nEveryone Else : Business as Usual.\n\nChapter 5 : Good Science\n\nGood Knowledge : The Scientific Method in Five Easy Steps\nGood Research : Scientific Articles and Literature Reviews\nGood Measures : Reliability and Validity\n\n\n\n\n\n\n2:10 - 3:00 | Check-In and Week 3 Review\n3:00 - 3:30 | Z-Scores\n3:30 - 3:42 | BREAK TIME\n3:42 - 4:45 | Likert Scales\n4:45 - 5:00 | Project Workshop / Office Hours"
  },
  {
    "objectID": "calstats/Lectures/4L_Scales.html#check-in-complete-this-survey",
    "href": "calstats/Lectures/4L_Scales.html#check-in-complete-this-survey",
    "title": "Class 4 | Scales",
    "section": "",
    "text": "check-in uses mini-data from Lab 3 (in the Dropbox Dataset Folder)\nlink to professor R script\n\n ##\n\n\n\nMini Exam is in TWO weeks [2/28]\n\ntake home; open-note; open-book; DO ON YOUR OWN.\nheld during normal class; 80 minutes (DSP students get extra time accommodations)\npractice exam will post next week\nNo new R content after today\n\n2/17 is a Holiday\n\nMonday Sections : attend extra GSI office hour OR jump to another GSIs section [email them for heads up]; no penalty if you can‚Äôt attend / makeup.\nEveryone Else : Business as Usual.\n\nChapter 5 : Good Science\n\nGood Knowledge : The Scientific Method in Five Easy Steps\nGood Research : Scientific Articles and Literature Reviews\nGood Measures : Reliability and Validity\n\n\n\n\n\n\n2:10 - 3:00 | Check-In and Week 3 Review\n3:00 - 3:30 | Z-Scores\n3:30 - 3:42 | BREAK TIME\n3:42 - 4:45 | Likert Scales\n4:45 - 5:00 | Project Workshop / Office Hours"
  },
  {
    "objectID": "calstats/Lectures/4L_Scales.html#part-1-the-z-score-scale-function",
    "href": "calstats/Lectures/4L_Scales.html#part-1-the-z-score-scale-function",
    "title": "Class 4 | Scales",
    "section": "Part 1 : The Z-Score (Scale Function)",
    "text": "Part 1 : The Z-Score (Scale Function)\n\nRECAP : What and Why Z-Score?\nThe Z-Score\n\ndistance from the mean\nin units of standard deviation\n\nWhy We Care :\n\ngives more context for how different an individual score is from other scores.\n\nexample : work for Meta and get paid an above average salary in zuckbucks!!!\n\nhow much above average???\nwtf is a difference of 10? Is that a lot or a little???\n\nexample : your sleep last night (compared to others)?\n\nmean sleep = 7\nsd sleep = 1.5\nstudent with 3 hours of sleep; mean = 7; sd = 1.5 . Z-SCORE = (3 - 7)/1.5\n\n\nremoves the units from a variable :\n\nmean of a z-scored variable will always be zero\nst of a z-scored variable will always be 1\nall variables described in units of standard deviation In R : The Z-Score\n\n\n\n\nZ-Scoring in R\nYou can calculate the z-score manually, or you can use the scale() function.\n\n## Manual Z-Score : distance from the mean / sd\nmini$hrs.sleepZM &lt;- (mini$hrs.sleep - mean(mini$hrs.sleep, na.rm = T))/ sd(mini$hrs.sleep, na.rm = T)\n\n## The scale() function\nmini$hrs.sleepZ &lt;- scale(mini$hrs.sleep) # the z-score transformation\n\n## Comparing these calculations side-by-side\nzcompare &lt;- with(mini, data.frame(hrs.sleep, hrs.sleepZM, hrs.sleepZ)) # organizing our two variables\nhead(zcompare) # looking at the first few rows\n\n  hrs.sleep hrs.sleepZM  hrs.sleepZ\n1       7.0 -0.05924517 -0.05924517\n2       6.0 -0.73228452 -0.73228452\n3       8.0  0.61379417  0.61379417\n4       7.5  0.27727450  0.27727450\n5        NA          NA          NA\n6       7.0 -0.05924517 -0.05924517\n\n\nIt might be easier to look at this in graphical form. In the graph below,\n\n\nClick Here to Show the Code\n## A PICTURE IS WORTH 1000 WORDS : Raw Scores vs. Z-Scores\n## REMEMBER : IT IS USUALLY BEST TO USE hist() FOR GRAPHING NUMERIC VARIABLES!\n## I AM USING PLOT TO ILLUSTRATE THE MEAN, STANDARD DEVIATION, AND Z-SCORES.\npar(mfrow = c(1,2)) # splits the graphics window\nplot(mini$hrs.sleep, main = \"Hours of Sleep [Raw Units]\", ylab = \"Hours of Sleep\")\nabline(h = mean(mini$hrs.sleep, na.rm = T), lwd = 5) # vertical line of mean\nabline(h = mean(mini$hrs.sleep, na.rm = T) + sd(mini$hrs.sleep, na.rm = T), # vertical line of mean + sd\n       lwd = 2, lty = 'dashed')\nabline(h = mean(mini$hrs.sleep, na.rm = T) - sd(mini$hrs.sleep, na.rm = T), # vertical line of mean - sd\n       lwd = 2, lty = 'dashed')\n\n## Z-SCORED GRAPH. WAIT TO RUN!!!! :)\nplot(mini$hrs.sleepZ, main = \"Hours of Sleep [Z-Scores]\", xlab = \"Hours of Sleep\")\nabline(h = mean(mini$hrs.sleepZ, na.rm = T), lwd = 5) # vertical line of mean\nabline(h = mean(mini$hrs.sleepZ, na.rm = T) + sd(mini$hrs.sleepZ, na.rm = T), # vertical line of mean + sd\n       lwd = 2, lty = 'dashed')\nabline(h = mean(mini$hrs.sleepZ, na.rm = T) - sd(mini$hrs.sleepZ, na.rm = T), # vertical line of mean - sd\n       lwd = 2, lty = 'dashed')\n\n\n\n\n\n\n\n\n\nMiscelaneous Student Questions and Professor Answers\n\nGo Here."
  },
  {
    "objectID": "calstats/Lectures/4L_Scales.html#part-2-likert-scales",
    "href": "calstats/Lectures/4L_Scales.html#part-2-likert-scales",
    "title": "Class 4 | Scales",
    "section": "Part 2 : Likert Scales",
    "text": "Part 2 : Likert Scales\n\nRECAP : The What and Why Likert Scale?\n\n\nIn R : Creating a Likert Scale\n\n\nProject Workshop : You Need a Likert Scale!"
  },
  {
    "objectID": "gradstats/gradlabs/4_LinearModels.html",
    "href": "gradstats/gradlabs/4_LinearModels.html",
    "title": "Lecture 4 | Linear Models",
    "section": "",
    "text": "Load the grad onboarding dataset (name this d to follow along with professor code in lecture). The variable can.forloop asked students whether they could write a for-loop or not. What is the difference in the number of students who said that YES they could for-loop, compared to the number who said either NO or MAYBE? Find a way to get R to calculate this difference using code (hint : use indexing and the summary function.)\nNow, write a for-loop to estimate how much sampling error might influence this number. Generate 1000 new samples, and re-calculate the difference between YES and NO + MAYBEs. What percentage of re-sampled groups would show that there are more people who CAN write a for-loop than NO or MAYBE?\nA bonus question. R wrote you a secret message. Run the code below to see it.\n\n\ngreyScale &lt;- colorRampPalette(c(\"pink\",\"red\"))\n\n# function to draw shape\nsecretmessage &lt;- function(r, col){\n  t &lt;- seq(0,2*pi,length.out=100)\n  x &lt;- r*sin(t)^3\n  y &lt;- (13*r/16)*cos(t) - (5*r/16)*cos(2*t) - (2*r/16)*cos(3*t) - (r/16)*cos(4*t)\n  polygon(x,y,col=col,border=NA)\n}\n\n# create new plot canvas\nplot.new()\n# limits are approximate here\nplot.window(xlim=c(-16,16),ylim=c(-16,13))\n\n# use mapply to loop; invisible to turn off an annoying output.\ninvisible(mapply(secretmessage,seq(16,0,length.out=100),greyScale(100)))\n\n## source : https://stackoverflow.com/questions/6542825/equation-driven-smoothly-shaded-concentric-shapes\n## https://stackoverflow.com/questions/12984991/stop-lapply-from-printing-to-console\n\n\n\n\n\n\n\n\nAgenda\n\n9:10 - 9:30 : Check-In.\n9:30 - 10:30 : Linear Models (Basics)\n10:30 - 11:00 : Break & Presentation\n11:00 - 12:00 : Linear Models (Continued)\n\nAnnouncements\nMini Exam in TWO Weeks.\n\nI give you data and a question, you generate a report in Quarto.\n\nData loading and cleaning.\nScale creating & descriptive statistics.\nLinear Models (TODAY!)\nBootstrapping\nA fun challenge problem worth 1 point.\n\nTake home (9AM-12PM).\nAsk questions on ZOOM if / when you have them. Okay? Don‚Äôt struggle on your own. Plenty of time to do that in other spaces!\nWe will practice / review next week (Lab 5 a practice exam.)\nThink it will be chill, and if not then professor takes the blame, alright?"
  },
  {
    "objectID": "gradstats/gradlabs/4_LinearModels.html#check-in-here",
    "href": "gradstats/gradlabs/4_LinearModels.html#check-in-here",
    "title": "Lecture 4 | Linear Models",
    "section": "Check-In Here",
    "text": "Check-In Here\n\nLoad the grad onboarding dataset (name this d to follow along with professor code in lecture). The variable can.forloop asked students whether they could write a for-loop or not. What is the difference in the number of students who said that YES they could for-loop, compared to the number who said either NO or MAYBE? Find a way to get R to calculate this difference using code (hint : use indexing and the summary function.)\n\nd &lt;- read.csv(\"../datasets/Grad Onboard 2025/grad_onboard_SP25.csv\",\n                           stringsAsFactors = T)\nplot(d$can.forloop)\n\n\n\n\n\n\n\nsummary(d$can.forloop) # finding the values for each group.\n\nMaybe    No   Yes \n   15    10     8 \n\n(15 + 10) - 8  # answering question with numbers\n\n[1] 17\n\n(summary(d$can.forloop)[1] + summary(d$can.forloop)[2]) - summary(d$can.forloop)[3] # answering question with R code. a lot of code!\n\nMaybe \n   17 \n\ns &lt;- summary(d$can.forloop) # saving this output, since using it multiple times\n(s[1] + s[2]) - s[3] # same answer; much easier to look at!\n\nMaybe \n   17 \n\n\nNow, write a for-loop to estimate how much sampling error might influence this number. Generate 1000 new samples, and re-calculate the difference between YES and NO + MAYBEs. What percentage of re-sampled groups would show that there are more people who CAN write a for-loop than NO or MAYBE?\n\nbucket &lt;- array()\nset.seed(424242) # \"locks\" in the randomization so everyone gets the same \"random\" result; must run immediately before the for-loop.\nfor(i in c(1:1000)){\n  nd &lt;- d[sample(1:nrow(d), nrow(d), replace = T), ]\n  s &lt;- summary(nd$can.forloop)\n  bucket[i] &lt;- (s[1] + s[2]) - s[3]\n}\nhist(bucket) # a normal distribution!\n\n\n\n\n\n\n\nmean(bucket) # very similar to my original answer\n\n[1] 17.132\n\nsd(bucket) # the estimate of sampling error\n\n[1] 4.954101\n\nsum(bucket &gt; 0) # all of the re-sampled slopes show the same positive difference (more people less familiar w/ for-loops than are familiar.)\n\n[1] 999\n\n\nA bonus question. R wrote you a secret message. Run the code below to see it.\n\n\ngreyScale &lt;- colorRampPalette(c(\"pink\",\"red\"))\nsecretmessage &lt;- function(r, col){\n  t &lt;- seq(0,2*pi,length.out=100)\n  x &lt;- r*sin(t)^3\n  y &lt;- (13*r/16)*cos(t) - (5*r/16)*cos(2*t) - (2*r/16)*cos(3*t) - (r/16)*cos(4*t)\n  polygon(x,y,col=col,border=NA)\n}\n\n# create new plot canvas\nplot.new()\n# limits are approximate here\nplot.window(xlim=c(-16,16),ylim=c(-16,13))\n\n# use mapply to loop; invisible to turn off an annoying output.\ninvisible(mapply(secretmessage,seq(16,0,length.out=100),greyScale(100)))\n\n## source : https://stackoverflow.com/questions/6542825/equation-driven-smoothly-shaded-concentric-shapes\n## source : https://stackoverflow.com/questions/12984991/stop-lapply-from-printing-to-console\n\n\nAnnouncements & Agenda\nAgenda\n\n9:10 - 9:30 : Check-In and Review\n9:30 - 10:30 : Linear Models (Basics)\n10:30 - 11:00 : Break & Presentation\n11:00 - 12:00 : Linear Models (Continued)\n\nAnnouncements\n\nDiscord!?!?\nLab Keys and Late Assignments.\n\nwould like to post key ASAP.\nbut late labs + key = TROUBLE. Ideas?\n\nMini Exam in TWO Weeks.\n\nI give you data and a question, you generate a report in Quarto.\n\nData loading and cleaning.\nScale creating & descriptive statistics.\nLinear Models (TODAY!)\nBootstrapping\nA fun challenge problem worth 1 point.\n\nTake home (9AM-12PM).\nAsk questions on ZOOM if / when you have them. Okay? Don‚Äôt struggle on your own. Plenty of time to do that in other spaces!\nWe will practice / review next week (Lab 5 a practice exam.)\nThink it will be chill, and if not then professor takes the blame, alright?"
  },
  {
    "objectID": "gradstats/gradlabs/4_LinearModels.html#recap-the-mean-as-prediction",
    "href": "gradstats/gradlabs/4_LinearModels.html#recap-the-mean-as-prediction",
    "title": "Lecture 4 | Linear Models",
    "section": "RECAP : The Mean as Prediction",
    "text": "RECAP : The Mean as Prediction\n\nThe Mean is a Prediction (of the Sample)\n\nWhere is the Mean?There is the Mean!\n\n\n\nplot(d$self.skills, \n     ylab = \"Self-Perception of Skills\",\n     xlab = \"Index\") \nabline(h = mean(d$self.skills, na.rm = T), lwd = 0)\n\n\n\n\n\n\n\n\n\n\n\nplot(d$self.skills, \n     ylab = \"Self-Perception of Skills\",\n     xlab = \"Index\") \nabline(h = mean(d$self.skills, na.rm = T), lwd = 5)"
  },
  {
    "objectID": "gradstats/gradlabs/4_LinearModels.html#linear-models-improving-our-predictions-numeric-iv",
    "href": "gradstats/gradlabs/4_LinearModels.html#linear-models-improving-our-predictions-numeric-iv",
    "title": "Lecture 4 | Linear Models",
    "section": "Linear Models : Improving our Predictions (Numeric IV)",
    "text": "Linear Models : Improving our Predictions (Numeric IV)\n\nThe Mean as a Model\nNote : I skipped over this in class today; the basic idea is that we can define a linear model to predict a variable from some constant value (1), and the result of that will be the mean, since the mean is our best prediction (minimizes the residual errors) when we don‚Äôt have any other information about the variable.\n\nlm(self.skills ~ 1, data = d) # predicting self.skills from a constant (1), using the datset = d\n\n\nCall:\nlm(formula = self.skills ~ 1, data = d)\n\nCoefficients:\n(Intercept)  \n      3.424  \n\nmod0 &lt;- lm(self.skills ~ 1, data = d) # saving this as a model object\ncoef(mod0) # looking at the coefficients\n\n(Intercept) \n   3.424242 \n\nmod0$residuals # finding the residuals\n\n         1          2          3          4          5          6          7 \n 0.5757576  0.5757576  1.5757576 -0.4242424  1.5757576  1.5757576  0.5757576 \n         8          9         10         11         12         13         14 \n-0.4242424  0.5757576  1.5757576  0.5757576 -1.4242424 -0.4242424 -0.4242424 \n        15         16         17         18         19         20         21 \n-0.4242424 -0.4242424  0.5757576 -0.4242424  1.5757576 -0.4242424 -2.4242424 \n        22         23         24         25         26         27         28 \n-2.4242424  1.5757576 -1.4242424  0.5757576 -0.4242424 -0.4242424  1.5757576 \n        29         30         31         32         33 \n-0.4242424 -1.4242424 -0.4242424 -1.4242424  0.5757576 \n\n\n\n\nUse information in the IV to predict the DV\nLet‚Äôs try the same activity, but now we will graph each individual‚Äôs self-skill (still on the y-axis) in relationship to their perception of their classmates‚Äô skill (on the x-axis).\n\nWhere is the Line?There is the Line!\n\n\n\nplot(jitter(self.skills) ~ class.skills, data = d, \n     ylab = \"Self-Perception of Skills\",\n     xlab = \"Perception of Classmates' Skills\") \nabline(lm(self.skills ~ class.skills, data = d), lwd = 0)\n\n\n\n\n\n\n\n\n\n\n\nplot(jitter(self.skills) ~ class.skills, data = d, \n     ylab = \"Self-Perception of Skills\",\n     xlab = \"Perception of Classmates' Skills\") \nabline(lm(self.skills ~ class.skills, data = d), lwd = 5)\n\n\n\n\n\n\n\n\n\n\n\nThe Linear Model :\n\n\n\n\n\nTo define a linear model, we will first use the lm() function to predict some DV from an IV.\nThen, we will graph the relationship between these two variables using the plot() function. I‚Äôm using jitter() on the DV in order to shift the points a little, since they are overlapping.\nThen, I draw a line (defined by the linear model) using the abline() function. I‚Äôve made the line width = 5 and color red to make it POP.\nI can look at the coefficients of the model with the coef() function. These coefficients are described by the starting place of the line when the x value is zero (the intercept), and the adjustment we make to Y as the X values increase.\n\nmod1 &lt;- lm(self.skills ~ class.skills, data = d)\n\nplot(jitter(self.skills) ~ class.skills, data = d, main = \"Jittered Data\") # jittered\nabline(mod1, lwd = 5, col = 'red')\n\n\n\n\n\n\n\ncoef(mod1)\n\n (Intercept) class.skills \n   1.9501188    0.3800475 \n\n# intercept = 1.95 = the predicted value of Y when ALL X values are ZERO.\n# slope = .38 = relationship between class.skills and our DV (self.skills)\n### as class.skills increase by ONE, then self.skills will increase by .38\n### these units are in the original unit of measurement (1-5 likert scale.)\n\n\n\nThere is Error in Our Prediction (residual error ‚Äì&gt; R^2)\nIn the graph above, I can see that the dots are not all exactly on the line. My predictions are wrong; this is residual error! For example, a person with an actual self-skill score of 2 has as a class-skill rating of 1. Our prediction of this person‚Äôs self-skill score, based on their class-skill rating of 1 is the the result of the linear model :\n\nself.skill ~ 1.95 + .38 * class.skill\nself.skill ~ 1.95 + .38 * 1\nself.skill ~ 2.33\n\nSo the person‚Äôs residual score = the difference between their actual score and our prediction = 2 - 2.33 = -.33. Fortunately, R does the residual calcualtions for us, from the linear model object.\n\nmod1$residuals # R does the residual calculation for us. what will happen if we add this up?\n\n          1           2           3           4           5           6 \n 0.52969121  0.52969121  1.90973872 -0.47030879  1.90973872  1.52969121 \n          7           8           9          10          11          12 \n 0.14964371 -0.47030879  0.52969121  1.14964371  0.52969121 -1.47030879 \n         13          14          15          16          17          18 \n-0.85035629 -0.09026128 -0.85035629 -0.09026128  0.52969121 -0.47030879 \n         19          20          21          22          23          24 \n 1.14964371 -0.47030879 -2.09026128 -2.85035629  1.14964371 -0.71021378 \n         25          26          27          28          29          30 \n 0.52969121 -0.47030879 -0.47030879  1.52969121 -0.47030879 -1.47030879 \n         31          32          33 \n-0.09026128 -0.33016627  0.52969121 \n\nsum(mod1$residuals) # they add to....\n\n[1] -1.776357e-15\n\nSSE &lt;- sum(mod1$residuals^2) # so I square them\nSSE # the total squared error when I use my model to make predictions.\n\n[1] 38.3753\n\n## Visualizing Our Errors. (distance between actual scores and the line).\npar(mfrow = c(1,2))\nplot(d$self.skills, \n     ylab = \"Self-Perception of Skills\",\n     xlab = \"Index\", main = \"Mean as Model \\n(SST = Total Sum of Squared Errors)\") \nabline(h = mean(d$self.skills, na.rm = T), lwd = 5)\nplot(jitter(self.skills) ~ class.skills, data = d, main = \"Linear Model \\n(SSE = Sum of Squared Errors When Model Making Predictions)\", \n     xlim = c(1,5)) # jittered\nabline(mod1, lwd = 5, col = 'red')\n\n\n\n\n\n\n\nSST &lt;- sum((d$self.skills - mean(d$self.skills))^2) # defining the total error\nSST - SSE # a difference in errors when using the mean vs. our model\n\n[1] 3.685309\n\n(SST - SSE)/SST # the relative difference in errors = R^2 (R-squared.)\n\n[1] 0.08761902\n\nsummary(mod1)$r.squared # R does this for us. But good to do \"by hand\" to understand.\n\n[1] 0.08761902\n\n\n\n\nThere is Error in Our Prediction of the Population (sampling error)\nNote : I ran out of time to do this in class, but recorded a video at the VERY END of lecture recording that works through this code.\n\nbucket &lt;- array()\nfor(i in c(1:1000)){\n  nd &lt;- d[sample(1:nrow(d), nrow(d), replace = T), ]\n  modx &lt;- lm(self.skills ~ class.skills, data = nd)\n  bucket[i] &lt;- coef(modx)[2]\n}\nhist(bucket) # what do we expect to see?\nabline(v = mean(bucket), lwd = 5)\nabline(v = mean(bucket) + 1.96*sd(bucket), lwd = 2, lty = 'dashed')\nabline(v = mean(bucket) - 1.96*sd(bucket), lwd = 2, lty = 'dashed')\nmean(bucket)\nsd(bucket)\n\n\n\nTime for Another Example?\nHah! Next time :)\n\nnames(d) # what other (numeric, for now) variable might predict self.skills?\n\n [1] \"time\"           \"has.laptop\"     \"write.code\"     \"know.prog\"     \n [5] \"has.data\"       \"know.r\"         \"can.import\"     \"can.clean\"     \n [9] \"can.graph\"      \"can.render\"     \"can.lm\"         \"can.interp\"    \n[13] \"can.pvalue\"     \"can.sevsd\"      \"can.95ci\"       \"can.forloop\"   \n[17] \"self.skills\"    \"class.skills\"   \"self.excite\"    \"class.excite\"  \n[21] \"self.prep\"      \"class.prep\"     \"learn.concept\"  \"learn.r\"       \n[25] \"can.science\"    \"should.science\" \"truth.people\"   \"not.psychsci\"  \n[29] \"perf.pred\"      \"noperf.pred\"    \"too.complex\"    \"epistemology\"  \n[33] \"consent\""
  },
  {
    "objectID": "gradstats/gradlabs/4_LinearModels.html#break-time-meet-back-at-1045",
    "href": "gradstats/gradlabs/4_LinearModels.html#break-time-meet-back-at-1045",
    "title": "Lecture 4 | Linear Models",
    "section": "BREAK TIME : Meet Back at 10:45",
    "text": "BREAK TIME : Meet Back at 10:45"
  },
  {
    "objectID": "gradstats/gradlabs/4_LinearModels.html#presentations",
    "href": "gradstats/gradlabs/4_LinearModels.html#presentations",
    "title": "Lecture 4 | Linear Models",
    "section": "Presentations",
    "text": "Presentations"
  },
  {
    "objectID": "gradstats/gradlabs/4_LinearModels.html#linear-models-improving-our-predictions-categorical-iv",
    "href": "gradstats/gradlabs/4_LinearModels.html#linear-models-improving-our-predictions-categorical-iv",
    "title": "Lecture 4 | Linear Models",
    "section": "Linear Models : Improving our Predictions (Categorical IV)",
    "text": "Linear Models : Improving our Predictions (Categorical IV)\n\nUse information in the IV to predict the DV\n\n\nrelevel() to aid interpretation.\n\n\nEvaluate uncertainty in our prediction of the sample (R^2)\n\n\nEvaluate the uncertainty in our estimate of the population (sampling error)"
  },
  {
    "objectID": "gradstats/gradlabs/4_LinearModels.html#for-lab-4.",
    "href": "gradstats/gradlabs/4_LinearModels.html#for-lab-4.",
    "title": "Lecture 4 | Linear Models",
    "section": "FOR LAB 4.",
    "text": "FOR LAB 4.\n\nDefine linear models to predict a numeric DV from a numeric IV.\nInterpret the intercept, slope, and R^2 value.\nDo some bootstrapping.\nRepeat w/ a different dataset.\nPlay around with ggplot2()\nFind and evaluate a graph."
  },
  {
    "objectID": "calstats/Lectures/4L_Scales.html#part-2-normal-distributions-and-likert-scales",
    "href": "calstats/Lectures/4L_Scales.html#part-2-normal-distributions-and-likert-scales",
    "title": "Class 4 | Scales",
    "section": "Part 2 : ‚ÄúNormal‚Äù Distributions and Likert Scales",
    "text": "Part 2 : ‚ÄúNormal‚Äù Distributions and Likert Scales\n\nRECAP : The Normal Distribution\nWe think of psychological constructs as continuous variables :\n\nd &lt;- array()\nfor(i in c(1:1000)){\n  d[i] &lt;- sum(replicate(10, sample(c(0,1), 1)))\n  }\nhist(d, main = \"Theoretical Normal Distribution\", xlab = \"Self-Esteem\", breaks = 10)\n\n\n\n\n\n\n\n\nWe Expect This To Occur When :\n\nMultiple Explanations for Variation :\nThose Multiple Explanations are Independent :\n\nBelow are Two Examples :\n\nplot(as.factor(mini$selfesteem))\n\n\n\n\n\n\n\n\n\n\nRECAP : A Likert Scale\n\nWhy Are We Doing This?\nThe Likert Scale is designed to help approximate a normal distribution, by asking multiple questions about the same variable (Multiple Explanations for Variation!)\nNotice, that our single-item measures are not really continuous, but could be considered categories :\n\nplot(as.factor(mini$selfesteem), xlab = \"Self-Esteem\", main = \"Categories of Self-Esteem\")\n\n\n\n\n\n\n\n\n\n\nDISCUSSION : Evaluate the Satisfaction With Life Scale [below]\n\n\n\n\n\n\nICE-BREAKER : Best / worst present you received?\nDescribe features that you notice about this scale (try and use the terms we just learned!)\n\n# of items :\nresponse scale :\n\npositively keyed items :\nnegatively keyed items :\n\n\nDo you think this is a valid way to measure this construct?\n\nREASONS NO :\nREASONS YES :\n\nHow might you use this scale in a study (as an IV or a DV).\n\n\n\n\nIn R : Creating a Likert Scale\nLet‚Äôs create a variable about how GOOD people feel. What items from the mini dataset measure this (the high end and / or the low end)?\n\nnames(mini) # looking at the names of our variables.\n\n [1] \"pace\"            \"engaging\"        \"fb.friends\"      \"insta.followers\"\n [5] \"insta.follows\"   \"bored\"           \"thirsty\"         \"tired\"          \n [9] \"satlife\"         \"oskilove\"        \"rlove\"           \"socialmed.use\"  \n[13] \"class.attention\" \"hrs.sleep\"       \"selfpow.data\"    \"corppow.data\"   \n[17] \"success.work\"    \"success.priv\"    \"selfesteem\"      \"catdog\"         \n[21] \"tuhoburat\"       \"calgame\"         \"caffeine\"        \"breakfast\"      \n[25] \"is.female\"       \"long.hair\"       \"has.water\"       \"shoesize\"       \n[29] \"height\"          \"happy\"           \"drink\"           \"stoned72\"       \n[33] \"multilingual\"    \"waitlist\"        \"hrs.sleepZM\"     \"hrs.sleepZ\"     \n\n\n\nSTEP 1 : ORGANIZE AND EVALUATE THE ITEM RELIABILITY\n\nHAPPY.df &lt;- data.frame(mini$happy, mini$selfesteem, 10-mini$bored, 10-mini$tired, mini$satlife)\nHAPPY.df\n\n    mini.happy mini.selfesteem X10...mini.bored X10...mini.tired mini.satlife\n1            8               8                6                4            4\n2            8               7                2                0            7\n3            6               7                5                3            6\n4            8              10                6                2            9\n5           NA              NA               NA               NA           NA\n6            8               8                3                1            9\n7           10              10               10                3           10\n8            6               7                3                3            8\n9            7               5                4                2            6\n10           9               5                2                2           10\n11           7               1                7                0            7\n12           8               6                6                0            7\n13           4               5                4                0            2\n14          NA              NA                5                5            8\n15           7               6                5                7            7\n16           7               7                8                0            6\n17           8               8                4                6            8\n18           6               8                3                4            6\n19           6               7                5                4            8\n20           8               7                8                0            8\n21           6               8                5                1            4\n22           7               4                5                5            7\n23           7               8                5                7            9\n24           8               3                3                6            7\n25           8               8                6                0            9\n26           8               7               10                6            8\n27           7               7                3                5            6\n28          10              10               10                3           10\n29           7               7                8                3            6\n30          NA               6                3                5            7\n31           5               3                6                5            4\n32           8               7                5                7            8\n33           7               5                9                3            6\n34           8               8                0                2            8\n35           8               7                2                3            6\n36           7               7                3                2            8\n37           8               9                6                3            8\n38           9               8                8                8            8\n39           1               1                3                0            6\n40           9               8                8                1            8\n41           8               7                2                0            8\n42           7               6                5                2            8\n43           8               9                7                6            8\n44          10              10                5                8            7\n45           7               6                8                4            1\n46           3               2                7                4            4\n47           7               8                2                3            6\n48          10               2                6                8            7\n49           6               7                5                2            8\n50           8               8                3                4            8\n51          10              10                4                4           10\n52           9               8                0                3            8\n53           7               8                5                2            6\n54          10               8                6                0           10\n55           7               7                7                3            5\n56           5               4                4                6            5\n57           8               7                2                1            8\n58           8               9                2                3            8\n59           6               7                0                1            8\n60           5               4                7                7            6\n61           6               4                6                0            5\n62           9               7                8                4            8\n63           9               8                6                3            8\n64           9               3                4                7            8\n65           8               5                3                2            7\n66           7               9                3                5            8\n67           5               5                5                1            4\n68           8               8                4                4            8\n69           9               7                8                7            8\n70           9               8                7                3            9\n71           8               9                5                2            7\n72           5               6                8                0           10\n73           8               6               10                4            8\n74           9               8                6                2            9\n75           7               6                3                3            7\n76           6               5                8                1            6\n77          10               5                5                3           10\n78           7               3                5                2            8\n79           6               6                4                4            7\n80           6               7                9                2            2\n81           7               8                7                4            6\n82           9               8                3                0            8\n83          10               8                4                4            8\n84           7               8               NA                2            6\n85           8               8               10                3            8\n86           8               8                6                5            8\n87           7               9                4                3            6\n88           8               3                5                0            8\n89           7               6               10                4            7\n90           7               8                6                3            8\n91           9               9                3                5            7\n92           6               6                2                1            7\n93           8               7                7                3            9\n94           8               9                4                3            7\n95           6               6                5                4            7\n96           6               8                7                4            5\n97           7               6                8                4           NA\n98           7               5               10                6            7\n99           7               6                5                3            7\n100         10               9                8                3            9\n101          7               7                9                5            6\n102         NA               7                6                3            7\n103          8               9                4                2            8\n104          8               7                9               10            7\n105          8               7                6                4            6\n106          8               5                8                1            8\n107         10              10                7                4           10\n108          5               5                0                5            5\n109          9               8                1                1            8\n110          7               7                6                2            8\n111          5               0                5                0            6\n112          6               7                4                3            5\n113         NA               7                8                2            7\n114          3               4                6                1            2\n115          8               8                7                2            9\n116         10              10                5                3            9\n117          7               6               10                3            7\n118          7               7                5                8            6\n119          7               7                5                7            7\n120          9               3                6                3            2\n121          9               6                6                3            7\n122         10               7               10                9           10\n123          5               6                5                5            5\n124          7               8                3                6            7\n125          7               7                6                5            6\n126          6               6                9                3            7\n127          9               5                3                2            9\n128          9               6                4                5            7\n129          9               8                9                0            7\n130          6               6                6                3            7\n131          8               8                9                1            8\n132          8               7                5                1            8\n133          7               5                8                2            6\n134          9               4                9                9            8\n135          7              10                6                3           10\n136         10               8                7                8            8\n137          3               3                6                3            6\n138         10              10                7                0           10\n139          6               6                9                0            7\n140          6               5                5                4            4\n141          5               4                7                0            3\n142          8               8                2                3            9\n143          5               4                6                2            5\n144          7              10                6                1           10\n145          8              10                3                1            8\n146          8               9                6                3            9\n147          4               7                4                0            3\n148         10               8                6                2            9\n149          7               7                4                2            7\n150          9              10               10               10            9\n151          5               5                5                0            8\n152         NA              NA               NA               NA           NA\n153         NA               2                7                1            6\n154          5               4                6                3            8\n155          7               5                4                2            6\n156          7               5                5                1            5\n\nrange(HAPPY.df, na.rm = T) # 0 to 10 range means to reverse score I need to subtract variable from 0 + 10 = 10\n\n[1]  0 10\n\n# install.packages('psych') # only need to do this once!\nlibrary(psych) # do this every time you restart R.\nalpha(HAPPY.df)\n\nNumber of categories should be increased  in order to count frequencies. \n\n\n\nReliability analysis   \nCall: alpha(x = HAPPY.df)\n\n  raw_alpha std.alpha G6(smc) average_r S/N  ase mean  sd median_r\n      0.54      0.58     0.6      0.22 1.4 0.06    6 1.2     0.13\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.41  0.54  0.64\nDuhachek  0.42  0.54  0.66\n\n Reliability if an item is dropped:\n                 raw_alpha std.alpha G6(smc) average_r  S/N alpha se var.r\nmini.happy            0.35      0.37    0.36      0.13 0.58    0.087 0.027\nmini.selfesteem       0.45      0.49    0.51      0.19 0.97    0.074 0.045\nX10...mini.bored      0.62      0.65    0.64      0.32 1.88    0.052 0.053\nX10...mini.tired      0.56      0.61    0.61      0.28 1.54    0.061 0.073\nmini.satlife          0.43      0.47    0.46      0.18 0.87    0.076 0.033\n                 med.r\nmini.happy       0.072\nmini.selfesteem  0.132\nX10...mini.bored 0.331\nX10...mini.tired 0.273\nmini.satlife     0.132\n\n Item statistics \n                   n raw.r std.r r.cor r.drop mean  sd\nmini.happy       149  0.74  0.80  0.80   0.59  7.4 1.6\nmini.selfesteem  153  0.64  0.66  0.56   0.36  6.7 2.1\nX10...mini.bored 153  0.48  0.42  0.13   0.10  5.6 2.4\nX10...mini.tired 154  0.55  0.50  0.25   0.20  3.2 2.3\nmini.satlife     153  0.64  0.69  0.63   0.41  7.1 1.8\n\nHAPPY2.df &lt;- data.frame(mini$happy, mini$selfesteem, 10-mini$tired, mini$satlife)\nalpha(HAPPY2.df)\n\nNumber of categories should be increased  in order to count frequencies. \n\n\n\nReliability analysis   \nCall: alpha(x = HAPPY2.df)\n\n  raw_alpha std.alpha G6(smc) average_r S/N   ase mean  sd median_r\n      0.62      0.65    0.64      0.32 1.9 0.052  6.1 1.3     0.33\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.51  0.62  0.71\nDuhachek  0.51  0.62  0.72\n\n Reliability if an item is dropped:\n                 raw_alpha std.alpha G6(smc) average_r  S/N alpha se  var.r\nmini.happy            0.40      0.42    0.38      0.19 0.73    0.086 0.0454\nmini.selfesteem       0.51      0.56    0.54      0.29 1.25    0.072 0.0763\nX10...mini.tired      0.76      0.76    0.69      0.52 3.22    0.034 0.0063\nmini.satlife          0.49      0.53    0.48      0.27 1.12    0.073 0.0484\n                 med.r\nmini.happy       0.082\nmini.selfesteem  0.222\nX10...mini.tired 0.513\nmini.satlife     0.222\n\n Item statistics \n                   n raw.r std.r r.cor r.drop mean  sd\nmini.happy       149  0.80  0.83  0.80   0.64  7.4 1.6\nmini.selfesteem  153  0.73  0.73  0.59   0.44  6.7 2.1\nX10...mini.tired 154  0.55  0.49  0.19   0.14  3.2 2.3\nmini.satlife     153  0.71  0.75  0.66   0.48  7.1 1.8\n\n\n\n\nSTEP 2 : CREATE AND INTERPRET THE SCALE\n\nrowMeans(HAPPY2.df, na.rm = T)\n\n  [1] 6.000000 5.500000 5.500000 7.250000      NaN 6.500000 8.250000 6.000000\n  [9] 5.000000 6.500000 3.750000 5.250000 2.750000 6.500000 6.750000 5.000000\n [17] 7.500000 6.000000 6.250000 5.750000 4.750000 5.750000 7.750000 6.000000\n [25] 6.250000 7.250000 6.250000 8.250000 5.750000 6.000000 4.250000 7.500000\n [33] 5.250000 6.500000 6.000000 6.000000 7.000000 8.250000 2.000000 6.500000\n [41] 5.750000 5.750000 7.750000 8.750000 4.500000 3.250000 6.000000 6.750000\n [49] 5.750000 7.000000 8.500000 7.000000 5.750000 7.000000 5.500000 5.000000\n [57] 6.000000 7.000000 5.500000 5.500000 3.750000 7.000000 7.000000 6.750000\n [65] 5.500000 7.250000 3.750000 7.000000 7.750000 7.250000 6.500000 5.250000\n [73] 6.500000 7.000000 5.750000 4.500000 7.000000 5.000000 5.750000 4.250000\n [81] 6.250000 6.250000 7.500000 5.750000 6.750000 7.250000 6.250000 4.750000\n [89] 6.000000 6.500000 7.500000 5.000000 6.750000 6.750000 5.750000 5.750000\n [97] 5.666667 6.250000 5.750000 7.750000 6.250000 5.666667 6.750000 8.000000\n[105] 6.250000 5.500000 8.500000 5.000000 6.500000 6.000000 2.750000 5.250000\n[113] 5.333333 2.500000 6.750000 8.000000 5.750000 7.000000 7.000000 4.250000\n[121] 6.250000 9.000000 5.250000 7.000000 6.250000 5.500000 6.250000 6.750000\n[129] 6.000000 5.500000 6.250000 6.000000 5.000000 7.500000 7.500000 8.500000\n[137] 3.750000 7.500000 4.750000 4.750000 3.000000 7.000000 4.000000 7.000000\n[145] 6.750000 7.250000 3.500000 7.250000 5.750000 9.500000 4.500000      NaN\n[153] 3.000000 5.000000 5.000000 4.500000\n\nmini$HAPPYSCALE &lt;- rowMeans(HAPPY2.df, na.rm = T)\n\nhist(mini$HAPPYSCALE)"
  },
  {
    "objectID": "calstats/Lectures/4L_Scales.html#part-3-project-workshop-you-need-a-likert-scale",
    "href": "calstats/Lectures/4L_Scales.html#part-3-project-workshop-you-need-a-likert-scale",
    "title": "Class 4 | Scales",
    "section": "Part 3 : Project Workshop : You Need a Likert Scale!",
    "text": "Part 3 : Project Workshop : You Need a Likert Scale!\n\nStep 1 : Identify a (Dependent) Variable in Your Model\n\nStudent Example Go Here\n\n\n\nStep 2 : Measure this in a Continuous Way\n\nMethod 1 : Find or Adapt an Existing Scale\n\nList of (Free) Scales : https://ipip.ori.org/newIndexofScaleLabels.htm\nGoogle/Google Scholar : add ‚Äúconstruct‚Äù or ‚Äúvalidation‚Äù or ‚Äúscale‚Äù to your search\n\n\n\nMethod 2 : Write Your Own Scale\n\nKeep items phrased in the same way. (‚ÄúI am someone who‚Ä¶.‚Äù)\nKeep items on the same response scale (1-5 or 1-7); usually higher numbers indicate more agreement with the question.\nMake sure to include positive and negatively-keyed items.\n\n\n\n\nExamples for ‚ÄúMega‚Äù Class Survey\n\nLove for Water Scale :\n\nI think water is delicious.\nIf I‚Äôm thirsty, it‚Äôs water I crave.\nI‚Äôd rather be thirsty than drink water.\nIt‚Äôs a chore for me to drink water."
  },
  {
    "objectID": "gradstats/gradlabs/4_LinearModels.html#there-is-error-in-our-prediction-of-the-sample-residual-error",
    "href": "gradstats/gradlabs/4_LinearModels.html#there-is-error-in-our-prediction-of-the-sample-residual-error",
    "title": "Lecture 4 | Linear Models",
    "section": "There is Error in Our Prediction of the Sample (Residual Error)",
    "text": "There is Error in Our Prediction of the Sample (Residual Error)\nThis prediction of the sample has some error (residual error). We can (and will need to) quantify this error.\n\n## quantifying errors (residuals)\nresiduals &lt;- d$self.skills - mean(d$self.skills, na.rm = T)\nSST &lt;- sum(residuals^2)\nSST\n\n[1] 42.06061\n\nSST/length(residuals) # average of squared residuals (variance)\n\n[1] 1.274564\n\nsqrt(SST/length(residuals)) # average of residuals, unsquared (standard deviation)\n\n[1] 1.128966\n\nsd(d$self.skills) # slightly higher\n\n[1] 1.14647\n\nsqrt(SST/(length(residuals)-1)) # the 'real' equation; n-1 to inflate our estimate / adjust for small samples.\n\n[1] 1.14647"
  },
  {
    "objectID": "gradstats/gradlabs/4_LinearModels.html#the-mean-is-a-prediction-of-our-population-with-sampling-error",
    "href": "gradstats/gradlabs/4_LinearModels.html#the-mean-is-a-prediction-of-our-population-with-sampling-error",
    "title": "Lecture 4 | Linear Models",
    "section": "The Mean is a Prediction of our Population (with Sampling Error)",
    "text": "The Mean is a Prediction of our Population (with Sampling Error)\n\nm &lt;- array()\nfor(i in c(1:1000)){\n nd &lt;- d[sample(1:nrow(d), nrow(d), replace = T),] # a new sample\n m[i] &lt;- mean(nd$self.skills, na.rm = T)\n}\nmean(d$self.skills, na.rm = T)\n\n[1] 3.424242\n\nmean(m) # similar!\n\n[1] 3.420727\n\nsum(m &gt; 2.5) # all of them (100% greater than the midpoint of the scale.)\n\n[1] 1000\n\nsd(m) # sampling error!\n\n[1] 0.1943785\n\nhist(m, xlim = c(1,5)) # our distribution of sampling estimates \nabline(v = c(mean(d$self.skills),\n             mean(d$self.skills) + 1.96 * sd(m),\n             mean(d$self.skills) - 1.96 * sd(m)),\n       lwd = c(5,2,2), # two line widths\n       lty = c(1,2,2)) # two line types"
  },
  {
    "objectID": "gradstats/gradlabs/4_LinearModels.html#break-time-meet-back-at-1040",
    "href": "gradstats/gradlabs/4_LinearModels.html#break-time-meet-back-at-1040",
    "title": "Lecture 4 | Linear Models",
    "section": "BREAK TIME : Meet Back at 10:40",
    "text": "BREAK TIME : Meet Back at 10:40"
  },
  {
    "objectID": "gradstats/gradlabs/4_LinearModels.html#linear-models-again.",
    "href": "gradstats/gradlabs/4_LinearModels.html#linear-models-again.",
    "title": "Lecture 4 | Linear Models",
    "section": "Linear Models Again.",
    "text": "Linear Models Again.\nLoad the gradmini.csv dataset. Check to make sure the data loaded correctly. Let‚Äôs predict some variables."
  },
  {
    "objectID": "gradstats/gradlabs/4Lab_Models.html",
    "href": "gradstats/gradlabs/4Lab_Models.html",
    "title": "Lab 4: Linear Models",
    "section": "",
    "text": "Answer the questions below as a .PDF (you can either render from Quarto as .PDF, or render as .html and then print this html file as a .PDF). Make sure to reference any external sources (e.g., stack exchange; ChatGPT; peer help) that you used at the top of your lab assignment. If you use external resources, don‚Äôt just copy / paste the code (and source) blindly, but spend time thinking (and writing in your Lab) about what these external sources are doing with the code / what techniques you are learning from this source.\nAnd please ask for help if you get stuck / are confused / professor did something wrong on Discord!"
  },
  {
    "objectID": "gradstats/gradlabs/4Lab_Models.html#lab-instructions.",
    "href": "gradstats/gradlabs/4Lab_Models.html#lab-instructions.",
    "title": "Lab 4: Linear Models",
    "section": "",
    "text": "Answer the questions below as a .PDF (you can either render from Quarto as .PDF, or render as .html and then print this html file as a .PDF). Make sure to reference any external sources (e.g., stack exchange; ChatGPT; peer help) that you used at the top of your lab assignment. If you use external resources, don‚Äôt just copy / paste the code (and source) blindly, but spend time thinking (and writing in your Lab) about what these external sources are doing with the code / what techniques you are learning from this source.\nAnd please ask for help if you get stuck / are confused / professor did something wrong on Discord!"
  },
  {
    "objectID": "gradstats/gradlabs/4Lab_Models.html#problem-1-in-discussion-section",
    "href": "gradstats/gradlabs/4Lab_Models.html#problem-1-in-discussion-section",
    "title": "Lab 4: Linear Models",
    "section": "Problem 1 (In Discussion Section)",
    "text": "Problem 1 (In Discussion Section)\n\nUse the self-esteem dataset (from Labs 2 and 3) to determine whether there is a relationship between self-esteem and age. Feel free to re-use your code (or Professor‚Äôs code) from Labs 2 and 3. Include a graph of your linear model, and report the intercept and slope from the model. Below these statistics, explain what they tell you about the relationship between these two variables, and how you can see them on the graph.\n\n\nCalculate R^2 ‚Äúby hand‚Äù; confirm you did this correctly by comparing to the value that R calculates (you can access this by running summary(mod)$r.squared, with (mod) being the name of your model object. Note that in order to get the exact same value as R, you‚Äôll need to compare the residuals in your linear model to residuals when calculating the mean of only the participants who had data on both age and self-esteem. There are different ways to do this, but when calculating SST you can use the data from the linear model by calling mod$model$SELFES (assuming you named your model mod and your self-esteem variable SELFES). Below your R code, describe what this statistic tells you about the relationship between age and self-esteem.\n\n\nRun a for-loop to estimate the amount of sampling error in the slope from this linear model, and use this estimate of sampling error to define the 95% Confidence Interval for the slope. The steps you‚Äôll take are similar to what we‚Äôve done before with the for-loop; this time you‚Äôll need to extract the slope from a model of re-sampled data. I recorded a bonus video at the very end of our posted lecture video walking through how to do this (and code is also in the Lecture document.) Note : a 95% confidence interval for a slope is defined as slope ¬± 1.96 * sampling error."
  },
  {
    "objectID": "gradstats/gradlabs/4Lab_Models.html#problem-2.-step-problems.",
    "href": "gradstats/gradlabs/4Lab_Models.html#problem-2.-step-problems.",
    "title": "Lab 4: Linear Models",
    "section": "Problem 2. Step Problems.",
    "text": "Problem 2. Step Problems.\n\nLoad the two ‚ÄúSteps and BMI‚Äù data files from the Dataset Dropbox folder. There are two data files - data9b_m.txt describes the number of steps taken day for men, and data9b_w.txt for women. You will need to find a way to load .txt files into R, and then use rbind() (or another method) to merge the two files together into one dataset. (Make sure to add a column to each dataset to keep track of whether the data are for men or women.) Show that this worked.\n\n\nNow, determine whether there is a relationship between the number of steps that people take and their BMI. Estimate the sampling error of this relationship. What do you conclude, based on these results?"
  },
  {
    "objectID": "gradstats/gradlabs/4Lab_Models.html#problem-3.-ggplot2-problems.",
    "href": "gradstats/gradlabs/4Lab_Models.html#problem-3.-ggplot2-problems.",
    "title": "Lab 4: Linear Models",
    "section": "Problem 3. ggplot2 Problems.",
    "text": "Problem 3. ggplot2 Problems.\nSo far, we‚Äôve been doing our graphing in base R. Next week, we‚Äôll talk more about using ggplot2 - a powerful graphics package. Install and load the ggplot2() package if you haven‚Äôt done so already. Look over Hadley Wickham‚Äôs intro guide to ggplot2 and read through Chapter 1 of his Data Visualization Book. (Let us know on discord if you find another great ggplot2 reference to share!)\nRecreate one of the graphs from lecture or this lab using ggplot2. Play around with some of the other features to look at this graph from another perspective (e.g., add a variable to the graph for color or facet; shift the y-axis; etc.). How does this graph help you better understand the data?"
  },
  {
    "objectID": "gradstats/gradlabs/4Lab_Models.html#problem-4.-the-good-the-bad-and-the-ugly-graphs.",
    "href": "gradstats/gradlabs/4Lab_Models.html#problem-4.-the-good-the-bad-and-the-ugly-graphs.",
    "title": "Lab 4: Linear Models",
    "section": "Problem 4. The Good, the Bad, and the Ugly Graphs.",
    "text": "Problem 4. The Good, the Bad, and the Ugly Graphs.\nFind a graph published in an article relevant to your research interests. Evaluate the graph - what variables are on this graph? How does the graph adhere to / violate some of the principles of good graphics that we discussed in this week‚Äôs article / presentation? What is effective about the graph? What could be improved? Share the graph (and your analysis) here below, as well as in the 205 Vision Board"
  },
  {
    "objectID": "gradstats/gradlabs/4_LinearModels.html#section-1",
    "href": "gradstats/gradlabs/4_LinearModels.html#section-1",
    "title": "Lecture 4 | Linear Models",
    "section": "",
    "text": "names(d) # what variable might predict self.skills?\n\n [1] \"time\"           \"has.laptop\"     \"write.code\"     \"know.prog\"     \n [5] \"has.data\"       \"know.r\"         \"can.import\"     \"can.clean\"     \n [9] \"can.graph\"      \"can.render\"     \"can.lm\"         \"can.interp\"    \n[13] \"can.pvalue\"     \"can.sevsd\"      \"can.95ci\"       \"can.forloop\"   \n[17] \"self.skills\"    \"class.skills\"   \"self.excite\"    \"class.excite\"  \n[21] \"self.prep\"      \"class.prep\"     \"learn.concept\"  \"learn.r\"       \n[25] \"can.science\"    \"should.science\" \"truth.people\"   \"not.psychsci\"  \n[29] \"perf.pred\"      \"noperf.pred\"    \"too.complex\"    \"epistemology\"  \n[33] \"consent\"       \n\n\n\nThere is Error in Our Prediction (residual error ‚Äì&gt; R^2)\n\n\nThere is Error in Our Prediction of the Population (sampling error)"
  },
  {
    "objectID": "calstats/Lectures/4L_Scales.html#check-out-the-mega-survey",
    "href": "calstats/Lectures/4L_Scales.html#check-out-the-mega-survey",
    "title": "Class 4 | Scales",
    "section": "Check-Out : The Mega Survey",
    "text": "Check-Out : The Mega Survey\nwait to do this until professor says."
  },
  {
    "objectID": "calstats/labs/Lab4.html",
    "href": "calstats/labs/Lab4.html",
    "title": "Lab 4",
    "section": "",
    "text": "Use R to answer the following questions. Make sure that you show your R code and the output (result or graphs.) You can do this by taking a screenshot and pasting into your Google / Word Document, or (for students wanting an extra challenge) working in a Quarto document."
  },
  {
    "objectID": "calstats/labs/Lab4.html#problem-1.-mega-problems",
    "href": "calstats/labs/Lab4.html#problem-1.-mega-problems",
    "title": "Lab 4",
    "section": "Problem 1. MEGA Problems",
    "text": "Problem 1. MEGA Problems\nDownload the ‚Äúcal_mega_SP25.csv‚Äù dataset from Dropbox (this will be posted after lecture). Look at the codebook for a guide to the variables in this dataset. This dataset contains a mix of demographic, likert scale (both on a 1-5 and 0-4 response scale), categorical, and numeric data.\n\nLoad the dataset and check to make sure it loaded correctly.\nLook over the codebook to identify a variable that was measured with a likert scale (in the codebook, the name of the variable should be in bold, and then should see multiple items underneath that measure the variable.) Report the alpha reliability of the items in the scale - does it appear that the scale was reliable? If you had to cut one item, which would you cut (and why)?\nCreate the scale (save it to the original dataset), and graph it as a histogram. Make the graph look nice. Underneath the graph, report the mean, median, standard deviation, and range of the scale. Then, explain what you learn about the participants in the dataset for this graph.\nNow, z-score the scale and graph the z-scored version as a histogram. Make the graph look nice, and report the mean, standard deviation, and range of this z-scored variable. What changed when you did the z-score transformation? What is the same?"
  },
  {
    "objectID": "calstats/labs/Lab4.html#problem-2.-self-esteem-problems",
    "href": "calstats/labs/Lab4.html#problem-2.-self-esteem-problems",
    "title": "Lab 4",
    "section": "Problem 2. Self-Esteem Problems",
    "text": "Problem 2. Self-Esteem Problems\nDownload the ‚Äúselfesteemdata.csv‚Äù dataset, and look at the codebook for a guide to the variables.\nNote: zeros in this dataset are actually missing data. After loading the data, you‚Äôll need to run the following code to a) tell R that 0s are na values and then b) convert the data into a numeric format.\nselfes[selfes == 0] # finds all the 0 values\nselfes[selfes == 0] &lt;- NA # converts them to NA\n\nLoad the dataset, run the professor‚Äôs code to clean the data (you will need to name your dataset selfes, or adapt my code to match the name of your dataset), and check to make sure the data loaded correctly. Then, use R to confirm (somehow) that 0s were, in fact, converted into NA values.\nSelf-esteem was measured with Q1 - Q10. Note that some of the items are negatively-keyed, and will need to be reverse scored (use the codebook to figure this out.) Report the alpha reliability of the items in the scale - does it appear that the scale was reliable?\nCreate the self-esteem scale, and graph it as a histogram. Make the graph look nice. Underneath the graph, report the mean, median, standard deviation, and range of the scale. Then, explain what you learn about the participants in the dataset for this graph.\nGraph the variable age. Underneath the graph, report the mean, median, standard deviation, and range of age. Then, explain what you learn about the participants in the dataset for this graph."
  },
  {
    "objectID": "calstats/labs/Lab4.html#problem-3.-final-project-scale-problems",
    "href": "calstats/labs/Lab4.html#problem-3.-final-project-scale-problems",
    "title": "Lab 4",
    "section": "Problem 3. Final Project Scale Problems",
    "text": "Problem 3. Final Project Scale Problems\nIn lecture, we talked about likert scales, and how to use them to measure psychological variables (and the variables for your final project).\n\nWhat is your DV? What IVs do you think will predict or explain this DV?\nChoose one of the variables in your linear model (ideally the DV). How would you measure this variable using a likert scale? Report all the items you would use. If you used or adapted another scale, make sure to reference that here."
  },
  {
    "objectID": "calstats/labs/Lab4.html#problem-4.-research-problems",
    "href": "calstats/labs/Lab4.html#problem-4.-research-problems",
    "title": "Lab 4",
    "section": "Problem 4. Research Problems",
    "text": "Problem 4. Research Problems\nStart your literature review for your final project! Look over the material in Chapter 5 (Part 3) on how to use Google Scholar to find peer-reviewed articles for your final project. Then, try to find one article on your topic that you think is relevant / interesting. You don‚Äôt need to read the entire article, but just try to learn something about the topic that you didn‚Äôt know before (from the abstract, introduction, discussion, or even just the title).\nPaste the article citation below in APA format (you can export this using Google Scholar), and then your quick summary underneath the citation."
  },
  {
    "objectID": "gradstats/gradlabs/5_ModelReview.html",
    "href": "gradstats/gradlabs/5_ModelReview.html",
    "title": "Lecture 5 | Reviewing Models",
    "section": "",
    "text": "Instructions. Use the graphs and output below to answer the following questions about the relationship between how often students checked their phone (DV) and how many hours of sleep they had (IV in Model 1) and how happy they said they were with their advisor (Model 2). Here‚Äôs a link to the data if you want to follow along.\n\n\nCode\npar(mfrow = c(1,2))\nplot(check.phone ~ hrs.sleep, data = g,\n     ylab = \"# of Times Student Checked Phone\", \n     xlab = \"Hours of Sleep\",\n     main = \"Model 1\")\nmod1 &lt;- lm(check.phone ~ hrs.sleep, data = g)\nabline(mod1, lwd = 5, col = 'red')\n\nplot(check.phone ~ cooladvisor, data = g,\n     ylab = \"# of Times Student Checked Phone\",\n     xlab = \"Happy w/ Advisor (Student-Reported)\",\n     main = \"Model 2\")\nmod2 &lt;- lm(check.phone ~ cooladvisor, data = g)\nabline(mod2, lwd = 5, col = 'red')\n\n\n\n\n\n\n\n\n\nThe intercept and slope is reported for each linear model.\n\nModel 1 : phone checking ~ 38.8 + 8 * hours of sleep + ERROR\n\nphone checking if person got 0 hours of sleep = 38.8 + 8 * 0 = 38.8\nphone checking if person got 4 hours of sleep = 38.8 + 8 * 4 = 70.8\nresidual error = distance from each individual dot to the line; we square these errors and add them up to calcualte the sum of squared errors, then subtract them from SSE when using the mean as a prediction to estimate the reduction in error. ($R^2$ = (SST - SSE) / SST)\nsampling error = 11.2 = how much the line would vary if we were to resample the data (AN ESTIMATE / MADE UP GUESS).\n\nslope ¬± 1.96 * the sampling error = 95% confidence interval.\nsampling error in units of slope\nsampling error is kind of like standard deviation (how much an individual differs on average from our expected value)\n\nindividual = an individual slope if resampled (vs.¬†the individual data point)\nexpected value = the original slope in our sample (vs.¬†the mean of our sample)\n\n\n\n\n\n\nCode\nm1 &lt;- cbind(t(coef(mod1)), SampleError = sd(b1), R2 = summary(mod1)$r.squared)\nm2 &lt;- cbind(t(coef(mod2)), SampleError = sd(b2), R2 = summary(mod2)$r.squared)\n\nx &lt;- rbind(m1, m2)\nrownames(x) &lt;- c(\"Model 1\", \"Model 2\")\ncolnames(x) &lt;- c(\"Intercept\", \"Slope\", \"Sample Error (Slope)\", \"R-Squared\")\nround(x, 2)\n\n\n        Intercept Slope Sample Error (Slope) R-Squared\nModel 1     38.81  7.95                10.47      0.03\nModel 2     42.84  6.15                 5.59      0.05\n\n\nUse Models 1 and 2 to answer the questions below.\n\nDescribe the relationship in Model 1. What do you observe? Why do you think this relationship exists?\nDescribe the relationship in Model 2. What do you observe? Why do you think this relationship exists?\nWhat is the predicted amount of phone checking for someone who got zero hours of sleep?1. ‚Ä¶for someone who got four hours of sleep?\n‚Ä¶for someone who‚Äôs 0/10 happy with their advisor?\n‚Ä¶for someone who‚Äôs 10/10 happy with thier advisor?\nWhich variable is a better predictor of the number of times someone checks their phone?\nHow can you tell?"
  },
  {
    "objectID": "gradstats/gradlabs/5_ModelReview.html#check-in-here",
    "href": "gradstats/gradlabs/5_ModelReview.html#check-in-here",
    "title": "Lecture 5 | Reviewing Models",
    "section": "",
    "text": "Instructions. Use the graphs and output below to answer the following questions about the relationship between how often students checked their phone (DV) and how many hours of sleep they had (IV in Model 1) and how happy they said they were with their advisor (Model 2). Here‚Äôs a link to the data if you want to follow along.\n\n\nCode\npar(mfrow = c(1,2))\nplot(check.phone ~ hrs.sleep, data = g,\n     ylab = \"# of Times Student Checked Phone\", \n     xlab = \"Hours of Sleep\",\n     main = \"Model 1\")\nmod1 &lt;- lm(check.phone ~ hrs.sleep, data = g)\nabline(mod1, lwd = 5, col = 'red')\n\nplot(check.phone ~ cooladvisor, data = g,\n     ylab = \"# of Times Student Checked Phone\",\n     xlab = \"Happy w/ Advisor (Student-Reported)\",\n     main = \"Model 2\")\nmod2 &lt;- lm(check.phone ~ cooladvisor, data = g)\nabline(mod2, lwd = 5, col = 'red')\n\n\n\n\n\n\n\n\n\nThe intercept and slope is reported for each linear model.\n\nModel 1 : phone checking ~ 38.8 + 8 * hours of sleep + ERROR\n\nphone checking if person got 0 hours of sleep = 38.8 + 8 * 0 = 38.8\nphone checking if person got 4 hours of sleep = 38.8 + 8 * 4 = 70.8\nresidual error = distance from each individual dot to the line; we square these errors and add them up to calcualte the sum of squared errors, then subtract them from SSE when using the mean as a prediction to estimate the reduction in error. ($R^2$ = (SST - SSE) / SST)\nsampling error = 11.2 = how much the line would vary if we were to resample the data (AN ESTIMATE / MADE UP GUESS).\n\nslope ¬± 1.96 * the sampling error = 95% confidence interval.\nsampling error in units of slope\nsampling error is kind of like standard deviation (how much an individual differs on average from our expected value)\n\nindividual = an individual slope if resampled (vs.¬†the individual data point)\nexpected value = the original slope in our sample (vs.¬†the mean of our sample)\n\n\n\n\n\n\nCode\nm1 &lt;- cbind(t(coef(mod1)), SampleError = sd(b1), R2 = summary(mod1)$r.squared)\nm2 &lt;- cbind(t(coef(mod2)), SampleError = sd(b2), R2 = summary(mod2)$r.squared)\n\nx &lt;- rbind(m1, m2)\nrownames(x) &lt;- c(\"Model 1\", \"Model 2\")\ncolnames(x) &lt;- c(\"Intercept\", \"Slope\", \"Sample Error (Slope)\", \"R-Squared\")\nround(x, 2)\n\n\n        Intercept Slope Sample Error (Slope) R-Squared\nModel 1     38.81  7.95                10.47      0.03\nModel 2     42.84  6.15                 5.59      0.05\n\n\nUse Models 1 and 2 to answer the questions below.\n\nDescribe the relationship in Model 1. What do you observe? Why do you think this relationship exists?\nDescribe the relationship in Model 2. What do you observe? Why do you think this relationship exists?\nWhat is the predicted amount of phone checking for someone who got zero hours of sleep?1. ‚Ä¶for someone who got four hours of sleep?\n‚Ä¶for someone who‚Äôs 0/10 happy with their advisor?\n‚Ä¶for someone who‚Äôs 10/10 happy with thier advisor?\nWhich variable is a better predictor of the number of times someone checks their phone?\nHow can you tell?"
  },
  {
    "objectID": "gradstats/gradlabs/5_ModelReview.html#more-on-graphs-and-sample-bias.",
    "href": "gradstats/gradlabs/5_ModelReview.html#more-on-graphs-and-sample-bias.",
    "title": "Lecture 5 | Reviewing Models",
    "section": "More on Graphs, and Sample Bias.",
    "text": "More on Graphs, and Sample Bias.\n\nThe Monkey in Lab 4\n\nWhat are our takeaways from this example?\nDid you find the gorilla? Why / why not??\n\n\n\nResearch found that when people were given a hypothesis, they were less likely to see the gorilla in the data then when left to ‚Äúexplore‚Äù the dataset on their own.\n\n\n\n\n\n\n\nggplot2 review\nWhich graph (representing how the relationship between age and self-esteem changes depending on whether someone is Male, Female, or Other) do you like the most? Why??"
  },
  {
    "objectID": "gradstats/gradlabs/5_ModelReview.html#linear-models-review",
    "href": "gradstats/gradlabs/5_ModelReview.html#linear-models-review",
    "title": "Lecture 5 | Reviewing Models",
    "section": "Linear Models Review",
    "text": "Linear Models Review\n\nZ-Scores to Aid Interpretation.\nWhen we z-score a variable, we describe how far an individual score falls from the mean, in units of standard deviation. This gives us more context about how much an individual differs from the mean (relative to others). Z-scoring a variable also removes the units of measurement (since the units are on the numerator and denominator, so they divide out.) The z-score is considered a linear transformation, because the fundamental order of the data do not change. (We‚Äôll later talk about non-linear transformations, such as quadratic or logarithmic transformations.)\nWhen we z-score both variables in a model, we are describing the relationship between the two variables in units of standard deviation. This is commonly known as the correlation coefficient (\\(r\\)). And so the slope of a linear model where the DV and IV have been z-scored is equivalent to a correlation.\nWe can test this with the code below.\n\npar(mfrow = c(1,2))\nplot(check.phone ~ hrs.sleep, data = g,\n     ylab = \"# of Times Student Checked Phone\",\n     xlab = \"Hours of Sleep\", \n     main = \"Model 1 (Raw Units)\")\nmod1 &lt;- lm(check.phone ~ hrs.sleep, data = g)\nabline(mod1, lwd = 5, col = 'red')\n\nplot(scale(check.phone) ~ scale(hrs.sleep), data = g,\n     ylab = \"# of Times Student Checked Phone\",\n     xlab = \"Hours of Sleep\", \n     main = \"Model 2 (Z-Scored Units)\")\nmod1Z &lt;- lm(scale(check.phone) ~ scale(hrs.sleep), data = g)\nabline(mod1Z, lwd = 5, col = 'red')\n\n\n\n\n\n\n\n\nNote that the data and regression line do not change, but our units are different.\n\ndata.frame(coef(mod1), coef(mod1Z))\n\n            coef.mod1. coef.mod1Z.\n(Intercept)  38.806035 -0.01970378\nhrs.sleep     7.951007  0.19613194\n\n\n\nThe Intercept : The Predicted Value of Y when ALL X values are Zero.\n\nIn Raw Units :\nIn Z-Score Units :\n\nThe Slope : The Change in Our Predicted Value of Y when X Changes By One.\n\nIn Raw Units :\nIn Z-Score Units (Standardized Slope):\n\n\nNote that the correlation coefficient is equivalent to the standardized slope with one IV, and that this is the square root of our good friend \\(R^2\\).\n\ncor(data.frame(g$stress,g$hrs.sleep), use = \"pairwise.complete.obs\")\n\n               g.stress g.hrs.sleep\ng.stress     1.00000000 -0.06068984\ng.hrs.sleep -0.06068984  1.00000000\n\nlm(scale(hrs.sleep) ~ scale(stress), data = g)\n\n\nCall:\nlm(formula = scale(hrs.sleep) ~ scale(stress), data = g)\n\nCoefficients:\n  (Intercept)  scale(stress)  \n   -1.746e-16     -6.069e-02  \n\n\nThere‚Äôs also a function standardize() from the arm package (authored by stats wizards Andrew Gelman and Yu-Sung Su) that will automatically z-score the terms in your model. This uses a slightly different calculation for the z-score - rather than divide by one standard deviation, the authors recommend dividing by two standard deviations. You can read more about this logic here. This function is helpful when you have multiple IVs, or some IVs are not numeric (but you still want or need to standardize them; we‚Äôll talk more about these methods when we start working with categorical/binary variables.)\n\nlibrary(arm)\n\nLoading required package: MASS\n\n\nLoading required package: Matrix\n\n\nLoading required package: lme4\n\n\n\narm (Version 1.14-4, built: 2024-4-1)\n\n\nWorking directory is /Users/adcat/Documents/Documents - minicat/catterson.github.io/gradstats/gradlabs\n\nstandardize(mod1, standardize.y = T)\n\n\nCall:\nlm(formula = z.check.phone ~ z.hrs.sleep, data = g)\n\nCoefficients:\n(Intercept)  z.hrs.sleep  \n  -0.009852     0.196132"
  },
  {
    "objectID": "gradstats/gradlabs/5_ModelReview.html#reviewing-linear-models-z-scores-and-assumptions",
    "href": "gradstats/gradlabs/5_ModelReview.html#reviewing-linear-models-z-scores-and-assumptions",
    "title": "Lecture 5 | Reviewing Models",
    "section": "Reviewing Linear Models : Z-Scores and Assumptions",
    "text": "Reviewing Linear Models : Z-Scores and Assumptions\n\nZ-Scores to Aid Interpretation.\nWhen we z-score a variable, we describe how far an individual score falls from the mean, in units of standard deviation. This gives us more context about how much an individual differs from the mean (relative to others). Z-scoring a variable also removes the units of measurement (since the units are on the numerator and denominator, so they divide out.) The z-score is considered a linear transformation, because the fundamental order of the data do not change. (We‚Äôll later talk about non-linear transformations, such as quadratic or logarithmic transformations.)\nWhen we z-score both variables in a model, we are describing the relationship between the two variables in units of standard deviation. This is commonly known as the correlation coefficient (\\(r\\)). And so the slope of a linear model where the DV and IV have been z-scored is equivalent to a correlation.\n\nZ-Scoring in a Linear Model\nWe can test this with the code below. Look at the graphs - what‚Äôs different? what‚Äôs the same?\n\n\nCode\npar(mfrow = c(1,2))\nplot(check.phone ~ hrs.sleep, data = g,\n     ylab = \"# of Times Student Checked Phone (Z-Scored)\",\n     xlab = \"Hours of Sleep (Z-Scored)\", \n     main = \"Model 1 (Raw Units)\")\nmod1 &lt;- lm(check.phone ~ hrs.sleep, data = g)\nabline(mod1, lwd = 5, col = 'red')\n\nplot(scale(check.phone) ~ scale(hrs.sleep), data = g,\n     ylab = \"# of Times Student Checked Phone (Z-Scored)\",\n     xlab = \"Hours of Sleep (Z-Scored)\", \n     main = \"Model 1Z (Z-Scored Units)\")\nmod1Z &lt;- lm(scale(check.phone) ~ scale(hrs.sleep), data = g)\nabline(mod1Z, lwd = 5, col = 'red')\n\n\n\n\n\n\n\n\n\nNote that the data and regression line do not change, but our units are different.\n\n\nInterpreting a Z-Score Slope and Intercept.\n\n\nCode\nround(data.frame(coef(mod1), coef(mod1Z)), 2)\n\n\n            coef.mod1. coef.mod1Z.\n(Intercept)      38.81       -0.02\nhrs.sleep         7.95        0.20\n\n\n\nThe Intercept : The Predicted Value of Y when ALL X values are Zero.\n\nIn Raw Units :\nStandardized (Z-Score) :\n\nThe Slope : The Change in Our Predicted Value of Y when X Changes By One.\n\nIn Raw Units :\nStandardized (Z-Scored) :\n\n\nNote that the correlation coefficient should be equivalent to the standardized slope with one IV, and that this is the square root of our good friend \\(R^2\\).\n\n\nCode\ncoef(mod1Z)[2] # our standardized slope\n\n\nscale(hrs.sleep) \n       0.1961319 \n\n\nCode\nsummary(mod1)$r.squared # our R^2 value\n\n\n[1] 0.02955576\n\n\nCode\nsummary(mod1)$r.squared^.5 # the square root of R^2 = r = the correlation coefficient\n\n\n[1] 0.1719179\n\n\nCode\ncor(g$check.phone,g$hrs.sleep, use = \"pairwise.complete.obs\") # r = the correlation coefficient\n\n\n[1] 0.1719179\n\n\nUh oh, it‚Äôs not exactly the same! Close, but different‚Ä¶.why might this be?\nNOTE : There‚Äôs also a function standardize() from the arm package (authored by stats wizards Andrew Gelman and Yu-Sung Su) that will automatically z-score the terms in your model. This uses a slightly different calculation for the z-score - rather than divide by one standard deviation, the authors recommend dividing by two standard deviations. You can read more about this logic here. This function is helpful when you have multiple IVs, or some IVs are not numeric (but you still want or need to standardize them; we‚Äôll talk more about these methods when we start working with categorical/binary variables.)\n\n\nCode\nlibrary(arm)\n\n\nLoading required package: MASS\n\n\nLoading required package: Matrix\n\n\nLoading required package: lme4\n\n\n\narm (Version 1.14-4, built: 2024-4-1)\n\n\nWorking directory is /Users/adcat/Documents/Documents - minicat/catterson.github.io/gradstats/gradlabs\n\n\nCode\nstandardize(mod1, standardize.y = T)\n\n\n\nCall:\nlm(formula = z.check.phone ~ z.hrs.sleep, data = g)\n\nCoefficients:\n(Intercept)  z.hrs.sleep  \n  -0.009852     0.196132"
  },
  {
    "objectID": "gradstats/gradlabs/5_ModelReview.html#break-time-meet-back-at-1040",
    "href": "gradstats/gradlabs/5_ModelReview.html#break-time-meet-back-at-1040",
    "title": "Lecture 5 | Reviewing Models",
    "section": "BREAK TIME : Meet Back at 10:40",
    "text": "BREAK TIME : Meet Back at 10:40"
  },
  {
    "objectID": "gradstats/gradlabs/5_ModelReview.html#presentations",
    "href": "gradstats/gradlabs/5_ModelReview.html#presentations",
    "title": "Lecture 5 | Reviewing Models",
    "section": "Presentations",
    "text": "Presentations\n\nAssumptions of Linear Regression.\nThe interpretation of our linear model depends on a variety of different assumptions being met. You can read more about these from the great Gelman & Hill‚Äôs textbook : Data Analysis Using Regression. But below is a TLDR, with a few ideas of my own thrown in :)\n\nValidity. Is your linear model the right model to answer the research question that you have? This is the big question, and often goes beyond the specific statistics that we are focusing on. Did you include the right variables in your model, or are you leaving something out? Are you studying the right sample of people, drawn from the right population? Are your measures valid? This is hard to do, and good to remember that it is hard so you ask yourself ‚Äúam I doing a good job‚Äù?\nReliability of Measures. The variables in your linear model should be measured with low error (‚Äúgarbage in, garbage out‚Äù). There are different ways to assess reliability.\n\nCronbach‚Äôs alpha (psych::alpha()) is good for assessing the inter-item reliability of a likert scale.\nThe Intraclass Correlation Coefficient (ICC) is often used for observational ratings where multiple judges form impressions of the same target.\nTest-retest reliability is a great way to ensure that physiological or single-item measures will yield repeatable measures over time (assuming the target of measurement has not changed between time points). One way to test this is to define a linear model to predict the measure at one time point from the measure at another; you‚Äôd expect to see a strong relationship.\n\nIndependence. This is a big one - the residuals in your model need to be unrelated to each other. That is, I should not be able to predict the value of one error from another error. When the data in a linear model come from distinct individuals who are unrelated to each other, this assumption is usually considered to be met. However, there are often types of studies where the data are not independent.\n\nNested Data : Often times, individuals in a dataset belong to a group where there‚Äôs a clear dependence. For example, the happiness of individual members of a family is probably dependent on one another (since they share the same environment, stressors, etc.); the test-scores of children in a school are probably all related to each other (since they share the same teachers, administrators, funding, lead exposure in the drinking water, etc.)\nRepeated Measures : If a person is measured multiple times, then their data at one time point will be related to their data at the second time point, since they are coming from the same person, with the same past experiences and beliefs and genetics and all that other good stuff.\n\nIf the data are not independent, then we will need to account for the dependence in the data. We will learn to do this when we review Multilevel Linear Models. (Spoiler : it‚Äôs more lines.)\nLinearity. The dependent (outcome) variable should be the result of one or more linear functions (slopes). In other words, the outcome is the result of a bunch of straight lines. If the straight lines don‚Äôt properly ‚Äúfit‚Äù or ‚Äúexplain‚Äù your data, then maybe you need some non-straight lines‚Ä¶you could bend the line (add a quadratic term), look for an interaction term (that tests whether there‚Äôs a multiplicative relationship between the variables), or apply a non-linear transformation to the data (i.e., often times income is log transformed, since a 9,000 point difference between 1000 and 10,000 dollars/month is not the same as a 9,000 point difference between 1,000,000 and 1,009,000 dollars/month).\nEqual Variance of Errors (Homoscedasticity). The errors associated with the predicted values of the DV (the residuals) should be similar for all different values of the IV. Homoscedasticity means that the residual errors in our outcome variable are distributed the same across the different values of the IV. Heteroscedasticity means there is some non-constant variability in errors, which means that your model may not be an appropriate explanation of the data.\nNormality of Errors (Residuals and Sampling). This assumption is less emphasized as our datasets have gotten larger, and methods for estimating sampling error have improved. But the basic idea is that statistics like confidence intervals and estimates of slopes assume that the errors in our model are normally distributed. If they are not, it‚Äôs likely that your model is not appropriately fitting the data (i.e., maybe some outliers are influencing the results.)\n\n\n\nTesting Assumptions in R\nAssumptions 1-3 require critical thinking about your methods and measures.\nAssumptions 4 and 5 can be examined using the plot() function in base R - you may have accidentally come across these when trying to graph your linear model.\nWe talked about how to interpret these plots a little in lecture; here‚Äôs another tutorial that walks through the interpretation. Let us know (on Discord!) if you find another good example / explanation.\n\n\nCode\npar(mfrow = c(2,2))\nplot(mod1)\n\n\n\n\n\n\n\n\n\nCode\ng[10,]\n\n\n                        Time howgo howlab3 howR hunger stress social organized\n10 2025/02/14 9:24:46 AM PST     7       9    8     10      8      9         6\n   open agreeable givedata love2scroll tired cooladvisor lovebay had.breakfast\n10    8         7        2           8     9          10       7            No\n   have.water food.pref check.phone hrs.sleep checkin.go\n10        Yes  burritos         280       5.5          7\n\n\nAssumption 6 can be examined by graphing the residuals of your model object with a histogram.\n\n\nCode\npar(mfrow = c(1,1))\nhist(mod1$residuals)\n\n\n\n\n\n\n\n\n\nOkay! These are the basics of linear models. For Lab 5, you‚Äôll take a practice mini exam that brings all these ideas together. I‚Äôll post a key so you can check your work / understanding before the mini exam. We will continue to work with linear models for many weeks; and build more complexity to try and make better, more valid, predictions. Yeah!!"
  },
  {
    "objectID": "gradstats/gradlabs/5_ModelReview.html#evaluating-linear-models-z-scores-and-assumptions",
    "href": "gradstats/gradlabs/5_ModelReview.html#evaluating-linear-models-z-scores-and-assumptions",
    "title": "Lecture 5 | Reviewing Models",
    "section": "Evaluating Linear Models : Z-Scores and Assumptions",
    "text": "Evaluating Linear Models : Z-Scores and Assumptions\n\nZ-Scores to Aid Interpretation.\nWhen we z-score a variable, we describe how far an individual score falls from the mean, in units of standard deviation. This gives us more context about how much an individual differs from the mean (relative to others). Z-scoring a variable also removes the units of measurement (since the units are on the numerator and denominator, so they divide out.) The z-score is considered a linear transformation, because the fundamental order of the data do not change. (We‚Äôll later talk about non-linear transformations, such as quadratic or logarithmic transformations.)\nWhen we z-score both variables in a model, we are describing the relationship between the two variables in units of standard deviation. This is commonly known as the correlation coefficient (\\(r\\)). And so the slope of a linear model where the DV and IV have been z-scored is equivalent to a correlation.\n\nZ-Scoring in a Linear Model\nWe can test this with the code below. Look at the graphs - what‚Äôs different? what‚Äôs the same?\n\n\nCode\npar(mfrow = c(1,2))\nplot(check.phone ~ hrs.sleep, data = g,\n     ylab = \"# of Times Student Checked Phone (Z-Scored)\",\n     xlab = \"Hours of Sleep (Z-Scored)\", \n     main = \"Model 1 (Raw Units)\")\nmod1 &lt;- lm(check.phone ~ hrs.sleep, data = g)\nabline(mod1, lwd = 5, col = 'red')\n\nplot(scale(check.phone) ~ scale(hrs.sleep), data = g,\n     ylab = \"# of Times Student Checked Phone (Z-Scored)\",\n     xlab = \"Hours of Sleep (Z-Scored)\", \n     main = \"Model 1Z (Z-Scored Units)\")\nmod1Z &lt;- lm(scale(check.phone) ~ scale(hrs.sleep), data = g)\nabline(mod1Z, lwd = 5, col = 'red')\n\n\n\n\n\n\n\n\n\nNote that the data and regression line do not change, but our units are different.\n\n\nInterpreting a Z-Score Slope and Intercept.\n\n\nCode\nround(data.frame(coef(mod1), coef(mod1Z)), 2)\n\n\n            coef.mod1. coef.mod1Z.\n(Intercept)      38.81       -0.02\nhrs.sleep         7.95        0.20\n\n\n\nThe Intercept : The Predicted Value of Y when ALL X values are Zero.\n\nIn Raw Units : The predicted value of phone checking for someone with zero hours of sleep.\nStandardized (Z-Score) :The predicted value of phone checking (in z-score units) for someone with the average sleep (zscore = 0).\n\nz-scored intercept will always be zero, since our prediction is the ‚Äúthe average person on X will be average on Y‚Äù (without information on how people differ on X, our best guess is that they are average in Y, since the average is the best guess of a variable.\n\n\nThe Slope : The Change in Our Predicted Value of Y when X Changes By One.\n\nIn Raw Units : for every one hour of sleep, a person checks their phone 38.81 more times.\nStandardized (Z-Scored) : for every 1-standard deviation increase in hours of sleep, a person increases their phone checking by .2 standard deviations\n\nthis is called a standardized beta\nAND : when you have two numeric variables that been z-scored, it is equal to the correlation coefficient (r)\n\n\n\nNote that the correlation coefficient should be equivalent to the standardized slope with one IV, and that this is the square root of our good friend \\(R^2\\).\n\n\nCode\ncoef(mod1Z)[2] # our standardized slope\n\n\nscale(hrs.sleep) \n       0.1961319 \n\n\nCode\nsummary(mod1)$r.squared # our R^2 value\n\n\n[1] 0.02955576\n\n\nCode\nsummary(mod1)$r.squared^.5 # the square root of R^2 = r = the correlation coefficient\n\n\n[1] 0.1719179\n\n\nCode\ncor(g$check.phone,g$hrs.sleep, use = \"pairwise.complete.obs\") # r = the correlation coefficient\n\n\n[1] 0.1719179\n\n\nCode\n?cor\n\n\nUh oh, it‚Äôs not exactly the same! Close, but different‚Ä¶.why might this be?\n\n\nCode\ngmod &lt;- with(g, data.frame(check.phone, hrs.sleep))\ngmod\n\n\n   check.phone hrs.sleep\n1        46.00       6.5\n2        15.00       5.5\n3        90.00       7.0\n4        52.00       2.0\n5        84.00       6.0\n6        23.00       7.0\n7        90.00       8.0\n8       148.00       7.0\n9         6.41       6.0\n10      280.00       5.5\n11          NA       2.0\n12       40.00       6.0\n13       72.00       7.0\n14       48.00       7.0\n15      182.00       7.0\n16      128.00       6.0\n17        7.00       3.0\n18      180.00       8.0\n19      136.00       8.0\n20       92.00       7.0\n21        8.00       7.0\n22      110.00       7.0\n23       95.00       7.0\n24      100.00       7.0\n25      164.00       4.0\n26       30.00       6.5\n\n\nCode\ngmod &lt;- na.omit(gmod)\n\n\nmoddyZ &lt;- lm(scale(check.phone) ~ scale(hrs.sleep), data = gmod)\nround(coef(moddyZ), 2)\n\n\n     (Intercept) scale(hrs.sleep) \n            0.00             0.17 \n\n\nNOTE : There‚Äôs also a function standardize() from the arm package (authored by stats wizards Andrew Gelman and Yu-Sung Su) that will automatically z-score the terms in your model. This uses a slightly different calculation for the z-score - rather than divide by one standard deviation, the authors recommend dividing by two standard deviations. You can read more about this logic here. This function is helpful when you have multiple IVs, or some IVs are not numeric (but you still want or need to standardize them; we‚Äôll talk more about these methods when we start working with categorical/binary variables.)\n\n\nCode\nlibrary(arm)\n\n\nLoading required package: MASS\n\n\nLoading required package: Matrix\n\n\nLoading required package: lme4\n\n\n\narm (Version 1.14-4, built: 2024-4-1)\n\n\nWorking directory is /Users/adcat/Documents/Documents - minicat/catterson.github.io/gradstats/gradlabs\n\n\nCode\nstandardize(mod1, standardize.y = T)\n\n\n\nCall:\nlm(formula = z.check.phone ~ z.hrs.sleep, data = g)\n\nCoefficients:\n(Intercept)  z.hrs.sleep  \n  -0.009852     0.196132"
  },
  {
    "objectID": "gradstats/gradlabs/5_ModelReview.html#practice.",
    "href": "gradstats/gradlabs/5_ModelReview.html#practice.",
    "title": "Lecture 5 | Reviewing Models",
    "section": "PRACTICE.",
    "text": "PRACTICE.\nWork with a buddy. Load the mini_grad dataset (also in our Dropbox Dataset folder) and use these data to do the following.\n\nGraph the variable stress. What do you observe?\nChoose another numeric variable that you think will predict stress. Why did you choose this variable? What theory do you have?\nDefine a linear model to predict stress from this IV. What is the slope, intercept, and \\(R^2\\) of this model (in raw and z-scored units)? Graph the relationship between these variables, making sure to illustrate the linear model.\nThen, use bootstrapping to estimate the 95% Confidence Interval for the slope and estimate the power researchers had to detect any observed relationship. (Cool if you can add lines to illustrate the 95% Confidence Interval) on your graph. Below the graph, describe what these statistics tell you about the relationship between these two variables.\nAdd your results to the Grad Student Vision Board. Is this thing on?"
  },
  {
    "objectID": "calstats/Lectures/5L_Methods.html",
    "href": "calstats/Lectures/5L_Methods.html",
    "title": "5L_Methods",
    "section": "",
    "text": "Use the cal_mega dataset (from Lab 4) to answer the questions in the check-in. Make sure to save your work in an R script.\n\nm &lt;- read.csv(\"~/Dropbox/!WHY STATS/Class Datasets/cal_mega_SP25.csv\",\n              stringsAsFactors = T)\nhead(m) # checking my data.\n\n                       time    sex age                       eth\n1 2025/02/14 4:52:49 PM PST   Male  20                     Asian\n2 2025/02/14 4:52:49 PM PST Female  20 White / European American\n3 2025/02/14 4:53:00 PM PST Female  20 White / European American\n4 2025/02/14 4:53:21 PM PST Female  19                     Asian\n5 2025/02/14 4:54:09 PM PST Female  20           Asian and white\n6 2025/02/14 4:54:29 PM PST   Male  24                     Asian\n                               ed.mom                               ed.dad\n1                        Some college                         Some college\n2                        Some college  Professional / Post-graduate degree\n3 Professional / Post-graduate degree High school graduation or equivalent\n4                        Some college High school graduation or equivalent\n5                  College graduation                   College graduation\n6 Professional / Post-graduate degree  Professional / Post-graduate degree\n               income e1 a1 c1r n1 o1 e2 a2r c2r n2 o2r e3 a3 c3 n4r o4 e4 a4r\n1       Over $175,001  3  3   4  4  4  3   3   4  4   3  3  3  3   4  4  4   4\n2       Over $175,001  3  4   3  5  5  3   3   3  3   3  3  3  4   2  3  4   3\n3       Over $175,001  2  4   2  4  4  4   3   2  3   1  4  4  4   4  4  4   1\n4       Over $175,001  4  4   2  4  4  3   1   3  4   3  3  4  4   2  3  2   2\n5 $125,001 - $175,000  2  5   2  4  5  3   2   2  2   1  4  4  4   3  4  5   2\n6       Over $175,001  5  5   5  5  5  5   5   5  5   5  5  5  5   5  5  5   5\n  c4 n4r.1 o4r e5r a5 c5 n5r o5 e6r a6r c6r n6 o6r swls1 swls2 swls3 swls4\n1  2     2   2   3  2  2   3  3   2   2   3  3   2     2     3     3     3\n2  3     2   2   3  3  3   3  3   3   3   3  3   3     3     3     3     3\n3  2     3   1   1  5  5   4  4   2   2   1  1   1     5     5     5     4\n4  3     3   2   3  4 NA  NA NA  NA  NA  NA NA  NA    NA    NA    NA    NA\n5  4     3   1   3  5  5   5  5   2   3   2  4   1     4     4     4     4\n6  5     5   5   5  5  5   5  5   5   5   5  5   5     5     5     5     5\n  swls5 pow1 pow2r soul1 soul2r soul3 soul4r water1 water2 water3r water4r\n1     3    3     3     4      3     3      3      3      3       3       3\n2     3    3     3     3      4     4      4      4      4       2       3\n3     4    4     2     5      2     4      5      4      4       1       4\n4    NA   NA    NA    NA     NA    NA     NA     NA     NA      NA      NA\n5     4    4     2     4      4     3      4      5      5       1       1\n6     5    5     5     5      5     5      5      5      5       5       5\n  brekky1 brekky2 brekky3r thinkcap1 thinkcap2 thinkcap3r thinkcap4r goodcap1\n1       3       3        2         2         2          4          4        4\n2       4       3        3         3         3          3          3        2\n3       1       5        1         4         4          1          1        2\n4      NA      NA       NA        NA        NA         NA         NA       NA\n5       2       5        2         5         2          2          4        3\n6       5       5        5         5         5          5          5        5\n  goodcap2r goodcap3 goodcap4r pwe1 pwe2 pwe3 pwe4r dance1 dance2 dance3r\n1         4        3         3    4    4    3     2      2      2       2\n2         4        1         4    4    4    4     1      4      4       3\n3         4        2         4    1    4    2     2      5      4       1\n4        NA       NA        NA   NA   NA   NA    NA     NA     NA      NA\n5         4        2         4    2    1    1     3      5      4       1\n6         5        5         5    5    5    5     5      5      5       5\n  dance4r phone1 phone2 phone3r diffpeep1 diffpeep2 diffpeep3r diffpeep4r\n1       2      2      2       3         3         4          4          4\n2       3      3      3       3         3         3          3          3\n3       1      4      1       5         5         5          1          1\n4      NA     NA     NA      NA        NA        NA         NA         NA\n5       1      4      3       3         5         4          1          2\n6       5      5      5       5         5         5          5          5\n  timer1 timer2 timer3r timer4r nature1 nature2 nature3r nature4r parents1\n1      3      4       4       4       4       4        4        3        5\n2      3      3       3       3       3       3        3        3        5\n3      4      1       4       4       4       4        1        1        5\n4     NA     NA      NA      NA      NA      NA       NA       NA       NA\n5      4      3       2       4       5       4        2        3        4\n6      5      5       5       5       5       5        5        5        5\n  parents2 parents3r parents4r justworld1 justworld2 justworld3 justworld4 narc\n1        5         3         2          5          4          4          3    3\n2        5         3         3          3          1          3          3    1\n3        4         1         1          4          2          2          3    1\n4       NA        NA        NA         NA         NA         NA         NA   NA\n5        4         2         2          3          3          2          3    3\n6        5         5         5          5          5          5          5    5\n  sleep1 sleep2 sleep3R sleep4 sleep5 sleep6 sleep7 sleep8 pss1 pss2R pss3R\n1      2      2       2      2      3      3      2      3    3     3     2\n2      2      2       3      2      2      3      2      2    2     2     1\n3      1      1       3      1      1      0      0      4    0     3     3\n4      2      2       2      2     NA     NA     NA     NA   NA     2    NA\n5      2      2       3      1      1      1      3      2    3     3     2\n6      4      4       4      4      4      4      4      4    4     4     4\n  pss4 onlineshop1 onlineshop2 onlineshop3 onlineshop4 onlineshop5 onlineshop6R\n1    2           3           3           3           3           3            3\n2    3           1           2           1           1           1            3\n3    0           0           3           3           0           0            0\n4   NA          NA          NA          NA          NA          NA           NA\n5    1           1           1           1           0           0            3\n6    4           4           4           4           4           4            4\n  onlineshop7R taco.best smoke.pot pinepizzatasty procrastinate boba pref.book\n1            4       Yes        No             No            No  Yes       Yes\n2            3       Yes       Yes            Yes           Yes   No        No\n3            0        No        No            Yes            No   No        No\n4           NA        No        No             No           Yes   No        No\n5            3        No        No             No           Yes   No        No\n6            4        No        No             No            No   No        No\n  read.scifi need.caff love.music clubbed read.marx been.disney attend.class\n1         No        No        Yes      No        No          No          Yes\n2         No       Yes        Yes     Yes       Yes         Yes          Yes\n3         No       Yes        Yes     Yes       Yes         Yes          Yes\n4         No       Yes        Yes     Yes       Yes         Yes           No\n5        Yes        No        Yes      No       Yes         Yes          Yes\n6         No        No         No      No        No          No           No\n  had.breakfast call.folks tried.take\n1           Yes         No        Yes\n2           Yes        Yes        Yes\n3           Yes        Yes         No\n4            No         No         No\n5           Yes         No        Yes\n6            No         No         No\n                                                                                        job.work\n1                                                             No, I am just a student right now.\n2                                                             No, I am just a student right now.\n3   Yes, I am working a full-time job in addition to being a student (40 hours or more per week)\n4                                                             No, I am just a student right now.\n5 Yes, I am working a part-time job in addition to being a student (less than 40 hours per week)\n6                                                             No, I am just a student right now.\n  fav.weather hrs.sleep screens units hrs.screen items.amazon\n1       sunny         8       2    18        6.0            3\n2       sunny         8       2    16        8.0            1\n3    #fireszn         8       2    20        5.0            1\n4       rainy         8       2    15         NA            2\n5       sunny         8       2    16        4.5            0\n6       snowy         9       2    16        3.0            1\n\n\n\nWhat is the sample size of this dataset?\n\nnrow(m)\n\n[1] 122\n\n\nCreate a likert scale to measure people‚Äôs love of Dancing. This was measured with the following variables : dance1, dance2, dance3r, and dance4r.¬†Note that dance3r and dance4r are negatively-keyed, and will need to be reverse scored. What number will you need to subtract dance3r and dance4r from in order to reverse score them?\n\n## STEP 1. CREATE A DATA FRAME TO ISOLATE THE SCALE ITEMS!\nDANCE.df &lt;- data.frame(m$dance1, m$dance2, m$dance3r, m$dance4r)\nDANCE.df\n\n    m.dance1 m.dance2 m.dance3r m.dance4r\n1          2        2         2         2\n2          4        4         3         3\n3          5        4         1         1\n4         NA       NA        NA        NA\n5          5        4         1         1\n6          5        5         5         5\n7          4        4         4         4\n8          3        2         2         2\n9          3        3         2         2\n10         3        2         3         3\n11         3        2         3         2\n12         4        1         1         1\n13         2        1         4         3\n14         3        4         2         2\n15         3        2         1         2\n16         4        5         1         1\n17         4        4         2         2\n18         3        1         1         1\n19         5        5         5         5\n20         4        3         2         2\n21         2        4         1         1\n22         5        4         2         1\n23         4        4         1         1\n24         1        1         5         5\n25         5        4         1         1\n26         4        4         1         1\n27         5        1         2         2\n28         5        4         1         2\n29         4        4         1         2\n30         2        2         3         3\n31         5        5         1         1\n32         2        1         4         4\n33         3        3         3         3\n34         4        4         2         2\n35         2        2         3         4\n36         4        4         1         1\n37         2        2         4         4\n38         3        3         3         2\n39         2        1         4         4\n40         4        3         1         1\n41         3        2         2         2\n42         5        5         1         1\n43         5        5         1         1\n44         3        3         2         2\n45         5        1         1         1\n46         4        5         1         1\n47         5        5         1         1\n48         2        2         4         2\n49         3        4         1         1\n50         3        2         2         1\n51         4        4         1         1\n52         1        1         1         1\n53         4        4         2         2\n54         3        2         2         3\n55         5        5         1         1\n56         3        2         2         1\n57         5        4         1         1\n58         4        3         1         1\n59         3        2         3         4\n60         5        5         1         1\n61         5        5         1         1\n62         5        2         1         2\n63         4        2         1         2\n64         1        1         5         3\n65         4        4         1         1\n66         4        3         1         1\n67         4        2         2         3\n68         1        1         5         4\n69         5        5         1         1\n70         5        2         1         2\n71         1        2         4         3\n72         5        5         1         1\n73         4        1         2         2\n74         1        1         4         4\n75         2        1         4         3\n76         4        1         3         4\n77         3        1         3         5\n78         4        2         1         1\n79         4        1         1         5\n80         4        1         2         4\n81         1        1         5         2\n82         5        2         1         2\n83         4        2         2         2\n84         5        4         2         3\n85         4        4         2         2\n86         2        2         3         2\n87         4        2         2         2\n88         5        4         1         2\n89         2        3         2         4\n90         4        4         2         2\n91         2        2         3         2\n92         4        2         1         2\n93         1        1         3         4\n94         4        5         2         1\n95         5        4         1         1\n96         2        2         4         4\n97         5        5         1         1\n98         1        1         5         5\n99         3        2         2         2\n100        4        4         4         2\n101        4        4         2         2\n102        2        2         4         4\n103        4        5         1         1\n104        3        3         3         2\n105        3        3         1         2\n106        3        1         1         1\n107        4        1         2         3\n108        4        2         3         2\n109        5        4         1         2\n110        3        3         3         3\n111        5        4         1         1\n112        1        1         5         2\n113        4        4         2         3\n114        3        1         2         3\n115        5        4         1         1\n116        4        3         2         2\n117        2        2         4         4\n118        4        3         2         2\n119       NA       NA        NA        NA\n120        2        1         4         5\n121       NA        3         3         3\n122        4        3         2         1\n\n## STEP 2 : reverse score the negatively keyed items in the scale.\nDANCE.df &lt;- data.frame(m$dance1, m$dance2, 6-m$dance3r, 6-m$dance4r)\nrange(DANCE.df, na.rm = T) # a 1-5 scale = 1 + 5 = 6 = # to subract NK items from.\n\n[1] 1 5\n\nDANCE.df\n\n    m.dance1 m.dance2 X6...m.dance3r X6...m.dance4r\n1          2        2              4              4\n2          4        4              3              3\n3          5        4              5              5\n4         NA       NA             NA             NA\n5          5        4              5              5\n6          5        5              1              1\n7          4        4              2              2\n8          3        2              4              4\n9          3        3              4              4\n10         3        2              3              3\n11         3        2              3              4\n12         4        1              5              5\n13         2        1              2              3\n14         3        4              4              4\n15         3        2              5              4\n16         4        5              5              5\n17         4        4              4              4\n18         3        1              5              5\n19         5        5              1              1\n20         4        3              4              4\n21         2        4              5              5\n22         5        4              4              5\n23         4        4              5              5\n24         1        1              1              1\n25         5        4              5              5\n26         4        4              5              5\n27         5        1              4              4\n28         5        4              5              4\n29         4        4              5              4\n30         2        2              3              3\n31         5        5              5              5\n32         2        1              2              2\n33         3        3              3              3\n34         4        4              4              4\n35         2        2              3              2\n36         4        4              5              5\n37         2        2              2              2\n38         3        3              3              4\n39         2        1              2              2\n40         4        3              5              5\n41         3        2              4              4\n42         5        5              5              5\n43         5        5              5              5\n44         3        3              4              4\n45         5        1              5              5\n46         4        5              5              5\n47         5        5              5              5\n48         2        2              2              4\n49         3        4              5              5\n50         3        2              4              5\n51         4        4              5              5\n52         1        1              5              5\n53         4        4              4              4\n54         3        2              4              3\n55         5        5              5              5\n56         3        2              4              5\n57         5        4              5              5\n58         4        3              5              5\n59         3        2              3              2\n60         5        5              5              5\n61         5        5              5              5\n62         5        2              5              4\n63         4        2              5              4\n64         1        1              1              3\n65         4        4              5              5\n66         4        3              5              5\n67         4        2              4              3\n68         1        1              1              2\n69         5        5              5              5\n70         5        2              5              4\n71         1        2              2              3\n72         5        5              5              5\n73         4        1              4              4\n74         1        1              2              2\n75         2        1              2              3\n76         4        1              3              2\n77         3        1              3              1\n78         4        2              5              5\n79         4        1              5              1\n80         4        1              4              2\n81         1        1              1              4\n82         5        2              5              4\n83         4        2              4              4\n84         5        4              4              3\n85         4        4              4              4\n86         2        2              3              4\n87         4        2              4              4\n88         5        4              5              4\n89         2        3              4              2\n90         4        4              4              4\n91         2        2              3              4\n92         4        2              5              4\n93         1        1              3              2\n94         4        5              4              5\n95         5        4              5              5\n96         2        2              2              2\n97         5        5              5              5\n98         1        1              1              1\n99         3        2              4              4\n100        4        4              2              4\n101        4        4              4              4\n102        2        2              2              2\n103        4        5              5              5\n104        3        3              3              4\n105        3        3              5              4\n106        3        1              5              5\n107        4        1              4              3\n108        4        2              3              4\n109        5        4              5              4\n110        3        3              3              3\n111        5        4              5              5\n112        1        1              1              4\n113        4        4              4              3\n114        3        1              4              3\n115        5        4              5              5\n116        4        3              4              4\n117        2        2              2              2\n118        4        3              4              4\n119       NA       NA             NA             NA\n120        2        1              2              1\n121       NA        3              3              3\n122        4        3              4              5\n\n## STEP 3 : evaluate the reliability\nlibrary(psych)\nalpha(DANCE.df)\n\n\nReliability analysis   \nCall: alpha(x = DANCE.df)\n\n  raw_alpha std.alpha G6(smc) average_r S/N   ase mean sd median_r\n      0.84      0.84    0.85      0.57 5.4 0.024  3.5  1     0.56\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.79  0.84  0.88\nDuhachek  0.79  0.84  0.89\n\n Reliability if an item is dropped:\n               raw_alpha std.alpha G6(smc) average_r S/N alpha se  var.r med.r\nm.dance1            0.78      0.79    0.74      0.55 3.7    0.036 0.0276  0.47\nm.dance2            0.84      0.84    0.80      0.63 5.1    0.026 0.0175  0.67\nX6...m.dance3r      0.77      0.77    0.71      0.53 3.4    0.035 0.0091  0.48\nX6...m.dance4r      0.80      0.81    0.76      0.58 4.2    0.031 0.0158  0.64\n\n Item statistics \n                 n raw.r std.r r.cor r.drop mean  sd\nm.dance1       119  0.85  0.85  0.79   0.72  3.5 1.2\nm.dance2       120  0.79  0.77  0.66   0.60  2.8 1.4\nX6...m.dance3r 120  0.86  0.86  0.83   0.73  3.8 1.3\nX6...m.dance4r 120  0.81  0.82  0.75   0.66  3.8 1.2\n\nNon missing response frequency for each item\n                  1    2    3    4    5 miss\nm.dance1       0.08 0.14 0.19 0.34 0.24 0.02\nm.dance2       0.22 0.26 0.13 0.26 0.13 0.02\nX6...m.dance3r 0.07 0.12 0.14 0.28 0.40 0.02\nX6...m.dance4r 0.06 0.12 0.13 0.34 0.34 0.02\n\n## STEP 4 : IF the items in the scale are reliable, then I want to squish (or average) all the items into ONE NUMBER for each person. This will be a NEW VARIABLE created from the likert scale.\n\nm$DANCELOVE &lt;- rowMeans(DANCE.df)\n## USE THIS VARIABLE IN YOUR ANALYSES!!!\n\nWhat is the alpha reliability of this scale? Round to two decimal places.\n\nsee above!\n\nWhat is the mean of this scale? Round to two decimal places.\n\nmean(m$DANCELOVE, na.rm = T)\n\n[1] 3.491597\n\n\nGraph the variable as a histogram. Describe the shape; what do you learn about our class from this variable?\n\nhist(m$DANCELOVE)\n\n\n\n\n\n\n\n\nThe variable hrs.screen asked students to report how many hours of screens they used (according to their phone) in the last day. What is the mean of this variable? Round to two decimal places.\n\nmean(m$hrs.screen, na.rm = T)\n\n[1] 5.943551\n\nm$hrs.screen[m$hrs.screen &lt; 0] &lt;- NA\n\nhist(m$hrs.screen)\n\n\n\n\n\n\n\n\nHow many people did not report their hours of screen usage (i.e., how many NAs are there in this variable?)\n\nsummary(m$hrs.screen)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  1.680   4.000   6.000   6.004   7.375  15.000      16 \n\nlength(m$hrs.screen[m$hrs.screen == \"NA\"]) # another way!\n\n[1] 16\n\nsum(is.na(m$hrs.screen)) # another way!!!\n\n[1] 16\n\n\nWhat‚Äôs the z-score for someone who stared at their screen for 8-hours a day? Round to two decimal places.\n\n8 - mean(m$hrs.screen, na.rm = T) # distance from the mean\n\n[1] 1.99566\n\nsd(m$hrs.screen, na.rm = T) # the average person differs from the mean by 2.7 hours!\n\n[1] 2.74497\n\n(8 - mean(m$hrs.screen, na.rm = T))/sd(m$hrs.screen, na.rm = T) # distance from the mean/sd\n\n[1] 0.7270244\n\n# Z = .73\n\nscale(m$hrs.screen)\n\n               [,1]\n  [1,] -0.001580936\n  [2,]  0.727024443\n  [3,] -0.365883626\n  [4,]           NA\n  [5,] -0.548034971\n  [6,] -1.094489006\n  [7,]  1.455629823\n  [8,] -0.001580936\n  [9,] -0.730186316\n [10,]  0.362721754\n [11,]  0.362721754\n [12,]  0.727024443\n [13,] -0.001580936\n [14,]           NA\n [15,]  0.727024443\n [16,] -0.730186316\n [17,] -1.094489006\n [18,] -0.730186316\n [19,] -0.001580936\n [20,] -1.458791695\n [21,] -0.311238223\n [22,]  1.455629823\n [23,]           NA\n [24,]           NA\n [25,]  0.362721754\n [26,] -1.276640350\n [27,] -0.730186316\n [28,]  0.909175788\n [29,]  2.184235203\n [30,] -0.365883626\n [31,]           NA\n [32,]  1.455629823\n [33,]  1.455629823\n [34,]  0.362721754\n [35,] -0.001580936\n [36,] -1.094489006\n [37,]           NA\n [38,] -0.001580936\n [39,] -1.458791695\n [40,]  0.362721754\n [41,]  0.362721754\n [42,] -0.001580936\n [43,] -1.458791695\n [44,] -0.001580936\n [45,] -0.001580936\n [46,] -1.094489006\n [47,]  2.912840582\n [48,] -0.730186316\n [49,]  2.548537892\n [50,] -1.094489006\n [51,] -1.094489006\n [52,]           NA\n [53,]  0.362721754\n [54,] -0.365883626\n [55,]  1.455629823\n [56,]  0.144140140\n [57,] -0.001580936\n [58,]  0.727024443\n [59,] -1.094489006\n [60,]           NA\n [61,] -0.001580936\n [62,] -0.365883626\n [63,]           NA\n [64,] -0.730186316\n [65,] -0.001580936\n [66,]  0.362721754\n [67,] -0.365883626\n [68,] -0.365883626\n [69,] -1.094489006\n [70,]  0.362721754\n [71,] -0.730186316\n [72,]  0.727024443\n [73,]           NA\n [74,] -0.730186316\n [75,]           NA\n [76,] -0.001580936\n [77,]  0.727024443\n [78,]  2.184235203\n [79,] -1.032557548\n [80,] -1.094489006\n [81,] -0.365883626\n [82,] -0.001580936\n [83,] -1.458791695\n [84,] -0.365883626\n [85,] -1.094489006\n [86,]           NA\n [87,] -0.730186316\n [88,] -0.912337661\n [89,] -1.094489006\n [90,]  0.180570409\n [91,] -0.001580936\n [92,] -1.458791695\n [93,] -0.001580936\n [94,] -1.575368556\n [95,] -0.730186316\n [96,]           NA\n [97,] -0.001580936\n [98,] -0.730186316\n [99,]  1.819932513\n[100,]  0.727024443\n[101,] -0.001580936\n[102,]  1.455629823\n[103,] -0.821261988\n[104,]  0.727024443\n[105,] -0.365883626\n[106,]  1.455629823\n[107,]  3.277143272\n[108,]  0.362721754\n[109,] -0.799403827\n[110,]  1.455629823\n[111,]           NA\n[112,]  0.727024443\n[113,]  0.727024443\n[114,]  0.544873098\n[115,] -0.183732281\n[116,] -1.094489006\n[117,] -0.730186316\n[118,]           NA\n[119,]           NA\n[120,] -0.365883626\n[121,]  1.091327133\n[122,] -0.001580936\nattr(,\"scaled:center\")\n[1] 6.00434\nattr(,\"scaled:scale\")\n[1] 2.74497\n\nscale(m$hrs.screen)[m$hrs.screen == 8]\n\n [1] 0.7270244        NA 0.7270244        NA 0.7270244        NA        NA\n [8]        NA        NA        NA 0.7270244        NA        NA 0.7270244\n[15]        NA        NA 0.7270244        NA        NA 0.7270244 0.7270244\n[22]        NA 0.7270244 0.7270244        NA        NA\n\n\nWhat does this z-score tell you?\n\n\n\n\nMini Exam is in NEXT weeks [2/28]\n\nSTUDY GUIDE HERE\ntake home; open-note; open-book; DO ON YOUR OWN.\nheld during normal class; 85 minutes (DSP students get extra time accommodations)\npractice exam will post next week\n\nLab 5. Do the Fall 2024 Mini Exam. Look over the key (video key!) after you‚Äôve done. Submit your completed exam. Auto graded, but I want you do to. (PRACTICE PRACTICE PRACTICE!)\nMilestone #1 Due in TWO weeks [3/9]\n\n\n\n\n\n2:10 - 3:10 | Check-In and Review\n3:10 - 3:30 | Reliability and Validity\n3:30 - 3:42 | Break Time\n3:42 - 4:30 | Project Workshop."
  },
  {
    "objectID": "calstats/Lectures/5L_Methods.html#check-in-review-time",
    "href": "calstats/Lectures/5L_Methods.html#check-in-review-time",
    "title": "5L_Methods",
    "section": "",
    "text": "Use the cal_mega dataset (from Lab 4) to answer the questions in the check-in. Make sure to save your work in an R script.\n\nm &lt;- read.csv(\"~/Dropbox/!WHY STATS/Class Datasets/cal_mega_SP25.csv\",\n              stringsAsFactors = T)\nhead(m) # checking my data.\n\n                       time    sex age                       eth\n1 2025/02/14 4:52:49 PM PST   Male  20                     Asian\n2 2025/02/14 4:52:49 PM PST Female  20 White / European American\n3 2025/02/14 4:53:00 PM PST Female  20 White / European American\n4 2025/02/14 4:53:21 PM PST Female  19                     Asian\n5 2025/02/14 4:54:09 PM PST Female  20           Asian and white\n6 2025/02/14 4:54:29 PM PST   Male  24                     Asian\n                               ed.mom                               ed.dad\n1                        Some college                         Some college\n2                        Some college  Professional / Post-graduate degree\n3 Professional / Post-graduate degree High school graduation or equivalent\n4                        Some college High school graduation or equivalent\n5                  College graduation                   College graduation\n6 Professional / Post-graduate degree  Professional / Post-graduate degree\n               income e1 a1 c1r n1 o1 e2 a2r c2r n2 o2r e3 a3 c3 n4r o4 e4 a4r\n1       Over $175,001  3  3   4  4  4  3   3   4  4   3  3  3  3   4  4  4   4\n2       Over $175,001  3  4   3  5  5  3   3   3  3   3  3  3  4   2  3  4   3\n3       Over $175,001  2  4   2  4  4  4   3   2  3   1  4  4  4   4  4  4   1\n4       Over $175,001  4  4   2  4  4  3   1   3  4   3  3  4  4   2  3  2   2\n5 $125,001 - $175,000  2  5   2  4  5  3   2   2  2   1  4  4  4   3  4  5   2\n6       Over $175,001  5  5   5  5  5  5   5   5  5   5  5  5  5   5  5  5   5\n  c4 n4r.1 o4r e5r a5 c5 n5r o5 e6r a6r c6r n6 o6r swls1 swls2 swls3 swls4\n1  2     2   2   3  2  2   3  3   2   2   3  3   2     2     3     3     3\n2  3     2   2   3  3  3   3  3   3   3   3  3   3     3     3     3     3\n3  2     3   1   1  5  5   4  4   2   2   1  1   1     5     5     5     4\n4  3     3   2   3  4 NA  NA NA  NA  NA  NA NA  NA    NA    NA    NA    NA\n5  4     3   1   3  5  5   5  5   2   3   2  4   1     4     4     4     4\n6  5     5   5   5  5  5   5  5   5   5   5  5   5     5     5     5     5\n  swls5 pow1 pow2r soul1 soul2r soul3 soul4r water1 water2 water3r water4r\n1     3    3     3     4      3     3      3      3      3       3       3\n2     3    3     3     3      4     4      4      4      4       2       3\n3     4    4     2     5      2     4      5      4      4       1       4\n4    NA   NA    NA    NA     NA    NA     NA     NA     NA      NA      NA\n5     4    4     2     4      4     3      4      5      5       1       1\n6     5    5     5     5      5     5      5      5      5       5       5\n  brekky1 brekky2 brekky3r thinkcap1 thinkcap2 thinkcap3r thinkcap4r goodcap1\n1       3       3        2         2         2          4          4        4\n2       4       3        3         3         3          3          3        2\n3       1       5        1         4         4          1          1        2\n4      NA      NA       NA        NA        NA         NA         NA       NA\n5       2       5        2         5         2          2          4        3\n6       5       5        5         5         5          5          5        5\n  goodcap2r goodcap3 goodcap4r pwe1 pwe2 pwe3 pwe4r dance1 dance2 dance3r\n1         4        3         3    4    4    3     2      2      2       2\n2         4        1         4    4    4    4     1      4      4       3\n3         4        2         4    1    4    2     2      5      4       1\n4        NA       NA        NA   NA   NA   NA    NA     NA     NA      NA\n5         4        2         4    2    1    1     3      5      4       1\n6         5        5         5    5    5    5     5      5      5       5\n  dance4r phone1 phone2 phone3r diffpeep1 diffpeep2 diffpeep3r diffpeep4r\n1       2      2      2       3         3         4          4          4\n2       3      3      3       3         3         3          3          3\n3       1      4      1       5         5         5          1          1\n4      NA     NA     NA      NA        NA        NA         NA         NA\n5       1      4      3       3         5         4          1          2\n6       5      5      5       5         5         5          5          5\n  timer1 timer2 timer3r timer4r nature1 nature2 nature3r nature4r parents1\n1      3      4       4       4       4       4        4        3        5\n2      3      3       3       3       3       3        3        3        5\n3      4      1       4       4       4       4        1        1        5\n4     NA     NA      NA      NA      NA      NA       NA       NA       NA\n5      4      3       2       4       5       4        2        3        4\n6      5      5       5       5       5       5        5        5        5\n  parents2 parents3r parents4r justworld1 justworld2 justworld3 justworld4 narc\n1        5         3         2          5          4          4          3    3\n2        5         3         3          3          1          3          3    1\n3        4         1         1          4          2          2          3    1\n4       NA        NA        NA         NA         NA         NA         NA   NA\n5        4         2         2          3          3          2          3    3\n6        5         5         5          5          5          5          5    5\n  sleep1 sleep2 sleep3R sleep4 sleep5 sleep6 sleep7 sleep8 pss1 pss2R pss3R\n1      2      2       2      2      3      3      2      3    3     3     2\n2      2      2       3      2      2      3      2      2    2     2     1\n3      1      1       3      1      1      0      0      4    0     3     3\n4      2      2       2      2     NA     NA     NA     NA   NA     2    NA\n5      2      2       3      1      1      1      3      2    3     3     2\n6      4      4       4      4      4      4      4      4    4     4     4\n  pss4 onlineshop1 onlineshop2 onlineshop3 onlineshop4 onlineshop5 onlineshop6R\n1    2           3           3           3           3           3            3\n2    3           1           2           1           1           1            3\n3    0           0           3           3           0           0            0\n4   NA          NA          NA          NA          NA          NA           NA\n5    1           1           1           1           0           0            3\n6    4           4           4           4           4           4            4\n  onlineshop7R taco.best smoke.pot pinepizzatasty procrastinate boba pref.book\n1            4       Yes        No             No            No  Yes       Yes\n2            3       Yes       Yes            Yes           Yes   No        No\n3            0        No        No            Yes            No   No        No\n4           NA        No        No             No           Yes   No        No\n5            3        No        No             No           Yes   No        No\n6            4        No        No             No            No   No        No\n  read.scifi need.caff love.music clubbed read.marx been.disney attend.class\n1         No        No        Yes      No        No          No          Yes\n2         No       Yes        Yes     Yes       Yes         Yes          Yes\n3         No       Yes        Yes     Yes       Yes         Yes          Yes\n4         No       Yes        Yes     Yes       Yes         Yes           No\n5        Yes        No        Yes      No       Yes         Yes          Yes\n6         No        No         No      No        No          No           No\n  had.breakfast call.folks tried.take\n1           Yes         No        Yes\n2           Yes        Yes        Yes\n3           Yes        Yes         No\n4            No         No         No\n5           Yes         No        Yes\n6            No         No         No\n                                                                                        job.work\n1                                                             No, I am just a student right now.\n2                                                             No, I am just a student right now.\n3   Yes, I am working a full-time job in addition to being a student (40 hours or more per week)\n4                                                             No, I am just a student right now.\n5 Yes, I am working a part-time job in addition to being a student (less than 40 hours per week)\n6                                                             No, I am just a student right now.\n  fav.weather hrs.sleep screens units hrs.screen items.amazon\n1       sunny         8       2    18        6.0            3\n2       sunny         8       2    16        8.0            1\n3    #fireszn         8       2    20        5.0            1\n4       rainy         8       2    15         NA            2\n5       sunny         8       2    16        4.5            0\n6       snowy         9       2    16        3.0            1\n\n\n\nWhat is the sample size of this dataset?\n\nnrow(m)\n\n[1] 122\n\n\nCreate a likert scale to measure people‚Äôs love of Dancing. This was measured with the following variables : dance1, dance2, dance3r, and dance4r.¬†Note that dance3r and dance4r are negatively-keyed, and will need to be reverse scored. What number will you need to subtract dance3r and dance4r from in order to reverse score them?\n\n## STEP 1. CREATE A DATA FRAME TO ISOLATE THE SCALE ITEMS!\nDANCE.df &lt;- data.frame(m$dance1, m$dance2, m$dance3r, m$dance4r)\nDANCE.df\n\n    m.dance1 m.dance2 m.dance3r m.dance4r\n1          2        2         2         2\n2          4        4         3         3\n3          5        4         1         1\n4         NA       NA        NA        NA\n5          5        4         1         1\n6          5        5         5         5\n7          4        4         4         4\n8          3        2         2         2\n9          3        3         2         2\n10         3        2         3         3\n11         3        2         3         2\n12         4        1         1         1\n13         2        1         4         3\n14         3        4         2         2\n15         3        2         1         2\n16         4        5         1         1\n17         4        4         2         2\n18         3        1         1         1\n19         5        5         5         5\n20         4        3         2         2\n21         2        4         1         1\n22         5        4         2         1\n23         4        4         1         1\n24         1        1         5         5\n25         5        4         1         1\n26         4        4         1         1\n27         5        1         2         2\n28         5        4         1         2\n29         4        4         1         2\n30         2        2         3         3\n31         5        5         1         1\n32         2        1         4         4\n33         3        3         3         3\n34         4        4         2         2\n35         2        2         3         4\n36         4        4         1         1\n37         2        2         4         4\n38         3        3         3         2\n39         2        1         4         4\n40         4        3         1         1\n41         3        2         2         2\n42         5        5         1         1\n43         5        5         1         1\n44         3        3         2         2\n45         5        1         1         1\n46         4        5         1         1\n47         5        5         1         1\n48         2        2         4         2\n49         3        4         1         1\n50         3        2         2         1\n51         4        4         1         1\n52         1        1         1         1\n53         4        4         2         2\n54         3        2         2         3\n55         5        5         1         1\n56         3        2         2         1\n57         5        4         1         1\n58         4        3         1         1\n59         3        2         3         4\n60         5        5         1         1\n61         5        5         1         1\n62         5        2         1         2\n63         4        2         1         2\n64         1        1         5         3\n65         4        4         1         1\n66         4        3         1         1\n67         4        2         2         3\n68         1        1         5         4\n69         5        5         1         1\n70         5        2         1         2\n71         1        2         4         3\n72         5        5         1         1\n73         4        1         2         2\n74         1        1         4         4\n75         2        1         4         3\n76         4        1         3         4\n77         3        1         3         5\n78         4        2         1         1\n79         4        1         1         5\n80         4        1         2         4\n81         1        1         5         2\n82         5        2         1         2\n83         4        2         2         2\n84         5        4         2         3\n85         4        4         2         2\n86         2        2         3         2\n87         4        2         2         2\n88         5        4         1         2\n89         2        3         2         4\n90         4        4         2         2\n91         2        2         3         2\n92         4        2         1         2\n93         1        1         3         4\n94         4        5         2         1\n95         5        4         1         1\n96         2        2         4         4\n97         5        5         1         1\n98         1        1         5         5\n99         3        2         2         2\n100        4        4         4         2\n101        4        4         2         2\n102        2        2         4         4\n103        4        5         1         1\n104        3        3         3         2\n105        3        3         1         2\n106        3        1         1         1\n107        4        1         2         3\n108        4        2         3         2\n109        5        4         1         2\n110        3        3         3         3\n111        5        4         1         1\n112        1        1         5         2\n113        4        4         2         3\n114        3        1         2         3\n115        5        4         1         1\n116        4        3         2         2\n117        2        2         4         4\n118        4        3         2         2\n119       NA       NA        NA        NA\n120        2        1         4         5\n121       NA        3         3         3\n122        4        3         2         1\n\n## STEP 2 : reverse score the negatively keyed items in the scale.\nDANCE.df &lt;- data.frame(m$dance1, m$dance2, 6-m$dance3r, 6-m$dance4r)\nrange(DANCE.df, na.rm = T) # a 1-5 scale = 1 + 5 = 6 = # to subract NK items from.\n\n[1] 1 5\n\nDANCE.df\n\n    m.dance1 m.dance2 X6...m.dance3r X6...m.dance4r\n1          2        2              4              4\n2          4        4              3              3\n3          5        4              5              5\n4         NA       NA             NA             NA\n5          5        4              5              5\n6          5        5              1              1\n7          4        4              2              2\n8          3        2              4              4\n9          3        3              4              4\n10         3        2              3              3\n11         3        2              3              4\n12         4        1              5              5\n13         2        1              2              3\n14         3        4              4              4\n15         3        2              5              4\n16         4        5              5              5\n17         4        4              4              4\n18         3        1              5              5\n19         5        5              1              1\n20         4        3              4              4\n21         2        4              5              5\n22         5        4              4              5\n23         4        4              5              5\n24         1        1              1              1\n25         5        4              5              5\n26         4        4              5              5\n27         5        1              4              4\n28         5        4              5              4\n29         4        4              5              4\n30         2        2              3              3\n31         5        5              5              5\n32         2        1              2              2\n33         3        3              3              3\n34         4        4              4              4\n35         2        2              3              2\n36         4        4              5              5\n37         2        2              2              2\n38         3        3              3              4\n39         2        1              2              2\n40         4        3              5              5\n41         3        2              4              4\n42         5        5              5              5\n43         5        5              5              5\n44         3        3              4              4\n45         5        1              5              5\n46         4        5              5              5\n47         5        5              5              5\n48         2        2              2              4\n49         3        4              5              5\n50         3        2              4              5\n51         4        4              5              5\n52         1        1              5              5\n53         4        4              4              4\n54         3        2              4              3\n55         5        5              5              5\n56         3        2              4              5\n57         5        4              5              5\n58         4        3              5              5\n59         3        2              3              2\n60         5        5              5              5\n61         5        5              5              5\n62         5        2              5              4\n63         4        2              5              4\n64         1        1              1              3\n65         4        4              5              5\n66         4        3              5              5\n67         4        2              4              3\n68         1        1              1              2\n69         5        5              5              5\n70         5        2              5              4\n71         1        2              2              3\n72         5        5              5              5\n73         4        1              4              4\n74         1        1              2              2\n75         2        1              2              3\n76         4        1              3              2\n77         3        1              3              1\n78         4        2              5              5\n79         4        1              5              1\n80         4        1              4              2\n81         1        1              1              4\n82         5        2              5              4\n83         4        2              4              4\n84         5        4              4              3\n85         4        4              4              4\n86         2        2              3              4\n87         4        2              4              4\n88         5        4              5              4\n89         2        3              4              2\n90         4        4              4              4\n91         2        2              3              4\n92         4        2              5              4\n93         1        1              3              2\n94         4        5              4              5\n95         5        4              5              5\n96         2        2              2              2\n97         5        5              5              5\n98         1        1              1              1\n99         3        2              4              4\n100        4        4              2              4\n101        4        4              4              4\n102        2        2              2              2\n103        4        5              5              5\n104        3        3              3              4\n105        3        3              5              4\n106        3        1              5              5\n107        4        1              4              3\n108        4        2              3              4\n109        5        4              5              4\n110        3        3              3              3\n111        5        4              5              5\n112        1        1              1              4\n113        4        4              4              3\n114        3        1              4              3\n115        5        4              5              5\n116        4        3              4              4\n117        2        2              2              2\n118        4        3              4              4\n119       NA       NA             NA             NA\n120        2        1              2              1\n121       NA        3              3              3\n122        4        3              4              5\n\n## STEP 3 : evaluate the reliability\nlibrary(psych)\nalpha(DANCE.df)\n\n\nReliability analysis   \nCall: alpha(x = DANCE.df)\n\n  raw_alpha std.alpha G6(smc) average_r S/N   ase mean sd median_r\n      0.84      0.84    0.85      0.57 5.4 0.024  3.5  1     0.56\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.79  0.84  0.88\nDuhachek  0.79  0.84  0.89\n\n Reliability if an item is dropped:\n               raw_alpha std.alpha G6(smc) average_r S/N alpha se  var.r med.r\nm.dance1            0.78      0.79    0.74      0.55 3.7    0.036 0.0276  0.47\nm.dance2            0.84      0.84    0.80      0.63 5.1    0.026 0.0175  0.67\nX6...m.dance3r      0.77      0.77    0.71      0.53 3.4    0.035 0.0091  0.48\nX6...m.dance4r      0.80      0.81    0.76      0.58 4.2    0.031 0.0158  0.64\n\n Item statistics \n                 n raw.r std.r r.cor r.drop mean  sd\nm.dance1       119  0.85  0.85  0.79   0.72  3.5 1.2\nm.dance2       120  0.79  0.77  0.66   0.60  2.8 1.4\nX6...m.dance3r 120  0.86  0.86  0.83   0.73  3.8 1.3\nX6...m.dance4r 120  0.81  0.82  0.75   0.66  3.8 1.2\n\nNon missing response frequency for each item\n                  1    2    3    4    5 miss\nm.dance1       0.08 0.14 0.19 0.34 0.24 0.02\nm.dance2       0.22 0.26 0.13 0.26 0.13 0.02\nX6...m.dance3r 0.07 0.12 0.14 0.28 0.40 0.02\nX6...m.dance4r 0.06 0.12 0.13 0.34 0.34 0.02\n\n## STEP 4 : IF the items in the scale are reliable, then I want to squish (or average) all the items into ONE NUMBER for each person. This will be a NEW VARIABLE created from the likert scale.\n\nm$DANCELOVE &lt;- rowMeans(DANCE.df)\n## USE THIS VARIABLE IN YOUR ANALYSES!!!\n\nWhat is the alpha reliability of this scale? Round to two decimal places.\n\nsee above!\n\nWhat is the mean of this scale? Round to two decimal places.\n\nmean(m$DANCELOVE, na.rm = T)\n\n[1] 3.491597\n\n\nGraph the variable as a histogram. Describe the shape; what do you learn about our class from this variable?\n\nhist(m$DANCELOVE)\n\n\n\n\n\n\n\n\nThe variable hrs.screen asked students to report how many hours of screens they used (according to their phone) in the last day. What is the mean of this variable? Round to two decimal places.\n\nmean(m$hrs.screen, na.rm = T)\n\n[1] 5.943551\n\nm$hrs.screen[m$hrs.screen &lt; 0] &lt;- NA\n\nhist(m$hrs.screen)\n\n\n\n\n\n\n\n\nHow many people did not report their hours of screen usage (i.e., how many NAs are there in this variable?)\n\nsummary(m$hrs.screen)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  1.680   4.000   6.000   6.004   7.375  15.000      16 \n\nlength(m$hrs.screen[m$hrs.screen == \"NA\"]) # another way!\n\n[1] 16\n\nsum(is.na(m$hrs.screen)) # another way!!!\n\n[1] 16\n\n\nWhat‚Äôs the z-score for someone who stared at their screen for 8-hours a day? Round to two decimal places.\n\n8 - mean(m$hrs.screen, na.rm = T) # distance from the mean\n\n[1] 1.99566\n\nsd(m$hrs.screen, na.rm = T) # the average person differs from the mean by 2.7 hours!\n\n[1] 2.74497\n\n(8 - mean(m$hrs.screen, na.rm = T))/sd(m$hrs.screen, na.rm = T) # distance from the mean/sd\n\n[1] 0.7270244\n\n# Z = .73\n\nscale(m$hrs.screen)\n\n               [,1]\n  [1,] -0.001580936\n  [2,]  0.727024443\n  [3,] -0.365883626\n  [4,]           NA\n  [5,] -0.548034971\n  [6,] -1.094489006\n  [7,]  1.455629823\n  [8,] -0.001580936\n  [9,] -0.730186316\n [10,]  0.362721754\n [11,]  0.362721754\n [12,]  0.727024443\n [13,] -0.001580936\n [14,]           NA\n [15,]  0.727024443\n [16,] -0.730186316\n [17,] -1.094489006\n [18,] -0.730186316\n [19,] -0.001580936\n [20,] -1.458791695\n [21,] -0.311238223\n [22,]  1.455629823\n [23,]           NA\n [24,]           NA\n [25,]  0.362721754\n [26,] -1.276640350\n [27,] -0.730186316\n [28,]  0.909175788\n [29,]  2.184235203\n [30,] -0.365883626\n [31,]           NA\n [32,]  1.455629823\n [33,]  1.455629823\n [34,]  0.362721754\n [35,] -0.001580936\n [36,] -1.094489006\n [37,]           NA\n [38,] -0.001580936\n [39,] -1.458791695\n [40,]  0.362721754\n [41,]  0.362721754\n [42,] -0.001580936\n [43,] -1.458791695\n [44,] -0.001580936\n [45,] -0.001580936\n [46,] -1.094489006\n [47,]  2.912840582\n [48,] -0.730186316\n [49,]  2.548537892\n [50,] -1.094489006\n [51,] -1.094489006\n [52,]           NA\n [53,]  0.362721754\n [54,] -0.365883626\n [55,]  1.455629823\n [56,]  0.144140140\n [57,] -0.001580936\n [58,]  0.727024443\n [59,] -1.094489006\n [60,]           NA\n [61,] -0.001580936\n [62,] -0.365883626\n [63,]           NA\n [64,] -0.730186316\n [65,] -0.001580936\n [66,]  0.362721754\n [67,] -0.365883626\n [68,] -0.365883626\n [69,] -1.094489006\n [70,]  0.362721754\n [71,] -0.730186316\n [72,]  0.727024443\n [73,]           NA\n [74,] -0.730186316\n [75,]           NA\n [76,] -0.001580936\n [77,]  0.727024443\n [78,]  2.184235203\n [79,] -1.032557548\n [80,] -1.094489006\n [81,] -0.365883626\n [82,] -0.001580936\n [83,] -1.458791695\n [84,] -0.365883626\n [85,] -1.094489006\n [86,]           NA\n [87,] -0.730186316\n [88,] -0.912337661\n [89,] -1.094489006\n [90,]  0.180570409\n [91,] -0.001580936\n [92,] -1.458791695\n [93,] -0.001580936\n [94,] -1.575368556\n [95,] -0.730186316\n [96,]           NA\n [97,] -0.001580936\n [98,] -0.730186316\n [99,]  1.819932513\n[100,]  0.727024443\n[101,] -0.001580936\n[102,]  1.455629823\n[103,] -0.821261988\n[104,]  0.727024443\n[105,] -0.365883626\n[106,]  1.455629823\n[107,]  3.277143272\n[108,]  0.362721754\n[109,] -0.799403827\n[110,]  1.455629823\n[111,]           NA\n[112,]  0.727024443\n[113,]  0.727024443\n[114,]  0.544873098\n[115,] -0.183732281\n[116,] -1.094489006\n[117,] -0.730186316\n[118,]           NA\n[119,]           NA\n[120,] -0.365883626\n[121,]  1.091327133\n[122,] -0.001580936\nattr(,\"scaled:center\")\n[1] 6.00434\nattr(,\"scaled:scale\")\n[1] 2.74497\n\nscale(m$hrs.screen)[m$hrs.screen == 8]\n\n [1] 0.7270244        NA 0.7270244        NA 0.7270244        NA        NA\n [8]        NA        NA        NA 0.7270244        NA        NA 0.7270244\n[15]        NA        NA 0.7270244        NA        NA 0.7270244 0.7270244\n[22]        NA 0.7270244 0.7270244        NA        NA\n\n\nWhat does this z-score tell you?\n\n\n\n\nMini Exam is in NEXT weeks [2/28]\n\nSTUDY GUIDE HERE\ntake home; open-note; open-book; DO ON YOUR OWN.\nheld during normal class; 85 minutes (DSP students get extra time accommodations)\npractice exam will post next week\n\nLab 5. Do the Fall 2024 Mini Exam. Look over the key (video key!) after you‚Äôve done. Submit your completed exam. Auto graded, but I want you do to. (PRACTICE PRACTICE PRACTICE!)\nMilestone #1 Due in TWO weeks [3/9]\n\n\n\n\n\n2:10 - 3:10 | Check-In and Review\n3:10 - 3:30 | Reliability and Validity\n3:30 - 3:42 | Break Time\n3:42 - 4:30 | Project Workshop."
  },
  {
    "objectID": "calstats/Lectures/5L_Methods.html#reliability-and-validity",
    "href": "calstats/Lectures/5L_Methods.html#reliability-and-validity",
    "title": "5L_Methods",
    "section": "Reliability and Validity",
    "text": "Reliability and Validity\n\nRelevance to Psychological Science\n\nReliability in Neuroscience1\n\nValidity in Neuroscience2\n\n\n\n\n\nRelevance to Real-Life.\n\nHow would you evaluate the reliability and validity of the STEP COUNTER on your phone???\n\n\n\n\n\n\n\nTerm\nWay of Testing\n\n\nface :¬†does our measure or result look like what it should look like?\nthe unit of measurement looks like what I want it to look like (STEPS); anywhere from 0 to 20,000 (A BIG DAY?!)\n\n\nconvergent :¬†is our measure similar to related concepts?\ndoes the step counter increase when I take steps.\n\n\ndiscriminant :¬†is our measure different from unrelated concepts?\nthe number of steps should be unrelated to : shaking your phone; miles driven; # of tired students in a classroom on a Friday.\n\n\ntest-retest :¬†do we get the same result if we take multiple measures?\ntake X steps at one time point, then take X steps at another time point. X should be the same both times!\n\n\ninterrater reliability :¬†would another observer make the same measurements?\ncarry two phones; take X steps‚Ä¶phones should give the same answer (Z; not necessarily X!)\n\n\ninter-item reliability :¬†would one item in the likert scale be related to others?\nNOT RELEVANT FOR LIKERT SCALES."
  },
  {
    "objectID": "calstats/Lectures/5L_Methods.html#break-time-meet-back-at-342",
    "href": "calstats/Lectures/5L_Methods.html#break-time-meet-back-at-342",
    "title": "5L_Methods",
    "section": "BREAK TIME : MEET BACK AT 3:42",
    "text": "BREAK TIME : MEET BACK AT 3:42"
  },
  {
    "objectID": "calstats/Lectures/5L_Methods.html#milestone-1-final-project-workshop",
    "href": "calstats/Lectures/5L_Methods.html#milestone-1-final-project-workshop",
    "title": "5L_Methods",
    "section": "Milestone #1 | Final Project Workshop",
    "text": "Milestone #1 | Final Project Workshop\nFinal Project Description & Rubric + Milestone #1\nRECAP : Questions About the Research Process / Scientific Articles?\nIntroduction Deconstruction :\nThe introduction starts broad, but then quickly focuses on the variables in your model so readers can understand a) what your study is about and b) why you‚Äôre doing your study.¬†\n\n\n\n\n\n\n\nSection\nBrief Explanation\n\n\n1. The Opening\nDescribe the question you have, and explain why this question matters\n\n\n2. The Review\nDescribe what past research and theory has to say on the question and your theory. Your goal is to give the reader the background they need to understand why you are doing your study; you don‚Äôt need to cover EVERY single issue on your topic..\n\n\n3. The Critique\nExplain why the past research is not ‚Äúthe final truth‚Äù, and what other new questions might be important to consider (and why these questions matter). Only point out limitations with past research that you will address in your study; other limitations that you think future research will address should go in the discussion section.\n\n\n4. The Current Research\nExplain what specific questions your study will address. Be clear by stating each idea as a hypothesis with language like, ‚ÄúI predict‚Äù or ‚ÄúMy first hypothesis‚Äù.\n\n\n\n\nSTEP 1 : read an excerpt from the introduction3; identify (in the margins) each of part of the introduction (‚ÄúThe Opening‚Äù, ‚ÄúThe Review‚Äù, ‚ÄúThe Critique‚Äù, and ‚ÄúThe Current Research‚Äù)\n\n\nACTIVITY : Doing a Lit Review\n\nLit Review Template\n\nOpen document.\nClick File ‚Äì&gt; Make a copy\n\n\n\nFill in with your own notes about the research you find!\n\nLink to Professor Example"
  },
  {
    "objectID": "calstats/Lectures/5L_Methods.html#check-out",
    "href": "calstats/Lectures/5L_Methods.html#check-out",
    "title": "5L_Methods",
    "section": "Check-Out",
    "text": "Check-Out"
  },
  {
    "objectID": "calstats/Lectures/5L_Methods.html#footnotes",
    "href": "calstats/Lectures/5L_Methods.html#footnotes",
    "title": "5L_Methods",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBrandt, D. J., Sommer, J., Krach, S., Bedenbender, J., Kircher, T., Paulus, F. M., & Jansen, A. (2013). Test-retest reliability of fMRI brain activity during memory encoding. Frontiers in psychiatry, 4, 163. [Link to Full Article]‚Ü©Ô∏é\nBennett, C. M., Miller, M. B., & Wolford, G. L. (2009). Neural correlates of interspecies perspective taking in the post-mortem Atlantic Salmon: an argument for multiple comparisons correction. Neuroimage, 47(Suppl 1), S125. [Link to Full Article]‚Ü©Ô∏é\nFull article here‚Ü©Ô∏é"
  },
  {
    "objectID": "gradstats/gradlabs/5_ModelReview.html#break-time-meet-back-at-1050",
    "href": "gradstats/gradlabs/5_ModelReview.html#break-time-meet-back-at-1050",
    "title": "Lecture 5 | Reviewing Models",
    "section": "BREAK TIME : Meet Back at 10:50",
    "text": "BREAK TIME : Meet Back at 10:50"
  },
  {
    "objectID": "gradstats/gradlabs/5Lab_MiniPractice.html",
    "href": "gradstats/gradlabs/5Lab_MiniPractice.html",
    "title": "Lab 5 - Mini Practice",
    "section": "",
    "text": "Use the dataset to answer the questions below. Make sure your report a) includes the code that you used, b) shows the result / output in a clear and organized way (e.g., do not include junk output that we don‚Äôt need to see!), and c) answers the questions to explain your code / the result. This is representative of the kind of work you‚Äôll do on the actual exam, though the specific questions / tasks will change depending on the dataset. The key has been (or will be) posted - try on your own, but use as a guide if you get stuck. Yeah!"
  },
  {
    "objectID": "gradstats/gradlabs/5Lab_MiniPractice.html#instructions.",
    "href": "gradstats/gradlabs/5Lab_MiniPractice.html#instructions.",
    "title": "Lab 5 - Mini Practice",
    "section": "",
    "text": "Use the dataset to answer the questions below. Make sure your report a) includes the code that you used, b) shows the result / output in a clear and organized way (e.g., do not include junk output that we don‚Äôt need to see!), and c) answers the questions to explain your code / the result. This is representative of the kind of work you‚Äôll do on the actual exam, though the specific questions / tasks will change depending on the dataset. The key has been (or will be) posted - try on your own, but use as a guide if you get stuck. Yeah!"
  },
  {
    "objectID": "gradstats/gradlabs/5Lab_MiniPractice.html#the-dataset-and-problem.",
    "href": "gradstats/gradlabs/5Lab_MiniPractice.html#the-dataset-and-problem.",
    "title": "Lab 5 - Mini Practice",
    "section": "The Dataset and Problem.",
    "text": "The Dataset and Problem.\nLoad the ‚ÄúNarcissism‚Äù dataset (on Dropbox) into R. Note that these data are not ‚Äòcomma‚Äô separated values, but separated by tabs; you‚Äôll need to import this using the following argument. Check to make sure the data loaded correctly, and report the sample size. Look over the codebook (also posted to Dropbox).\nDr.¬†Professor wants to see whether there‚Äôs a relationship between narcissism (the DV; variable = score in the dataset) and age (the IV; variable = age). Use the dataset to test Dr.¬†Professor‚Äôs prediction that as people get older, they get less narcissistic."
  },
  {
    "objectID": "gradstats/gradlabs/5Lab_MiniPractice.html#problem-1.-data-loading-and-cleaning",
    "href": "gradstats/gradlabs/5Lab_MiniPractice.html#problem-1.-data-loading-and-cleaning",
    "title": "Lab 5 - Mini Practice",
    "section": "Problem 1. Data Loading and Cleaning",
    "text": "Problem 1. Data Loading and Cleaning\nLoad the data and check to make sure the data loaded correctly. Report the sample size. The variable elapse describes how long participants took to complete the survey (time started - time submitted). Decide on a rule - justify this using a mix of logic and statistics - and use this rule to remove these individuals from your dataset. (Note that you should remove the entire individual from the dataset, not just their elapse score.) Graph the variable elapse after removing the outliers, and report the number of individuals who were removed from the dataset using your rule."
  },
  {
    "objectID": "gradstats/gradlabs/5Lab_MiniPractice.html#problem-2.-descriptive-statistics-graphs.",
    "href": "gradstats/gradlabs/5Lab_MiniPractice.html#problem-2.-descriptive-statistics-graphs.",
    "title": "Lab 5 - Mini Practice",
    "section": "Problem 2. Descriptive Statistics Graphs.",
    "text": "Problem 2. Descriptive Statistics Graphs.\nGraph the variables needed to test Dr.¬†Professor‚Äôs theory. Make the graphs look nice, as if ready for a publication. Below the graphs, report the relevant descriptive statistics and describe what these statistics / the graphs tell you about these variables. What did you learn?"
  },
  {
    "objectID": "gradstats/gradlabs/5Lab_MiniPractice.html#problem-3.-linear-models.",
    "href": "gradstats/gradlabs/5Lab_MiniPractice.html#problem-3.-linear-models.",
    "title": "Lab 5 - Mini Practice",
    "section": "Problem 3. Linear Models.",
    "text": "Problem 3. Linear Models.\nDefine a linear model to test Dr.¬†Professor‚Äôs question. What is the slope, intercept, and \\(R^2\\) of this model (in raw and z-scored units)? Then, use bootstrapping to estimate the 95% Confidence Interval for the slope and estimate the power researchers had to detect any observed relationship. Graph the relationship between these variables, making sure to illustrate the linear model (and lines to illustrate the 95% Confidence Interval) on your graph. Below the graph, describe what these statistics tell you about the relationship between these two variables. Finally, evaluate the assumptions of this linear model."
  },
  {
    "objectID": "gradstats/gradlabs/5Lab_MiniPractice.html#problem-4.-creating-a-scale.",
    "href": "gradstats/gradlabs/5Lab_MiniPractice.html#problem-4.-creating-a-scale.",
    "title": "Lab 5 - Mini Practice",
    "section": "Problem 4. Creating a Scale.",
    "text": "Problem 4. Creating a Scale.\nDr.¬†Professor is worried that the narcissism variable (score) was not calculated correctly. In this study, narcissism was measured by giving people 40-items with two options and adding up the total number of ‚Äúnarcissism‚Äù options that people selected. At the end of the codebook, the researchers have identified which responses were used in calculating the score; however this code will not work in R. Re-create the narcissism score from these 40-items and report the alpha reliability of this scale. Then, confirm that your calculation of narcissism is exactly the same as the one that was already calculated in the dataset (score).\nHeads up. This question took professor 30 minutes to figure out‚Ä¶so okay if you struggle a little! Feel free to peek at my key (and there‚Äôs definitely a simpler way to do this, but I was feeling stubborn about the approach I initially took). I think this represents the kind of weird data you might be asked to interpret. But I do like to have a small challenge problem on each exam, worth only 1/21 points, just to keep things interesting. FWIW, the likert scale you do on the actual exam will be more straightforward."
  },
  {
    "objectID": "calstats/labs/Lab5.html",
    "href": "calstats/labs/Lab5.html",
    "title": "Lab5",
    "section": "",
    "text": "For Lab 5, take last semester‚Äôs Mini R Exam. There‚Äôs a key posted for this, but you should try to work on the exam on your own, and use the key as a guide / learning experience after getting stuck for a few minutes :)\nUpload your completed practice exam to receive credit. Note you will not receive feedback from your TA, so make sure to look over the key."
  },
  {
    "objectID": "calstats/Lectures/5L_Methods.html#break-time-meet-back-at-348",
    "href": "calstats/Lectures/5L_Methods.html#break-time-meet-back-at-348",
    "title": "5L_Methods",
    "section": "BREAK TIME : MEET BACK AT 3:48",
    "text": "BREAK TIME : MEET BACK AT 3:48"
  },
  {
    "objectID": "gradstats/gradlabs/6_TTests.html",
    "href": "gradstats/gradlabs/6_TTests.html",
    "title": "Lecture 6 - T-Tests Are [SPOILER ALERT] Linear Models",
    "section": "",
    "text": "9:10 - 9:20 | Check-In / Mini Exam Debrief\n9:20 - 10:30 | Linear Model when the IV is categorical\n10:30 - 10:45 | BREAK\n10:45 - 11:05 | Student Presentations!\n11:10 - 12:00 | T-Tests and Work on Lab 6\n\n\n\n\n\nNot yet graded!\nProfessor should post key? [Y/N]\n\n\n\n\n\n\nRescheduled : to 4/18 or 4/25 [???]\nDe-emphasized : now worth 10% of your grade (other 10% shifted to the chill final project)\nWill be shorter? Group exam? A whole week to do like a lab? IDK. Let‚Äôs check in later and see where we are at.\n\n\n\n\n\nMore practice in class; group work (in lecture, in section)??\nMixed responses about article discussions.\nY‚ÄôALL : Please Message / Email / Discord if Confused or Stuck (on Lab, in Class, etc.)"
  },
  {
    "objectID": "gradstats/gradlabs/6_TTests.html#check-in-here",
    "href": "gradstats/gradlabs/6_TTests.html#check-in-here",
    "title": "Lecture 6 - T-Tests Are [SPOILER ALERT] Linear Models",
    "section": "",
    "text": "9:10 - 9:20 | Check-In / Mini Exam Debrief\n9:20 - 10:30 | Linear Model when the IV is categorical\n10:30 - 10:45 | BREAK\n10:45 - 11:05 | Student Presentations!\n11:10 - 12:00 | T-Tests and Work on Lab 6\n\n\n\n\n\nNot yet graded!\nProfessor should post key? [Y/N]\n\n\n\n\n\n\nRescheduled : to 4/18 or 4/25 [???]\nDe-emphasized : now worth 10% of your grade (other 10% shifted to the chill final project)\nWill be shorter? Group exam? A whole week to do like a lab? IDK. Let‚Äôs check in later and see where we are at.\n\n\n\n\n\nMore practice in class; group work (in lecture, in section)??\nMixed responses about article discussions.\nY‚ÄôALL : Please Message / Email / Discord if Confused or Stuck (on Lab, in Class, etc.)"
  },
  {
    "objectID": "gradstats/gradlabs/6_TTests.html#the-linear-model-in-categorical-iv-form",
    "href": "gradstats/gradlabs/6_TTests.html#the-linear-model-in-categorical-iv-form",
    "title": "Lecture 6 - T-Tests Are [SPOILER ALERT] Linear Models",
    "section": "The Linear Model in (Categorical IV) Form",
    "text": "The Linear Model in (Categorical IV) Form\n\nTwo-Level Categorical IV : Simple!\n\nStep 1 : Develop a Question and Set Hypotheses.\nLet‚Äôs determine whether people who had breakfast were more or less hungry than people who did not have breakfast.\n\nOUR THEORY : people who eat breakfast are less hungry than those who do not.\nNull Hypothesis (latin for ‚Äúnot any‚Äù / none) : no difference or eating breakfast ‚Äì&gt; More hunger.\nAlternative Hypothesis : people who eat breakfast are less hungry than those who do not.\nQUESTION : what do you do when you feel the urge / see to HARK (hypothesis after results known)?\n\nwhy it‚Äôs a problem ??? :\n\nthe data could be a form of error (type I or type II); but more likely to cause problems if your theory is only based on the data, and not past research.\nwe are always going to be biased to find a result that works in our favor.\n\nbe transparent : ‚Äúoh! cool!! Now I‚Äôm interested in this idea‚Ä¶.let‚Äôs replicate.‚Äù\npre-registration : make your hypotheses in advance (and time-stamp them) and then update them if they change.\n\n\n\n\n\nStep 2 : Load Data, Graph and Evaluate Your Individual Variables.\n\nm &lt;- read.csv(\"~/Dropbox/!GRADSTATS/gradlab/Datasets/MiniGrad/mini_grad_data.csv\", stringsAsFactors = T)\nhead(m) # looks good \n\n                        Time howgo howlab3 howR hunger stress social organized\n1 2025/02/14 12:56:41 AM PST     6       9    9      5      5      8         3\n2  2025/02/14 9:15:47 AM PST     8       7    7      2      2      4         8\n3  2025/02/14 9:17:10 AM PST     7       8    6      6      6      3         4\n4  2025/02/14 9:18:26 AM PST     9       6    8      3      3      4         7\n5  2025/02/14 9:22:44 AM PST     6       6    8      3      7      5         4\n6  2025/02/14 9:23:36 AM PST     6       7    5      1      3      2         7\n  open agreeable givedata love2scroll tired cooladvisor lovebay had.breakfast\n1    7         5        0           2     7           2      10           Yes\n2    9         5        2           2     4           5       7           Yes\n3    9         7        3           7     7           9       7           Yes\n4    9         8        0          10    10          10      10           Yes\n5   10        10        5           9     6           7      10            No\n6    8         8        5           5     3           0       0           Yes\n  have.water food.pref check.phone hrs.sleep checkin.go\n1         No    tortas          46       6.5          8\n2        Yes  burritos          15       5.5          8\n3         No     tacos          90       7.0          8\n4        Yes     tacos          52       2.0          6\n5        Yes     tacos          84       6.0          7\n6        Yes     tacos          23       7.0          5\n\nnrow(m) # yep.\n\n[1] 26\n\npar(mfrow = c(1,2))\nhist(m$hunger)\nplot(m$had.breakfast, xlab = \"Had Breakfast\")\n\n\n\n\n\n\n\n\n\n\nStep 3 : Define and Interpret Your Linear Model.\n\nOur model will predict hunger from whether people had breakfast.\nAs before, we define a linear model using the lm function, and will see an intercept and slope.\n\n\nmod &lt;- lm(hunger ~ had.breakfast, data = m)\ncoef(mod)\n\n     (Intercept) had.breakfastYes \n        5.750000        -2.107143 \n\n\n\nThe Model : \\(\\hat{Y} = a + b_1 * X_1\\)\n\n\\(a\\) (sometimes = Intercept = Predicted Value of Y when all X values are zero.\n\\(X_1\\) = A dummy coded categorical variable. This is often the default method of comparing two groups, and compares each level of the factor to a reference level.\n\n\\(X_1\\) = 0 = had.breakfast = No (Not Yes)\n\\(X_1\\) = 1 = had.breakfast = Yes\n\n\\(b_1\\) = the slope = the predicted change in Y as X changes by 1\n\nUsing the Model to Make Predictions\n\npredicted hunger if had.breakfast = Yes?\n\npredicted hunger = 5.75 - 2.1 * had.breakfastYes\npredicted hunger = 5.75 + -2.1 * 1 = 3.65\n\npredicted hunger if had.breakfast = No?\n\npredicted hunger = 5.75 - 2.1 * had.breakfastYes\npredicted hunger = 5.75 - 2.1 * 0 = 5.75\n\n\nNote that these values are the same as the average hunger for people who had breakfast and for people who did not have breakfast.\n\n\nbrekY &lt;- m[m$had.breakfast == \"Yes\",]\nbrekN &lt;- m[m$had.breakfast == \"No\",]\n\nmean(brekY$hunger) # the intercept + the slope\n\n[1] 3.642857\n\nmean(brekN$hunger) # the intercept\n\n[1] 5.75\n\ntapply(m$hunger, m$had.breakfast, mean) # another way to do this.\n\n      No      Yes \n5.750000 3.642857 \n\n\n\nThe Graph. A few ways to graph the linear model with the IV is categorical.\nNote that the plot() function draws a boxplot that illustrates the median of each group; whereas the linear model emphasizes the mean.\n\n\nplot(hunger ~ had.breakfast, data = m)\n\n\n\n\n\n\n\n\n\nFor ‚Äúquick‚Äù methods, I like plotmeans() from the gplots() library. This plots the mean of each group.\n\n\n#install.packages(\"gplots\")\nlibrary(gplots) \n\n\nAttaching package: 'gplots'\n\n\nThe following object is masked from 'package:stats':\n\n    lowess\n\nplotmeans(hunger ~ had.breakfast, data = m, connect = F, ylim = c(0,10))\n\n\n\n\n\n\n\n\n\nggplot2 also has a method; nice for illustrating the individual points and making the graph look more professional?\n\n\nlibrary(ggplot2)\nlibrary(ggthemes)\nggplot(m, aes(x=had.breakfast, y=hunger)) + # defining my space\n  stat_summary(fun=mean, geom=\"bar\", fill=\"gray\") + # adding a bar for the means\n  stat_summary(fun.data=mean_se, geom=\"errorbar\", width=0.25) + # adding sampling error [more on this later!]\n  geom_dotplot(binaxis='y', binwidth=0.1, stackdir=\"center\", alpha=0.5, stroke=0, dotsize=0.8) + # adding dots for the individual data; sorting them\n  coord_cartesian(ylim=c(0,10)) + # changing the limits of my y-axis.\n  xlab(\"Brekky\") + ylab(\"Hunger\") + # adding labels\n  theme_wsj() # adding a theme\n\n\n\n\n\n\n\n\n\n\nStep 4 : Evaluate Your Model\n\nDiagnostic Plots to see if those assumptions are met!\n\n\npar(mfrow = c(2,2))\nplot(mod)\n\n\n\n\n\n\n\n\n\n\\(R^2\\) (Effect Size)\n\nHow large is this difference? \\(R^2\\) gives some context.\n\nsummary(mod)$r.squared\n\n[1] 0.1791375\n\n## ILLUSTRATING R^2 WHEN THE VARIABLE IS CATEGORICAL\npar(mfrow = c(1,2))\nplot(m$hunger)\nabline(h = mean(m$hunger, na.rm = T), lwd = 5)\nresiduals &lt;- m$hunger - mean(m$hunger, na.rm = T)\nSST &lt;- sum(residuals^2)\nSST\n\n[1] 160.1538\n\n## THE MODEL\nplot(m$hunger, col = m$had.breakfast, pch = 19)\nabline(h = coef(mod)[1], lwd = 5, col = 'black') # line for non breakfast folks\nabline(h = coef(mod)[1] + coef(mod)[2], lwd = 5, col = 'red') # line for non breakfast folks\n\n\n\n\n\n\n\nSSM &lt;- sum(mod$residuals^2)\n\n## R^2\n\n(SST - SSM)/SST\n\n[1] 0.1791375\n\n\nThis describes the percentage of variation in hunger that is explained by our model (in this case, using whether someone had breakfast to make predictions of hunger.) So, using the model to make predictions of hunger reduces our squared errors by %17 (vs.¬†using the mean to make predictions.)\n\n\n\nSomething New : Cohen‚Äôs D (Effect Size)\nOkay, but people find \\(R^2\\) confusing and unitless and about squared numbers and describes how good the model is as a whole; sometimes I just want to understand how BIG the difference between the groups is. One method people us is to compare the distance between groups, relative to the average distance between individuals (the standard deviation.)\nHowever, if we are assuming these groups are different in some important way, then we may not want to use a standard deviation statistic that considers individuals to belong to the same group. The pooled standard deviation is a weighted average of the standard deviation from each group.\n\nnY &lt;- nrow(brekY)\nnN &lt;- nrow(brekN)\ndfY &lt;- nY-1 # the sample size of breakfast eaters, minus 1\ndfN &lt;- nN-1\n\nvarY &lt;- var(brekY$hunger)\nvarN &lt;- var(brekN$hunger)\n\npoolvar &lt;- ((dfY * varY) + (dfN * varN))/(dfY + dfN)\npoolsd &lt;- poolvar^.5\n\ndiff &lt;- coef(mod)[2] # the difference between groups\ndiff/poolsd\n\nhad.breakfastYes \n      -0.9003175 \n\n\n\nThe psych package has a cohen‚Äôs d function built in that calculates this for you. (Not sure why we are getting different manual estimate from the package tho?)\n\n\nlibrary(psych)\n\n\nAttaching package: 'psych'\n\n\nThe following objects are masked from 'package:ggplot2':\n\n    %+%, alpha\n\ncohen.d(m$hunger, m$had.breakfast)\n\nCall: cohen.d(x = m$hunger, group = m$had.breakfast)\nCohen d statistic of difference between two means\n     lower effect upper\n[1,] -1.74  -0.94 -0.11\n\nMultivariate (Mahalanobis) distance between groups\n[1] 0.94\nr equivalent of difference between two means\n data \n-0.42 \n\ncohen.d(hunger ~ had.breakfast, data = m)\n\nCall: cohen.d(x = hunger ~ had.breakfast, data = m)\nCohen d statistic of difference between two means\n       lower effect upper\nhunger -1.74  -0.94 -0.11\n\nMultivariate (Mahalanobis) distance between groups\n[1] 0.94\nr equivalent of difference between two means\nhunger \n -0.42 \n\n\nOkay, so what does this statistic mean? Well, this tells us that the difference between folks who had breakfast and those who did not (the slope in our model) is not even as large as the difference between any two random individuals (the pooled sd). But is this a little? A lot?\n\nMany people reference Cohen‚Äôs ‚Äúconvention‚Äù of what defines effect size to be small (d = .2), medium (.5), and large effect (d = .8). However, this summary glosses Cohen‚Äôs disclaimer, as well as a ton of other context that Cohen provides in his original book.\n\n\n\nACTIVITY : Look over this nice interactive guide for interpreting Cohen‚Äôs D. How does this visualization help you think about the difference? Does this difference seem like a little? A lot??\nHere‚Äôs a great article working through some of the issues in effect size calculation and interpretation.\n\n\n\nA Note on Dummy Coding\n\nR will default to a reference level based on alphabetical order, so in this case, ‚ÄúNo‚Äù will be the reference group. To change the reference group, you can relevel the factor.\n\n\nm$had.breakfastR &lt;- relevel(m$had.breakfast, ref = \"Yes\")\nplot(m$had.breakfastR) # same data, different orientation\n\n\n\n\n\n\n\nmodR &lt;- lm(hunger ~ had.breakfastR, data = m)\ncoef(modR)\n\n     (Intercept) had.breakfastRNo \n        3.642857         2.107143 \n\npar(mfrow = c(1,2))\nplotmeans(hunger ~ had.breakfast, data = m, main = \"No as Reference Group\")\nplotmeans(hunger ~ had.breakfastR, data = m, main = \"Yes as Reference Group\")\n\n\n\n\n\n\n\n\n\nWhen the IV has more than 2 levels‚Ä¶.professor demo on the board."
  },
  {
    "objectID": "gradstats/gradlabs/6_TTests.html#presentations",
    "href": "gradstats/gradlabs/6_TTests.html#presentations",
    "title": "Lecture 6 - T-Tests Are [SPOILER ALERT] Linear Models",
    "section": "Presentations",
    "text": "Presentations\n\nRacecraft - historical and sociological critique of race as a category.\n\ninspired a few psych research articles; prof will find."
  },
  {
    "objectID": "gradstats/gradlabs/6_TTests.html#the-t-test",
    "href": "gradstats/gradlabs/6_TTests.html#the-t-test",
    "title": "Lecture 6 - T-Tests Are [SPOILER ALERT] Linear Models",
    "section": "The T-Test",
    "text": "The T-Test\nOur linear model compares the difference between two groups. This is similar to what a t-test does. However, the t-test also compares this difference to an estimate of sampling error - the standard error - that estimates how much of a difference we might find if we were drawing a random sample from a population where there was no difference in groups (the null population.)\nThe basic equation for the standard error is : \\(se = sd(x) / \\sqrt{n}\\)\nWe can easily pull up these statistics using the summary() function. The interpretation of these statistics will take more time!\n\nsummary(mod)\n\n\nCall:\nlm(formula = hunger ~ had.breakfast, data = m)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.7500 -1.7232 -0.6429  2.2500  4.2500 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        5.7500     0.6756   8.511 1.04e-08 ***\nhad.breakfastYes  -2.1071     0.9207  -2.289   0.0312 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.34 on 24 degrees of freedom\nMultiple R-squared:  0.1791,    Adjusted R-squared:  0.1449 \nF-statistic: 5.238 on 1 and 24 DF,  p-value: 0.03121\n\n\nStandard Error is similar to the sampling error we estimated through bootstrapping (you will test this in Lab 6!).1\n\nsqrt((poolvar/nY) + (poolvar/nN))\n\n[1] 0.9207258\n\n\nHowever, there are a few key conceptual and computational differences.\n\n\n\n\n\n\n\n\n\nBootstrapping\nNull Hypothesis Significance Testing (Standard Error)\n\n\n\n\nhow to estimate sampling error.\nwe want to estimate how much our statistics might change due to re-sampling, because our sample isn‚Äôt a perfect representation of the population.\nwe generate lots of ‚Äúnew‚Äù samples from our original dataset. these new samples are the same size as our original sample, but we use sampling with replacement to make sure we don‚Äôt get the exact same people in the sample every time. the goal is to see how small changes to our sample (that we might find with sampling error) influence our results (the model).\nwe calculate a statistic that is based on:\n\nthe variance in our sample (with the idea that the more individuals vary in the sample, the more sampling error we might have)\nour sample size (with the idea that the larger our sample, the less sampling error we will find.)\n\n\n\nstatistic we care about that defines sampling error\nstandard deviation of the 1,000 (or however many) slopes we generated from bootstrapping.\nstandard error (estimates how much the average slope would differ from b = 0‚Ä¶.the expected slope assuming the null)\n\n\nhow to evaluate our slope, relative to sampling error\ncalculate the % of slopes in the same direction as our slope\ncalculate 95% confidence intervals, and see whether that range includes zero and / or numbers in the opposite direction of the slope you found. (e.g., if you found a negative number, does the range include positive numbers? If so, then likely we‚Äôd find a positive relationship due to chance)\nt-value : evaluates slope you found, relative to slope you might find due to random chance.\nuse the t-value to calculate the probability given your distribution, and reject if p &lt; .05\n(or be more conservative and reject if p &lt; .01 or p &lt; .001).\n\n\n\nNote that the t-test does the same thing that our linear model does; evaluates the difference in groups, relative to an estimate of the sampling error we might observe.\n\nsummary(mod)\n\n\nCall:\nlm(formula = hunger ~ had.breakfast, data = m)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.7500 -1.7232 -0.6429  2.2500  4.2500 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        5.7500     0.6756   8.511 1.04e-08 ***\nhad.breakfastYes  -2.1071     0.9207  -2.289   0.0312 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.34 on 24 degrees of freedom\nMultiple R-squared:  0.1791,    Adjusted R-squared:  0.1449 \nF-statistic: 5.238 on 1 and 24 DF,  p-value: 0.03121\n\nt.test(brekY$hunger, brekN$hunger, var.equal = T)\n\n\n    Two Sample t-test\n\ndata:  brekY$hunger and brekN$hunger\nt = -2.2886, df = 24, p-value = 0.03121\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -4.0074275 -0.2068582\nsample estimates:\nmean of x mean of y \n 3.642857  5.750000"
  },
  {
    "objectID": "gradstats/gradlabs/6_TTests.html#activity-work-on-lab-6.",
    "href": "gradstats/gradlabs/6_TTests.html#activity-work-on-lab-6.",
    "title": "Lecture 6 - T-Tests Are [SPOILER ALERT] Linear Models",
    "section": "Activity : Work on Lab 6.",
    "text": "Activity : Work on Lab 6.\n\nDefine a linear model to see whether breakfast (the most important meal of the day) is related to another numeric outcome variable.\n\nGraph the relationship between the two variables.\nReport and interpret statistics : intercept, slope, \\(R^2\\), cohen‚Äôs d.\nCompare bootstrapping method to NHST method of estimating sampling error.\nRead more about NHST and linear models when there‚Äôs 3 or more levels."
  },
  {
    "objectID": "gradstats/gradlabs/6_TTests.html#footnotes",
    "href": "gradstats/gradlabs/6_TTests.html#footnotes",
    "title": "Lecture 6 - T-Tests Are [SPOILER ALERT] Linear Models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(And side note : to get what R calculates in our model, we will weight each pooled variance by the sample size of each group.)‚Ü©Ô∏é"
  },
  {
    "objectID": "calstats/Lectures/6AL_ModelIntro.html",
    "href": "calstats/Lectures/6AL_ModelIntro.html",
    "title": "Introduction to Linear Models",
    "section": "",
    "text": "Mini Exam Is Over!\n\nTAs working hard to grade!\nProfessor regrets the time error :(\n\nMega Exam Changes :\n\nRescheduled to 4/25\n85 Minutes\nWill be shorter than the Mini Exam (2-3 Problems!)\n\nFinal Project Milestone #1 Due SUNDAY AT 11:59 PM\n\nGoals : Identify your model, and support your ideas with some past research.\nDRAFT : DONE IS BETTER THAN PERFECT!\n\nFinal Project Milestone #2 Due Next Friday (After Class is Fine‚Ä¶Sunday okay?)\n\nGoals : Define your IV2 and draft your survey items.\nWill work on in section next week.\n\n\n\n\n\n\n2:10 - 2:30 | Check-In and Announcements / Exam Debrief.\n2:30 - 3:15 | The Linear Model (I LOVE LINES)\n3:15 - 3:30 | Break Time\n3:30 - 4:30 | Final Project Workshop (Milestone #1 ‚Äì&gt; Milestone #2)"
  },
  {
    "objectID": "calstats/Lectures/6AL_ModelIntro.html#check-in-post-mini-exam-survey",
    "href": "calstats/Lectures/6AL_ModelIntro.html#check-in-post-mini-exam-survey",
    "title": "Introduction to Linear Models",
    "section": "",
    "text": "Mini Exam Is Over!\n\nTAs working hard to grade!\nProfessor regrets the time error :(\n\nMega Exam Changes :\n\nRescheduled to 4/25\n85 Minutes\nWill be shorter than the Mini Exam (2-3 Problems!)\n\nFinal Project Milestone #1 Due SUNDAY AT 11:59 PM\n\nGoals : Identify your model, and support your ideas with some past research.\nDRAFT : DONE IS BETTER THAN PERFECT!\n\nFinal Project Milestone #2 Due Next Friday (After Class is Fine‚Ä¶Sunday okay?)\n\nGoals : Define your IV2 and draft your survey items.\nWill work on in section next week.\n\n\n\n\n\n\n2:10 - 2:30 | Check-In and Announcements / Exam Debrief.\n2:30 - 3:15 | The Linear Model (I LOVE LINES)\n3:15 - 3:30 | Break Time\n3:30 - 4:30 | Final Project Workshop (Milestone #1 ‚Äì&gt; Milestone #2)"
  },
  {
    "objectID": "calstats/Lectures/6AL_ModelIntro.html#the-linear-model",
    "href": "calstats/Lectures/6AL_ModelIntro.html#the-linear-model",
    "title": "Introduction to Linear Models",
    "section": "The Linear Model",
    "text": "The Linear Model\n\nRECAP : The Mean as Prediction\nPreviously, we discussed how the mean could be used to make predictions of individuals.\n\\(\\huge y_i = \\hat{Y} + \\epsilon_i\\)\n\n\n\\(\\Large y_i\\) = the DV = the individual‚Äôs actual score we are trying to predict (remember \\(_i\\) = index; a specific individual.)\n\non the graph: each individual dot (on the y-axis; the x-axis just describes when people submitted the survey.\n\n\\(\\Large \\hat{Y}\\) = our prediction (the mean).\n\non the graph: the solid red line\n\n\\(\\Large \\epsilon\\) = residual error = distance between the predicted values of y and the individual‚Äôs actual value of y\n\non the graph: the distance between each dot and the line.\n\n\nd &lt;- read.csv(\"~/Dropbox/!WHY STATS/Class Datasets/cal_mini_SP25.csv\", stringsAsFactors = T)\nplot(d$insta.follows, main = \"Mean as a Model (Red Line)\",\n     xlab = \"Index (Row in Dataset)\",\n     ylab = \"# Of Accounts a Person Follows\")\nabline(h = mean(d$insta.follows, na.rm = T), lwd = 5, col = 'red')\n\n\n\n\n\n\n\n\nWe also talked about how we could quantify the total error in these predictions, by adding up the squared residual errors (the sum of squared errors).\n\nresidual &lt;- d$insta.follows - mean(d$insta.follows, na.rm = T)\nSST &lt;- sum(residual^2, na.rm = T)\nSST\n\n[1] 39584116\n\n\nThis number made no sense, but it is a critical statistic, since it quantifies how valid our predictions of individuals were when using the mean to make predictions. To give the statistics some context, we divided the sum of squared errors by the sample size (this is the variance) and then un-squared this number (by taking the square root). This new statistic - the standard deviation - served as an average of residual error that describes how far the average person differs from the mean.\n\nn &lt;- length(na.omit(d$insta.follows)) # total number of individuals; omitting missing data.\nsqrt(SST/(n-1)) # the equation for the standard deviation\n\n[1] 515.4272\n\nsd(d$insta.follows, na.rm = T) # the function to get the same answer.\n\n[1] 515.4272\n\n\nAs scientists, our goal is to make accurate predictions of individuals. So we would want to find a way to make the sum of squared errors equal zero - have no error in our predictions. The mean is a good starting place, but it‚Äôs one number. And people are complex.\n\n\n\n\n\n\n\nthe mean\nthe linear model ‚Ñ¢ ¬©\n\n\n\n\n\n\n\nThe mean is an okay starting place for our predictions, but we can try to do better!\n\nnames(d)\n\n [1] \"pace\"            \"engaging\"        \"fb.friends\"      \"insta.followers\"\n [5] \"insta.follows\"   \"bored\"           \"thirsty\"         \"tired\"          \n [9] \"satlife\"         \"oskilove\"        \"rlove\"           \"socialmed.use\"  \n[13] \"class.attention\" \"hrs.sleep\"       \"selfpow.data\"    \"corppow.data\"   \n[17] \"success.work\"    \"success.priv\"    \"selfesteem\"      \"catdog\"         \n[21] \"tuhoburat\"       \"calgame\"         \"caffeine\"        \"breakfast\"      \n[25] \"is.female\"       \"long.hair\"       \"has.water\"       \"shoesize\"       \n[29] \"height\"          \"happy\"           \"drink\"           \"stoned72\"       \n[33] \"multilingual\"    \"waitlist\"       \n\n\nDISCUSS :\n\nICE BREAKER : if you had to live inside one social media platform, what would it be and why???\nTHINK ABOUT A LINEAR MODEL : how do you think the variables (above) would help (or not help) us predict the number of accounts someone follows on instagram (insta.follows)? Why / why not???\n\n\n\n\n\n\n\n\n\nVariables we think would help us make predictions\nVariables we think would not help us make predictions\nVariables we have NO IDEA if they would help us make predictions?!?\n\n\nnumber of followers (the amount of followers you have is very similar to the amount of people you follow‚Ä¶.unless you have an ego :\n\nif you have an ego; then the number of accounts you follow will be lower and less related to the number of accounts that follow you.\nif you don‚Äôt have an ego; then there should be a strong connection.\n\ntime spent on social media :\nsocmedia.use : people who say they use social media a lot will follow more accounts than people who say they do not use social media.\nbeing thirsty :\nhair length\ncaffeine intake?\nheight?\n\n\n\n\n\n\nThe Linear Model in FOUR EASY STEPS.\nThe model is a line that updates our predictions of one variable based on knowledge of another.\n\nDefine your model : what is your DV? What are your IVs? How do you think they will be related???\n# of accounts people follow on instagram ~ soc.mediause + error\nGraph your DV and IV(s) : make sure the data look good.\n\n\npar(mfrow = c(1,2))\nhist(d$insta.follows)\nmax(scale(d$insta.follows), na.rm = T) # looking to see the max value and its z-score to determine whether its an outlier or not\n\n[1] 4.089203\n\nhist(d$socialmed.use)\n\n\n\n\n\n\n\n\n\nPlot the relationship between the two variables.\n\n\nplot(insta.follows ~ socialmed.use, data = d)\nmod &lt;- lm(insta.follows ~ socialmed.use, data = d)\nabline(mod, lwd = 5, col = 'red')\n\n\n\n\n\n\n\n\n\nDefine the linear model and interpret the intercept and slope of the model.\n\n\nmod &lt;- lm(insta.follows ~ socialmed.use, data = d) # defines the model; saves as mod\nmod # shows me what is inside mod\n\n\nCall:\nlm(formula = insta.follows ~ socialmed.use, data = d)\n\nCoefficients:\n  (Intercept)  socialmed.use  \n       284.72          43.98  \n\nplot(insta.follows ~ socialmed.use, data = d) # graphs the relationship \nabline(mod, lwd = 5, col = 'red') # draws a red line of width five based on mod\n\n\n\n\n\n\n\n\nequation for a line : y = a + bX\n\ncoef(mod)\n\n  (Intercept) socialmed.use \n    284.71701      43.97825 \n\n# THE MODEL : insta.follows = 284 + 43.9 * X\n## predicted value of insta.follows for someone with a social media usage of 5\n284 + 43.9 * 5\n\n[1] 503.5\n\n\n\n\n\\(\\Large y_i\\) = the DV = each individual‚Äôs actual score on the dependent variable.\n\non the graph: the value of each dot on the y-axis\n\n\\(\\Large a\\) = the intercept = the starting place for our prediction. You can think of the intercept as ‚Äúthe predicted value of y when all x values are zero‚Äù.)\n\non the graph: the value of the line at X = 0\n\n\\(\\Large X_i\\) = the IV = the individual‚Äôs actual score on the independent variable (a different variable than the DV).\n\non the graph: the value of each dot on the x-axis\n\n\\(\\Large b_1\\) = the slope = an adjustment we make in our prediction of y, based on the individual‚Äôs x value.\n\non the graph: how much the line increases in y value when x-values increase by 1 unit.\n\n\\(\\Large \\epsilon_i\\) = residual error = the distance between our prediction and the individual‚Äôs actual y value.\n\non the graph: the distance between each individual data point and the line."
  },
  {
    "objectID": "calstats/Lectures/6AL_ModelIntro.html#break-time",
    "href": "calstats/Lectures/6AL_ModelIntro.html#break-time",
    "title": "Introduction to Linear Models",
    "section": "BREAK TIME",
    "text": "BREAK TIME"
  },
  {
    "objectID": "calstats/Lectures/6AL_ModelIntro.html#final-project-workshop",
    "href": "calstats/Lectures/6AL_ModelIntro.html#final-project-workshop",
    "title": "Introduction to Linear Models",
    "section": "Final Project Workshop",
    "text": "Final Project Workshop\n\nMilestone 1. Some Questions\n\ndo we need to use the template? [NO!!!! don‚Äôt worry about the template; the point of the template is to help you organize and summarize the past reseach you are reading; your TA will need to see that you‚Äôve found and thought about SOME past resaerch.]\ni don‚Äôt wanna outline an intro. [just a draft‚Ä¶keep it simple]\n\nexample outline : sleep quality\n\nTHE POINT : sleep quality is how people feel after they sleep, such as how restful people are.\nEVIDENCE / WHO CARES : sleep quality is importatn because it‚Äôs related to productivity (CITATION), morale (CITATION), and social interaction (CITATION).\nTHE POINT : screen time is related to sleep quality.\n\nEVIDENCE : In one study, researchers found Blah blah blah.\nWHO CARES : Screen use is increasing (CITATION), and important to know how that might be affecting sleep quality in ways that might interfere with well-being.\n\n\n\nFOCUS ON :\n\ndefining a linear model\nfiguring out how to measure your variables.\n\n\n\n\nMilestone 1. Defining IV2\nConsider the pattern that we found earlier in class. Do you think this pattern will always hold for all people? Or would it change for some people? How so??\n\nplot(insta.follows ~ insta.followers, data = d) # graphs the relationship \nabline(mod5, lwd = 5, col = 'red') # draws a red line of width five based on mod\n\n\n\n\n\n\n\n\nThese other variables are called moderator variables, and they can be a good way to identify a second independent variable for your study / help advance science. Life is complex, and some established ‚Äúeffect‚Äù or pattern in the data will likely change as a result of some other variable!\n\n\nMilestone 2. Creating Your Survey.\nI strongly recommend using Google Forms to build your survey. A few tips.\n\nMake sure your DV is measured numerically. A likert scale or number response is fine.\nUse Multiple Choice Grid for likert scales.\n\nEach row is an item.\nUse a 1-5 scale to make it easy for participants to answer.\nTry to make all the items fit on the same ‚Äústem‚Äù, so it‚Äôs easier for participants to get in the flow.\nI do not require responses; good to let folks skip questions if they want (and know to exclude their data) rather than force them to give an answer, which might be bad data.\nYou could add an ‚Äúattention check‚Äù (e.g., ‚ÄúMark Strongly Agree for this question.‚Äù) if you are worried about people not taking your survey seriously. But BETTER to just keep the survey short, and give your participant some motivation (‚ÄúI can send you the results of my paper if you want!‚Äù)\n\n\nInclude demographic variables like age and sex; give people options to self-identify as something outside a forced binary!\nKeep categorical variables to just a few levels; you will need a LOT of data to capture variation if there are too many levels! 3-4 groups per variable.\n\nMake sure categorical variables are not better measured numerically; set ‚Äúresponse validation‚Äù for any open-ended numbers you hope to collect to make data cleaning easier.\n\nMake sure each variable is measured independently of the others. For example, if I want to measure the relationship between happiness and reading, I would want to measure these separately.\n\nI am happy.\nI like to read.\nReading makes me happy. This mixes up the two variables (the DV and IV). It could be a cool measure on its own (love for reading scale?)."
  },
  {
    "objectID": "gradstats/gradlabs/6Lab_TTests.html",
    "href": "gradstats/gradlabs/6Lab_TTests.html",
    "title": "Lab 6 - Key",
    "section": "",
    "text": "Answer the questions below as a .PDF (you can either render from Quarto as .PDF, or render as .html and then print this html file as a .PDF). Make sure to reference any external sources (e.g., stack exchange; ChatGPT; peer help) that you used at the top of your lab assignment. If you use external resources, don‚Äôt just copy / paste the code (and source) blindly, but spend time thinking (and writing in your Lab) about what these external sources are doing with the code / what techniques you are learning from this source. And please ask for help if you get stuck / are confused / professor did something wrong on Discord!"
  },
  {
    "objectID": "gradstats/gradlabs/6Lab_TTests.html#lab-instructions.",
    "href": "gradstats/gradlabs/6Lab_TTests.html#lab-instructions.",
    "title": "Lab 6 - Key",
    "section": "",
    "text": "Answer the questions below as a .PDF (you can either render from Quarto as .PDF, or render as .html and then print this html file as a .PDF). Make sure to reference any external sources (e.g., stack exchange; ChatGPT; peer help) that you used at the top of your lab assignment. If you use external resources, don‚Äôt just copy / paste the code (and source) blindly, but spend time thinking (and writing in your Lab) about what these external sources are doing with the code / what techniques you are learning from this source. And please ask for help if you get stuck / are confused / professor did something wrong on Discord!"
  },
  {
    "objectID": "gradstats/gradlabs/6Lab_TTests.html#problem-1.",
    "href": "gradstats/gradlabs/6Lab_TTests.html#problem-1.",
    "title": "Lab 6 - Key",
    "section": "Problem 1.",
    "text": "Problem 1.\nIn lecture, Professor predicted hunger from whether a person had breakfast or not. Work with your buddies (or go solo) to define a new linear model to test whether having breakfast (the most important meal of the day) predicts any other numeric variable in the grad mini dataset. Before you define the model, write out your null and alternative hypothesis and explain the possible reasons why you might see a pattern in the data. Make sure to include a graph of the linear model (make the graph look nice), and report and interpret the intercept, slope, \\(R^2\\), and cohen‚Äôs d value of this model."
  },
  {
    "objectID": "gradstats/gradlabs/6Lab_TTests.html#problem-2.-sampling-error-via-bootstrapping-and-standard-error",
    "href": "gradstats/gradlabs/6Lab_TTests.html#problem-2.-sampling-error-via-bootstrapping-and-standard-error",
    "title": "Lab 6 - Key",
    "section": "Problem 2. Sampling Error via Bootstrapping and Standard Error",
    "text": "Problem 2. Sampling Error via Bootstrapping and Standard Error\nEstimate sampling error of the slope in Problem 1 using bootstrapping, and then using the formula for standard error (as output by the summary function). Note that the sampling error estimate via bootstrapping and via standard error should be very similar to each other. (Spooky!) Below these statistics, describe what they tell you about the relationship you observed in Problem 1. Then, draw on your past stats knowledge and / or go through the supplemental readings on NHST in the syllabus - what does the p-value of the slope tell you? Okay to struggle here; it is confusing and we will chat more about this next week.\nAs an optional challenge worth zero points, see if you can calculate the standard error by hand; you‚Äôll need to use the equation for pooled variance (and equation for standard error when using pooled variance). This is not important and I don‚Äôt really know why I‚Äôm including it here‚Ä¶just tradition I guess."
  },
  {
    "objectID": "gradstats/gradlabs/6Lab_TTests.html#problem-3.-t-tests-are-a-linear-model.",
    "href": "gradstats/gradlabs/6Lab_TTests.html#problem-3.-t-tests-are-a-linear-model.",
    "title": "Lab 6 - Key",
    "section": "Problem 3. T-Tests are A Linear Model.",
    "text": "Problem 3. T-Tests are A Linear Model.\nConfirm that the t.test() function reports the same statistics as your linear model. (You will need to add an argument to force R to assume equal variances between the groups. We will talk more about this next week!)"
  },
  {
    "objectID": "gradstats/gradlabs/6Lab_TTests.html#problem-4.-more-factors-no-problem.",
    "href": "gradstats/gradlabs/6Lab_TTests.html#problem-4.-more-factors-no-problem.",
    "title": "Lab 6 - Key",
    "section": "Problem 4. More Factors, No Problem.",
    "text": "Problem 4. More Factors, No Problem.\nThe variable food.pref measures whether people prefer tacos, tortas, or burritos. Predict hunger from this categorical variable that has three levels; interpret the intercept, slope(s), and \\(R^2\\) value and include a graph. Feel free to define some other linear model; key idea is to get practice working with linear models when the IV has more than two categories. You can use Prof‚Äôs 101 notes as a guide on how to interpret a dummy coded variable when there are three (or more) levels."
  },
  {
    "objectID": "calstats/Lectures/6AL_ModelIntro.html#activity-define-another-model-to-predict-insta.follows-from-another-numeric-iv",
    "href": "calstats/Lectures/6AL_ModelIntro.html#activity-define-another-model-to-predict-insta.follows-from-another-numeric-iv",
    "title": "Introduction to Linear Models",
    "section": "Activity : Define another model to predict insta.follows from another numeric IV!!!",
    "text": "Activity : Define another model to predict insta.follows from another numeric IV!!!\n\n# Zara! \nhist(d$bored)\n\n\n\n\n\n\n\nplot(insta.follows ~ bored, data = d)\nmod2 &lt;- lm(insta.follows ~ bored, data = d)\nabline(mod2)\n\n\n\n\n\n\n\n# Height!! (With the outliers)\n\nmod3 &lt;- lm(insta.follows ~ height, data = d)\nplot(insta.follows ~ height, data = d)\nabline(mod3, lwd = 5)\n\n\n\n\n\n\n\n# Height!! (Without the outliers)\nd$H2 &lt;- d$height\nd$H2[d$H2 &lt; 40 | d$H2 &gt; 100] &lt;- NA # removing outliers!\nhist(d$H2) \n\n\n\n\n\n\n\nmod4 &lt;- lm(insta.follows ~ H2, data = d)\nplot(insta.follows ~ H2, data = d)\nabline(mod4, lwd = 5)\n\n\n\n\n\n\n\n# INSTA.FOLLOWERS\nd$insta.followers[d$insta.followers &gt; 10000] &lt;- NA # removing the outlier!\nmod5 &lt;- lm(insta.follows ~ insta.followers, data = d)\nplot(insta.follows ~ insta.followers, data = d)\nabline(mod5, lwd = 5)"
  },
  {
    "objectID": "calstats/Lectures/6AL_ModelIntro.html#evaluating-error",
    "href": "calstats/Lectures/6AL_ModelIntro.html#evaluating-error",
    "title": "Introduction to Linear Models",
    "section": "Evaluating Error",
    "text": "Evaluating Error\n\npar(mfrow = c(1,2))\nplot(d$insta.follows, main = \"Mean as a Model (Red Line)\",\n     xlab = \"Index (Row in Dataset)\",\n     ylab = \"# Of Accounts a Person Follows\")\nabline(h = mean(d$insta.follows, na.rm = T), lwd = 5, col = 'red')\n\nplot(insta.follows ~ socialmed.use, data = d) # graphs the relationship \nabline(mod, lwd = 5, col = 'red') # draws a red line of width five based on mod\n\n\n\n\n\n\n\n\n\nSST # sum of squared errors when using the mean (we did this an hour ago)\n\n[1] 39584116\n\nmod$residuals # the residuals from my model\n\n           1            2            3            4            6            7 \n 447.4134794 2173.3917317  585.4569746  -27.5865206 -579.5430254 -592.5647730 \n           8            9           10           11           12           13 \n  -3.5647730 -360.6300159 -328.6952588 -474.5430254 1301.4569746 -113.5430254 \n          15           16           17           18           19           20 \n 507.4569746 1987.4569746 -423.5865206  323.4352270  919.4134794 -175.5647730 \n          21           22           23           24           25           26 \n 195.3917317  -76.5430254  373.4134794  -30.5212777 -265.5212777 -463.5647730 \n          27           28           29           31           32           33 \n-265.5430254  -92.5647730 -224.4995301 -328.6952588 -590.5647730 -548.5865206 \n          34           35           36           37           38           39 \n 244.4569746 -226.5430254  263.4352270 -176.4995301  449.3482365   -0.5430254 \n          40           41           42           43           44           45 \n-401.5212777  412.4787223  248.4787223 1207.4352270  110.4569746  667.4352270 \n          46           48           49           50           51           52 \n-173.4995301  -32.5647730 -633.5430254 -504.6082683  463.4569746  -74.4995301 \n          53           54           55           56           57           58 \n-459.5647730 -230.5212777  451.4134794 -192.5647730 1155.4569746 1451.4134794 \n          59           60           61           62           63           64 \n 121.5004699  521.4134794 -315.4995301 -136.5647730 -482.5430254 -316.5647730 \n          65           66           67           68           69           70 \n-374.5430254  454.3264889 -287.5647730 -372.6735111 -100.5647730  348.4352270 \n          71           72           73           74           75           76 \n-369.5865206 -579.5212777 -247.6082683  -20.5865206  426.4787223  319.4787223 \n          77           78           80           81           82           83 \n-166.6517635  900.3917317  189.4352270 -254.6082683 -236.5430254   41.3699841 \n          84           85           86           87           88           89 \n   9.4787223 -262.4995301 -457.6300159  -46.5865206 -524.4995301 -484.5865206 \n          90           91           92           93           95           96 \n-372.6735111 -472.5647730   63.4569746 -372.6735111 -499.5647730  441.4134794 \n          97           98           99          100          101          102 \n-284.7170064 -122.6082683 -615.5212777 -508.4995301  234.4352270 -284.7170064 \n         103          104          105          106          107          108 \n-459.6300159 -376.5647730  701.4134794 -155.5430254  995.3917317  273.4352270 \n         109          110          111          112          113          114 \n 550.3917317  326.4569746 -504.6082683   32.4352270 -328.6952588  -86.5430254 \n         115          116          117          118          119          120 \n-448.5865206 -261.7170064 -213.5430254  261.4352270 -460.6300159  309.4787223 \n         121          122          123          124          125          126 \n -11.6517635 -100.5647730  584.4352270 -502.6082683 -362.5430254 -193.5865206 \n         127          128          129          130          131          132 \n-576.5430254  130.4569746  806.3482365  476.3917317 -424.4995301   61.4134794 \n         134          135          136          137          138          139 \n  39.3482365 -284.7170064 -478.4995301  633.4569746 -304.6082683 -450.6300159 \n         140          141          142          143          144          145 \n-277.5430254 -242.5647730  289.4352270 -484.6082683  144.4352270 -580.5212777 \n         146          147          148          149          150          151 \n  47.4134794  208.4787223 -429.5865206 -151.4995301 -550.5212777 -134.6300159 \n         153          154          155          156 \n 988.3917317   83.3482365 -116.5430254  643.4352270 \n\nsum(mod$residuals) # add up to zero\n\n[1] -1.904255e-12\n\nSSM &lt;- sum(mod$residuals^2) \nSSM # 37million errors.\n\n[1] 37685836\n\nSST - SSM # 1898280 LESS error when I use my model to make predictions.\n\n[1] 1898280\n\n(SST - SSM)/SST # about .05 or 5% reduction in the total error. \n\n[1] 0.04795561\n\n\n^^^ This is called \\(R^2\\) ^^^\n\npar(mfrow = c(1,2))\nplot(d$insta.follows, main = \"Mean as a Model (Red Line)\",\n     xlab = \"Index (Row in Dataset)\",\n     ylab = \"# Of Accounts a Person Follows\")\nabline(h = mean(d$insta.follows, na.rm = T), lwd = 5, col = 'red')\n\nplot(insta.follows ~ insta.followers, data = d) # graphs the relationship \nabline(mod5, lwd = 5, col = 'red') # draws a red line of width five based on mod\n\n\n\n\n\n\n\nSSM5 &lt;- sum(mod5$residuals^2)\nSSM5\n\n[1] 19751193\n\nSST - SSM5\n\n[1] 19832923\n\n(SST - SSM5)/SST\n\n[1] 0.5010324"
  },
  {
    "objectID": "calstats/Lectures/6AL_ModelIntro.html#break-time-meet-back-at-352",
    "href": "calstats/Lectures/6AL_ModelIntro.html#break-time-meet-back-at-352",
    "title": "Introduction to Linear Models",
    "section": "BREAK TIME : MEET BACK AT 3:52",
    "text": "BREAK TIME : MEET BACK AT 3:52"
  },
  {
    "objectID": "gradstats/gradlabs/7_PowerMR.html",
    "href": "gradstats/gradlabs/7_PowerMR.html",
    "title": "Lecture 7 - Power & Multiple Regression",
    "section": "",
    "text": "Load this dataset into R. Collected from undergrads at UC Berkeley. Yay students.\n\nThe researchers wanted to know whether people who ate breakfast (IV = breakfast; a categorical factor with two levels 0 = No; 1 = Yes) would happier (DV = happy) than people who did not have breakfast.\nUse the R output from the linear model below to answer the following questions :\n\n\nhist(u$happy)\n\n\n\n\n\n\n\nplot(u$breakfast)\n\n\n\n\n\n\n\nlevels(u$breakfast)[1]# identifying the empty level.\n\n[1] \"\"\n\nlevels(u$breakfast)[1] &lt;- NA\nplot(u$breakfast)\n\n\n\n\n\n\n\nmod1 &lt;- lm(happy ~ breakfast, data = u)\nsummary(mod1)\n\n\nCall:\nlm(formula = happy ~ breakfast, data = u)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.5684 -0.5684 -0.0370  0.9630  2.9630 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    7.0370     0.2220  31.698   &lt;2e-16 ***\nbreakfastYes   0.5314     0.2780   1.911   0.0579 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.631 on 147 degrees of freedom\n  (7 observations deleted due to missingness)\nMultiple R-squared:  0.02425,   Adjusted R-squared:  0.01761 \nF-statistic: 3.653 on 1 and 147 DF,  p-value: 0.05792\n\n\n\nWhat is the difference in happiness for someone who did vs.¬†did not eat breakfast? [.53]\nWhat is the predicted happiness for someone who did not eat breakfast? (Round to 1 decimal place.)\n```         \nhappiness = 7 + .5 * 0 = 7\n```\nWhat is the predicted happiness for someone who did eat breakfast? (Round to 1 decimal place.)\n```         \nhappiness = 7 + .5 * 1 = 7.5\n```\nWhat are some reasons we might we observe this difference?\n\nCAUSAL : having breakfast makes people feel good\nREVERSE CAUSASTION : happiness causes you to eat breakfast (not being happy causes you to not eat breakfast.)\nCONFOUNDS & COLLIDERS :\n\nCONFOUNDS : a variable that is related to both the DV and the IV (free time is related to both happiness and ability to have breakfast; stress; SES statsus; age)\nINTERACTION EFFECT [NEXT WEEK] : a variable that is an outcome of both the DV and IV.\n\nexample : inherent talent ~ hard work; but in a sample of high-performing people, these are related. but in a sample of low-performing people, these two groups are not related.\n\n\nCHANCE : the relationship is due to sampling error.\n\nIs this difference considered statistically significant? Why / why not?\nNow, use R to define a linear model to test whether people with more instagram followers (IV = insta.followers) would be happier (DV = happy) than people with fewer instagram followers. What is the slope of this model? Round to two decimal places.\nDescribe this relationship between these two variables in terms of the slope, R^2 value, and statistical significance.\n\n\nhist(u$insta.followers)\n\n\n\n\n\n\n\nmod2 &lt;- lm(happy ~ insta.followers, data = u)\nsummary(mod2)\n\n\nCall:\nlm(formula = happy ~ insta.followers, data = u)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.3931 -0.3956  0.3023  0.6126  2.6099 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      7.396e+00  1.382e-01  53.509  &lt; 2e-16 ***\ninsta.followers -3.698e-06  8.350e-07  -4.429 1.86e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.664 on 144 degrees of freedom\n  (10 observations deleted due to missingness)\nMultiple R-squared:  0.1199,    Adjusted R-squared:  0.1138 \nF-statistic: 19.61 on 1 and 144 DF,  p-value: 1.863e-05\n\nplot(happy ~ insta.followers, data = u)\nabline(mod2, lwd = 5)\n\n\n\n\n\n\n\ndev.new()\npar(mfrow = c(2,2))\nplot(mod2)\n\nWarning in sqrt(crit * p * (1 - hh)/hh): NaNs produced\n\nWarning in sqrt(crit * p * (1 - hh)/hh): NaNs produced\n\nu$insta.followers[u$insta.followers &gt; 10000] &lt;- NA\nmod2 &lt;- lm(happy ~ insta.followers, data = u)\nsummary(mod2)\n\n\nCall:\nlm(formula = happy ~ insta.followers, data = u)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.4042 -0.6869  0.4325  0.6578  2.6578 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     7.342e+00  2.121e-01  34.619   &lt;2e-16 ***\ninsta.followers 8.939e-05  2.662e-04   0.336    0.738    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.674 on 142 degrees of freedom\n  (12 observations deleted due to missingness)\nMultiple R-squared:  0.0007933, Adjusted R-squared:  -0.006243 \nF-statistic: 0.1127 on 1 and 142 DF,  p-value: 0.7375\n\nplot(happy ~ insta.followers, data = u)\nabline(mod2, lwd = 5)\ndev.new()\npar(mfrow = c(2,2))\nplot(mod2)"
  },
  {
    "objectID": "gradstats/gradlabs/7_PowerMR.html#check-in",
    "href": "gradstats/gradlabs/7_PowerMR.html#check-in",
    "title": "7_PowerMR",
    "section": "",
    "text": "dataset; define a model - is there a relationship between the two variables?\nplot-twist; need to remove outliers AND there‚Äôs a quadratic term?\ninterpreting the significance of the model."
  },
  {
    "objectID": "gradstats/gradlabs/7_PowerMR.html#p-values",
    "href": "gradstats/gradlabs/7_PowerMR.html#p-values",
    "title": "7_PowerMR",
    "section": "p-values",
    "text": "p-values"
  },
  {
    "objectID": "gradstats/gradlabs/7_PowerMR.html#power-tests",
    "href": "gradstats/gradlabs/7_PowerMR.html#power-tests",
    "title": "Lecture 7 - Power & Multiple Regression",
    "section": "power tests",
    "text": "power tests\n\nwhat‚Äôs the point, professor?\n\nPower : the probability that you would ‚Äúcorrectly‚Äù observe a ‚Äútrue‚Äù relationship between two variables that exists.\n\ngoal : you want power to be HIGH. Power increases as‚Ä¶\n\nthe effect size increases : the bigger the difference, the more likely you‚Äôll detect it.\nyour sample size increases : the more people, the less sampling error, and the easier it is to have conidence that any difference you found is not just chance.\nyou increase the threshold for rejecting the null hypothesis : if the probability\n\nassumptions : there is a true relationship; you have observed this relationship.\n\nReasons to Calculate Power :\n\nPost-Hoc Power : You did a study, and want to evaluate\nPower Planning : You are planning to run a study, and want to know how many people to recruit to have the highest probability of observing the ‚Äútrue‚Äù effect (if it exists.)\n\n\n\n\na tour of null and alternative realities\nWatch the lecture recording (posted to bCourses) for a tour through these slides.\n\n\n\n\ncalculating in R (by hand)\n\nmod1 # a model object\n\n\nCall:\nlm(formula = happy ~ breakfast, data = u)\n\nCoefficients:\n (Intercept)  breakfastYes  \n      7.0370        0.5314  \n\nsummary(mod1) # a function applied to the object\n\n\nCall:\nlm(formula = happy ~ breakfast, data = u)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.5684 -0.5684 -0.0370  0.9630  2.9630 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    7.0370     0.2220  31.698   &lt;2e-16 ***\nbreakfastYes   0.5314     0.2780   1.911   0.0579 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.631 on 147 degrees of freedom\n  (7 observations deleted due to missingness)\nMultiple R-squared:  0.02425,   Adjusted R-squared:  0.01761 \nF-statistic: 3.653 on 1 and 147 DF,  p-value: 0.05792\n\nsm1 &lt;- summary(mod1) # saving this as an object\nobjects(sm1) # there is more inside.\n\n [1] \"adj.r.squared\" \"aliased\"       \"call\"          \"coefficients\" \n [5] \"cov.unscaled\"  \"df\"            \"fstatistic\"    \"na.action\"    \n [9] \"r.squared\"     \"residuals\"     \"sigma\"         \"terms\"        \n\nsm1$coefficients # tadaa\n\n             Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept)  7.037037  0.2220042 31.697766 1.363981e-67\nbreakfastYes 0.531384  0.2780306  1.911243 5.791958e-02\n\nsm1$coefficients[2,3] # our t-value\n\n[1] 1.911243\n\nm1tval &lt;- sm1$coefficients[2,3]\n\nqt(.975, df = 147) # t-distribution approaches the normal distribution (with a 95% Interval cutoff of 1.96....) but we are not quite there.\n\n[1] 1.976233\n\nm1cut &lt;- qt(.975, 147) # t-distribution approaches the normal distribution (with a 95% Interval cutoff of 1.96....) but we are not quite there.\n\npt(m1tval - m1cut, df = 147) # our power.\n\n[1] 0.4741351\n\n\n\n\ncalculating in R (a package)\n\n# install.packages(\"pwr\")\nlibrary(pwr)\nsummary(mod1)\n\n\nCall:\nlm(formula = happy ~ breakfast, data = u)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.5684 -0.5684 -0.0370  0.9630  2.9630 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    7.0370     0.2220  31.698   &lt;2e-16 ***\nbreakfastYes   0.5314     0.2780   1.911   0.0579 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.631 on 147 degrees of freedom\n  (7 observations deleted due to missingness)\nMultiple R-squared:  0.02425,   Adjusted R-squared:  0.01761 \nF-statistic: 3.653 on 1 and 147 DF,  p-value: 0.05792\n\nm1r &lt;- summary(mod1)$r.squared^.5\npwr.r.test(n = 149, r = m1r)\n\n\n     approximate correlation power calculation (arctangh transformation) \n\n              n = 149\n              r = 0.1557139\n      sig.level = 0.05\n          power = 0.4770911\n    alternative = two.sided\n\n\n\n\nmore power examples!\n\nPower Illustrated.\nIn lecture, professor did some scribbles on the whiteboard to illustrate power, and tried to record these. He also said that he would record a few other videos.\nProf.¬†did not, in fact, find time in the present moment to record new videos. Bummer! But he did remember that he had recorded similar videos, and found some from 2018. (What were you doing in 2018?? Let us know on Discord; and as always - reach out if you still have questions about power!!)\n\nRecording #1 : Conceptual Example of Power\nRecording #2 : Another Example. Couldn‚Äôt immediately track down the original data analyses these refer to, but the slope (b = .44) and other statistics come from a paper I had rejected in part because reviewers complained that I only replicated the main result in 4 out of 5 studies. (The paper was also a hot mess.) Bummer! But it was a cool phenomenon; I sadly never published on it (parenting * teaching career activated), but truth got out eventually someone published a very clearly written, much better, and perfectly replicating paper (across six studies!) on it 8 years later. Ahhh, one thing off the to-do list!\n\n\n\nEstimating Sample Size.\nAs discussed in the lecture slides (see recording), power is a function of effect size, sample size, and the alpha level (alpha = the Type I error that the researcher sets). This means that you can use these functions to estimate the sample size you need for a given power (the convention is often 80%).\nLet‚Äôs say I want to know what sample size I need to detect a slope of r = .23.\nFrom the pwr package, I can define this effect, specifcy the power, type I error level, and whether I want to do a 1- or 2-tailed test.\n\npwr.r.test(r = .23, power = .80, alternative = \"two.sided\")\n\n\n     approximate correlation power calculation (arctangh transformation) \n\n              n = 145.2367\n              r = 0.23\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\n\nI can also plot the result of this output and see how power increases as a function of my sample size.\n\np.ex &lt;- pwr.r.test(r = .23, power = .80, alternative = \"two.sided\")\nplot(p.ex)\n\n\n\n\n\n\n\n\n\n\n\n‚Ä¶but you don‚Äôt have to take my word for it.\nThe approach to power described above assumes a normally distribution of sampling error. This is a good starting place, but not all distributions are gaussian! Below are a few different methods to help you estimate power across a wide variety of types of data.\n\nHere‚Äôs a nice overview of how to conduct sample size power analysis in R; it works through a few examples and discusses how to\nThis is a modern, thorough, and good overview of power, that also discusses ways to generate simulated data to estimate power..\nHere‚Äôs another tutorial of an R package that works for a wide variety of different tests; many of which can be used when youthe assumptions of linear regression are violated.\nLet me know if you find other useful resources to help calcualte or conceptualize power!"
  },
  {
    "objectID": "gradstats/gradlabs/7_PowerMR.html#presentation",
    "href": "gradstats/gradlabs/7_PowerMR.html#presentation",
    "title": "7_PowerMR",
    "section": "presentation!!",
    "text": "presentation!!"
  },
  {
    "objectID": "gradstats/gradlabs/7_PowerMR.html#more-regression-multiple-regression",
    "href": "gradstats/gradlabs/7_PowerMR.html#more-regression-multiple-regression",
    "title": "Lecture 7 - Power & Multiple Regression",
    "section": "more regression (multiple regression)",
    "text": "more regression (multiple regression)\n\nBenefits of Multiple Regression: Life is Complex!\n\nAccount for multiple variables to help predict and explain the DV.\nControl for the effect of on IV on the relationship between another IV and the DV.\nCompare unique effects of each IV on the DV. See how the slope of one IV compares to the slope of another IV. (Need to standardize your variables, to make sure that they are all on the same scale.)\nNEXT WEEK : Interaction Effects!!! See how the relationship between one IV and the DV changes depending on another IV.\n\n\n\nIn Practice.\n\nu$height[u$height &gt; 85 | u$height &lt; 20] &lt;- NA\nlevels(u$long.hair)[1] &lt;- NA\nlevels(u$is.female)[1] &lt;- NA\nlibrary(gplots)\n\n\nAttaching package: 'gplots'\n\n\nThe following object is masked from 'package:stats':\n\n    lowess\n\npar(mfrow = c(1,2))\nplotmeans(height ~ long.hair, data = u, ylim = c(60,70)) \nplotmeans(height ~ is.female, data = u, ylim = c(60,70))\n\n\n\n\n\n\n\n\n\nHeight ~ long.hairHeight ~ is.femaleheight ~ long.hair + is.female\n\n\n\nmoda &lt;- lm(height ~ long.hair, data = u)\nsummary(moda)\n\n\nCall:\nlm(formula = height ~ long.hair, data = u)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-14.2143  -2.4129  -0.2143   2.5871  11.7857 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   66.2143     0.5501 120.361   &lt;2e-16 ***\nlong.hairYes  -1.8013     0.7085  -2.542   0.0121 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.117 on 139 degrees of freedom\n  (15 observations deleted due to missingness)\nMultiple R-squared:  0.04443,   Adjusted R-squared:  0.03756 \nF-statistic: 6.463 on 1 and 139 DF,  p-value: 0.01211\n\n\n\n\n\nmodb &lt;- lm(height ~ is.female, data = u)\nsummary(modb)\n\n\nCall:\nlm(formula = height ~ is.female, data = u)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-11.833  -1.833  -0.425   2.167   9.575 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   68.4250     0.5799 117.993  &lt; 2e-16 ***\nis.femaleYes  -4.5923     0.6852  -6.702 4.72e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.668 on 139 degrees of freedom\n  (15 observations deleted due to missingness)\nMultiple R-squared:  0.2442,    Adjusted R-squared:  0.2388 \nF-statistic: 44.92 on 1 and 139 DF,  p-value: 4.724e-10\n\n\n\n\n\nmodc &lt;- lm(height ~ long.hair + is.female, data = u)\nsummary(modc)\n\n\nCall:\nlm(formula = height ~ long.hair + is.female, data = u)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.2631  -2.0006  -0.2671   1.9994   9.7288 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   68.2712     0.6010 113.599  &lt; 2e-16 ***\nlong.hairYes   0.7375     0.7699   0.958     0.34    \nis.femaleYes  -5.0080     0.8414  -5.952  2.1e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.68 on 137 degrees of freedom\n  (16 observations deleted due to missingness)\nMultiple R-squared:  0.243, Adjusted R-squared:  0.2319 \nF-statistic: 21.99 on 2 and 137 DF,  p-value: 5.226e-09\n\n\n\n\n\n\nDiscussion : What Do You Observe Changing About the Slopes?\n\n\n\nReporting Effects in a Regression Table (see Sierra review in Discussion Section!!!)\nTable 1. Unstandardized Regression Coefficients; Predicting Height from Long.Hair and Is.Female.\n\n\n\n\nModel 1\nModel 2\nModel 3\n\n\n\n\nIntercept\n\n\n\n\n\nLong.Hair (0 = No; 1 = Yes)\n\n\n\n\n\nIs.Female (0 = No; 1 = Yes)\n\n\n\n\n\n\\(R^2\\)\n\n\n\n\n\n\n\n\nMultiple Regression : Visualized in Multi-Dimensional Space!\nThe code below won‚Äôt run in quarto, and may not work on your comptuer; for teaching purposes!\n\n#install.packages('rgl')\n#install.packages('car')\nlibrary(car)\nlibrary(rgl)\n\nscatter3d(as.numeric(u$is.female), # IV1 - must be numeric (if not already)\n          u$height, # DV\n          as.numeric(u$long.hair)) # IV2 - must be numeric (if not already)\n\n\nWould You Like to Learn More?\n\nTake Aaron Fisher‚Äôs class on Structural Equation Modeling.\nRead through Peng Ding (Prof in Cal Stats Department) Book on Causal Inference"
  },
  {
    "objectID": "gradstats/gradlabs/7_PowerMR.html#lab-7.",
    "href": "gradstats/gradlabs/7_PowerMR.html#lab-7.",
    "title": "Lecture 7 - Power & Multiple Regression",
    "section": "lab 7.",
    "text": "lab 7.\n\nGet more practice working with multiple regression, and calculating power (both post-hoc and for power planning purposes).\nRecreate some of the analyses from the ‚ÄúInterrogating Objectivity‚Äù paper."
  },
  {
    "objectID": "calstats/Lectures/6BL_MascotModels.html",
    "href": "calstats/Lectures/6BL_MascotModels.html",
    "title": "Using Linear Models",
    "section": "",
    "text": "Link to Mascot Dataset\nChatGPT Explanation of Implicit and Explicit Prejudice\n\n\n\n\n\n\n\nImplicit and Explicit Prejudice in the Mascot Dataset\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\nattitude\na scale created based on a 13-item survey about attitudes toward native american mascots at UIUC, measured with items such as ‚ÄúI wish the Chief were still the mascot‚Äù and ‚ÄúChief Illiniwek is a racist symbol (negatively-keyed item)‚Äù. Higher numbers = more positive rating of the mascot.\n\n\nIATscore\na continuous implicit measure of the person‚Äôs unconscious prejudice toward / against Native Americans, measured in terms of how fast people are to associate Native American (vs.¬†White) names with Past (vs.¬†Present) terms. Higher scores = more Present-White and Past-Native American bias.\n\n\nprejudiceNatAm\na continuous explicit measure of the person‚Äôs conscious prejudice against Native Ameicans, measured with items such as, ‚ÄúNative Americans are a vanishing culture and there are few ‚Äúreal‚Äù Indians‚Äù and ‚ÄúIt is now unnecessary for the U.S. government to honor their treaty obligations to Native tribes.‚Äù Higher numbers = more prejudice against Native Americans.\n\n\n\n\nSyllabus Guidelines :\n\nSpecific Guidelines for Today‚Äôs Lecture :\n\ndisagree with the point not the person.\nno name-calling or labels; instead of ‚Äúyou‚Äôre racist‚Äù‚Ä¶focus on the specific opinion / belief.\nroot things in your experiences; realize that your experiences will bias you; seek to understand and listen from others.\nnot your responsibility to change another person‚Äôs mind; not going to happen today.\nkeep it slow; let people finish their thought before starting a new one.\n\n\n\n\n\n\nMini Exam Grades Posted\n\nExtra Credit (for timing error & rubric reconsideration).\nLearning from the R Exam.\nRegrades as needed.\n\nMega Exam : April 25th from 2:10 - 3:35 PM; DSP has Extra Time.\n\nShorter!\nWorth More Points!!\nWill be practice exams!!!\nCan learn from your mistakes again!!!!\n\nMilestone #2 : A draft of your survey.\n\n\n\n\n\n2:10 - 2:35 | Check-In & Announcements\n2:35 - 3:00 | The Mascot Dataset\n3:00 - 3:30 | Work on Lab 6\n3:30 - 3:40 | BREAK\n3:40 - 4:00 | Linear Model Recap\n4:00 - 4:05 | MINI BREAK\n4:05 - 4:30 | Milestone #2 Examples\n4:30 - 5:00 | Final Project Questions"
  },
  {
    "objectID": "calstats/Lectures/6BL_MascotModels.html#check-in-working-with-the-mascot-dataset",
    "href": "calstats/Lectures/6BL_MascotModels.html#check-in-working-with-the-mascot-dataset",
    "title": "Using Linear Models",
    "section": "",
    "text": "Link to Mascot Dataset\nChatGPT Explanation of Implicit and Explicit Prejudice\n\n\n\n\n\n\n\nImplicit and Explicit Prejudice in the Mascot Dataset\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\nattitude\na scale created based on a 13-item survey about attitudes toward native american mascots at UIUC, measured with items such as ‚ÄúI wish the Chief were still the mascot‚Äù and ‚ÄúChief Illiniwek is a racist symbol (negatively-keyed item)‚Äù. Higher numbers = more positive rating of the mascot.\n\n\nIATscore\na continuous implicit measure of the person‚Äôs unconscious prejudice toward / against Native Americans, measured in terms of how fast people are to associate Native American (vs.¬†White) names with Past (vs.¬†Present) terms. Higher scores = more Present-White and Past-Native American bias.\n\n\nprejudiceNatAm\na continuous explicit measure of the person‚Äôs conscious prejudice against Native Ameicans, measured with items such as, ‚ÄúNative Americans are a vanishing culture and there are few ‚Äúreal‚Äù Indians‚Äù and ‚ÄúIt is now unnecessary for the U.S. government to honor their treaty obligations to Native tribes.‚Äù Higher numbers = more prejudice against Native Americans.\n\n\n\n\nSyllabus Guidelines :\n\nSpecific Guidelines for Today‚Äôs Lecture :\n\ndisagree with the point not the person.\nno name-calling or labels; instead of ‚Äúyou‚Äôre racist‚Äù‚Ä¶focus on the specific opinion / belief.\nroot things in your experiences; realize that your experiences will bias you; seek to understand and listen from others.\nnot your responsibility to change another person‚Äôs mind; not going to happen today.\nkeep it slow; let people finish their thought before starting a new one.\n\n\n\n\n\n\nMini Exam Grades Posted\n\nExtra Credit (for timing error & rubric reconsideration).\nLearning from the R Exam.\nRegrades as needed.\n\nMega Exam : April 25th from 2:10 - 3:35 PM; DSP has Extra Time.\n\nShorter!\nWorth More Points!!\nWill be practice exams!!!\nCan learn from your mistakes again!!!!\n\nMilestone #2 : A draft of your survey.\n\n\n\n\n\n2:10 - 2:35 | Check-In & Announcements\n2:35 - 3:00 | The Mascot Dataset\n3:00 - 3:30 | Work on Lab 6\n3:30 - 3:40 | BREAK\n3:40 - 4:00 | Linear Model Recap\n4:00 - 4:05 | MINI BREAK\n4:05 - 4:30 | Milestone #2 Examples\n4:30 - 5:00 | Final Project Questions"
  },
  {
    "objectID": "calstats/Lectures/6BL_MascotModels.html#the-mascot-dataset",
    "href": "calstats/Lectures/6BL_MascotModels.html#the-mascot-dataset",
    "title": "Using Linear Models",
    "section": "The Mascot Dataset",
    "text": "The Mascot Dataset\n\nRecap of Implict and Explicit Prejudice\n\n\n\nCheck-In Answers\nMascot Data Examples\n\n\n\n\ncheck-in go here\n\n\n\ncheck-in go here\n\n\n\n\n\n\nOverview of the IAT (Measure of Implicit Prejudice)\n\n\nNote : the IAT is not a perfect measure, and there‚Äôs a fair amount of debate about whether it is a reliable and valid measure as claimed (see here and here and here for academic examples of some of this debate). FWIW my hot take is that I believe a) implicit measures are really hard to quantify, b) the underlying mechanism that experiences shape our cognition is real, c) we live in a society that prioritizes white and male voices in various ways (history education; modern media; etc.), d) that A-C together would suggest it is very likely people would hold unconscious biases that reflect those in society, and E) it‚Äôs important to identify and name those biases if you want to address them. Happy to chat more; thx for attending my footnote talk. Anyway, here‚Äôs a link to learn more or take a test for yourself.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Study\n\nResearch Question : ‚ÄúWhy do people differ in their beliefs about native american mascots?‚Äù\n\nResearcher (& Berkeley alumnus) Michael Kraus : Informed by his experiences as a new faculty member at the University of Illinois at Urbana-Champaign.\n\n\n\nChief Illiniwik at a Basketball Game\nMembers of one of the over 10 tribes represented by the Illinois Confederation at a Powwow in 2024\n\n\n\n\n\n\n\nDo we need science? What are the arguments you‚Äôve heard for why native american mascots are / are not racist?\n\n‚ÄúIS RACIST‚Äù\n\nit appropriates the culture; inaccurate protrayals of a complex culture; perpetuates a defamation culture that doesn‚Äôt really lead people to seek understanding / gain knowledge.\nimportant to recognize the brutal history of the US genocide against native americans, and historical views of native americans.\ndehumanizing : mascots are ‚Äúless than‚Äù human; a caricature / cartoon / exaggeration / mockery (often used for comedy).\ncultural clothing has important significance within culture that is not recognized / ignored / mischaracterized.\nmost members of the community are a) not involved in the creation or profits of the mascot and b) are actively against it [see Florida Seminoles for an alternative model.]\n\n‚ÄúNOT RACIST‚Äù\n\nit celebrates the culture :\n\nhard to celebrate a culture if they are not consulted / approving.\nthe ‚Äúproud warrior‚Äù stereotype is actually rooted in racist and hateful dehumanization that was used to justify genocide.\n\nother groups have mascots, like the ‚ÄúFighting Irish!‚Äù or ‚ÄúThe Vikings!‚Äù\n\nIrish people were and currently are disciminated against (an interesting history of their ‚ÄúWhiteness‚Äù)\n‚ÄúVikings‚Äù / Nordic people were not subject to the same violence that native groups were subject to / are valued / have sovereign country.\n\n\n\nDo science to test this question [https://osf.io/zbu3x/]¬†\n\nDISCUSS :\n\nWhat has been your experience learning about indigenous culture? What indigenous authors, artists, or other forms of culture have you encountered?\n\n\n\nHow do you think implicit prejudice (IATscore) will be related to people‚Äôs attitudes toward the native american mascot? Why?\n\nPOS / NEG / NO RELATIONSHIP\n\nHow do you think explicit prejudice (prejudiceNatAm) will be related to people‚Äôs attitudes toward the native american mascot? Why?\n\nPOS / NEG / NO RELATIONSHIP\n\nWhich variable (explicit or implicit prejudice) do you think will be a BETTER predictor of people‚Äôs attitudes about the native american mascot? Why?\n\nIMPLICIT BETTER / EXPLICIT BETTER / NO DIFF\n\nimplicit : not everyone will be open to their thought and beliefs, but that will still influence them; harder to hide (than explicit); IAT emphasizes past, which might connect to how people perceive the mascot\nexplicit : maybe people don‚Äôt report, but the people who are explicit are gonna let that racism alllll hang out, and they will drive the relationship; easier to measure since it‚Äôs conscious.\n\n\nOther Questions About\n\nhow do you account for people lying on surveys?????\n\nmodern racism a ‚Äúsafer‚Äù form of racism to measure.\nconsistency checks : overclaiming questionnaire\naccept that error is an inherent part of the research process let‚Äôs goooooooo!!!!!!!\n\n\n\nOther Resources to Learn More About Indigenous Culture :\n\nViolence against indigenous peoples in history and today and in our language\nHistory of Indigenous peoples in California and on Berkeley‚Äôs campus\nRobin Wall Kimmerer‚Äôs Braiding Sweetgrass : Indigenous Wisdom, Scientific Knowledge, and the Teaching of Plants"
  },
  {
    "objectID": "calstats/Lectures/6BL_MascotModels.html#working-on-lab-6.",
    "href": "calstats/Lectures/6BL_MascotModels.html#working-on-lab-6.",
    "title": "Using Linear Models",
    "section": "Working on Lab 6.",
    "text": "Working on Lab 6.\nSee the link above for Lab 6. We will work on the lab together in class! Yeah!\n\nProblem 1.\nThese data measure Midwestern college students‚Äô implicit and explicit prejudiced attitudes toward Native Americans, and their support for a college Native American mascot considered racist by local tribes. The researchers predicted that both implicit prejudice (IV1 = IATscore) and explicit prejudice (IV2 = prejudiceNatAm) would be related to more positive attitudes toward the native american mascot (DV = attitude). Test these researchers theory.\n\nLoad the data, check to make sure the data loaded correctly, and graph each variable that will be used in your models.\n\n\nmascot &lt;- read.csv(\"~/Dropbox/!WHY STATS/Class Datasets/mascot_data.csv\", stringsAsFactors = T)\nhead(mascot)\n\n  Income MotherEducation FatherEducation attitude liberal  Race PayCredit\n1      8               5               6 93.40659     4.5 White    Credit\n2      6               4               7 54.94505     4.0 White    Credit\n3      7               5               4 85.71429     2.0 White    Credit\n4      7               5               6 47.25275     2.5 White    Credit\n5      3               5               5 69.23077     4.0 White    Credit\n6      1               3               9 47.25275     4.0 White    Credit\n  IATscore prejudiceNatAm\n1     0.29       3.571429\n2     0.46       3.238095\n3     0.77       2.714286\n4    -0.10       2.428571\n5     0.73       3.238095\n6     0.79       1.619048\n\nnrow(mascot)\n\n[1] 201\n\nhist(mascot$attitude)\n\n\n\n\n\n\n\nhist(mascot$IATscore, main = \"Higher Numbers = Associate NatAm with Old\\nLower Number = Associate NatAm with Modern\")\nabline(v = mean(mascot$IATscore), lwd = 5)\n\n\n\n\n\n\n\nhist(mascot$prejudiceNatAm)\n\n\n\n\n\n\n\n\n\nDefine a linear model to predict the variable attitudes (DV) from implicit prejudice (IV1 = IATscore). Include a graph of the linear model, and report the slope and R2 value of this model beneath this graph.\n\n\nmod1 &lt;- lm(attitude ~ IATscore, data = mascot)\nplot(attitude ~ IATscore, data = mascot)\nabline(mod1, lwd = 5)\n\n\n\n\n\n\n\n\n\ncoef(mod1)\n\n(Intercept)    IATscore \n   60.55966    10.80089 \n\n# INTERCEPT = THE PREDICTED VALUE OF Y WHEN ALL X VALUES ARE ZERO.\n\nSSM &lt;- sum(mod1$residuals^2)\nSSM\n\n[1] 72193.25\n\nSST &lt;- sum((mascot$attitude - mean(mascot$attitude))^2)\nSST\n\n[1] 76487.21\n\nSST-SSM\n\n[1] 4293.957\n\n(SST - SSM)/SST\n\n[1] 0.05613955\n\n\n\nBREAK TIME : MEET BACK AT 4:00\nEVAN HAS QUESTIONS!!!\n\n\nDefine a linear model to predict the variable attitudes (DV) from explicit prejudice (IV2 = prejudiceNatAm). Include a graph of the linear model, and report the slope and R2 value of this model beneath this graph.\n\n\nmod2 &lt;- lm(attitude ~ prejudiceNatAm, data = mascot)\nplot(attitude ~ prejudiceNatAm, data = mascot)\nabline(mod2, lwd = 5)\n\n\n\n\n\n\n\nmod2\n\n\nCall:\nlm(formula = attitude ~ prejudiceNatAm, data = mascot)\n\nCoefficients:\n   (Intercept)  prejudiceNatAm  \n        35.436           9.946  \n\n\n\nsummary(mod2)$r.squared\n\n[1] 0.1825174\n\n\n\nDescribe what you observe about this relationship. Were the reserachers‚Äô theory supported? Why do you think these patterns exist? Which measure of prejudice (implicit or explicit) was a better predictor of the DV?\nPROFESSOR BONUS QUESTION LET‚ÄôS GO!\n\n\ncoef(mod1)\n\n(Intercept)    IATscore \n   60.55966    10.80089 \n\ncoef(mod2)\n\n   (Intercept) prejudiceNatAm \n     35.435715       9.945856 \n\npar(mfrow = c(1,3))\nplot(mascot$attitude, main = \"SST = Mean as Prediction\")\nabline(h = mean(mascot$attitude), lwd = 5)\nplot(attitude ~ IATscore, data = mascot, main = \"SSM = IV1 as Prediction\")\nabline(mod1, lwd = 5)\nplot(attitude ~ prejudiceNatAm, data = mascot, main = \"SSM = IV2 as Prediction\")\nabline(mod2, lwd = 5)\n\n\n\n\n\n\n\n\nZ-Scored!\n\npar(mfrow = c(1,2))\nplot(scale(attitude) ~ scale(IATscore), data = mascot)\nmod1Z &lt;- lm(scale(attitude) ~ scale(IATscore), data = mascot)\nabline(mod1Z, lwd = 5)\nplot(scale(attitude) ~ scale(prejudiceNatAm), data = mascot)\nmod2Z &lt;- lm(scale(attitude) ~ scale(prejudiceNatAm), data = mascot)\nabline(mod2Z, lwd = 5)\n\n\n\n\n\n\n\n\n\nround(coef(mod1Z), 2)\n\n    (Intercept) scale(IATscore) \n           0.00            0.24 \n\nround(coef(mod2Z), 2)\n\n          (Intercept) scale(prejudiceNatAm) \n                 0.00                  0.43 \n\n\n\n\nProblem 2.\nHow strong is the relationship between implicit and explicit prejudice? Define a linear model to test this prediction. Then, z-score your DV and IV in the linear model, and report (and interpret) the slope and \\(R^2\\) value. How did these statistics change? How did they remain the same?\n\n\nNote : Work on Problem 3 (On Your Own) and Problem 4 (In Discussion Section!)\nMake sure to include Problems 1 and 2 in your lab assignment, along with Problems 3 and 4. Thanks!"
  },
  {
    "objectID": "calstats/Lectures/6BL_MascotModels.html#check-in-break-time-meet-back-at-ttt",
    "href": "calstats/Lectures/6BL_MascotModels.html#check-in-break-time-meet-back-at-ttt",
    "title": "Using Linear Models",
    "section": "Check-In & Break Time (Meet Back at T:TT)",
    "text": "Check-In & Break Time (Meet Back at T:TT)\n\nTake the check-out.\nFeel free to read an excerpt from Tommy Orange‚Äôs THERE THERE.\n\n\n\n\n\nThe RCA ‚ÄúIndian Head Test Pattern‚Äù, as referenced in the text. \n\n‚ÄúIn the dark times\nWill there also be singing?\nYes, there will also be singing.\nAbout the dark times?‚Äù\n- Bertolt Brecht\nThere was an Indian head, the head of an Indian, the drawing of the head of a headdressed, long haired, Indian depicted, drawn by an unknown artist in 1939, broadcast until the late 1970s to American TVs everywhere after all the shows ran out. It‚Äôs called the Indian Head Test Pattern. If you left the TV on, you‚Äôd hear a tone at 440 hertz‚Äîthe tone used to tune instruments‚Äîand you‚Äôd see that Indian, surrounded by circles that looked like sights through rifle scopes. There was what looked like a bullseye in the middle of the screen, with numbers like coordinates. The Indian head was just above the bullseye, like all you‚Äôd need to do was nod up in agreement to set the sights on the target. This was just a test.¬†\nIn 1621, colonists invited Massasoit, chief of the Wampanoags, to a feast after a recent land deal. Massasoit came with ninety of his men. That meal is why we still eat a meal together in November. Celebrate it as a nation. But that one wasn‚Äôt a thanksgiving meal. It was a land deal meal. Two years later there was another, similar meal, meant to symbolize eternal friendship. Two hundred Indians dropped dead that night from supposed unknown poison.\nBy the time Massasoit‚Äôs son Metacomet became chief, there were no Indian-Pilgrim meals being eaten together. Metacomet, also known as King Phillip, was forced to sign a peace treaty to give up all Indian guns. Three of his men were hanged. His brother Wamsutta was, let‚Äôs say, very likely poisoned after being summoned and seized by the Plymouth court. All of which lead to the first official Indian war. The first war with Indians. King Phillip‚Äôs War. Three years later the war was over and Metacomet was on the run. He was caught by Benjamin Church, Captain of the very first American Ranger force and an Indian by the name of John Alderman. Metacomet was beheaded and dismembered. Quartered. They tied his four body sections to nearby trees for the birds to pluck. John Alderman was given Metacomet‚Äôs hand, which he kept in a jar of rum and for years took it around with him‚Äîcharged people to see it. Metacomet‚Äôs head was sold to the Plymouth Colony for thirty shillings‚Äîthe going rate for an Indian head at the time. The head was spiked and carried through the streets of Plymouth before it was put on display at Plymouth Colony Fort for the next twenty five years.¬†\nIn 1637, anywhere from four to seven hundred Pequot were gathered for their annual green corn dance. Colonists surrounded the Pequot village, set it on fire, and shot any Pequot who tried to escape. The next day the Massachusetts Bay Colony had a feast in celebration, and the governor declared it a day of thanksgiving. Thanksgivings like these happened everywhere, whenever there were, what we have to call: successful massacres. At one such celebration in Manhattan, people were said to have celebrated by kicking the heads of Pequot people through the streets like soccer balls.\nThe first novel ever written by a Native person, and the first novel written in California, was written in 1854, by a Cherokee guy named John Rollin Ridge. His novel, The Life and Adventures of Joaquin Murieta, was based on a supposed real-life Mexican bandit from California by the same name, who, in 1853, was killed by a group of Texas rangers. To prove they‚Äôd killed Murrieta and collect the five thousand dollar reward put on his head‚Äîthey cut it off. Kept it in a jar of whiskey. They also took the hand of his fellow bandit Three Fingered Jack. The rangers took Joaquin‚Äôs head and the hand on a tour throughout California, charged a dollar for the show.¬†\nThe Indian head in the jar, the Indian head on a pike were like flags flown, to be seen, cast broadly. Just like the Indian head test pattern was broadcast to sleeping Americans as we set sail from our living rooms, over the ocean blue green glowing airwaves, to the shores, the screens of the new world."
  },
  {
    "objectID": "calstats/Lectures/6BL_MascotModels.html#r2",
    "href": "calstats/Lectures/6BL_MascotModels.html#r2",
    "title": "Using Linear Models",
    "section": "\\(R^2\\)",
    "text": "\\(R^2\\)\n\nThe Formula\n\\(\\Huge R^2 = \\frac{SS_{total} - SS_{model}}{SS_{total}}\\)\n\nSSTSSM\n\n\nSST refers to the sum of squared errors when using the mean to make predictions. This is called total because the mean is the starting place for our predictions; our predictions will never be worse than the mean, so we expect the error around the mean to be the largest it will ever be.\n\n\nSSM refers to the sum of squared residuals when using the linear model to make predictions.\n\n\n\n\n\nIn Real-Life\n\n\n\nDISCUSS : what do these linear models tell us about the relationship between GPA, SAT (IVs) and freshman grades (DV)?"
  },
  {
    "objectID": "calstats/Lectures/6BL_MascotModels.html#check-in-mini-break",
    "href": "calstats/Lectures/6BL_MascotModels.html#check-in-mini-break",
    "title": "Using Linear Models",
    "section": "CHECK-IN & MINI BREAK",
    "text": "CHECK-IN & MINI BREAK\n\nGENERATING SOME MORE CLASS DATA FOR NEXT WEEK!\nNO TALKING / LOOKING THINGS UP!"
  },
  {
    "objectID": "gradstats/gradlabs/7_PowerMR.html#check-in-interpreting-linear-models.",
    "href": "gradstats/gradlabs/7_PowerMR.html#check-in-interpreting-linear-models.",
    "title": "Lecture 7 - Power & Multiple Regression",
    "section": "",
    "text": "Load this dataset into R. Collected from undergrads at UC Berkeley. Yay students.\n\nThe researchers wanted to know whether people who ate breakfast (IV = breakfast; a categorical factor with two levels 0 = No; 1 = Yes) would happier (DV = happy) than people who did not have breakfast.\nUse the R output from the linear model below to answer the following questions :\n\n\nhist(u$happy)\n\n\n\n\n\n\n\nplot(u$breakfast)\n\n\n\n\n\n\n\nlevels(u$breakfast)[1]# identifying the empty level.\n\n[1] \"\"\n\nlevels(u$breakfast)[1] &lt;- NA\nplot(u$breakfast)\n\n\n\n\n\n\n\nmod1 &lt;- lm(happy ~ breakfast, data = u)\nsummary(mod1)\n\n\nCall:\nlm(formula = happy ~ breakfast, data = u)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.5684 -0.5684 -0.0370  0.9630  2.9630 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    7.0370     0.2220  31.698   &lt;2e-16 ***\nbreakfastYes   0.5314     0.2780   1.911   0.0579 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.631 on 147 degrees of freedom\n  (7 observations deleted due to missingness)\nMultiple R-squared:  0.02425,   Adjusted R-squared:  0.01761 \nF-statistic: 3.653 on 1 and 147 DF,  p-value: 0.05792\n\n\n\nWhat is the difference in happiness for someone who did vs.¬†did not eat breakfast? [.53]\nWhat is the predicted happiness for someone who did not eat breakfast? (Round to 1 decimal place.)\n```         \nhappiness = 7 + .5 * 0 = 7\n```\nWhat is the predicted happiness for someone who did eat breakfast? (Round to 1 decimal place.)\n```         \nhappiness = 7 + .5 * 1 = 7.5\n```\nWhat are some reasons we might we observe this difference?\n\nCAUSAL : having breakfast makes people feel good\nREVERSE CAUSASTION : happiness causes you to eat breakfast (not being happy causes you to not eat breakfast.)\nCONFOUNDS & COLLIDERS :\n\nCONFOUNDS : a variable that is related to both the DV and the IV (free time is related to both happiness and ability to have breakfast; stress; SES statsus; age)\nINTERACTION EFFECT [NEXT WEEK] : a variable that is an outcome of both the DV and IV.\n\nexample : inherent talent ~ hard work; but in a sample of high-performing people, these are related. but in a sample of low-performing people, these two groups are not related.\n\n\nCHANCE : the relationship is due to sampling error.\n\nIs this difference considered statistically significant? Why / why not?\nNow, use R to define a linear model to test whether people with more instagram followers (IV = insta.followers) would be happier (DV = happy) than people with fewer instagram followers. What is the slope of this model? Round to two decimal places.\nDescribe this relationship between these two variables in terms of the slope, R^2 value, and statistical significance.\n\n\nhist(u$insta.followers)\n\n\n\n\n\n\n\nmod2 &lt;- lm(happy ~ insta.followers, data = u)\nsummary(mod2)\n\n\nCall:\nlm(formula = happy ~ insta.followers, data = u)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.3931 -0.3956  0.3023  0.6126  2.6099 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      7.396e+00  1.382e-01  53.509  &lt; 2e-16 ***\ninsta.followers -3.698e-06  8.350e-07  -4.429 1.86e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.664 on 144 degrees of freedom\n  (10 observations deleted due to missingness)\nMultiple R-squared:  0.1199,    Adjusted R-squared:  0.1138 \nF-statistic: 19.61 on 1 and 144 DF,  p-value: 1.863e-05\n\nplot(happy ~ insta.followers, data = u)\nabline(mod2, lwd = 5)\n\n\n\n\n\n\n\ndev.new()\npar(mfrow = c(2,2))\nplot(mod2)\n\nWarning in sqrt(crit * p * (1 - hh)/hh): NaNs produced\n\nWarning in sqrt(crit * p * (1 - hh)/hh): NaNs produced\n\nu$insta.followers[u$insta.followers &gt; 10000] &lt;- NA\nmod2 &lt;- lm(happy ~ insta.followers, data = u)\nsummary(mod2)\n\n\nCall:\nlm(formula = happy ~ insta.followers, data = u)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.4042 -0.6869  0.4325  0.6578  2.6578 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     7.342e+00  2.121e-01  34.619   &lt;2e-16 ***\ninsta.followers 8.939e-05  2.662e-04   0.336    0.738    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.674 on 142 degrees of freedom\n  (12 observations deleted due to missingness)\nMultiple R-squared:  0.0007933, Adjusted R-squared:  -0.006243 \nF-statistic: 0.1127 on 1 and 142 DF,  p-value: 0.7375\n\nplot(happy ~ insta.followers, data = u)\nabline(mod2, lwd = 5)\ndev.new()\npar(mfrow = c(2,2))\nplot(mod2)"
  },
  {
    "objectID": "calstats/labs/Lab6.html",
    "href": "calstats/labs/Lab6.html",
    "title": "Lab6",
    "section": "",
    "text": "Problem 1.\nThese data measure Midwestern college students‚Äô implicit and explicit prejudiced attitudes toward Native Americans, and their support for a college Native American mascot considered racist by local tribes. The researchers predicted that both implicit prejudice (IV1 = IATscore) and explicit prejudice (IV2 = prejudiceNatAm) would be related to more positive attitudes toward the native american mascot (DV = attitude). Test these researchers theory.\n\nLoad the data, check to make sure the data loaded correctly, and graph each variable that will be used in your models.\nDefine a linear model to predict the variable attitudes (DV) from implicit prejudice (IV1 = IATscore). Include a graph of the linear model, and report the slope and R2 value of this model beneath this graph.\nDefine a linear model to predict the variable attitudes (DV) from explicit prejudice (IV2 = prejudiceNatAm). Include a graph of the linear model, and report the slope and R2 value of this model beneath this graph.\nDescribe what you observe about this relationship. Were the reserachers‚Äô theory supported? Why do you think these patterns exist? Which measure of prejudice (implicit or explicit) was a better predictor of the DV?\n\n\n\nProblem 2.\nHow strong is the relationship between implicit and explicit prejudice? Define a linear model to test this prediction. Then, z-score your DV and IV in the linear model, and report (and interpret) the slope and \\(R^2\\) value. How did these statistics change (from the non-z-scored model to the z-scored model)? How did they remain the same?\n\n\nProblem 3 (On Your Own).\nIdentify another numeric variable from the mascot dataset that you think will predict attitudes toward the mascot. Define the linear model, report the statistics and graph needed to understand the relationship, and explain what you observe about the relationship between the two variables below.\n\n\nProblem 4 (In Discussion Section).\nWork with your classmates to identify a numeric DV that you are interested in predicting. Then, work with a buddy to choose a numeric IV that y‚Äôall think will BEST predict this DV. Write out your prediction (how do you think these two variables will be related?) Define the linear model needed to test this prediction, report the statistics and graph needed to understand the relationship, and explain what you observe below your graph. Why do you think this relationship exists? Then, z-score your DV and IV in the linear model, and report (and interpret) the slope and \\(R^2\\) value. How did these statistics change? How did they remain the same? Compare the results of your model to another team - whose model was a ‚Äúbetter‚Äù model? Why / how do you know?"
  },
  {
    "objectID": "gradstats/gradlabs/7_PowerMR.html#presentations",
    "href": "gradstats/gradlabs/7_PowerMR.html#presentations",
    "title": "Lecture 7 - Power & Multiple Regression",
    "section": "Presentations",
    "text": "Presentations"
  },
  {
    "objectID": "gradstats/gradlabs/7Lab_MRPower.html",
    "href": "gradstats/gradlabs/7Lab_MRPower.html",
    "title": "Lab 7 - Multiple Regression & Power",
    "section": "",
    "text": "Choose a DV from a dataset (either the mini grad dataset, or the undergrad dataset we worked with this week would be easy datasets to work with.) Identify 2 independent variables that you think will help you predict this DV. Before you analyze the data, write out specific predictions about the relationship(s) you expect for the relationship between IV1 and the DV, IV2 and the DV, and how these relationships might change in a multiple regression.\nDefine a series of linear models to predict the DV from each IV (separately, and then together in a linear model with two IVs.) Include a regression table to summarize these data, and report the intercept, slope(s), standard error, t-test, and p-value for each coefficient, and \\(R^2\\) for the model. You can either create a table manually in quarto, or - if you want to spend more time on this Lab - there are ways to have R generate ‚Äúpublication ready‚Äù regression tables for you that might save you time in the long run.\n\n\nBelow the table, report what you observe about the relationships between each variable as you would in a paper, and any other comments or observations about the fit of the model.\nFinally, choose one statistic you most care about, and report i) the power that you had to detect this effect in the model, and ii) the sample size that you would need to have 80% power. (Note : we ran out of time to talk about how to do ii; prof. will try to record a video / add this to the notes; but you can play around with the pwr functions to calculate n given an effect size, power, and alpha.)"
  },
  {
    "objectID": "gradstats/gradlabs/7Lab_MRPower.html#problem-1.-more-regression-more-predictors-more-power.",
    "href": "gradstats/gradlabs/7Lab_MRPower.html#problem-1.-more-regression-more-predictors-more-power.",
    "title": "Lab 7 - Multiple Regression & Power",
    "section": "",
    "text": "Choose a DV from a dataset (either the mini grad dataset, or the undergrad dataset we worked with this week would be easy datasets to work with.) Identify 2 independent variables that you think will help you predict this DV. Before you analyze the data, write out specific predictions about the relationship(s) you expect for the relationship between IV1 and the DV, IV2 and the DV, and how these relationships might change in a multiple regression.\nDefine a series of linear models to predict the DV from each IV (separately, and then together in a linear model with two IVs.) Include a regression table to summarize these data, and report the intercept, slope(s), standard error, t-test, and p-value for each coefficient, and \\(R^2\\) for the model. You can either create a table manually in quarto, or - if you want to spend more time on this Lab - there are ways to have R generate ‚Äúpublication ready‚Äù regression tables for you that might save you time in the long run.\n\n\nBelow the table, report what you observe about the relationships between each variable as you would in a paper, and any other comments or observations about the fit of the model.\nFinally, choose one statistic you most care about, and report i) the power that you had to detect this effect in the model, and ii) the sample size that you would need to have 80% power. (Note : we ran out of time to talk about how to do ii; prof. will try to record a video / add this to the notes; but you can play around with the pwr functions to calculate n given an effect size, power, and alpha.)"
  },
  {
    "objectID": "gradstats/gradlabs/7Lab_MRPower.html#problem-2.-back-to-the-mini-exam.",
    "href": "gradstats/gradlabs/7Lab_MRPower.html#problem-2.-back-to-the-mini-exam.",
    "title": "Lab 7 - Multiple Regression & Power",
    "section": "Problem 2. Back to the Mini Exam.",
    "text": "Problem 2. Back to the Mini Exam.\nThe remaining questions from this lab ask you to recreate and interpret some of the analyses from the ‚ÄúInterrogating Objectivity‚Äù article that we read (and whose data we worked with for the ‚ÄúMini Exam‚Äù.)\nThe authors report the results of a between subject analyses:\n\n\n\n\n\nThis is really just the results of a linear regression where the categorical variable pocu has been dummy coded. Convert pocu to a categorical factor, and use this dummy-coded variable in three separate linear models that predict the number of power words, the number of positive emotion words, and the nubmer of negative emotion words used in an abstract. You should be able to recreate the analyses from the text above."
  },
  {
    "objectID": "gradstats/gradlabs/7Lab_MRPower.html#problem-3.-alternative-coding-schemes.",
    "href": "gradstats/gradlabs/7Lab_MRPower.html#problem-3.-alternative-coding-schemes.",
    "title": "Lab 7 - Multiple Regression & Power",
    "section": "Problem 3. Alternative Coding Schemes.",
    "text": "Problem 3. Alternative Coding Schemes.\nThe linear regression for Problem 2 defaults to dummy coded variables which you can use to report the predicted values of the DV for authors of color and white authors. In the paper, the authors report that they have coded scholars of color (‚Äú1‚Äù), and White scholars (‚Äú-1‚Äù). This is a form of ‚Äúeffect coding‚Äù called ‚Äúsum coding‚Äù or ‚Äúdeviation coding‚Äù; you can read more these coding schemes here or here is another example - note this is a clear example of the default tendency to use white people as the reference group.\nSee how the linear model (where you predict power words from pocu) changes from the dummy coded pocu variable to one that has been effect coded. You can do this by creating a copy of the pocu categorical factor and re-level the variable to have values of -1 and 1, using one of the methods described in the links above, or specifying the contrasts as an argument in the lm function.\nIn your own words, interpret the intercept and slope from this effect coded model. Which interpretation (dummy coded from P2 or effect coded in this problem) makes more sense? Do you feel like ‚Äúeffect coding‚Äù de-emphasizes the reference category, as described in last week‚Äôs reading?"
  },
  {
    "objectID": "gradstats/gradlabs/7Lab_MRPower.html#problem-4.-multiple-regression-problems",
    "href": "gradstats/gradlabs/7Lab_MRPower.html#problem-4.-multiple-regression-problems",
    "title": "Lab 7 - Multiple Regression & Power",
    "section": "Problem 4. Multiple Regression Problems",
    "text": "Problem 4. Multiple Regression Problems\n\nIn the next paragraph, the authors describe linear regression analyses where they predict the same three DVs (power, positive emotion words, and negative emotion words) from author race, controlling for several other variables : ‚Äúword count, journal impact factor, author h-index (a correlate of author status), and publication year‚Äù. You can access the full supplementary regression tables here.\nDefine three separate models to recreate the authors‚Äô analyses in R. The authors report standardized (z-scored) slopes; you can do this by using the scale function to transform each numeric variable in the linear model. The authors use the ‚Äúeffect coded‚Äù categorical pocu variable that you created in Problem 3. Note : I was not able to perfectly replicate the authors regression coefficients (though did replicate the t-tests and p-values they reported). I‚Äôm not sure if this is a software issue, publication error, or something else. But a good example of why it‚Äôs important to share not only data but code as well. Let me know if you get a perfect replication of the tables though!"
  },
  {
    "objectID": "calstats/Lectures/6BL_MascotModels.html#thinking-more-about-r2",
    "href": "calstats/Lectures/6BL_MascotModels.html#thinking-more-about-r2",
    "title": "Using Linear Models",
    "section": "Thinking More About \\(R^2\\)",
    "text": "Thinking More About \\(R^2\\)\n\nThe Formula\n\\(\\Huge R^2 = \\frac{SS_{total} - SS_{model}}{SS_{total}}\\)\n\nSSTSSM\n\n\nSST refers to the sum of squared errors when using the mean to make predictions. This is called total because the mean is the starting place for our predictions; our predictions will never be worse than the mean, so we expect the error around the mean to be the largest it will ever be.\n\n## visualizing SST here\n\n\n\nSSM refers to the sum of squared residuals when using the linear model to make predictions.\n\n## visualizing SSM here\n\n\n\n\n\n\nIn Real-Life\n\n\nDISCUSS : what do these linear models tell us about the relationship between GPA, SAT (IVs) and freshman grades (DV)?"
  },
  {
    "objectID": "calstats/Lectures/6BL_MascotModels.html#milestone-2",
    "href": "calstats/Lectures/6BL_MascotModels.html#milestone-2",
    "title": "Using Linear Models",
    "section": "Milestone #2",
    "text": "Milestone #2\n\nProf.¬†looks over some examples of student surveys!\nOther Notes From Student Questions Will Go Here."
  },
  {
    "objectID": "calstats/Lectures/1L_WhyStats.html",
    "href": "calstats/Lectures/1L_WhyStats.html",
    "title": "Lecture 1 | Welcome to Psych 101",
    "section": "",
    "text": "access these lecture notes on bCourses!\nclick on this link to check-in (or visit : tinyurl.com/first101class)\n\n\n\n\n\n\n\n\n\nSection Swap : post on bCourses to find someone to swap with.\nWaitlisted Students : Thanks for your patience! Nothing I can do :(\nJoin the Class Discord : link on bCourses\nNext Week :\n\nAttend Discussion Section\nComplete Lab 1 (will start in lecture!)\nRead Chapter 2\nComplete Quiz 2\n\n\n\n\n\n\n2:10 - 2:20 | Check-In & Announcements\n2:20 - 3:10 | RECAP : Science as Prediction\n3:10 - 3:20 | Break #1\n3:20 - 3:50 | Positivism and Linear Models\n3:50 - 3:55 | Break #2\n3:55 - 4:30 | In R : Defining Variables\n4:30 - 5:00 | So you‚Äôre interested in being a researcher / going to graduate school?"
  },
  {
    "objectID": "calstats/Lectures/1L_WhyStats.html#welcome-and-check-in",
    "href": "calstats/Lectures/1L_WhyStats.html#welcome-and-check-in",
    "title": "Lecture 1 | Welcome to Psych 101",
    "section": "",
    "text": "access these lecture notes on bCourses!\nclick on this link to check-in (or visit : tinyurl.com/first101class)"
  },
  {
    "objectID": "calstats/Lectures/1L_WhyStats.html#course-announcements",
    "href": "calstats/Lectures/1L_WhyStats.html#course-announcements",
    "title": "Lecture 1 | Welcome to Psych 101",
    "section": "",
    "text": "Section Swap : post on bCourses to find someone to swap with.\nWaitlisted Students : Thanks for your patience! Nothing I can do :(\nJoin the Class Discord : link on bCourses\nNext Week :\n\nAttend Discussion Section\nComplete Lab 1 (will start in lecture!)\nRead Chapter 2\nComplete Quiz 2"
  },
  {
    "objectID": "calstats/Lectures/1L_WhyStats.html#agenda",
    "href": "calstats/Lectures/1L_WhyStats.html#agenda",
    "title": "Lecture 1 | Welcome to Psych 101",
    "section": "",
    "text": "2:10 - 2:20 | Check-In & Announcements\n2:20 - 3:10 | RECAP : Science as Prediction\n3:10 - 3:20 | Break #1\n3:20 - 3:50 | Positivism and Linear Models\n3:50 - 3:55 | Break #2\n3:55 - 4:30 | In R : Defining Variables\n4:30 - 5:00 | So you‚Äôre interested in being a researcher / going to graduate school?"
  },
  {
    "objectID": "calstats/Lectures/1L_WhyStats.html#activity-variables-and-variation-in-the-room",
    "href": "calstats/Lectures/1L_WhyStats.html#activity-variables-and-variation-in-the-room",
    "title": "Lecture 1 | Welcome to Psych 101",
    "section": "Activity : Variables and Variation in the Room",
    "text": "Activity : Variables and Variation in the Room\nClass Activity. Let‚Äôs create a list of variables that we observe in this classroom.\n\nhair style\nhair color\nethnicity\neye color\nbody size\ndevices\ngender\nidentity\n\naffect : feeling of affinity for fitting in w/ your culture.\nbehavior : how you express it\ncognition : self-perception (do you think about this identity / perceive it in others?)\n\nglasses\nhats vs.¬†no hats\n\nKey Terms. From the readings.\n\nAffect, Behavior, Cognition\nBetween vs.¬†Within-Person Variation\n\n[7 Minutes] Answer the following questions with your buddy.\nFind a buddy in the class! (There‚Äôs a discord thread if you prefer to communicate with someone online.)\n\nIf you could have dinner with anyone in the world (living or dead) who would it be?\nWhy are you a psychology major? What interests you about people (or non-human animals)?\nHow would you label this interest as a variable?\n\nAre you interested in the between-person or within-person version of this variable?\nAre you interested in the Affective, Behavioral, or Cognitive aspect of this variable?\n\n\nStudent Examples\n\nHamza : motivation for goal changes over time [within person - starts as affect ‚Äì&gt; behavior]\nMax : emotional effect of languages (how much the same word in a western language influences someone compared to that word in an eastern language) [between person - comparing people who speak one language to people who speak another.]\nCamille : Moral regret (you did something bad and feel bad about it) and how that changes over time."
  },
  {
    "objectID": "calstats/Lectures/1L_WhyStats.html#thinking-about-programming-free-association-activity",
    "href": "calstats/Lectures/1L_WhyStats.html#thinking-about-programming-free-association-activity",
    "title": "Lecture 1 | Welcome to Psych 101",
    "section": "Thinking about Programming (Free Association Activity)",
    "text": "Thinking about Programming (Free Association Activity)\n\nClose your eyes\nTake a deep breath (inhale / exhale)\nVisualize an image based on the word that you hear me say.\nWhat do you observe?"
  },
  {
    "objectID": "calstats/Lectures/1L_WhyStats.html#r-is-your-friend",
    "href": "calstats/Lectures/1L_WhyStats.html#r-is-your-friend",
    "title": "Lecture 1 | Welcome to Psych 101",
    "section": "R is Your Friend",
    "text": "R is Your Friend\n\nThe Console\nThe console is where R does its work.\n\nACTIVITY : Look at the image below. What do you see? What makes sense / what seems confusing?\n\n\n\n\n\n\nSome Additional Notes on R Studio and the Source File (.R)\nIn this class, we‚Äôll be using RStudio. RStudio is an IDE (Integrated Development Environment) that includes the console along with other useful windows and tools.\n\nThe Console is at the bottom left of the IDE. Hi console!\nThe R script is at the top left of the IDE, and is a document that you use to write (and organize) code. You will want to do most of your work in the R script, and feel an appropriate level of anxiety when you notice that your Rscript is unsaved (as indicated by the red text and *).\nThe Environment is at the top right of the IDE, and shows you all of the ‚Äúobjects‚Äù that you have defined in R.\nThe File Window is at the bottom right of the IDE, and shows you the files. Note that there are tabs here for Plots (where graphs will pop up), Packages (things you can download to give R extra features), a Help viewer (sometimes very useful!).\n\n\n\n\nACTIVTY : open up RStudio\n\nType some math into an Rscript, and send it to the console. Yeah, you are programming!\nDefine two variables in R - one numeric and one string variable (these can be unrelated to your project topic!) Make sure to collect at least ten data points for each variable, and show that you successfully defined the variable in R by ‚Äúprinting‚Äù it in R. Yeah, you‚Äôre programming!\nCopy/paste (or screenshot) your code and output from the question above to a document to answer Lab 1, Question 1."
  },
  {
    "objectID": "calstats/Lectures/1L_WhyStats.html#in-r-variables-and-variation",
    "href": "calstats/Lectures/1L_WhyStats.html#in-r-variables-and-variation",
    "title": "Lecture 1 | Welcome to Psych 101",
    "section": "In R : Variables and Variation",
    "text": "In R : Variables and Variation\n\nNumeric Variables in R\n\n\n\n\n\n\n\nCode\nDescription\n\n\n\n\nvariable &lt;- c(#, #, #, #, etc.)\n\n\n\ntired &lt;- c(1,2,3,4)\nvariable = an object that you will define in R\n&lt;- = ‚Äúassign‚Äù; tells R to save whatever comes on the right to whatever object is on the left.\nc = combine : tells R to combine whatever happens in the parentheses\n() = parentheses to group related terms\n# = what you store in the variable; each item should be separated by a comma and space.\n\n\nhist(dat$variable)\nFor continuous variables : draws a histogram.\n\n\n\n\n\nExample : Creating Numeric Variables\n\ncounting &lt;- c(1,2,3,4,5) # the numbers one through five\nprint(counting) # one way to \"print\" the variable\n\n[1] 1 2 3 4 5\n\ncounting # another way to \"print\" the variable\n\n[1] 1 2 3 4 5\n\nhist(counting) # a way to graph the variable (a histogram)\n\n\n\n\n\n\n\n\n\n\nString Variables\n\n\n\n\n\n\n\nvariable &lt;- c(‚Äúname1‚Äù, ‚Äúname2‚Äù, ‚Äúname1‚Äù, etc.)\n\nemotion &lt;- c(‚Äúsad‚Äù, ‚Äúhappy‚Äù, ‚Äúsad‚Äù)\nvariable = an object that you will define in R\n&lt;- = ‚Äúassign‚Äù; tells R to save whatever comes on the right to whatever object is on the left.\nc = combine : tells R to combine whatever happens in the parentheses\n() = parentheses to group related terms\n# = what you store in the variable; each item should be separated by a comma and space.\n\n\nas.factor(variable)\n\nas.factor(emotion)\nas.factor() # converts a string variable into a categorical factor\n\n\nvariable &lt;- as.factor(variable)\n# ‚Äúsaves‚Äù this conversion as the original variable\n\n\nplot(dat$variable)\nFor categorical variables : draws a barplot. For continuous variables :¬† illustrates values of the variable (y-axis) as a function of their index (x-axis).\n\n\n\n\n\nExample : Creating Non-Numeric Variables\nThe data below describe the categories of family laundry that was hanging in my apartment to dry.\n\nlaundryhang &lt;- c(\"shirt\", \"shirt\", \"leggings\", \"leggings\", \"shirt\", \n             \"shirt\", \"leggings\", \"pants\", \"sweater\", \"sweater\") # defining a string variable\nprint(laundryhang)\n\n [1] \"shirt\"    \"shirt\"    \"leggings\" \"leggings\" \"shirt\"    \"shirt\"   \n [7] \"leggings\" \"pants\"    \"sweater\"  \"sweater\" \n\nlaundryhang # another way to \"print\" the variable\n\n [1] \"shirt\"    \"shirt\"    \"leggings\" \"leggings\" \"shirt\"    \"shirt\"   \n [7] \"leggings\" \"pants\"    \"sweater\"  \"sweater\" \n\nlaundryhang &lt;- as.factor(laundryhang) # changing the format of the sting variable into a categorical factor\nplot(laundryhang) # a way to graph the non-numeric variable"
  },
  {
    "objectID": "calstats/Lectures/1L_WhyStats.html#break-time-meet-back-at-325",
    "href": "calstats/Lectures/1L_WhyStats.html#break-time-meet-back-at-325",
    "title": "Lecture 1 | Welcome to Psych 101",
    "section": "Break Time: Meet Back at 3:25",
    "text": "Break Time: Meet Back at 3:25"
  },
  {
    "objectID": "calstats/Lectures/1L_WhyStats.html#prediction-power",
    "href": "calstats/Lectures/1L_WhyStats.html#prediction-power",
    "title": "Lecture 1 | Welcome to Psych 101",
    "section": "Prediction & Power",
    "text": "Prediction & Power\n\nPredictions in Real Life\n\nWill it rain tonight?\n\nKnowledge (what information did you use to make the prediction)?\nPower (ways your predictions influence future behaviors)\n\nProfessor predicted that attendance would be HIGH today, but will dip later in the semester :(\n\nKnowledge (what information did you use to make the prediction)?\n\nfirst day of the semester; students are STOKED and MINIMALLY STRESSED.\npattern seen in almost every past semester.\nFriday afternoons are basiscally the weekend.\n\nPower (ways your predictions influence future behaviors)\n\nprepared; made sure to arrive on time; tucked in shirt.\nremind students to attend with weekly announcements.\ncreate a positive classroom environment where students feel supported and like attendance is helpful.\ncreated an example that serves as meta-commentary on the importance of attendance.\n\nWas Professor Valid? TBD! &lt;3\n\nWork on Lab 1, Question 1. [In Lecture] What‚Äôs a prediction about people that you made today? What information did you use to make this prediction? How did (or could) you use this prediction to influence outcomes? Were you valid in your predictions?\n\nJESSICA :\n\nprediction : this professor is awesome!\nknowledge : other people told me the class was managable and friendly.\npower : felt excited to enter the class\nvalid : maybe? let‚Äôs see.\n\nTRISHA :\n\nprediction : thought people would come out of apartments when fire alarm went off in building; there was an emergency.\nknowledge : we are taught to do this\npower : left my apartment (without thinking too much about grabbing my keys)\nvalid : no, I was the only one outside.\nmodel : behavior in response to fire alarm ~ training + motivation not to die + social norms + error\n\n\n\n\n\nScientific Predictions\nPsychological scientists seek to better understand variation, in order to help make valid predictions in ways that help exert power over our environments.\n\n\n\nTopic\nOther Questions We Might Ask?\n\n\n\n\nVideo | |\n\n\n\n1 | Are there differences between how long people actually use social media and how long people want to use social media? | | | What dopamine reactions do people get after using social media? | | How do reactions to visual cues in the environemnt differ when people are on their phones? | | What factors keep people off social media long-term? | | Do the engineers who create infinite scrolling feel guilty.\n\n\n\n2 | |"
  },
  {
    "objectID": "calstats/Lectures/1L_WhyStats.html#the-linear-model",
    "href": "calstats/Lectures/1L_WhyStats.html#the-linear-model",
    "title": "Lecture 1 | Welcome to Psych 101",
    "section": "The Linear Model",
    "text": "The Linear Model\n\nDefinition\nStatistical Model : DV ~ IV1 + IV2 + ‚Ä¶ + IVk + error\n\nDV = dependent variable = the variable you want to predict\nIV = independent variable(s) = the variable(s) you think will predict the DV\n\nk = any number = there can be MANY IVs\nany variable can be an IV or DV - it‚Äôs up to the researcher to choose\n\n~ = a squiggly line / tilde = our model is uncertain (not equal)\nerror = other factors that are not part of your model that would also explain the DV\nWe say: ‚Äúthe DV is a function of‚Ä¶‚Äù; ‚Äúthe DV depends on the IV(s)‚Äù\n\n\n\nExamples\n\nrain ~ number of umbrellas other people have + personal experience + weather app + clouds + sun + smell + pressure + error\nclass quality ~ communication + study habits + lecture attendance + quality of lecturing + mood / vibes + professor and student adaptability to the class + workload + participation + time management + hunger + sleep + life shit + 3 hours lectures on a friday afternoon + respect + right amount of material that is relevant and interetsting + study buddy + LEARNING somethign useful and relevant + ERROR\n\n\n\nAnother Example\nKEY IDEA : Linear Models Help Make and Quantify Prediction\n\nwhat information (IV) is related to the DV (predict)\nwhich IV allows us to make the best predictions (effect size)\nthe amount of error in your prediction (error can come from our measures, our models, and maybe is just inherent to science?)"
  },
  {
    "objectID": "calstats/Lectures/1L_WhyStats.html#work-on-lab-1-predictions-research-questions.",
    "href": "calstats/Lectures/1L_WhyStats.html#work-on-lab-1-predictions-research-questions.",
    "title": "Lecture 1 | Welcome to Psych 101",
    "section": "Work on Lab 1 : Predictions & Research Questions.",
    "text": "Work on Lab 1 : Predictions & Research Questions.\n\nOn Your Own\nDuring our break, think a little bit about what research questions you might want to address for the project in this class. Below are some ideas to help you get started thinking of a research question if you are feeling stuck!\n\nIs there a real-world issue that you care about? What variables make up this questions?\nWhat is something about people (your friends, parents, classmates) that you think is interesting or confusing? What variables are the focus of this interest or question?\nWhat‚Äôs a future career you might want to pursue with your psychology degree? What‚Äôs a variable that is related to this career? What questions might you ask abut this variable?\n\nQuestion 3 (In Lecture / On Your Own). Get started on the final project by thinking through a research question you might be interested in studying as a psychology researcher. (Totally fine to change this, but great to start focusing on a question.)\n\nWhat is your question? Why do you care about this question (and / or why does this question matter to others)? How interested in this question are you on a scale from 0 (just doing to get credit for this question) to 10 (this is what motivates you to wake up each day and you will answer this question with the energy and passion of 1000 suns)?\nHow do your past experiences and background inform this question?\nWhat is the variable that is the focus of this question? How does this variable relate to affect, behavior, and cognition? Which aspect of this variable are you most interested in focusing on for your project?\nWhat is the between-person form of variation for this variable? What is the within-person form of variation for this variable? Note: for the final project, I strongly recommend focusing on a between-person variation version of the variable for the final project.\nDo you have ideas about what might predict or explain this variable (the answer to your question)? How would you write this out as a linear model?\n\n\n\nStudent Examples"
  },
  {
    "objectID": "calstats/Lectures/1L_WhyStats.html#getting-research-experience-as-an-ra",
    "href": "calstats/Lectures/1L_WhyStats.html#getting-research-experience-as-an-ra",
    "title": "Lecture 1 | Welcome to Psych 101",
    "section": "Getting Research Experience as an RA",
    "text": "Getting Research Experience as an RA\n\nRA = Research Assistant\n\nMostly Unpaid Experiences\nSome paid experiences exist!\n\nFrom the berkeley website‚Ä¶\nBusine$$ $chool\nStanford [maybe paid]\nA list a student sent me that they found.\n‚ÄúCold calling‚Äù labs who are doing work you think is cool.\nChat with your TAs / Professors\n\n\nAs an RA :\n\nwork with data : transcribing data; behavioral coding data; recruiting and participants to collect data; setting up psychophysiological recordings; cleaning data; etc.\nother opportunities to gain skills you can demonstrate :\n\nreading & discussing papers\nworking with IRB (institutional review board - an ethics thing)\nanalyzing data ‚Üí presenting research at a conference (poster) or submitting a paper for publication [your golden ticket]\ngeneral mentorship (how to apply to grad school; where to apply; who to talk to & e-mail; etc.)\nNOTE : this work and these skills apply to other work outside of research applications [time management; coordinating schedules; juggling responsibilities; etc.]\n\nget a sense of whether this [work or lab] is for you?\n\ndo you enjoy the work? are you going to look forward to showing up and doing the work / fulfilling the commitment?¬†\nare you working with a horrible monster?\n\nnot responsive\ninconsistent work / no plan for your work\nkind of a bully (emotionally abusive ‚Üí stealing your work)¬†\n\nor are you working with someone who is super cool and a positive influence on mentoring young minds!?!?! [YES!!!!]"
  },
  {
    "objectID": "calstats/Lectures/1L_WhyStats.html#applying-to-graduate-school",
    "href": "calstats/Lectures/1L_WhyStats.html#applying-to-graduate-school",
    "title": "Lecture 1 | Welcome to Psych 101",
    "section": "Applying to Graduate School",
    "text": "Applying to Graduate School\n\nYou are applying to work on research with a specific professor(s) at a school.\n\nShould have a sense of the topic you want to pursue.\nGood to have a narrative about how your past work and studies have prepared you for this topic / demonstrate an enduring interest in the topic.\n\nIndependent Thesis / Research Project :\n\nan official honors‚Äô thesis\nundergraduate research project (e.g., SURF; Psych 101!)\nyour own independent study / advanced work you did as an RA\n\nPersonal Statement : Experiences with Research You Can Write About\n\nI‚Äôm fascinated by people‚Ä¶Over the last year, I worked on an independent research study to better understand‚Ä¶.\nWorking as an RA; your research project; attending / presenting at a conference; etc.\n\n3-4 Letters of Recommendation : folks who can speak personally to your ability to do research.\nClinical Students : some kind of clinical internship / experience üòü\nTalk to people who are doing the thing you want to be doing about their journey\n\n\nThe Academic Job Market\nSome Data [Source]\n\n\n\nPhDs get jobs?\n\n\n\nbut not in academia‚Ä¶\n\n\n\n$$$$$$$$"
  },
  {
    "objectID": "calstats/Lectures/1L_WhyStats.html#footnotes",
    "href": "calstats/Lectures/1L_WhyStats.html#footnotes",
    "title": "Lecture 1 | Welcome to Psych 101",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHere‚Äôs a link to the article where this headline comes from. These data are a little dated, and I couldn‚Äôt immediately find more recent data - my guess is Meta does not really want to advertise that people are using the product more and more. However, in reports to investors reports consistent growth in metrics like ‚Äúad impressions‚Äù and ‚Äúdaily active users‚Äù. Let me know if you find other sources to show how technology companies are capturing more and more of our attention!‚Ü©Ô∏é\nOscar Grant, Trayvon Martin, Philando Castile, Eric Garner, George Floyd, Tamir Rice, Breonna Taylor, Ahmaud Aubrey, Jacob Blake. Here‚Äôs a more comprehensive list, and here‚Äôs a summary article on policing and race.\n‚Ü©Ô∏é"
  },
  {
    "objectID": "gradstats/gradlabs/8_InteractionFX.html",
    "href": "gradstats/gradlabs/8_InteractionFX.html",
    "title": "Lecture 8 | Interaction Effects",
    "section": "",
    "text": "Lab 8 : Self-Care Lab. Do something nice for yourself you wouldn‚Äôt have done if you had a lab. Share on discord (or not!)\nFinal Project : A Discussion Post; identify a dataset and research question & think about how you might answer this question with the data (and a linear model??)\nOur Remaining Class Time\n\n4/4 Class : On Zoom or Wizard of Oz Style?? (And section??). Topic : Logistic Regression and Quadratic Terms and Other Linear Model Stuff.\n4/11 : IRL, on Hierarchical Linear Models.\n4/18 : MEGA Exam. Will do my best to keep it chill.\n4/25 : More Hierarchical Linear Models. PCA? We will see!!\n5/2 : Our last class can u believe it???? The learning has stopped. So review, discussion, tears, project time."
  },
  {
    "objectID": "gradstats/gradlabs/8_InteractionFX.html#check-in",
    "href": "gradstats/gradlabs/8_InteractionFX.html#check-in",
    "title": "8_InteractionFX",
    "section": "",
    "text": "4/4 class\npower examples\n\n\nlibrary(jtools)\nh &lt;- readxl::read_xls(\"~/Downloads/in.excel.for.daniel.hormone.data.xls\")\nh &lt;- h[,-1]\n\nh$sexF &lt;- as.factor(h$sex)\nlevels(h$sexF) &lt;- c(\"Male\", \"Female\")\nh$sexF &lt;- relevel(h$sexF, ref = \"Female\")\n\nmod1 &lt;- lm(test_mean ~ narcicissm, data = h)\nmod2 &lt;- lm(test_mean ~ sexF, data = h)\nmod3 &lt;- lm(test_mean ~ sexF + narcicissm, data = h)\n\nzm1 &lt;- scale_mod(mod1, scale.response = T)\nzm2 &lt;- scale_mod(mod2, scale.response = T)\nzm3 &lt;- scale_mod(mod3, scale.response = T)\n?summ\nexport_summs(mod1, mod2, mod3, \n             coefs = c(\"Narcissism\" = \"narcicissm\", \"Sex (0 = Female; 1 = Male)\" = \"sexFMale\"),\n             transform.response = T, scale = T, confint = T)\n\n\n\nModel 1Model 2Model 3\n\nNarcissism0.28 **¬†¬†¬†¬†¬†¬†¬†0.17 *¬†¬†\n\n(0.10)¬†¬†¬†¬†¬†¬†¬†¬†¬†(0.09)¬†¬†¬†\n\nSex (0 = Female; 1 = Male)¬†¬†¬†¬†¬†¬†1.32 ***1.25 ***\n\n¬†¬†¬†¬†¬†¬†(0.18)¬†¬†¬†(0.18)¬†¬†¬†\n\nN86¬†¬†¬†¬†¬†¬†90¬†¬†¬†¬†¬†¬†¬†86¬†¬†¬†¬†¬†¬†¬†\n\nR20.08¬†¬†¬†0.37¬†¬†¬†¬†0.40¬†¬†¬†¬†\n\nAll continuous variables are mean-centered and scaled by 1 standard deviation.  *** p &lt; 0.001;  ** p &lt; 0.01;  * p &lt; 0.05."
  },
  {
    "objectID": "gradstats/gradlabs/8_InteractionFX.html#power-planning",
    "href": "gradstats/gradlabs/8_InteractionFX.html#power-planning",
    "title": "8_InteractionFX",
    "section": "Power Planning",
    "text": "Power Planning\n\nCalculating\n\n\nConceptualizing\n\n\nCaring\n\nFull Article : Button, K. S., Ioannidis, J. P., Mokrysz, C., Nosek, B. A., Flint, J., Robinson, E. S., & Munaf√≤, M. R. (2013). Power failure: why small sample size undermines the reliability of neuroscience.¬†Nature reviews neuroscience,¬†14(5), 365-376."
  },
  {
    "objectID": "gradstats/gradlabs/8_InteractionFX.html#interaction-effects",
    "href": "gradstats/gradlabs/8_InteractionFX.html#interaction-effects",
    "title": "8_InteractionFX",
    "section": "Interaction Effects",
    "text": "Interaction Effects\n\nlibrary(pwr)\npwr.r.test(r = .2, power = .8, alternative = \"two.sided\")\n\n\n     approximate correlation power calculation (arctangh transformation) \n\n              n = 193.0867\n              r = 0.2\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\n\n\nlibrary(ggplot2)\nggplot(data = h, aes(x = test_mean, y = narcicissm, color = as.factor(sex))) + \n  geom_point(size = .5, alpha = .3, position = \"jitter\") + \n  labs(title = \"My Graph\", x = \"Age (Years)\", y = \"Self-Esteem (1-4 Scale)\") + \n  geom_smooth(method = lm)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 36 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 36 rows containing missing values or values outside the scale range\n(`geom_point()`)."
  },
  {
    "objectID": "calstats/Lectures/7_CatModels.html#announcements-reminders",
    "href": "calstats/Lectures/7_CatModels.html#announcements-reminders",
    "title": "Lecture 7 - Categorical Models",
    "section": "Announcements & Reminders :",
    "text": "Announcements & Reminders :\n\nIt is almost Spring Break. Class will end early today :)\n4/4 Lecture is ON ZOOM. Will keep it short and engaging! Plan to participate / engage if you attend, or just watch the recording.\nStart Collecting Final Project Data?! Your GSI hopefully has (or will) give you feedback on your final project; start collecting data. Make sure :\n\nYour DV is numeric.\nYou measure each variable in your linear model as a separate variable.\nEach question has one response option (no ‚Äúmultiple multiple choice‚Äù or ‚Äúcheckbox‚Äù.)\nOther issues?"
  },
  {
    "objectID": "calstats/Lectures/7_CatModels.html#r2-in-real-life",
    "href": "calstats/Lectures/7_CatModels.html#r2-in-real-life",
    "title": "Lecture 7 - Categorical Models",
    "section": "\\(R^2\\) In Real-Life",
    "text": "\\(R^2\\) In Real-Life\n\n\n\n\n\nDISCUSS : what do these linear models tell us about the relationship between GPA, SAT (IVs) and freshman grades (DV)?"
  },
  {
    "objectID": "calstats/Lectures/7_CatModels.html#when-the-iv-is-categorical",
    "href": "calstats/Lectures/7_CatModels.html#when-the-iv-is-categorical",
    "title": "Lecture 7 - Categorical Models",
    "section": "When the IV is Categorical‚Ä¶",
    "text": "When the IV is Categorical‚Ä¶\n\nWe Load the Data\n\nd &lt;- read.csv(\"~/Dropbox/!WHY STATS/Class Datasets/cal_mini_SP25.csv\", stringsAsFactors = T)\n\n\n\nWe Graph our Variables.\n\nhist(d$satlife)\n\n\n\n\n\n\n\nplot(d$stoned72) # an empty level!\n\n\n\n\n\n\n\nlevels(d$stoned72)[1] &lt;- NA\nplot(d$stoned72) # an empty level is GONE.\n\n\n\n\n\n\n\n\n\n\nWe Define and Graph Our Linear Model.\nThe Model Coefficients : An Intercept and Slope\n\nmod &lt;- lm(satlife ~ stoned72, data = d)\ncoef(mod)\n\n(Intercept) stoned72yes \n   6.960317    0.994228 \n\n\nThe Graph Will Help Our Interpretation.\n\nlibrary(gplots)\n\n\nAttaching package: 'gplots'\n\n\nThe following object is masked from 'package:stats':\n\n    lowess\n\nplotmeans(satlife ~ stoned72, data = d)\n\n\n\n\n\n\n\ncoef(mod)\n\n(Intercept) stoned72yes \n   6.960317    0.994228 \n\nsummary(mod)$r.squared\n\n[1] 0.03745752\n\n\n\nThe IV has been DUMMY CODED.\n\nwhen stoned72 = 0 = ‚ÄúNo‚Äù (Or not ‚ÄúYes‚Äù)\nwhen stoned72 = 1 = ‚ÄúYes‚Äù\n\nThe Intercept is : The Predicted Value of Y when all X-Values are Zero.\nThe Slope is : How our Predicted Value of Y changes when X changes by one.\n\n\n\nWe Interpret Our Linear Model.\n\nWhat is the relationship?\nHow strong is this relationship?\nWhy do we find this relationship in the data?\n\nCAUSALITY : smoking marijuana causes people to feel more satisfied with their lives.\n\nmarijuana cuts off oxygen to brain; makes them delirious?\nit is an effective stress management strategy\ndopamine spikes highest when anticipating; the question triggered the response and life is good TM\nyou are numbing yourself to the horrors in the world, escaping into a void of short-term pleasure and a delusion that things will be okay omg is everyone looking at me right now okay I‚Äôm gonna stop next time‚Ä¶I promise‚Ä¶\nsatlife is a temporary state.\nmaybe some recency bias; people who were stoned only thinking about short-term positive feelings; not contextualizng the pain; the horror.\ndrugs make you feel good!!!!\nnot smoking / doing drugs might cause you to be lower in satisfaction with life because you are attending to things?\n\nREVERSE CAUSATION : satisfaction with life might cause people to smoke weed?\n\npeople who are feelin great feel like they can sit back; blaze one up; just relaaaaaxxxxxx.\npeople who are HECKA STRESSED might not want to invite marijuana into their lives.\n\nCONFOUND / THIRD VARIABLE :\n\na chill personality (someone low in Negative Emotion) may a) be more likely to use marijuana and b) more likely to be satisfied w/ their life.\nmoney : someone with a lot of $$ might be more satisfied with their lives, and more likley to buy weed (which does have a cost).\nfree time / sleep / stress.\n\nCHANCE : we found this pattern due to chance.\n\nWho cares about this relationship?\nWhat other questions do we have about these data?\n\n\n\nYOUR TURN : Define another linear model to predict a numeric DV from a categorical IV (with 2 levels.)\nStudnet Example Go Here.\nStudnet Example Go Here."
  },
  {
    "objectID": "calstats/Lectures/7_CatModels.html#final-project-questions-go-here",
    "href": "calstats/Lectures/7_CatModels.html#final-project-questions-go-here",
    "title": "Lecture 7 - Categorical Models",
    "section": "Final Project Questions Go Here!",
    "text": "Final Project Questions Go Here!"
  },
  {
    "objectID": "calstats/Lectures/7_CatModels.html#all-done-yay.",
    "href": "calstats/Lectures/7_CatModels.html#all-done-yay.",
    "title": "Lecture 7 - Categorical Models",
    "section": "ALL DONE! YAY.",
    "text": "ALL DONE! YAY."
  },
  {
    "objectID": "gradstats/gradlabs/8_InteractionFX.html#announcements",
    "href": "gradstats/gradlabs/8_InteractionFX.html#announcements",
    "title": "Lecture 8 | Interaction Effects",
    "section": "",
    "text": "Lab 8 : Self-Care Lab. Do something nice for yourself you wouldn‚Äôt have done if you had a lab. Share on discord (or not!)\nFinal Project : A Discussion Post; identify a dataset and research question & think about how you might answer this question with the data (and a linear model??)\nOur Remaining Class Time\n\n4/4 Class : On Zoom or Wizard of Oz Style?? (And section??). Topic : Logistic Regression and Quadratic Terms and Other Linear Model Stuff.\n4/11 : IRL, on Hierarchical Linear Models.\n4/18 : MEGA Exam. Will do my best to keep it chill.\n4/25 : More Hierarchical Linear Models. PCA? We will see!!\n5/2 : Our last class can u believe it???? The learning has stopped. So review, discussion, tears, project time."
  },
  {
    "objectID": "gradstats/gradlabs/8_InteractionFX.html#check-in-interpreting-a-multiple-regression-table.",
    "href": "gradstats/gradlabs/8_InteractionFX.html#check-in-interpreting-a-multiple-regression-table.",
    "title": "Lecture 8 | Interaction Effects",
    "section": "Check-In : Interpreting a Multiple Regression Table.",
    "text": "Check-In : Interpreting a Multiple Regression Table.\nNote : these data come from the ‚Äúhormone_datset.csv‚Äù; in the dropbox folder. But not necessary to download?\n\nmod1 &lt;- lm(narcicissm ~ sexF, data = hmod)\nmod2 &lt;- lm(narcicissm ~ test_mean, data = hmod)\nmod3 &lt;- lm(narcicissm ~ sexF + test_mean, data = hmod)\nexport_summs(mod1, mod2, mod3, \n             coefs = c(\"Sex (0 = Female; 1 = Male)\" = \"sexFMale\", \"Testosterone\" = \"test_mean\"),\n             error_format = \"[se = {std.error}, t = {statistic}]\", transform.response = T, scale = T, confint = T)\n\n\n\nModel 1Model 2Model 3\n\nSex (0 = Female; 1 = Male)0.39¬†¬†¬†¬†¬†¬†¬†0.04¬†¬†\n\n[se = 0.23, t = 1.68]¬†¬†¬†¬†¬†¬†[se = 0.29, t = 0.13]¬†\n\nTestosterone¬†¬†¬†¬†0.28 **0.27 *\n\n¬†¬†¬†¬†[se = 0.10, t = 2.65]¬†¬†[se = 0.13, t = 2.00]¬†\n\nN86¬†¬†¬†¬†86¬†¬†¬†¬†¬†¬†86¬†¬†¬†¬†¬†\n\nR20.03¬†0.08¬†¬†¬†0.08¬†¬†\n\nAll continuous variables are mean-centered and scaled by 1 standard deviation.  *** p &lt; 0.001;  ** p &lt; 0.01;  * p &lt; 0.05."
  },
  {
    "objectID": "gradstats/gradlabs/8_InteractionFX.html#multiple-regression-more-regression",
    "href": "gradstats/gradlabs/8_InteractionFX.html#multiple-regression-more-regression",
    "title": "Lecture 8 | Interaction Effects",
    "section": "Multiple Regression : More Regression",
    "text": "Multiple Regression : More Regression\n\nLabeling Changes\n\n‚ÄúIndependent‚Äù Effect. The slope of the IV does not change when other variables are added to the model.\n‚ÄúSuppression‚Äù Effect. The slope of the IV gets stronger (in either direction) when other variables are added to the model.\n‚ÄúMediation‚Äù Effect. The slope of the IV gets closer to zero when other variables are added to the model. Careful : ‚Äúmediation‚Äù often implies some causal relationship, and it is very hard (/impossible?) to estimate causal relationships without experimental methods. Some links below that go deeper into this.\n\n\n\nHow Large a Change is Enough to Matter???\nIs the difference in slope between Model 1 and Model 3 significant?\n\nexport_summs(mod1, mod2, mod3, \n             coefs = c(\"Sex (0 = Female; 1 = Male)\" = \"sexFMale\", \"Testosterone\" = \"test_mean\"),\n             error_format = \"[se = {std.error}, t = {statistic}]\", transform.response = T, scale = T, confint = T)\n\n\n\nModel 1Model 2Model 3\n\nSex (0 = Female; 1 = Male)0.39¬†¬†¬†¬†¬†¬†¬†0.04¬†¬†\n\n[se = 0.23, t = 1.68]¬†¬†¬†¬†¬†¬†[se = 0.29, t = 0.13]¬†\n\nTestosterone¬†¬†¬†¬†0.28 **0.27 *\n\n¬†¬†¬†¬†[se = 0.10, t = 2.65]¬†¬†[se = 0.13, t = 2.00]¬†\n\nN86¬†¬†¬†¬†86¬†¬†¬†¬†¬†¬†86¬†¬†¬†¬†¬†\n\nR20.03¬†0.08¬†¬†¬†0.08¬†¬†\n\nAll continuous variables are mean-centered and scaled by 1 standard deviation.  *** p &lt; 0.001;  ** p &lt; 0.01;  * p &lt; 0.05.\n\n\n\n\n\nbucket &lt;- array()\nfor(i in c(1:1000)){\n  hsim &lt;- h[sample(1:nrow(h), nrow(h), replace = T),]\n  xm1 &lt;- lm(narcicissm ~ sexF, data = hsim)\n  xm2 &lt;- lm(narcicissm ~ sexF + test_mean, data = hsim)\n  bucket[i] &lt;- coef(xm1)[2] - coef(xm2)[2]\n}\nsum(bucket &gt; 0)\n\n[1] 994\n\n\n\nexport_summs(mod1, mod2, mod3,\n             coefs = c(\"Sex (0 = Female; 1 = Male)\" = \"sexFMale\", \"Testosterone\" = \"test_mean\"),\n             error_format = \"[se = {std.error}, t = {statistic}]\", \n             transform.response = T, scale = T, confint = T)\n\n\n\nModel 1Model 2Model 3\n\nSex (0 = Female; 1 = Male)0.39¬†¬†¬†¬†¬†¬†¬†0.04¬†¬†\n\n[se = 0.23, t = 1.68]¬†¬†¬†¬†¬†¬†[se = 0.29, t = 0.13]¬†\n\nTestosterone¬†¬†¬†¬†0.28 **0.27 *\n\n¬†¬†¬†¬†[se = 0.10, t = 2.65]¬†¬†[se = 0.13, t = 2.00]¬†\n\nN86¬†¬†¬†¬†86¬†¬†¬†¬†¬†¬†86¬†¬†¬†¬†¬†\n\nR20.03¬†0.08¬†¬†¬†0.08¬†¬†\n\nAll continuous variables are mean-centered and scaled by 1 standard deviation.  *** p &lt; 0.001;  ** p &lt; 0.01;  * p &lt; 0.05.\n\n\n\n\n\n# install.packages(\"mediation\")\nlibrary(mediation)\n\nLoading required package: mvtnorm\n\n\n\nAttaching package: 'mvtnorm'\n\n\nThe following object is masked from 'package:arm':\n\n    standardize\n\n\nThe following object is masked from 'package:jtools':\n\n    standardize\n\n\nLoading required package: sandwich\n\n\nmediation: Causal Mediation Analysis\nVersion: 4.5.0\n\nmodM &lt;- lm(test_mean ~ sexF, data = h) # defining a model to predict our mediator (testosterone) from the IV (sex)\nmedmod &lt;- mediate(modM, mod3, treat = \"sexF\", mediator = \"test_mean\", dropobs = \"TRUE\", boot = TRUE, sims = 1000)\n\nWarning in mediate(modM, mod3, treat = \"sexF\", mediator = \"test_mean\", dropobs\n= \"TRUE\", : treatment and control values do not match factor levels; using\nFemale and Male as control and treatment, respectively\n\n\nRunning nonparametric bootstrap\n\nsummary(medmod)\n\n\nCausal Mediation Analysis \n\nNonparametric Bootstrap Confidence Intervals with the Percentile Method\n\n               Estimate 95% CI Lower 95% CI Upper p-value  \nACME             0.1947       0.0303         0.43   0.018 *\nADE              0.0199      -0.3163         0.33   0.906  \nTotal Effect     0.2146      -0.0400         0.47   0.082 .\nProp. Mediated   0.9072      -3.3018         5.70   0.100 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nSample Size Used: 86 \n\n\nSimulations: 1000 \n\n\n\nTotal Effect : slope of sexFMale in Model 1\nAverage Direct Effect : slope of sexFMale in Model 3\nACME = Average Causal Mediation Effects = Total Effect - Direct Effect; influence our mediatior has on the pre-existing relationship.\n\nNOTE : the output here doesn‚Äôt match the effects in the table for two reasons:\n\nMy results in the table are standardized (and mediate doesn‚Äôt seem to work well with z-scored models? I couldn‚Äôt figure it out at least‚Ä¶)\nmediate will only include complete cases across all the models. Notice that my first linear model has a larger sample size than the models with testosterone. It would be a more fair comparison to remove the individuals from Model 1 who are also missing in my other models. Will chat more about missing data next lecture.\n\n\n\nWould You Like to Learn More?\n\nThe Difference Between ‚ÄúSignificant‚Äù and ‚ÄúNot Significant‚Äù is not Itself Statistically Significant\nA conceptual review of mediation, and tour through the ‚Äòmediation‚Äô package.\nPreacher KJ, Hayes AF. Asymptotic and resampling strategies for assessing and comparing indirect effects in multiple mediator models. Behav Res Methods. 2008;40(3):879-891. doi:10.3758/BRM.40.3.879\nA nice and recent summary of why caution is needed for mediation analysis."
  },
  {
    "objectID": "gradstats/gradlabs/8_InteractionFX.html#presentations",
    "href": "gradstats/gradlabs/8_InteractionFX.html#presentations",
    "title": "Lecture 8 | Interaction Effects",
    "section": "Presentations",
    "text": "Presentations"
  },
  {
    "objectID": "gradstats/gradlabs/8_InteractionFX.html#multiple-regression-moderation-interaction-effects",
    "href": "gradstats/gradlabs/8_InteractionFX.html#multiple-regression-moderation-interaction-effects",
    "title": "Lecture 8 | Interaction Effects",
    "section": "Multiple Regression : Moderation / Interaction Effects",
    "text": "Multiple Regression : Moderation / Interaction Effects\n\nFrantz Fanon Reading.\n\nWhat did you think about the reading???\nWhat is his main point?\nHave you observed this in the wild / our modern society?\nHow would or might you test this idea with data, as a modern psychologist would???\nAny other questions / ideas / comments?\n\n\n\nSee Professor Handout.\n\n\nPicture is worth‚Ä¶that‚Äôs right class‚Ä¶1000 words.\n\nlibrary(ggplot2)\nlibrary(ggthemes)\nggplot(data = h, aes(x = scale(test_mean), y = scale(narcicissm), color = sexF)) + \n  geom_point(size = .5, alpha = .3, position = \"jitter\") + \n  labs(title = \"The Interaction Effect\", x = \"Testosterone (Z-Scored)\", y = \"Narcissism (Z-Scored)\", color = \"Sex\") + \n  geom_smooth(method = lm) + theme_apa()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 36 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 36 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\nStill just a linear model‚Ä¶\nA linear model whose IVs must be standardized!!\n\nmod4 &lt;- lm(narcicissm ~ sexF * test_mean, data = h)\nm4z &lt;- scale_lm(mod4, scale.response = T) # you \nm4z\n\n\nCall:\nlm(formula = narcicissm ~ sexF * test_mean)\n\nCoefficients:\n       (Intercept)            sexFMale           test_mean  sexFMale:test_mean  \n            0.2967             -0.2628              0.6242             -0.4118  \n\n\n\ncf4 &lt;- coef(m4z)\nplot(scale(narcicissm) ~ scale(test_mean), col = sexF, data = h, pch = 19)\nabline(a = cf4[1],\n       b = cf4[3], \n       col = \"black\", lwd = 5) # line for females\nabline(a = cf4[1] + cf4[2],\n       b = cf4[3] + cf4[4], \n       col = \"red\", lwd = 5) # line for males\n\n\n\n\n\n\n\n\n\n\nReporting, and in a table.\n\nggplot(data = h, aes(x = scale(test_mean), y = scale(narcicissm), color = sexF)) + \n  geom_point(size = .5, alpha = .3, position = \"jitter\") + \n  labs(title = \"The Interaction Effect\", x = \"Testosterone (Z-Scored)\", y = \"Narcissism (Z-Scored)\", color = \"Sex\") + \n  geom_smooth(method = lm) + theme_apa()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 36 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 36 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nexport_summs(mod1, mod2, mod3, mod4,\n             coefs = c(\"Sex (0 = Female; 1 = Male)\" = \"sexFMale\", \"Testosterone\" = \"test_mean\", \"Sex * Testosterone\" = \"sexFMale:test_mean\"),\n             error_format = \"[se = {std.error}, t = {statistic}]\", transform.response = T, scale = T, confint = T)\n\n\n\nModel 1Model 2Model 3Model 4\n\nSex (0 = Female; 1 = Male)0.39¬†¬†¬†¬†¬†¬†¬†0.04¬†¬†-0.26¬†\n\n[se = 0.23, t = 1.68]¬†¬†¬†¬†¬†¬†[se = 0.29, t = 0.13]¬†[se = 0.40, t = -0.65]\n\nTestosterone¬†¬†¬†¬†0.28 **0.27 *0.62¬†\n\n¬†¬†¬†¬†[se = 0.10, t = 2.65]¬†¬†[se = 0.13, t = 2.00]¬†[se = 0.37, t = 1.71]\n\nSex * Testosterone¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†-0.41¬†\n\n¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†[se = 0.39, t = -1.05]\n\nN86¬†¬†¬†¬†86¬†¬†¬†¬†¬†¬†86¬†¬†¬†¬†¬†86¬†¬†¬†¬†\n\nR20.03¬†0.08¬†¬†¬†0.08¬†¬†0.09¬†\n\nAll continuous variables are mean-centered and scaled by 1 standard deviation.  *** p &lt; 0.001;  ** p &lt; 0.01;  * p &lt; 0.05.\n\n\n\n\n\n\nWould You Like To Learn More?\nAs always, let me know if you find great resources :)\n\nThe Interactions Package. The author of jtools has made what looks like a super clean way to graph interaction effects in R, using ggplot2.\nVisualization Tutorials. Other, more cumbersome methods of plotting interaction effects exist!"
  },
  {
    "objectID": "gradstats/gradlabs/9_ModelsTransformed.html",
    "href": "gradstats/gradlabs/9_ModelsTransformed.html",
    "title": "Lecture 9 - Generalized Linear Models",
    "section": "",
    "text": "Hi folks, it‚Äôs me; your professor. These are words that I wrote; kind of neat that you can read them with a voice that is your own. Anyway, sorry to not be in class today. I hope y‚Äôall are well, and very much appreciate your flexibility as I travel to be with family. Look forward to being back next week :)\nI‚Äôve tried to organize a narrative about logistic regression - why we do this, how to do it in R, and how to interpret the results. You‚Äôll use this skill as part of completing Lab 9.\nNext week, we‚Äôll review logistic regression, talk more about model comparison, work through the final project, and start talking about hierarchical linear models.\nHave a great week, feel free to post on Discord if you have any questions, something is confusing, you found an error in the document or helpful resource I could add, or you are listening to a super cool track you want to share."
  },
  {
    "objectID": "gradstats/gradlabs/9_ModelsTransformed.html#issues-with-multiple-regression",
    "href": "gradstats/gradlabs/9_ModelsTransformed.html#issues-with-multiple-regression",
    "title": "9_ModelsTransformed",
    "section": "Issues with Multiple Regression",
    "text": "Issues with Multiple Regression\n\n1. Overfitting.\nWhen your model is too complex, each variable in the model (parameter) increases the model complexity.\n\ncomplex models that perfectly fit the data are problematic: you essentially describing your sample, and not the underlying population (which is usually the goal of multiple regression.)\nWe don‚Äôt expect over-fit models to generalize to other samples.\n\nTo ensure your model generalizes to other samples, you can a) replicate, or b) cross-validate (divide your sample into sub-samples; define a model on one sample, then test the model in the other(s). Lots of different ways to do this! Here‚Äôs one.)\n\n\nh &lt;- read.csv(\"~/Dropbox/!GRADSTATS/gradlab/Datasets/hormone_dataset.csv\")\nmodz &lt;- lm(narcicissm ~ ., data = h)\nsummary(modz)\n\n\nCall:\nlm(formula = narcicissm ~ ., data = h)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.81086 -0.19232  0.02552  0.18077  0.67338 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          2.480e+00  7.895e-01   3.141 0.002594 ** \nsubj                 3.148e-03  1.358e-03   2.318 0.023831 *  \nsection              1.200e-01  9.085e-02   1.321 0.191447    \nage                 -6.391e-02  1.806e-02  -3.539 0.000776 ***\nsex                 -4.649e-02  1.329e-01  -0.350 0.727696    \ntest1               -1.011e+01  5.667e+00  -1.785 0.079307 .  \ntest2               -1.012e+01  5.666e+00  -1.787 0.078925 .  \ntest_mean            2.024e+01  1.133e+01   1.786 0.079052 .  \ncortisol1            1.116e+02  5.907e+01   1.890 0.063554 .  \ncortisol2            1.100e+02  6.067e+01   1.813 0.074719 .  \ncortisol_mean       -2.194e+02  1.197e+02  -1.833 0.071639 .  \ntest.x.cort         -1.701e-02  2.362e-02  -0.720 0.474272    \ninternal_motivation -4.711e-02  7.348e-02  -0.641 0.523880    \nexternal_motivation  1.536e-01  6.159e-02   2.494 0.015351 *  \ndom_social           2.903e-01  6.716e-02   4.322  5.8e-05 ***\ndom_aggressive       1.995e-01  5.402e-02   3.692 0.000476 ***\nscore_econ_ideology -1.131e-01  4.553e-02  -2.485 0.015732 *  \nsocial_ideology     -5.902e-02  3.929e-02  -1.502 0.138187    \noverall_ideology     8.840e-02  5.825e-02   1.517 0.134313    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3313 on 61 degrees of freedom\n  (42 observations deleted due to missingness)\nMultiple R-squared:  0.704, Adjusted R-squared:  0.6167 \nF-statistic: 8.062 on 18 and 61 DF,  p-value: 2.76e-10\n\n\n\nMulticollinearity. If your independent variables are highly related, then your multivariate regression slope estimates are not uniquely determined. weird things happen to your coefficients, and this makes it hard to interpret your effects.\n\nIN R : check the ‚Äúvariance inflation factor‚Äù (VIF); a measure of how much one IV is related to all the other IVs in the model.\n\\(\\huge VIF_j=\\frac{1}{1-R_{j}^{2}}\\)\n\n\n```{r}\nlibrary(car)\n\n```\n\n\nWould You Like to Learn More??\n\nSome more notes on multicollinearity and VIFs.\nTake Aaron Fisher‚Äôs class on Structural Equation Modeling?\nRead through Peng Ding (Prof in Cal Stats Department) Book on Causal Inference"
  },
  {
    "objectID": "gradstats/gradlabs/9_ModelsTransformed.html#more-on-power",
    "href": "gradstats/gradlabs/9_ModelsTransformed.html#more-on-power",
    "title": "9_ModelsTransformed",
    "section": "",
    "text": "Using drawings of distributions.\nUsing the pwr package.\n\n\n\n\n\n\n\n\n\n\nWhy should we care about power? : Button, K. S., Ioannidis, J. P., Mokrysz, C., Nosek, B. A., Flint, J., Robinson, E. S., & Munaf√≤, M. R. (2013). Power failure: why small sample size undermines the reliability of neuroscience.¬†Nature reviews neuroscience,¬†14(5), 365-376.\nMany examples of pwr used for different types of models.\nAnother nice tutorial."
  },
  {
    "objectID": "calstats/Lectures/7_CatModels.html",
    "href": "calstats/Lectures/7_CatModels.html",
    "title": "Lecture 7 - Categorical Models",
    "section": "",
    "text": "No talking, no looking up answers!\nWill discuss next week :)"
  },
  {
    "objectID": "calstats/Lectures/7_CatModels.html#check-in-a-quick-study",
    "href": "calstats/Lectures/7_CatModels.html#check-in-a-quick-study",
    "title": "Lecture 7 - Categorical Models",
    "section": "",
    "text": "No talking, no looking up answers!\nWill discuss next week :)"
  },
  {
    "objectID": "calstats/labs/Lab7.html",
    "href": "calstats/labs/Lab7.html",
    "title": "",
    "section": "",
    "text": "Lab 7 is a self-care lab. Your assignment is to do something nice to yourself that you wouldn‚Äôt have normally done if you had a HW assignment :). Submit a sentence here to get credit, and feel free to post on Discord if you‚Äôd like to share publically.\nSpring Break!"
  },
  {
    "objectID": "calstats/labs/Lab7.html#lab-7",
    "href": "calstats/labs/Lab7.html#lab-7",
    "title": "",
    "section": "",
    "text": "Lab 7 is a self-care lab. Your assignment is to do something nice to yourself that you wouldn‚Äôt have normally done if you had a HW assignment :). Submit a sentence here to get credit, and feel free to post on Discord if you‚Äôd like to share publically.\nSpring Break!"
  },
  {
    "objectID": "calstats/Lectures/8X_3LevelExperiment_Asynch.html",
    "href": "calstats/Lectures/8X_3LevelExperiment_Asynch.html",
    "title": "Lecture 8 - Experiments and Models - Asynchronous Class",
    "section": "",
    "text": "Hi class! It‚Äôs me, your professor. In text format. These are words that I wrote; are you hearing my voice in your head? In any case, thanks for your patience and flexibility; it is much appreciated (and needed).\nHere‚Äôs a short video where I say hi and go over this document.\n\n\n\nBefore next Friday (4/11) please complete the following.\n\nWork through this lecture document and complete the Lecture 8 Quiz. The quiz focuses on analyzing and interpreting the dataset we collected before spring break, on ‚Äúcat speeds‚Äù. I have some videos to help walk y‚Äôall through this.\nRead Chapter 9 and Complete Quiz 9. This chpater is a lot. We will 100% review this in lecture next week; but good to get an introduction to the topic. It‚Äôs very easy to do in R :)\nComplete Milestone #3. You‚Äôll work on this in discussion section. There are two parts - a draft of your methods section, and getting your final project data exported from Google Forms and imported into R so you can start data analysis :)\nThere is no lab assignment this week. Just all the other stuff haha. :) &lt;3\n\n\n\n\nThere are three parts to this lecture document; I think this document will take an average of 60 minutes to complete (sd = 30 minutes?) so please plan accordingly.\n\nCheck-In. On interpreting linear regression with a categorical IV.\nLecture. More thoughts on experiments.\nLecture and Quiz. On the ‚ÄúAnchoring‚Äù dataset we collected last week. With a short quiz, based on the lecture and video.\n\nI‚Äôm looking forward to seeing everyone again on Friday, April 11th. Let me know if you have questions or comments on the Discord. Take care!\n:Prof."
  },
  {
    "objectID": "calstats/Lectures/8X_3LevelExperiment_Asynch.html#introductions",
    "href": "calstats/Lectures/8X_3LevelExperiment_Asynch.html#introductions",
    "title": "Lecture 8 - Experiments and Models - Asynchronous Class",
    "section": "",
    "text": "Hi class! It‚Äôs me, your professor. In text format. These are words that I wrote; are you hearing my voice in your head? In any case, thanks for your patience and flexibility; it is much appreciated (and needed).\nHere‚Äôs a short video where I say hi and go over this document.\n\n\n\nBefore next Friday (4/11) please complete the following.\n\nWork through this lecture document and complete the Lecture 8 Quiz. The quiz focuses on analyzing and interpreting the dataset we collected before spring break, on ‚Äúcat speeds‚Äù. I have some videos to help walk y‚Äôall through this.\nRead Chapter 9 and Complete Quiz 9. This chpater is a lot. We will 100% review this in lecture next week; but good to get an introduction to the topic. It‚Äôs very easy to do in R :)\nComplete Milestone #3. You‚Äôll work on this in discussion section. There are two parts - a draft of your methods section, and getting your final project data exported from Google Forms and imported into R so you can start data analysis :)\nThere is no lab assignment this week. Just all the other stuff haha. :) &lt;3\n\n\n\n\nThere are three parts to this lecture document; I think this document will take an average of 60 minutes to complete (sd = 30 minutes?) so please plan accordingly.\n\nCheck-In. On interpreting linear regression with a categorical IV.\nLecture. More thoughts on experiments.\nLecture and Quiz. On the ‚ÄúAnchoring‚Äù dataset we collected last week. With a short quiz, based on the lecture and video.\n\nI‚Äôm looking forward to seeing everyone again on Friday, April 11th. Let me know if you have questions or comments on the Discord. Take care!\n:Prof."
  },
  {
    "objectID": "calstats/Lectures/8X_3LevelExperiment_Asynch.html#check-in.",
    "href": "calstats/Lectures/8X_3LevelExperiment_Asynch.html#check-in.",
    "title": "Lecture 8 - Experiments and Models - Asynchronous Class",
    "section": "Check-In.",
    "text": "Check-In.\nProfessor wanted to see if calling your parents / guardians was related to having a more satisfied life. He analyzed data from the mega dataset, and defined the linear model below (satisfaction with life ~ call.folks (yes / no) + error. Use the following output to answer the check-in questions (submit answers to the link above).\n\nmod &lt;- lm(SWLS ~ call.folks, data = m)\nround(coef(mod), 2)\n\n  (Intercept) call.folksYes \n         3.26          0.12 \n\n\n\nThe Questions.\n\nWhat is the predicted value of satisfaction with life for someone who does not call their parents?\nWhat is the predicted value of satisfaction with life for someone who calls their parents?\nWhat is the difference in satisfaction with life between someone who does and does not call their parents?\nCan professor say that calling parents CAUSES people to have a different satisfaction with life? Why / why not?\n\n\n\nCheck-In Key.\nWhen you are done, watch the video below to see a review of the answers. Yes, you can just watch the video (or submit nothing) but trying on your own is the point!\n\nHere‚Äôs a link to the R Script professor used to generate the model / the key."
  },
  {
    "objectID": "calstats/Lectures/8X_3LevelExperiment_Asynch.html#lecture-experimental-methods",
    "href": "calstats/Lectures/8X_3LevelExperiment_Asynch.html#lecture-experimental-methods",
    "title": "Lecture 8 - Experiments and Models - Asynchronous Class",
    "section": "Lecture : Experimental Methods",
    "text": "Lecture : Experimental Methods\n\nManipulation in Depth : Watch out for Misleading Control Variables\nThe manipulation is when researchers create multiple groups (experimental and / or control conditions) and change ONE THING about a person‚Äôs experience in each group, and then observe the outcome. The change that experimenters make to the conditions is a source of variation (the independent variable), and the outcome is the dependent variable.\n\nthe treatment / experimental condition is when the IV is present (the change happens). For example, in a drug study, the treatment / experimental condition would be the drug that the researchers give the participant.\ncontrol / comparison condition is when the IV is absent (the default experience / no change). Critically, you want everything about the treatment and control condition to be the same except for the one thing (the IV) that you are manipulating. For example, in a drug study the control condition would be a pill that looks like the drug, but doesn‚Äôt have the key chemical compound the researchers are testing.\n\nKEY IDEA : the control / comparison group matters! In drug studies, the control group is clear. However, in other contexts what a ‚Äúgood control‚Äù is much harder to define, and researchers can be a little misleading (or biased) when creating the control and experimental groups. So it‚Äôs an important skill to think about the control condition, and whether it‚Äôs a fair comparison.\nFor example, let‚Äôs say I wanted to test whether attending a 3-hour statsiticss and research methods lecture decreases student boredom. What‚Äôs the control condition here?\n\nif I wanted to show that a 3-hour statistics and research methods lecture DECREASES boredom, I could compare the lecture to things that are more boring (like watching paint dry for three hours, or reading the dictionary for three hours.)\nIf I wanted to show that a 3-hour statistics and research methods lecture INCREASES boredom, I could compare the lecture to things that are less boring, like watching a super-exciting movie for 3-hours, or maybe a professional lecture guest starring super-famous celebrities like Chancellor Rich Lyons.\n\nThis example may seem obvious, but thinking through what the experimental / control conditions are can yield surprising results.\n\n\nReal-Life Examples of Difficult Control Conditions\nWatch the video below, where I walk through two famous studies that have difficult control conditions.\n\n\nPower Posing Study1\n\n\n\n\nGratitude Study.\n\nArticle : Seligman, M. E., Steen, T. A., Park, N., & Peterson, C. (2005). Positive psychology progress: empirical validation of interventions.¬†American psychologist,¬†60(5), 410.\nPractice\n\n\n\n\n\n\n\n\n\n\n\n\n1. What did the experimenters manipulate? Which of these were experimental and control conditions?\n\n\n\n\n\nThe experimenters manipulated the specific instructions that people received; there were five experimental conditions (giving people tasks like practicing gratitude) and one control condition - a task to write about early memories.\n\n\n\n\n\n\n\n\n\n2. What are some other things that differ between the experimental and control conditions (potential confounds)?\n\n\n\n\n\nEarly memories and gratitude are both about writing some kind of self-reflection. Yet gratitude is focused on the present, and early memories are on the past. Furthermore, gratitude is focused on positive attributes, while early memories are focused on potentially negative experiences. I wonder how much of the benefit of gratidue reported in these studies is about not triggering negative memories.\n\n\n\n\n3. ON DISCORD (Optional) :\n\nWhat‚Äôs something that you are grateful for?\nWhat are some other (better) control conditions that you might include in this study?"
  },
  {
    "objectID": "calstats/Lectures/8X_3LevelExperiment_Asynch.html#experimental-methods",
    "href": "calstats/Lectures/8X_3LevelExperiment_Asynch.html#experimental-methods",
    "title": "Lecture 8 - Experiments and Models - Asynchronous Class",
    "section": "Experimental Methods",
    "text": "Experimental Methods\n\nThe Definition of Causality\n\nThe cause and effect are contiguous in space and time.\nThe cause must be prior to the effect. (no reverse causation)\nThere must be a constant union betwixt the cause and effect. (‚ÄúTis chiefly this quality, that constitutes the relation.‚Äù) (no random chance)\nThe same cause always produces the same effect, and the same effect never arises but from the same cause. (not ‚Äújust‚Äù some third variable)[^1]\n\n[^1]but remember, life is complex and there are often multiple causes of human behavior!\n\n\nManipulation : Watch out for Misleading Control Variables\n\nRECAP : the manipulation (A/B Testing) : researchers create multiple groups (conditions) and change ONE THING (the IV) about a person‚Äôs experience in each group & observe the result (the DV).\n\ntreatment / experimental condition : the IV is present (the change happens)\ncontrol / comparison condition : the IV is absent (the default experience / no change)\nKEY IDEA : the comparison group matters!\n\nExample : sitting in a 3 statistics and research methods class‚Ä¶\n\nDECREASES boredom compared to‚Ä¶\n\nexample go here\n\nINCREASES boredom compared to‚Ä¶\n\nexample go here\n\n\n\n\n\nReal-Life Examples of Difficult Control Conditions\n\nPower Posing Study**1 Is this a fair comparison / manipulation? Why / why not?\n\n\n\nGratitude Study:\nRead the prompt below. Answer the following questions in your breakout room discussion.\n\nICE-BREAKER : What‚Äôs something that you are grateful for?\nWhat did the experimenters manipulate? Which of these were experimental and control conditions?\nWhat are some other things that differ between the experimental and control conditions (potential confounds)?\nWhat are some other (better) control conditions that you might include in this study?"
  },
  {
    "objectID": "calstats/Lectures/8X_3LevelExperiment_Asynch.html#lecture-anchoring-study-from-lecture-7",
    "href": "calstats/Lectures/8X_3LevelExperiment_Asynch.html#lecture-anchoring-study-from-lecture-7",
    "title": "Lecture 8 - Experiments and Models - Asynchronous Class",
    "section": "Lecture : Anchoring Study (From Lecture 7)",
    "text": "Lecture : Anchoring Study (From Lecture 7)\n\n\nAnchoring : Experimental Design\nBelow are the three versions of the study that students saw in Week 7. Do you remember which condition you were in? Could you tell that you were in an experiment??\n\n\n\n\n\n\n\n\nHigh Condition\nLow Condition\nControl Condition\n\n\n\n\n\n\n\n\n\n\nBelow are examples of how the anchoring study was an example of an experiment.\n\n\n\n\n\n\n\nKey Term and Definition\nExample in the Anchoring Study\n\n\noutcome = THE DV = what was being measured after the manipulation?\nThe person‚Äôs own answer to the specific question (i.e., their idea about the speed of a cat, the height of a tree, and the amount of meat eaten.)\n\n\nmanipulation = THE IV = what were ALL the things that the researcher changed about a person‚Äôs experience (across experimental conditions)?\nThe context given before the person provided their own answer was manipulated; there were three groups :\n\nHIGH : the person was asked whether the answer was more or less than a very large number (e.g., ‚Äúcan a cat run more or less than 40 miles per hour.‚Äù)\nLOW : the person was asked whether the answer was more or less than a very small number (e.g., ‚Äúcan a cat run more or less than 3 miles per hour.‚Äù)\nCONTROL : no context was given before the person made their own rating.\n\nEverything else was the same across the conditions; the font was the same; the types of questions was the same; the wording was the same; the color was the same; etc.\n\n\nrandom assignment = were all possible confound variables balanced across conditions?\nI‚Äôll show evidence of random assignment in the lecture video that supports these notes.\nThere was one variable that was not randomly assigned - the month you were born in was used to assign people to one of the three groups (Google forms does not have a built-in random assignment feature.) For example, people in the HIGH condition were all born between January and April.\n\n\ndouble-blind = did the study avoid demand characteristics (where experimenter might have influenced behavior when giving the study) & placebo effects (where participants might have acted in a certain way because they knew they were being experimented on)?\nThe google form didn‚Äôt really change its behavior when assigning people to one group or another; we can say that it was blind to the participants‚Äô condition because it is a computer.\nThe participants may or may not have been blind to the study condition; I didn‚Äôt advertise this was an experiment, you were supposed to do the survey on your own (‚Äúno talking with your buddy‚Äù) and we hadn‚Äôt yet done (or learned) about experiments yet. Still, I imagine there‚Äôs some amount of skepticism among students that may have influenced the results. This lowers the external validity / generalizability of the study somewhat. (However, we gain a lot of power in being able to control people‚Äôs experience with this study.)\n\n\ngeneralizability = did the study have external validity? what was the effect size (\\(R^2\\))?\nIt‚Äôs unclear how questions about cat speeds, tree heights, and meat consumption are relevant to the real world. But as I describe in the video, this study was partially responsible for the researchers‚Äô winning the Nobel Prize‚Ä¶.in economics. For what it‚Äôs worth, I‚Äôve seen this study replicate in every. single. semester. over the last‚Ä¶.8 years? I‚Äôve been using this as an example. It‚Äôs a very reliable effect; and has a fairly large \\(R^2\\) value.\n\n\nethics = should researchers do this type of study? (Predict & Control)\nCurious your thoughts! Did you think this research is ethical???\n\n\n\n\n\nAnchoring : Data to Test the Theory\n\nQuestion : Will the number that people see BEFORE making their own rating influence their decision?\nTheory : Which pattern in the data do you expect to observe? Vote on Discord BEFORE analyzing the data / watching the video (or not.)\n\nOPTION A: People who see a HIGHER number before making their own rating will make a HIGHER number than people who see the LOWER number.\nOPTION B : People who see a LOWER number before making their own rating will make a HIGHER number than people who see the HIGHER number.\nOPTION C : There will be NO DIFFERENCES between the groups.\n\nLinear Models (DV ~ IV). To test this theory, we will need to define three separate linear models.\n\ncat speeds ~ condition\ntree heights ~ condition\nmeat consumption ~ condition\n\nData : Download the ‚Äúanchor_SP25.csv‚Äù dataset here. These data were collected\n\n\n\nAnchoring : Cat Speed Example\nWatch the video tutorial below, where I walk through how to test the researchers‚Äô question (does the number that people see before making their own rating influence their own rating?)\n\n\nLink to R Script\n\n\n## Loading Data and Data Cleaning\nlibrary(gplots)\n\n\nAttaching package: 'gplots'\n\n\nThe following object is masked from 'package:stats':\n\n    lowess\n\nanchor &lt;- read.csv(\"~/Dropbox/!WHY STATS/Class Datasets/anchorSP25.csv\", stringsAsFactors = T)\nanchor$catspeed[anchor$catspeed &gt; 100] &lt;- NA\n\n#### Define and Graph the Linear Model\ncat.mod &lt;- lm(catspeed ~ condition, data = anchor)\nplotmeans(catspeed ~ condition, data = anchor, connect = F)\n\n\n\n\n\n\n\ncoef(cat.mod)\n\n  (Intercept) conditionHIGH  conditionLOW \n    15.379545     18.297874     -5.650974 \n\n#### Interpret the Linear Model\nsummary(cat.mod)$r.squared\n\n[1] 0.3169796\n\n\n\n\nAnchoring Study in Conclusion (Who Cares About Cat Speeds?)\n\n\n\nLecture Quiz : Anchoring for Tree Heights & Meat Consumption\nUse the ‚Äúanchor_SP25.csv‚Äù dataset to test whether the experimental manipulation influenced people‚Äôs judgments about how tall trees were, and how much meat the average american would eat.\nNote that the variables treeheight and meateat have outliers. Remove outliers for these variables based on the following rules:\n\ntreeheight : any response greater than 1000 ft (the height of a 10-story building)\nmeateat : any response greater than 1095 (which would be the average american eating three pounds of meat per day, every day for a year.)\n\nThen, define two linear models (model 1 = treeheight ~ condition; model 2 = meateat ~ condition), and use these models to answer the questions below. Round to the nearest whole number (no decimal places).\n\nWhat is the predicted value of treeheight for someone in the control condition?\nWhat is the predicted value of treeheight for someone in the high condition?\nWhat is the predicted value of treeheight for someone in the low condition?\nWhat percentage of variation in people‚Äôs ideas about tree heights is explained by the experimental manipulation (i.e., what is the \\(R^2\\) value for this model?)\nWhat is the predicted value of meateat for someone in the control condition?\nWhat is the predicted value of meateat for someone in the high condition?\nWhat is the predicted value of meateat for someone in the low condition?\nWhat percentage of variation in people‚Äôs ideas about meat consumption is explained by the experimental manipulation (i.e., what is the \\(R^2\\) value for this model?)\n\nFeeling stuck? Here‚Äôs a video where I walk through the code you‚Äôll need."
  },
  {
    "objectID": "calstats/Lectures/8X_3LevelExperiment_Asynch.html#introduction-to-chapter-9-sampling-error-and-sampling-bias",
    "href": "calstats/Lectures/8X_3LevelExperiment_Asynch.html#introduction-to-chapter-9-sampling-error-and-sampling-bias",
    "title": "Lecture 8 - Experiments and Models - Asynchronous Class",
    "section": "Introduction to Chapter 9 : Sampling Error (and Sampling Bias)",
    "text": "Introduction to Chapter 9 : Sampling Error (and Sampling Bias)"
  },
  {
    "objectID": "calstats/Lectures/8X_3LevelExperiment_Asynch.html#final-project-stuff",
    "href": "calstats/Lectures/8X_3LevelExperiment_Asynch.html#final-project-stuff",
    "title": "Lecture 8 - Experiments and Models - Asynchronous Class",
    "section": "Final Project Stuff",
    "text": "Final Project Stuff"
  },
  {
    "objectID": "calstats/Lectures/8X_3LevelExperiment_Asynch.html#the-end.",
    "href": "calstats/Lectures/8X_3LevelExperiment_Asynch.html#the-end.",
    "title": "Lecture 8 - Experiments and Models - Asynchronous Class",
    "section": "THE END.",
    "text": "THE END.\nThanks for reading and watching. Complete this check-out"
  },
  {
    "objectID": "calstats/Lectures/8X_3LevelExperiment_Asynch.html#footnotes",
    "href": "calstats/Lectures/8X_3LevelExperiment_Asynch.html#footnotes",
    "title": "Lecture 8 - Experiments and Models - Asynchronous Class",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nCarney, D. R., Cuddy, A. J., & Yap, A. J. (2010). Power posing: Brief nonverbal displays affect neuroendocrine levels and risk tolerance. Psychological science, 21(10), 1363-1368. link to article + link to summary of non-replication‚Ü©Ô∏é"
  },
  {
    "objectID": "calstats/Lectures/8_3LevelExperiment.html",
    "href": "calstats/Lectures/8_3LevelExperiment.html",
    "title": "Lecture 8 - Experiments and Models",
    "section": "",
    "text": "Professor wanted to see if calling your parents / guardians was related to having a more satisfied life. He analyzed data from the mega dataset, and defined the linear model below (satisfaction with life ~ call.folks (yes / no) + error. Use the following output to answer the check-in questions.\n\nmod &lt;- lm(SWLS ~ call.folks, data = m)\nround(coef(mod), 2)\n\n  (Intercept) call.folksYes \n         3.26          0.12 \n\n\n\nWhat is the predicted value of satisfaction with life for someone who calls their parents?\nWhat is the predicted value of satisfaction with life for someone who does not call their parents?\nWhat is the difference in satisfaction with life between someone who does and does not call their parents?\nCan professor say that calling parents CAUSES people to have a different satisfaction with life? Why / why not?"
  },
  {
    "objectID": "calstats/Lectures/8_3LevelExperiment.html#check-in-a-quick-study",
    "href": "calstats/Lectures/8_3LevelExperiment.html#check-in-a-quick-study",
    "title": "Lecture 8 - Experiments and Models",
    "section": "",
    "text": "Professor wanted to see if calling your parents / guardians was related to having a more satisfied life. He analyzed data from the mega dataset, and defined the linear model below (satisfaction with life ~ call.folks (yes / no) + error. Use the following output to answer the check-in questions.\n\nmod &lt;- lm(SWLS ~ call.folks, data = m)\nround(coef(mod), 2)\n\n  (Intercept) call.folksYes \n         3.26          0.12 \n\n\n\nWhat is the predicted value of satisfaction with life for someone who calls their parents?\nWhat is the predicted value of satisfaction with life for someone who does not call their parents?\nWhat is the difference in satisfaction with life between someone who does and does not call their parents?\nCan professor say that calling parents CAUSES people to have a different satisfaction with life? Why / why not?"
  },
  {
    "objectID": "calstats/Lectures/8_3LevelExperiment.html#announcements-agenda.",
    "href": "calstats/Lectures/8_3LevelExperiment.html#announcements-agenda.",
    "title": "Lecture 8 - Experiments and Models",
    "section": "Announcements & Agenda.",
    "text": "Announcements & Agenda.\n\nAnnouncements\n\nMilestone #3 | Methods Section & Descriptive Statistics.\nWe On Zoom!\n\nDefault mute.\nWebcam on in breakout rooms ‚Äúrequired‚Äù; nice during lecture.\nChat: keep it focused to questions during lecture; will build in time for social connection, etc. (feel free to spam discord w/ multitasking livestreamofconsciousness)\n\nQuestions, Comments, Concerns?\n\nAgenda\n\n2:10 - 2:30 | Check-In and Announcements\n2:30 - 2:45 | Experimental Methods Review\n2:45 - 3:15 | Anchoring Study (Part 1)\n3:15 - 3:30 | Mega Break Time\n3:30 - 3:55 | Anchoring Study (Part 2)\n3:55 - 4:00 | Mini Break Time\n4:00 - 4:30 | Introduction to Sampling Error and Bias\n4:30 - 5:00 | Conclusion"
  },
  {
    "objectID": "calstats/Lectures/8_3LevelExperiment.html#experimental-methods",
    "href": "calstats/Lectures/8_3LevelExperiment.html#experimental-methods",
    "title": "Lecture 8 - Experiments and Models",
    "section": "Experimental Methods",
    "text": "Experimental Methods\n\nThe Definition of Causality\n\nThe cause and effect are contiguous in space and time.\nThe cause must be prior to the effect. (no reverse causation)\nThere must be a constant union betwixt the cause and effect. (‚ÄúTis chiefly this quality, that constitutes the relation.‚Äù) (no random chance)\nThe same cause always produces the same effect, and the same effect never arises but from the same cause. (not ‚Äújust‚Äù some third variable)[^1]\n\n[^1]but remember, life is complex and there are often multiple causes of human behavior!\n\n\nManipulation : Watch out for Misleading Control Variables\n\nRECAP : the manipulation (A/B Testing) : researchers create multiple groups (conditions) and change ONE THING (the IV) about a person‚Äôs experience in each group & observe the result (the DV).\n\ntreatment / experimental condition : the IV is present (the change happens)\ncontrol / comparison condition : the IV is absent (the default experience / no change)\nKEY IDEA : the comparison group matters!\n\nExample : sitting in a 3 statistics and research methods class‚Ä¶\n\nDECREASES boredom compared to‚Ä¶\n\nexample go here\n\nINCREASES boredom compared to‚Ä¶\n\nexample go here\n\n\n\n\n\nReal-Life Examples of Difficult Control Conditions\n\nPower Posing Study**1 Is this a fair comparison / manipulation? Why / why not?\n\n\n\nGratitude Study:\nRead the prompt below. Answer the following questions in your breakout room discussion.\n\nICE-BREAKER : What‚Äôs something that you are grateful for?\nWhat did the experimenters manipulate? Which of these were experimental and control conditions?\nWhat are some other things that differ between the experimental and control conditions (potential confounds)?\nWhat are some other (better) control conditions that you might include in this study?"
  },
  {
    "objectID": "calstats/Lectures/8_3LevelExperiment.html#anchoring-as-an-experiment",
    "href": "calstats/Lectures/8_3LevelExperiment.html#anchoring-as-an-experiment",
    "title": "Lecture 8 - Experiments and Models",
    "section": "Anchoring as an Experiment",
    "text": "Anchoring as an Experiment\n\nThe Theory and Experimental Design\n\n\n\nHigh Condition\nLow Condition\nControl Condition\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKey Term and Definition\nExample in the Anchoring Study\n\n\n\n\noutcome = THE DV = what was being measured after the manipulation?\n\n\n\nmanipulation = THE IV = what were ALL the things that the researcher changed about a person‚Äôs experience (across experimental conditions)?\n\n\n\nrandom assignment = were all possible confound variables balanced across conditions?\n\n\n\ndouble-blind = did the study avoid demand characteristics (where experimenter might have influenced behavior when giving the study) & placebo effects (where participants might have acted in a certain way because they knew they were being experimented on)?\n\n\n\ngeneralizability = did the study have external validity? what was the effect size (R2)?\n\n\n\nethics = should researchers do this type of study? (Predict & Control)\n\n\n\n\n\n\nAnchoring : Data to Test the Theory\nIN BREAKOUT ROOMS!!!\n\nQuestion : Will the number that people see BEFORE making their own rating influence their decision?\nTheory :\n\nOPTION A: People who see a HIGHER number before making their own rating will make a HIGHER number than people who see the LOWER number.\nOPTION B : People who see a LOWER number before making their own rating will make a HIGHER number than people who see the HIGHER number.\nOPTION C : There will be NO DIFFERENCES between the groups.\n\nLinear Models : DV ~ IV\nData : the ‚Äúanchor_SP25.csv‚Äù dataset\n\nLoad the data and check to make sure the data loaded correctly.\nGraph the variables; remove outliers and / or empty levels.\n\nResults :\n\nDefine your Linear Models\nInterpret Your Results\n\nDiscuss :\n\nWhy do we observe this pattern?\nHow could we use this knowledge in real-life?\nWhat other questions do you have? What else would you want to learn about?"
  },
  {
    "objectID": "calstats/Lectures/8_3LevelExperiment.html#break-time",
    "href": "calstats/Lectures/8_3LevelExperiment.html#break-time",
    "title": "Lecture 8 - Experiments and Models",
    "section": "BREAK TIME :",
    "text": "BREAK TIME :"
  },
  {
    "objectID": "calstats/Lectures/8_3LevelExperiment.html#focus-your-power",
    "href": "calstats/Lectures/8_3LevelExperiment.html#focus-your-power",
    "title": "Lecture 8 - Experiments and Models",
    "section": "FOCUS YOUR POWER",
    "text": "FOCUS YOUR POWER"
  },
  {
    "objectID": "calstats/Lectures/8_3LevelExperiment.html#is-this-the-real-life-is-this-just-fantasy-sampling-error-and-bias",
    "href": "calstats/Lectures/8_3LevelExperiment.html#is-this-the-real-life-is-this-just-fantasy-sampling-error-and-bias",
    "title": "Lecture 8 - Experiments and Models",
    "section": "Is this the real life? Is this just fantasy? (Sampling Error and Bias)",
    "text": "Is this the real life? Is this just fantasy? (Sampling Error and Bias)\n\nKEY IDEA : A good sample is a random sample: each individual in the population has an equal chance of being selected in your sample. your sample will never equal the population.\nSampling bias : the sample is different from the population because of some error in our method of sampling that influences the dependent variable in a predictable way.\n\nExamples :\n\nSelf-Selection Bias : People CHOOSE to be in the study.\nSurvivor Bias : Participants may drop out, and you are only collecting data from a specific group of people who ‚Äúlasted‚Äù.\nMany Other Types Exist : Wikipedia has a very long list of types of sampling biases.\n\nGOAL : identify sources of bias and minimize / test to see their influence.\n\nSampling error : the sample is different from the population because of random reasons.\n\nExample : even if your sample has zero bias, there will always be some amount of sampling error unless you survey the ENTIRE population.\nGoal : Model the role of CHANCE in influencing the relationship between two variables."
  },
  {
    "objectID": "calstats/Lectures/8_3LevelExperiment.html#footnotes",
    "href": "calstats/Lectures/8_3LevelExperiment.html#footnotes",
    "title": "Lecture 8 - Experiments and Models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nCarney, D. R., Cuddy, A. J., & Yap, A. J. (2010). Power posing: Brief nonverbal displays affect neuroendocrine levels and risk tolerance. Psychological science, 21(10), 1363-1368. link to article + link to summary of non-replication‚Ü©Ô∏é"
  },
  {
    "objectID": "calstats/Lectures/8X_3LevelExperiment_Asynch.html#final-project-stuff-introduction-to-milestone-3",
    "href": "calstats/Lectures/8X_3LevelExperiment_Asynch.html#final-project-stuff-introduction-to-milestone-3",
    "title": "Lecture 8 - Experiments and Models - Asynchronous Class",
    "section": "Final Project Stuff : Introduction to Milestone #3",
    "text": "Final Project Stuff : Introduction to Milestone #3\nFor Milestone #3, you are going to need to"
  },
  {
    "objectID": "gradstats/gradlabs/X_MiscToDo.html",
    "href": "gradstats/gradlabs/X_MiscToDo.html",
    "title": "",
    "section": "",
    "text": "Calculating Power\nUsing drawings of distributions.\nUsing the pwr package.\n\n\nUsing Our Results to Plan Sample Size\n\n\nMore on Power\n\n\n\n\nWhy should we care about power? : Button, K. S., Ioannidis, J. P., Mokrysz, C., Nosek, B. A., Flint, J., Robinson, E. S., & Munaf√≤, M. R. (2013). Power failure: why small sample size undermines the reliability of neuroscience.¬†Nature reviews neuroscience,¬†14(5), 365-376.\nMany examples of pwr used for different types of models.\nAnother nice tutorial."
  },
  {
    "objectID": "gradstats/gradlabs/9_ModelsTransformed.html#logistic-regression",
    "href": "gradstats/gradlabs/9_ModelsTransformed.html#logistic-regression",
    "title": "Lecture 9 - Generalized Linear Models",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\nA Non-Logistic Example to Start\nLet‚Äôs look at the hormone dataset again. Previously, we saw that sex was related to testosterone.\n\nlibrary(gplots)\n\n\nAttaching package: 'gplots'\n\n\nThe following object is masked from 'package:stats':\n\n    lowess\n\nh &lt;- read.csv(\"~/Dropbox/!GRADSTATS/gradlab/Datasets/hormone_dataset.csv\")\nh$sexF &lt;- as.factor(h$sex)\nlevels(h$sexF) &lt;- c(\"Male\", \"Female\")\nh$sexF &lt;- relevel(h$sexF, ref = \"Female\")\n\nmod &lt;- lm(test_mean ~ sexF, data = h)\nplotmeans(test_mean ~ sexF, data = h, connect = F)\n\n\n\n\n\n\n\nsummary(mod)\n\n\nCall:\nlm(formula = test_mean ~ sexF, data = h)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-60.927 -17.886  -3.404  13.801 138.723 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   41.581      5.894   7.055 3.77e-10 ***\nsexFMale      50.586      7.045   7.181 2.11e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 30.63 on 88 degrees of freedom\n  (32 observations deleted due to missingness)\nMultiple R-squared:  0.3695,    Adjusted R-squared:  0.3623 \nF-statistic: 51.56 on 1 and 88 DF,  p-value: 2.111e-10\n\n\nAs practice, take a look at the output - what do you observe? Think about a) the pattern in the data; b) the interpretation of the significance and effect size; c) the ‚Äúwho cares‚Äù about these results.\n\n\n\n\n\n\nWhat Professor Observes (Think On Ur Own First?)\n\n\n\n\n\nI notice the following:\nA. The pattern shows that males have higher (b = 50.586) testosterone than females on average.\nB. The effect is fairly large - I didn‚Äôt (and don‚Äôt want) to calculate cohen‚Äôs D, but the \\(R^2\\) value is very high - biological sex explains 36% of the variation in testosterone. This is ‚Äúhighly significant‚Äù, meaning that if the null were true (if there were no differences in testosterone between males and females) the probability that we would observe a difference as large as 50.586 is very, very small (p &lt; .00000000001).\nC. Who cares about this? Gosh, there seem to be a lot of bad takes on hormones and sex out there these days, and I don‚Äôt really want to add to that chorus, but since I chose these data‚Ä¶I‚Äôm not really sure what to make out of hormone data. I‚Äôm not a hormone researcher, think it‚Äôs odd that our society is SO FOCUSED on quantifying the hormones of individuals, and think that all of the energy focused on hormone levels and women in sports could be better spent following and watching female athletes and supporting them in that way, and (while we‚Äôre at it) making sure that we create inclusive spaces where all people can belong? Seems easy. IDK. Feel free to lemme know if you disagree / I‚Äôm missing something / I have some learning to do. Okay, back to the show.\n\n\n\n\n\nFlipping the Model Around\nSo far, we‚Äôve predicted a numeric / continuous variable with our good friend the linear model. But researchers often want to make predictions of categorical variables.\nWe could treat sex as a numeric variable; in fact, the original variable was coded as numeric (1 = male; 2 = female). So why not include this ‚Äúnumeric‚Äù variable as a DV in our linear model? Seems easy; what could go wrong!??\n\nmod2 &lt;- lm(sex ~ test_mean, data = h)\nplot(sex ~ test_mean, data = h)\nabline(mod2)\n\n\n\n\n\n\n\nsummary(mod2)\n\n\nCall:\nlm(formula = sex ~ test_mean, data = h)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.6341 -0.2735 -0.1131  0.3306  0.9166 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.862302   0.087390  21.310  &lt; 2e-16 ***\ntest_mean   -0.007303   0.001017  -7.181 2.11e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.368 on 88 degrees of freedom\n  (32 observations deleted due to missingness)\nMultiple R-squared:  0.3695,    Adjusted R-squared:  0.3623 \nF-statistic: 51.56 on 1 and 88 DF,  p-value: 2.111e-10\n\n\n\n\n\n\n\n\nWhat Professor Thinks Went Wrong (Think On Ur Own First?)\n\n\n\n\n\n\nThe predicted values of the DV are continuous, and can range between 1 and 2. However, we think of (and measured) sex as a binary variable (male OR female) and while a continuous approach might better match the complex biological reality, that wasn‚Äôt the way the data were measured in this study.\nThe predicted values of gender can go beyond the range of our DV. For example, someone with a testosterone of 150 would be predicted to have a gender of .766. If 1 = male and 2 = female, does this make the person super-male? Sub male? No, the value makes no sense and is wrong.\nOur DV is not normally distributed, and the linear model depends on certain assumptions (specifically normality and linearity.) We are violating those assumptions here.\n\n\n\n\n\n\nLogistic Regression\nAs a solution to our problem, we can transform our linear model to one that conforms to a non-normal (or non-Gaussian) distribution.\nRather than predict a specific ‚Äúvalue‚Äù of male or female, we will estimate the probability of being male vs.¬†female. The predicted values should necessarily fall between 0 (estimated probability of being female = 0%) and 1 (estimated probability of being female = 100%).\nWe do this using a ‚Äúlink‚Äù function that ‚Äúlinks‚Äù the mean (i.e., expectaion) of your outcome variable(s), Y, to our linear predictor. Different types of outcome variables each have a different ‚Äúcanonical‚Äù link function, as summarized in the table below.\n\n\n\n\n\n\n\n\nDistribution\nLink Function\nExample Use Case\n\n\nNormal\nIdentity\nDV is continuous response.\n\n\nBinomial\nLogit\nDV is binary response\n\n\nPoisson\nLog\nDV is a fixed count response.\n\n\nGamma\nReciprocal\nContinuous, but highly skewed, distributions.\n\n\n\nYou don‚Äôt need to memorize these - the key idea is that sometimes you want (or need) to adjust the parameters of the linear model in order to better fit the type of data that you are working with.\nBy adapting our linear model, we have extended the general linear model to something called the generalized linear model (glm).\n\n\n\nOkay, But How Do I Do This in R, Professor?\nLet‚Äôs work through an example. We‚Äôll tweak our good friend lm slightly by calling the glm() function, and then specifying which family of distributions we are working with. in this case, the binomial family since our DV is binary.\n(Note that in order to adhere to the requirements of a binomial distribution, I need to encode the two groups of my categorical outcome as 0 and 1.\n\nh$sexR &lt;- h$sex - 1\nglmod &lt;- glm(sexR ~ test_mean, data = h, family = \"binomial\")\n\nBefore we get into the intercepts and slopes and all that, let‚Äôs graph the data because, as y‚Äôall know‚Ä¶.a picture is worth‚Ä¶yes, that‚Äôs right class‚Ä¶a thousand words.\n\nplot(sexR ~ test_mean, data = h,\n     xlab = \"Testosterone\",\n     ylab = \"Probability of Being Female\")\ncurve(predict(glmod, data.frame(test_mean=x), type = \"resp\"), add = T, col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\nThe red line is our estimated probability of being female for a person with a certain level of testosterone.\n\nUnlike the linear model, this function does not go beyond the limit of (0,1).\nThis function is not linear, meaning the slope is not constant for all levels of testosterone. For example, a difference in testosterone of 10 means less in terms of change in probability of being female at high levels of testosterone than at lower levels. (Carter! I believe this addresses your question from before Spring Break.)\n\nThe results of our model will describe these adjusted line. Unfortunately, things get a little confusing when we look at the intercept and slopes, because the default for the binomial family is to report terms in log-odds.\n\nInterpreting the Intercept\nThere are two methods of interpreting the intercept (that I know of.)\n\nExponentiate the Intercept\nWe can exponentiate the intercept to transform the estimate into the odds of the DV occuring when the X value is zero.\nOdds are defined as \\(\\text{odds} = \\frac{p}{(1-p)}\\).\n\nround(exp(coef(glmod)[1]), 2)\n\n(Intercept) \n     123.93 \n\n\nSo the intercept in this case describes the probability of being female, divided by the probability of not being female, for someone with zero testosterone. In other words, there‚Äôs a much, much higher odd of being female for someone with zero testosterone.\n\n\nInverse Logit Function\nIf we take the inverse of the logit function of the intercept, the intercept is the probability of Y, when all X values are zero. This is often the clearest way to interpret the intercept (if that statistic is relevant) in my opinion.\n\\(\\huge \\text{inverse logit = } \\frac{1}{1 + e^{-x}} \\text{ or } \\frac{e^{x}}{1 + e^{x}}\\)\n\nexp(coef(glmod)[1]) / (1 + exp(coef(glmod)[1]))\n\n(Intercept) \n  0.9919957 \n\n\nSo, there‚Äôs a 99.19% chance that a participant with zero testosterone would be female.\n\n\n\nInterpreting the Slope.\nWhen we exponentiate the slope, we convert the estimate into an odds ratio. The odds ratio describes how the odds change between two different outcomes.\nNote that an odds ratio of 1 would mean that there is no change in the odds of Y as X changes (similar to a slope of zero in a general linear model).\nOne nice feature of odds ratios is that they are are scalable - you can keep doubling the odds, and not go beyond a probability of 1.\n\nexp(coef(glmod)[2])\n\ntest_mean \n0.9121065 \n\n\nThe odds ratio is .91, which is .09 less than an odds ratio of 1. This means that each unit increase in testosterone decreases the probability of being female by .09 or 9%. (The change in probability is in reference to an odds ratio of 1.)\n\n\nGelman & Hill‚Äôs ‚ÄúDivide by Four‚Äù Rule.\nMany people find odds and odd ratios confusing. The estimable Gelman & Hill (2007) agree, and define a ‚Äúdivide by four‚Äù rule. Where you take the regression coefficient, divide it by four, and that number gives you the upper bound of the predictive difference in Y that corresponds to a 1-unit increase in X.\n\ncoef(glmod)/4\n\n(Intercept)   test_mean \n 1.20493609 -0.02299963 \n\n\nSo, this method would suggest that a unit increase in testosterone would decrease the probability of being female by no more than 2%.\n\n\n\n\nThat Inferential Statistics Stuff\nWe can extract inferential statistics using the summary() function, as before.\n\nsummary(glmod)\n\n\nCall:\nglm(formula = sexR ~ test_mean, family = \"binomial\", data = h)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  4.81974    1.19821   4.022 5.76e-05 ***\ntest_mean   -0.09200    0.02081  -4.420 9.86e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 109.96  on 89  degrees of freedom\nResidual deviance:  56.09  on 88  degrees of freedom\n  (32 observations deleted due to missingness)\nAIC: 60.09\n\nNumber of Fisher Scoring iterations: 7\n\n\nMuch is the same as before:\n\nWe have estimates of the slope. Note that these are not-yet exponentiated.\nWe have estimates of standard errors and p-values with stars that immediately show us whether our results are statistically significant or not.\n\nA few things are different:\n\nThere‚Äôs a dispersion parameter. This is part of the link function, and describes how variance in our outcome variable depends on the mean. This is always set to 1 in a logistic regression; there are other forms of regression (‚Äúquasi-logistic‚Äù and ‚Äúquasi-poisson‚Äù for example where the dispersion parameter can be increased to account for greater variablility in the outcome.)\nR no longer reports an \\(R^2\\) value. The \\(R^2\\) statistic is not appropriate for generalized linear models, since we are not calculating errors in the same way (i.e., we are not adding up . There are various methods of calculating what‚Äôs called a ‚Äúpseudo \\(R^2\\)‚Äù, which estimates this statistic, and often is reported via other functions (see below).\nInstead, R reports two deviance statistics - null and residual. Deviance is a measure of error - we ‚Äúwant‚Äù deviance to be low, and expect it to decrease by at least one for every new predictor we add to our model. The null deviance is the error when we have no predictor in our model (and are just using a constant term - the baseline probability of the outcome variable - to make predictions). The residual deviance is the deviance for this model; the fact that there‚Äôs a decrease of 53.87 exceeds our expected decrease of 1, and tells me the model has improved our predictions.\nR reports the Akaike Information Criterion (AIC). We will talk about this more next week, when we talk about model fit and comparing models, but the TLDR is this is a way to evaluate how ‚Äúgood‚Äù a model is at making predictions, with a lower AIC describing a model that better fits the data. The scale of this estimate - like the deviance statistics - is dependent on the data and sample size, so AIC is meant to compare one model to another from the same dataset.\n\nThere are a few different packages that make reporting the effects of a logistic regression easier. One example is the summ() function from the jtools package (which I think we looked at before to make nice multiple regression tables.) The function works the same, but we will add an argument to tell the function to exponentiate the coefficients to aid in the interpretation, and an argument to add confidence intervals. (See the documentation for the function for other arguments to add.)\n\nlibrary(jtools)\n\nWarning: package 'jtools' was built under R version 4.3.3\n\nexport_summs(glmod, exp = TRUE, error_format = \"[{conf.low}, {conf.high}]\")\n\n\n\nModel 1\n\n(Intercept)123.93 ***\n\n[11.84, 1297.49]¬†¬†¬†\n\ntest_mean0.91 ***\n\n[0.88, 0.95]¬†¬†¬†\n\nN90¬†¬†¬†¬†¬†¬†¬†\n\nAIC60.09¬†¬†¬†¬†\n\nBIC65.09¬†¬†¬†¬†\n\nPseudo R20.64¬†¬†¬†¬†\n\n *** p &lt; 0.001;  ** p &lt; 0.01;  * p &lt; 0.05.\n\n\n\n\nNote that the default output for this function reports another statistic - the Bayesian Information Criterion (which is another way to evaluate the model) as well as some Pseudo \\(R^2\\) statistics (there are different methods of calculating this; not sure which one the authors defaulted to.)\n\n\nWould You Like To Learn More?\nYou don‚Äôt have to take my word for it. Below are a few readings that will help support your understanding of generalized linear models. Let me know if you find other useful resources!\n\nGelman & Hill (2007). Chapter 5 is focused on logistic regression; they work through a few examples, talk about interaction effects and scaling / centering variables; making specific preditions‚Ä¶.very thorough. I think you can easily find this online, but let me know if you want a .pdf.\nA textbook chapter on generalized models. I really like this researcher‚Äôs approach to linear models, and while this textbook chapter is a little more spare than some of the other chapters, it presents a nice overview of why and how we use generalized linear models.\nA stats blog works through the output of a generalized linear model in R; focuses on a poisson distribution, but many of the principles are the same (and good to see a different example of a similar concept.)"
  },
  {
    "objectID": "gradstats/gradlabs/9_ModelsTransformed.html#lab-9.",
    "href": "gradstats/gradlabs/9_ModelsTransformed.html#lab-9.",
    "title": "9_ModelsTransformed",
    "section": "Lab 9.",
    "text": "Lab 9.\nSelf-care lab is over. Long live self-care lab. Feel free to work with others on this; think it would be very cool to treat this as a group assignment. Or go lone wolf (AWOOOOOO.)\n\nThe\nThe study author . See if you can recreate the author‚Äôs results using the variables in the datast.\nBefore spring break, we talked about (and learned about) interaction effects. Think of an interaction effect that you might want to test with these data.\nOkay, let‚Äôs practice"
  },
  {
    "objectID": "gradstats/gradlabs/X_MiscToDo.html#issues-with-multiple-regression",
    "href": "gradstats/gradlabs/X_MiscToDo.html#issues-with-multiple-regression",
    "title": "",
    "section": "## Issues with Multiple Regression",
    "text": "## Issues with Multiple Regression\n\n1. Overfitting.\nWhen your model is too complex, each variable in the model (parameter) increases the model complexity.\n\ncomplex models that perfectly fit the data are problematic: you essentially describing your sample, and not the underlying population (which is usually the goal of multiple regression.)\nWe don‚Äôt expect over-fit models to generalize to other samples.\n\nTo ensure your model generalizes to other samples, you can a) replicate, or b) cross-validate (divide your sample into sub-samples; define a model on one sample, then test the model in the other(s). Lots of different ways to do this! Here‚Äôs one.)\n\n\nh &lt;- read.csv(\"~/Dropbox/!GRADSTATS/gradlab/Datasets/hormone_dataset.csv\")\nmodz &lt;- lm(narcicissm ~ ., data = h)\nsummary(modz)\n\n\nCall:\nlm(formula = narcicissm ~ ., data = h)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.81086 -0.19232  0.02552  0.18077  0.67338 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          2.480e+00  7.895e-01   3.141 0.002594 ** \nsubj                 3.148e-03  1.358e-03   2.318 0.023831 *  \nsection              1.200e-01  9.085e-02   1.321 0.191447    \nage                 -6.391e-02  1.806e-02  -3.539 0.000776 ***\nsex                 -4.649e-02  1.329e-01  -0.350 0.727696    \ntest1               -1.011e+01  5.667e+00  -1.785 0.079307 .  \ntest2               -1.012e+01  5.666e+00  -1.787 0.078925 .  \ntest_mean            2.024e+01  1.133e+01   1.786 0.079052 .  \ncortisol1            1.116e+02  5.907e+01   1.890 0.063554 .  \ncortisol2            1.100e+02  6.067e+01   1.813 0.074719 .  \ncortisol_mean       -2.194e+02  1.197e+02  -1.833 0.071639 .  \ntest.x.cort         -1.701e-02  2.362e-02  -0.720 0.474272    \ninternal_motivation -4.711e-02  7.348e-02  -0.641 0.523880    \nexternal_motivation  1.536e-01  6.159e-02   2.494 0.015351 *  \ndom_social           2.903e-01  6.716e-02   4.322  5.8e-05 ***\ndom_aggressive       1.995e-01  5.402e-02   3.692 0.000476 ***\nscore_econ_ideology -1.131e-01  4.553e-02  -2.485 0.015732 *  \nsocial_ideology     -5.902e-02  3.929e-02  -1.502 0.138187    \noverall_ideology     8.840e-02  5.825e-02   1.517 0.134313    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3313 on 61 degrees of freedom\n  (42 observations deleted due to missingness)\nMultiple R-squared:  0.704, Adjusted R-squared:  0.6167 \nF-statistic: 8.062 on 18 and 61 DF,  p-value: 2.76e-10\n\n\n\nMulticollinearity. If your independent variables are highly related, then your multivariate regression slope estimates are not uniquely determined. weird things happen to your coefficients, and this makes it hard to interpret your effects.\n\nIN R : check the ‚Äúvariance inflation factor‚Äù (VIF); a measure of how much one IV is related to all the other IVs in the model.\n\\(\\huge VIF_j=\\frac{1}{1-R_{j}^{2}}\\)\n\n\n```{r}\nlibrary(car)\n\n```\n\n\nWould You Like to Learn More??\n\nSome more notes on multicollinearity and VIFs.\nTake Aaron Fisher‚Äôs class on Structural Equation Modeling?\nRead through Peng Ding (Prof in Cal Stats Department) Book on Causal Inference"
  },
  {
    "objectID": "gradstats/gradlabs/9_ModelsTransformed.html#prof.-says-hello.",
    "href": "gradstats/gradlabs/9_ModelsTransformed.html#prof.-says-hello.",
    "title": "Lecture 9 - Generalized Linear Models",
    "section": "",
    "text": "Hi folks, it‚Äôs me; your professor. These are words that I wrote; kind of neat that you can read them with a voice that is your own. Anyway, sorry to not be in class today. I hope y‚Äôall are well, and very much appreciate your flexibility as I travel to be with family. Look forward to being back next week :)\nI‚Äôve tried to organize a narrative about logistic regression - why we do this, how to do it in R, and how to interpret the results. You‚Äôll use this skill as part of completing Lab 9.\nNext week, we‚Äôll review logistic regression, talk more about model comparison, work through the final project, and start talking about hierarchical linear models.\nHave a great week, feel free to post on Discord if you have any questions, something is confusing, you found an error in the document or helpful resource I could add, or you are listening to a super cool track you want to share."
  },
  {
    "objectID": "gradstats/gradlabs/9_ModelsTransformed.html#lab-9-and-farewell.",
    "href": "gradstats/gradlabs/9_ModelsTransformed.html#lab-9-and-farewell.",
    "title": "Lecture 9 - Generalized Linear Models",
    "section": "Lab 9 and Farewell.",
    "text": "Lab 9 and Farewell.\nFor Lab 9, you‚Äôll work with a new dataset to define a model that you think would best test a researcher‚Äôs theory, then compare your model to the model the researcher defined in the original paper. You‚Äôll also practice working with an interaction effect (what we talked about in Week 8) and logistic regression.\nOkay, that‚Äôs all. Hope this made sense, and feel free to reach out on Discord / e-mail as needed. Yeah!"
  },
  {
    "objectID": "gradstats/gradlabs/9Lab_LogisticRegression.html",
    "href": "gradstats/gradlabs/9Lab_LogisticRegression.html",
    "title": "Lab 9",
    "section": "",
    "text": "Feel free to work with others on this; think it would be very cool to treat this as a group assignment. Or go lone wolf (AWOOOOOO.)"
  },
  {
    "objectID": "gradstats/gradlabs/9Lab_LogisticRegression.html#data-and-paper",
    "href": "gradstats/gradlabs/9Lab_LogisticRegression.html#data-and-paper",
    "title": "Lab 9",
    "section": "Data and Paper",
    "text": "Data and Paper\nLink to Dataset.\nDescription of Variables : These data were adapted from a study1 on Dehumanizing Language and Attitudes Toward Immigrants in the United States. There was no original codebook to the study, but Professor did some detective work and identified the following variables. Note : there are a few other variables whose name/label I couldn‚Äôt immediately figure out‚Ä¶.another reason researchers should have DATA DICTIONARIES :) Lemme know if you identify / track them down.\n\nv1-v10 : meta data collected by the survey tool (Qualtrics)\nconsent : whether the participant consented to the study.\nage : participant‚Äôs age\ngender : 1 = Male; 2 = Female\nrace : the researchers did not include a codebook for this variable; 5 = white participants; 6 = other.\nnondht : whether participants read a text using non-dehumanizing language about immigrants\npanas_3 : how much participants report feeling anger toward immigrants\ndh_treat : The experimental condition; participants read a text adapted from political speeches that described immigrants in non-dehumanizing (dh_treat = 0) or dehumanizing (dh_treat = 1) terms.¬†\nImmigration Measures : After reading the text, participants were asked about their attitudes toward immigration.\n\nimm1 : ‚ÄúDo you think the number of immigrants from foreign countries who are permitted to come to the United States to live should be INCREASED, LEFT THE SAME as it is now, or DECREASED?‚Äù [higher numbers = more support for immigration]\nimm2 : ‚ÄúWould you favor or oppose legislation to increase border security in order to make it more difficult for individuals to enter the country?‚Äù [higher numbers = less support for immigration]\nimm3 : ‚ÄúWould you favor or oppose legislation that would allow undocumented immigrants already in the country to apply for legal status?‚Äù [higher numbers = more support for immigration]\n\nEvaluation of Text : After reading the text, participants were asked about their thoughts toward the text.\n\ntexteval_1 : the text was realistic\ntexteval_3 : the text was persuasive\ntexteval_4 : ?????\ntexteval_5 : the text was unnatural"
  },
  {
    "objectID": "gradstats/gradlabs/9Lab_LogisticRegression.html#lab-questions",
    "href": "gradstats/gradlabs/9Lab_LogisticRegression.html#lab-questions",
    "title": "Lab 9",
    "section": "Lab Questions",
    "text": "Lab Questions\n\nThe author wanted to test ‚Äúhow the dehumanization of immigrants influences immigration policy attitudes‚Äù. To do this, they conducted an experiment, in which participants read either a dehumanizing or non-dehumanizing political speech about immigrants (variable = dh_treat) and then measured participants attitudes about three separate immigration questions (variables = imm1, imm2, imm3). Without looking at the results of the original paper, think about what linear model(s) you would define (based on the variables measured) to test this research question. Make sure you consider relevant control and / or confound variables to include, and explain why these are needed. Then, define and interpret this linear model. Was dehumanizing political speech related to immigration policy attitudes? Report your results as you might in a paper or conference talk.\nLook at Table 1 from the original paper (do this after you complete Question 1!!!). You can also view the author‚Äôs code here (Note that the author used a program called STATA; the code is different from R; think you‚Äôll be able to interpret what the models are but you‚Äôll need to open this file in a text editor). How did the researcher‚Äôs linear models differ from your own? See if you can recreate the author‚Äôs results using the variables in the dataset (feel free to just focus on one model). Skim the method section; do you feel like this was a valid way to answer the question? Why / why not? What other questions do you have? (Note : I think there‚Äôs a lot to unpack about this paper! We will do so next week.)\nBefore spring break, we talked about (and learned about) interaction effects. Think of an interaction effect that you might want to test with the data (e.g., does the relationship between experimental treatment and imm1 depend on the age of the participant, or whether the participant was female, or their education level?) Report the results of this interaction effect, and graph the interaction effect. Did the relationship between the two variables (DV and IV1) change depending on IV2???\nOkay, let‚Äôs practice logistic regression. Define a genearlized linear model to predict dh_treat from some of the other variables you included in your model in Question 1. Start with just one IV, and then if you are feeling the logistic regression vibes build complexity (work up to a model with at least 3 variables) and see how the statistics change. Or you can just do one model. Organize your results in a table and graph at least the first model. Self-care lab is over. Long live self-care lab."
  },
  {
    "objectID": "gradstats/gradlabs/9Lab_LogisticRegression.html#footnotes",
    "href": "gradstats/gradlabs/9Lab_LogisticRegression.html#footnotes",
    "title": "Lab 9",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nUtych, S. M. (2018). How dehumanization influences attitudes toward immigrants. Political Research Quarterly, 71(2), 440-452.‚Ü©Ô∏é"
  },
  {
    "objectID": "gradstats/gradlabs/10_ModelComparison.html",
    "href": "gradstats/gradlabs/10_ModelComparison.html",
    "title": "Lecture 10 - Model Comparisons",
    "section": "",
    "text": "Lab 10 : Our final lab, and a review of what we have learned. Write a tutorial on how to define and interpret a regression in R. Make sure to explain each step to a future student in this class using one of the datasets posted to Dropbox (or your final project data, if you can share it.) Your tutorial should include.\n\nA clearly stated research question and theory you can test with the dataset.\nGraphing the variables needed to for your model, and doing any data cleaning or transformations needed.\nDefining Linear Model(s), and interpreting the slope(s), intercept, and \\(R^2\\) for each model. Include a graph.\nExplaining how a linear model changes when you standardize (z-score) the variables.\nDefining, testing, and interpreting an interaction effect. Include a graph.\nEstimating and interpreting sampling error of the linear model.\n(Prof asks : is this too much???)\n\nR Exam : Nah.\n\nInstead, the 10% will be assigned to a group-based (or solo) assignment.\nI give you all the same dataset and two research questions. Y‚Äôall decide how to interpret and analyze the data to answer the question and prepare a 1-2 page summary report.\nGraded based on :\n\n1) Clear description of data cleaning and outlier / removal decisions used.\n2) Defining a linear model and reporting slopes and inferential statistics in a table.\n3) Interpreting the results of the model; doing relevant model diagnostics.\n4) Putting together results in an organized report.\n5) Peer-feedback.\n\n\nOur Remaining Class Time\n\n4/11 : Recap & Review & Model Comparisons [Lab 10 Assigned]\n4/18 : Hierarchical Linear Models [Not an R Exam Assigned]\n4/25 : More Hierarchical Linear Models? PCA? We will see!!\n5/2 : Our last class can u believe it???? The learning has stopped. So review, discussion, tears of joy, project time.\n5/9 : No RRR Week Class. We done.\n\nComments / Questions / Concerns / Ideas For How To Use The Time?\n\n\n\n\n\n\n\nFinal Project Vision Board"
  },
  {
    "objectID": "gradstats/gradlabs/10_ModelComparison.html#announcements",
    "href": "gradstats/gradlabs/10_ModelComparison.html#announcements",
    "title": "Lecture 10 - Model Comparisons",
    "section": "",
    "text": "Lab 10 : Our final lab, and a review of what we have learned. Write a tutorial on how to define and interpret a regression in R. Make sure to explain each step to a future student in this class using one of the datasets posted to Dropbox (or your final project data, if you can share it.) Your tutorial should include.\n\nA clearly stated research question and theory you can test with the dataset.\nGraphing the variables needed to for your model, and doing any data cleaning or transformations needed.\nDefining Linear Model(s), and interpreting the slope(s), intercept, and \\(R^2\\) for each model. Include a graph.\nExplaining how a linear model changes when you standardize (z-score) the variables.\nDefining, testing, and interpreting an interaction effect. Include a graph.\nEstimating and interpreting sampling error of the linear model.\n(Prof asks : is this too much???)\n\nR Exam : Nah.\n\nInstead, the 10% will be assigned to a group-based (or solo) assignment.\nI give you all the same dataset and two research questions. Y‚Äôall decide how to interpret and analyze the data to answer the question and prepare a 1-2 page summary report.\nGraded based on :\n\n1) Clear description of data cleaning and outlier / removal decisions used.\n2) Defining a linear model and reporting slopes and inferential statistics in a table.\n3) Interpreting the results of the model; doing relevant model diagnostics.\n4) Putting together results in an organized report.\n5) Peer-feedback.\n\n\nOur Remaining Class Time\n\n4/11 : Recap & Review & Model Comparisons [Lab 10 Assigned]\n4/18 : Hierarchical Linear Models [Not an R Exam Assigned]\n4/25 : More Hierarchical Linear Models? PCA? We will see!!\n5/2 : Our last class can u believe it???? The learning has stopped. So review, discussion, tears of joy, project time.\n5/9 : No RRR Week Class. We done.\n\nComments / Questions / Concerns / Ideas For How To Use The Time?\n\n\n\n\n\n\n\nFinal Project Vision Board"
  },
  {
    "objectID": "gradstats/gradlabs/10_ModelComparison.html#check-in-interpreting-interaction-effects.",
    "href": "gradstats/gradlabs/10_ModelComparison.html#check-in-interpreting-interaction-effects.",
    "title": "10_ModelComparison",
    "section": "[Check-In : Interpreting Interaction Effects.]",
    "text": "[Check-In : Interpreting Interaction Effects.]\n\nHere‚Äôs a link to the dataset\nHere‚Äôs a link to a description of the dataset.\n\n\nUse this Output to answer Part 1 of the Check-In.\n\nlibrary(jtools)\nd &lt;- read.csv(\"~/Dropbox/!GRADSTATS/gradlab/Datasets/Dehumanization - Utych/dehumanization_mturk_utych.csv\", stringsAsFactors = T)\nmod4x &lt;- glm(dh_treat ~ age + gender + educ + race+ timing, data = d, family = \"binomial\")\nexport_summs(mod4x, exp = T)\n\n\n\nModel 1\n\n(Intercept)1.95¬†¬†¬†\n\n(0.61)¬†¬†\n\nage0.99¬†¬†¬†\n\n(0.01)¬†¬†\n\ngender1.30¬†¬†¬†\n\n(0.24)¬†¬†\n\neduc1.18¬†¬†¬†\n\n(0.56)¬†¬†\n\nrace0.80 **\n\n(0.07)¬†¬†\n\ntiming1.00¬†¬†¬†\n\n(0.00)¬†¬†\n\nN354¬†¬†¬†¬†¬†¬†\n\nAIC490.51¬†¬†¬†\n\nBIC513.72¬†¬†¬†\n\nPseudo R20.05¬†¬†¬†\n\n *** p &lt; 0.001;  ** p &lt; 0.01;  * p &lt; 0.05.\n\n\n\n\n\n\nUse this Graph to answer Part 2 of the Check-In.\n\n\n\n\n\n\nR Code for the graph\n\n\n\n\n\n\nh &lt;- read.csv(\"~/Dropbox/!GRADSTATS/gradlab/Datasets/World Happiness Report - 2024/World-happiness-report-2024.csv\", stringsAsFactors = T)\nlibrary(ggplot2)\nlibrary(jtools)\n\n## Some data cleaning.\nh$GDPcat &lt;- ifelse(scale(h$Log.GDP.per.capita) &gt; sd(h$Log.GDP.per.capita, na.rm = T), \"High GDP\", \"Low GDP\")\nh$GDPcat &lt;- as.factor(h$GDPcat)\nplot(h$GDPcat)\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = subset(h, !is.na(h$GDPcat)), aes(x = scale(Ladder.score), y = scale(Social.support), color = GDPcat)) + \n  geom_point(alpha = .5, position = \"jitter\") +\n  geom_smooth(method = \"lm\") + labs(title = \"Check-In Graph\") + ylab(\"Social Support\") + xlab(\"Happiness (Ladder Score)\") +\n  theme_apa()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR Code for the models"
  },
  {
    "objectID": "gradstats/gradlabs/10_ModelComparison.html#lab-9-recap",
    "href": "gradstats/gradlabs/10_ModelComparison.html#lab-9-recap",
    "title": "10_ModelComparison",
    "section": "Lab 9 Recap",
    "text": "Lab 9 Recap\n\nThe ‚ÄúDehumanization‚Äù Paper\n\nModeling Issues\n\n\nOther Issues\n\n\n\nGeneralized Linear Models"
  },
  {
    "objectID": "gradstats/gradlabs/10_ModelComparison.html#issues-to-consider-with-multiple-regression",
    "href": "gradstats/gradlabs/10_ModelComparison.html#issues-to-consider-with-multiple-regression",
    "title": "10_ModelComparison",
    "section": "Issues to Consider with Multiple Regression",
    "text": "Issues to Consider with Multiple Regression\n\n1. ‚ÄúBuild‚Äù Your Model by Increasing Complexity.\n\n\n2. Overfitting.\nWhen your model is too complex, each variable in the model (parameter) increases the model complexity.\n\ncomplex models that perfectly fit the data are problematic: you essentially describing your sample, and not the underlying population (which is usually the goal of multiple regression.)\nWe don‚Äôt expect over-fit models to generalize to other samples.\n\nTo ensure your model generalizes to other samples, you can a) replicate, or b) cross-validate (divide your sample into sub-samples; define a model on one sample, then test the model in the other(s). Lots of different ways to do this! Here‚Äôs one.)\n\n\nh &lt;- read.csv(\"~/Dropbox/!GRADSTATS/gradlab/Datasets/hormone_dataset.csv\")\nmodz &lt;- lm(narcicissm ~ ., data = h)\nsummary(modz)\n\n\nCall:\nlm(formula = narcicissm ~ ., data = h)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.81086 -0.19232  0.02552  0.18077  0.67338 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          2.480e+00  7.895e-01   3.141 0.002594 ** \nsubj                 3.148e-03  1.358e-03   2.318 0.023831 *  \nsection              1.200e-01  9.085e-02   1.321 0.191447    \nage                 -6.391e-02  1.806e-02  -3.539 0.000776 ***\nsex                 -4.649e-02  1.329e-01  -0.350 0.727696    \ntest1               -1.011e+01  5.667e+00  -1.785 0.079307 .  \ntest2               -1.012e+01  5.666e+00  -1.787 0.078925 .  \ntest_mean            2.024e+01  1.133e+01   1.786 0.079052 .  \ncortisol1            1.116e+02  5.907e+01   1.890 0.063554 .  \ncortisol2            1.100e+02  6.067e+01   1.813 0.074719 .  \ncortisol_mean       -2.194e+02  1.197e+02  -1.833 0.071639 .  \ntest.x.cort         -1.701e-02  2.362e-02  -0.720 0.474272    \ninternal_motivation -4.711e-02  7.348e-02  -0.641 0.523880    \nexternal_motivation  1.536e-01  6.159e-02   2.494 0.015351 *  \ndom_social           2.903e-01  6.716e-02   4.322  5.8e-05 ***\ndom_aggressive       1.995e-01  5.402e-02   3.692 0.000476 ***\nscore_econ_ideology -1.131e-01  4.553e-02  -2.485 0.015732 *  \nsocial_ideology     -5.902e-02  3.929e-02  -1.502 0.138187    \noverall_ideology     8.840e-02  5.825e-02   1.517 0.134313    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3313 on 61 degrees of freedom\n  (42 observations deleted due to missingness)\nMultiple R-squared:  0.704, Adjusted R-squared:  0.6167 \nF-statistic: 8.062 on 18 and 61 DF,  p-value: 2.76e-10\n\n\n\n\n3. Multicollinearity.\nIf your independent variables are highly related, then your multivariate regression slope estimates are not uniquely determined. weird things happen to your coefficients, and this makes it hard to interpret your effects.\n\nIN R : check the ‚Äúvariance inflation factor‚Äù (VIF); a measure of how much one IV is related to all the other IVs in the model.\n\\(\\huge VIF_j=\\frac{1}{1-R_{j}^{2}}\\)\n\n```{r}\nlibrary(car)\n\n```\n\n\n4. Don‚Äôt Forget to Evaluate Those Regression Assumptions\n\n\n5. Okay, But Which Model is Best?????\n\n\nWould You Like to Learn More??\n\nSome more notes on multicollinearity and VIFs.\nTake Aaron Fisher‚Äôs class on Structural Equation Modeling?\nRead through Peng Ding (Prof in Cal Stats Department) Book on Causal Inference"
  },
  {
    "objectID": "gradstats/gradlabs/10_ModelComparison.html#other-issues-to-consider-with-multiple-regression",
    "href": "gradstats/gradlabs/10_ModelComparison.html#other-issues-to-consider-with-multiple-regression",
    "title": "Lecture 10 - Model Comparisons",
    "section": "Other Issues to Consider with Multiple Regression",
    "text": "Other Issues to Consider with Multiple Regression\n\n0. Transforming Models (Quadratic Terms)\n\nggplot(h, aes(x = Ladder.score, y = Social.support)) + \n  geom_point(alpha = .5) + \n  #geom_smooth(method = \"lm\") \n  geom_smooth(method = \"lm\", formula = y ~ x + I(x^2))\n\nWarning: Removed 3 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\nlm1 &lt;- lm(Social.support ~ Ladder.score, data = h)\nsummary(lm1)\n\n\nCall:\nlm(formula = Social.support ~ Ladder.score, data = h)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.74144 -0.09781  0.00007  0.11996  0.59888 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -0.13535    0.07899  -1.714   0.0889 .  \nLadder.score  0.22956    0.01397  16.435   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1945 on 138 degrees of freedom\n  (3 observations deleted due to missingness)\nMultiple R-squared:  0.6619,    Adjusted R-squared:  0.6594 \nF-statistic: 270.1 on 1 and 138 DF,  p-value: &lt; 2.2e-16\n\nlm2 &lt;- lm(Social.support ~ Ladder.score + I(Ladder.score^2), data = h)\nsummary(lm2)\n\n\nCall:\nlm(formula = Social.support ~ Ladder.score + I(Ladder.score^2), \n    data = h)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.74531 -0.09978 -0.00104  0.11676  0.60543 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)       -0.266626   0.264845  -1.007    0.316   \nLadder.score       0.282699   0.103251   2.738    0.007 **\nI(Ladder.score^2) -0.005086   0.009791  -0.519    0.604   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.195 on 137 degrees of freedom\n  (3 observations deleted due to missingness)\nMultiple R-squared:  0.6625,    Adjusted R-squared:  0.6576 \nF-statistic: 134.5 on 2 and 137 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n1. Which Model is ‚ÄúBest‚Äù???\nWe can evaluate whether a model is an improvement (compared to the mean) by evaluating the ‚Äúfit‚Äù.\nNot enough to see an increase in \\(R^2\\), because WHY????????\n\nanova(lm1) # evaluating the model vs. the mean\n\nAnalysis of Variance Table\n\nResponse: Social.support\n              Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nLadder.score   1 10.221 10.2209   270.1 &lt; 2.2e-16 ***\nResiduals    138  5.222  0.0378                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWhen the models are ‚Äúnested‚Äù, we can compare one model to another.\n\nanova(lm1, lm2) # evaluating model 2 vs. model 1\n\nAnalysis of Variance Table\n\nModel 1: Social.support ~ Ladder.score\nModel 2: Social.support ~ Ladder.score + I(Ladder.score^2)\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1    138 5.2220                           \n2    137 5.2118  1  0.010265 0.2698 0.6043\n\n\nThe F-Distribution, Visualized\n\n#install.packages(\"sjPlot\")\nlibrary(sjPlot)\n\ndist_f(deg.f1 = 1, deg.f2 = 138) # the f-distribution for model 1\n\n\n\n\n\n\n\ndist_f(deg.f1 = 2, deg.f2 = 136) # the f-distribution for comparing model 1 and model 4\n\n\n\n\n\n\n\n\nWhen comparing the fit of one model to a completely different model, will need another method.\n\nBootstrapping. You could resample the data, define two separate models, and evaluate the model fit in each resampled dataset, compare these models, then repeat the process many times. Yeah!\nThere are many other estimates of model performance, such as cross validation (see below) or fancier ML methods I don‚Äôt know :)\n\n\n\nCLASS ENDED HERE! OTHER STUFF WILL APPEAR IN A FUTURE LECTURE. THANKS!\n\n\n2. Watch Out For Overfitting.\nWhen your model is too complex, each variable in the model (parameter) increases the model complexity.\n\ncomplex models that perfectly fit the data are problematic: you essentially describing your sample, and not the underlying population (which is usually the goal of multiple regression.)\nWe don‚Äôt expect over-fit models to generalize to other samples. [Image source]\n\n\nCross Validation. To ensure your model generalizes to other samples, you can a) replicate, or b) cross-validate your data. Cross validation involves dividing your sample into sub-samples; define a model on one sample, then test the model in the other(s).\n\nHere‚Äôs the most simple example of cross-validation (‚Äútrain-test split‚Äù; ‚Äúholdout cross validation‚Äù)\n\nsample(0:1, nrow(h), replace = T, prob = c(.7, .3)) # using the sample function\n\n  [1] 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 1\n [38] 0 1 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 1 1 0 0 1 0 0 0 0 1 0 1 0 1 1 1 0 0 0 0\n [75] 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0\n[112] 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1 0 0 1 0\n\nset.seed(424242)\nrandom.selection &lt;- sample(0:1, nrow(h), replace = T, prob = c(.7, .3))\nhtrain &lt;- h[random.selection == 0,]\nhtest &lt;- h[random.selection == 1,]\n\n## Model in training Data\ntrain.mod &lt;- lm(Social.support ~ Ladder.score * Log.GDP.per.capita, data = htrain)\nsummary(train.mod)\n\n\nCall:\nlm(formula = Social.support ~ Ladder.score * Log.GDP.per.capita, \n    data = htrain)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.70478 -0.08978 -0.00238  0.09815  0.68575 \n\nCoefficients:\n                                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                     -0.53619    0.32701  -1.640    0.105    \nLadder.score                     0.27113    0.06472   4.189 7.39e-05 ***\nLog.GDP.per.capita               0.45651    0.27441   1.664    0.100    \nLadder.score:Log.GDP.per.capita -0.05874    0.04648  -1.264    0.210    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2043 on 77 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.6821,    Adjusted R-squared:  0.6697 \nF-statistic: 55.07 on 3 and 77 DF,  p-value: &lt; 2.2e-16\n\npredict(train.mod) # the predicted values of the DV, based on our model.\n\n        1         2         4         6         7         8         9        10 \n1.5658691 1.5408531 1.5020893 1.4986799 1.4973743 1.4764175 1.4602632 1.4549315 \n       12        13        17        18        19        20        21        22 \n1.4243403 1.4373031 1.4344788 1.4128527 1.4112355 1.4030480 1.3998894 1.4102398 \n       24        29        31        34        35        37        38        39 \n1.4011533 1.3396428 1.3641704 1.3482150 1.3461128 1.3248942 1.3221398 1.3289260 \n       41        42        44        46        51        53        54        55 \n1.3314375 1.2782423 1.2902556 1.3075235 1.2863680 1.2282990 1.2373548 1.2754498 \n       58        60        61        63        65        67        68        70 \n1.2405613 1.2413826 1.1974643 1.2585895 1.2202362 1.1927948 1.2029163 1.2210060 \n       74        75        79        84        85        86        87        89 \n1.1740772 1.1404057 0.9840313 1.1276332 1.1053595 1.1804468 1.1102798 1.0129970 \n       91        93        94        97        98        99       101       102 \n1.0924736 1.0104227 1.0439202 0.9598416 1.0922332 0.9636387 1.0327200 0.9640955 \n      104       105       108       110       111       112       114       116 \n0.9457778 1.0148509 0.9220134 0.8400436 0.8920740 0.8246022 0.8768503 0.8327770 \n      117       119       120       121       122       123       127       128 \n0.8033361 0.8444944 0.8469843 0.7486051 0.7665298 0.7408530 0.8474353 0.8303265 \n      129       130       132       133       134       140       141       142 \n0.7734868 0.6925578 0.6519904 0.5952518 0.6387592 0.5175113 0.5352949 0.6073950 \n      143 \n0.1536218 \n\n## Applying the model to our testing dataset.\npredict(train.mod, newdata = htest) # produces predicted values from our training model, using the testing data.\n\n        3         5        11        14        15        16        23        25 \n1.5312384 1.4997124 1.4484716 1.4318518 1.4287474 1.4292117 1.4062990 1.3720706 \n       26        27        28        30        32        33        36        40 \n1.3650023 1.3798016 1.3789961 1.3876634 1.3514627 1.3144987 1.3447837 1.3373362 \n       43        45        47        48        49        50        52        56 \n1.2634123 1.3119962 1.2556643 1.2868124 1.2923920 1.2884980 1.2889539 1.2726265 \n       57        59        64        66        69        71        72        73 \n1.2316819 1.2574693 1.2544183 1.2250269 1.2161932 1.1997594 1.2238602 1.1740769 \n       76        77        78        80        81        82        83        90 \n1.2016273 1.1730867 1.1831393 1.1495913 1.1658541 1.1392808 1.1255433 0.9620724 \n       92        95        96       100       103       106       107       109 \n1.0555989 1.0678431 1.0118759 1.0386631        NA 0.9924835 0.9759377 0.8072950 \n      113       115       118       124       125       126       131       135 \n0.7929260 0.9196825 0.8406228 0.7647379 0.8645359 0.8175708 0.6811526 0.7280377 \n      136       137       138       139 \n0.5490131 0.7535235 0.5643139 0.4975936 \n\npredval.test &lt;- predict(train.mod, newdata = htest)  # saves these predicted values from the testing dataset.\n\n## Calculating R^2\ntest.mod.resid &lt;- htest$Social.support - predval.test\nSSE &lt;- sum(test.mod.resid^2, na.rm = T)\nSSE\n\n[1] 1.541793\n\ntest.resid &lt;- htest$Social.support - mean(htest$Social.support, na.rm = T)\nsum(test.resid^2)\n\n[1] NA\n\nSST &lt;- sum(test.resid^2, na.rm = T)\n\n(SST - SSE)/SST\n\n[1] 0.7040524\n\n\nYou‚Äôll often see a few different methods of evaluating model fit.\n\n\\(R^2\\). Our good friend. The proportion of variance explained by the model (vs.¬†the mean)\nRooted Mean Squared Error (RMSE). The average amount of residual error (actual - predicted values).\nMean Absoulte Error. The average of the absolute value of the residuals; less sensitive to outliers than RMSE or \\(R^2\\).\n\nAnd there are different methods of defining the test and training datasets. And different packages and tutorials to do this. Here‚Äôs one, called ‚ÄúLeave one out cross validation - LOOCV‚Äù; gif below via Wikipedia.\n\n\n\n\n\n\n# install.packages(\"caret\")\nlibrary(caret)\n\nLoading required package: lattice\n\ntrain.control &lt;- trainControl(method = \"LOOCV\")\nloocvmod &lt;- train(Social.support ~ Ladder.score * Log.GDP.per.capita, data = h, method = \"lm\",\n                  trControl = train.control, na.action = \"na.omit\")\n\nprint(loocvmod)\n\nLinear Regression \n\n143 samples\n  2 predictor\n\nNo pre-processing\nResampling: Leave-One-Out Cross-Validation \nSummary of sample sizes: 139, 139, 139, 139, 139, 139, ... \nResampling results:\n\n  RMSE       Rsquared   MAE      \n  0.1902603  0.6720989  0.1361974\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n\n\n\n3. Watch Out For Multicollinearity.\nIf your independent variables are highly related, then your multivariate regression slope estimates are not uniquely determined. weird things happen to your coefficients, and this makes it hard to interpret your effects.\n\nIN R : check the ‚Äúvariance inflation factor‚Äù (VIF); a measure of how much one IV is related to all the other IVs in the model. ‚ÄúTradition‚Äù is that if VIF is &gt; 5 (or I‚Äôve also seen VIF &gt; 10) there‚Äôs a problem in the regression.\n\\(\\huge VIF_j=\\frac{1}{1-R_{j}^{2}}\\)\n\n\nlibrary(car)\n\nLoading required package: carData\n\nvif(mod4) # doesn't seem like multicollinearity is a problem.\n\nthere are higher-order terms (interactions) in this model\nconsider setting type = 'predictor'; see ?vif\n\n\n                          scale(Ladder.score) \n                                     2.490104 \n                    scale(Log.GDP.per.capita) \n                                     2.443312 \nscale(Ladder.score):scale(Log.GDP.per.capita) \n                                     1.057269 \n\n## creating a highly correlated second IV.\njitter(h$Healthy.life.expectancy, 300)\n\n  [1]  0.63649887  0.68930612  0.71828952  0.66823278  0.78147940  0.64639281\n  [7]  0.66783200  0.65547462  0.71468602  0.72225337  0.71773673  0.64999642\n [13]  0.62952106  0.64185465  0.72092831  0.69836634  0.64067483  0.60323449\n [19]  0.61564585  0.70589567  0.74527554  0.54082440  0.57966955  0.74032124\n [25]  0.52293196  0.54462176  0.75101008  0.55563685  0.60367356  0.74810486\n [31]  0.69789473  0.54448689  0.56294229  0.69718620  0.62746987  0.69055692\n [37]  0.62267421  0.70380172  0.62007029  0.68723153  0.74690172  0.49826800\n [43]  0.58734907  0.50056514  0.69238462  0.52731539  0.57692015  0.64389105\n [49]  0.57280871  0.69425428  0.82612563  0.77274534  0.38400182  0.57725457\n [55]  0.70352020  0.61930331  0.55762718  0.57337490  0.53553203  0.65959492\n [61]  0.45026458          NA  0.66845212  0.69307250  0.59281784  0.52861070\n [67]  0.54621673  0.69875356  0.45710762  0.43500797  0.56482900  0.54559839\n [73]  0.50964668  0.60188374  0.54674710  0.60113762  0.41675213  0.67774238\n [79]  0.45482879  0.51207586  0.53754193  0.65166685  0.26579890  0.58060330\n [85]  0.53234986  0.85807969  0.66611649          NA  0.25647428  0.12829974\n [91]  0.51774245  0.49561179  0.47517286  0.47895859  0.35411168  0.25447142\n [97]  0.21655523  0.58171885  0.35686944  0.51373138  0.48182973  0.20760587\n[103]          NA  0.31835400  0.46658725  0.34832926  0.52310604  0.28958395\n[109]  0.33832352  0.33133514  0.42088085  0.27791840  0.16285129  0.31895572\n[115]  0.62433954  0.30866350  0.37751970  0.43665557  0.41469131  0.38656597\n[121]  0.32196352  0.25386893  0.35348747  0.26579842  0.62022055  0.43845057\n[127]  0.49687114  0.62927456  0.48772773  0.38520825  0.37719937  0.40156511\n[133]  0.33585422  0.29027768  0.12197319  0.39450247  0.18769012  0.21595348\n[139]  0.24276263  0.24909294 -0.03799866  0.57172747  0.27471988\n\nh$health2 &lt;- jitter(h$Healthy.life.expectancy, 300)\n\nplot(h$health2, h$Healthy.life.expectancy) # yup.\n\n\n\n\n\n\n\nmultimod &lt;- lm(Ladder.score ~ Healthy.life.expectancy + health2, data = h) # both in the model.\nsummary(multimod) # results! Things look good....\n\n\nCall:\nlm(formula = Ladder.score ~ Healthy.life.expectancy + health2, \n    data = h)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.0155 -0.4313  0.1701  0.5191  1.5809 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               2.6514     0.2202  12.041  &lt; 2e-16 ***\nHealthy.life.expectancy   7.5885     1.9725   3.847 0.000182 ***\nhealth2                  -2.0676     1.8604  -1.111 0.268360    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7703 on 137 degrees of freedom\n  (3 observations deleted due to missingness)\nMultiple R-squared:  0.5809,    Adjusted R-squared:  0.5747 \nF-statistic: 94.93 on 2 and 137 DF,  p-value: &lt; 2.2e-16\n\nvif(multimod) # ...but wait!\n\nHealthy.life.expectancy                 health2 \n               24.78939                24.78939 \n\n\n\n\n4. Don‚Äôt Forget to Evaluate Those Regression Assumptions\n\npar(mfrow = c(2,2))\nplot(mod4)\n\n\n\n\n\n\n\n\n\n\nWould You Like to Learn More??\n\nAn overview of ML methods (including the partitioning approach) for evaluating models\nAnother overview of cross-validation methods.\nSome more notes on multicollinearity and VIFs."
  },
  {
    "objectID": "gradstats/gradlabs/10_ModelComparison.html#presentations",
    "href": "gradstats/gradlabs/10_ModelComparison.html#presentations",
    "title": "Lecture 10 - Model Comparisons",
    "section": "Presentations",
    "text": "Presentations"
  },
  {
    "objectID": "gradstats/gradlabs/10_ModelComparison.html#check-in-interpreting-interaction-effects",
    "href": "gradstats/gradlabs/10_ModelComparison.html#check-in-interpreting-interaction-effects",
    "title": "Lecture 10 - Model Comparisons",
    "section": "Check-In : Interpreting Interaction Effects",
    "text": "Check-In : Interpreting Interaction Effects\n\nHere‚Äôs a link to the dataset\nHere‚Äôs a link to a description of the dataset.\n\n\nUse this Output to answer Part 1 of the Check-In.\n\nlibrary(jtools)\nd &lt;- read.csv(\"~/Dropbox/!GRADSTATS/gradlab/Datasets/Dehumanization - Utych/dehumanization_mturk_utych.csv\", stringsAsFactors = T)\nmod4x &lt;- glm(dh_treat ~ age + gender + educ + race+ timing, data = d, family = \"binomial\")\nexport_summs(mod4x, exp = T)\n\n\n\nModel 1\n\n(Intercept)1.95¬†¬†¬†\n\n(0.61)¬†¬†\n\nage0.99¬†¬†¬†\n\n(0.01)¬†¬†\n\ngender1.30¬†¬†¬†\n\n(0.24)¬†¬†\n\neduc1.18¬†¬†¬†\n\n(0.56)¬†¬†\n\nrace0.80 **\n\n(0.07)¬†¬†\n\ntiming1.00¬†¬†¬†\n\n(0.00)¬†¬†\n\nN354¬†¬†¬†¬†¬†¬†\n\nAIC490.51¬†¬†¬†\n\nBIC513.72¬†¬†¬†\n\nPseudo R20.05¬†¬†¬†\n\n *** p &lt; 0.001;  ** p &lt; 0.01;  * p &lt; 0.05.\n\n\n\n\n\n\nUse this Graph to answer Part 2 of the Check-In.\n\n\n\n\n\n\nR Code for the graph\n\n\n\n\n\n\nh &lt;- read.csv(\"~/Dropbox/!GRADSTATS/gradlab/Datasets/World Happiness Report - 2024/World-happiness-report-2024.csv\", stringsAsFactors = T)\nlibrary(ggplot2)\nlibrary(jtools)\n\n## Some data cleaning.\nh$GDPcat &lt;- ifelse(scale(h$Log.GDP.per.capita) &gt; sd(h$Log.GDP.per.capita, na.rm = T), \"High GDP\", \"Low GDP\")\nh$GDPcat &lt;- as.factor(h$GDPcat)\nplot(h$GDPcat)\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = subset(h, !is.na(h$GDPcat)), aes(x = scale(Ladder.score), y = scale(Social.support), color = GDPcat)) + \n  geom_point(alpha = .5, position = \"jitter\") +\n  geom_smooth(method = \"lm\") + labs(title = \"Check-In Graph\") + ylab(\"Social Support\") + xlab(\"Happiness (Ladder Score)\") +\n  theme_apa()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nInteraction Effect : social support = .08 + .25GDP + .60  Ladder - .10 * (GDP * Ladder) BLUE LINE : social support = .08 + .25-1 + .60  Ladder - .10 * (-1 * Ladder) = -.17 + .7 * Ladder RED LINE : = .08 + .251 + .60  Ladder - .10 * (1 * Ladder) = .32 + .5 * Ladder\n\n\n\n\n\n\nR Code for the models\n\n\n\n\n\n\nmod1 &lt;- lm(scale(Social.support) ~ scale(Log.GDP.per.capita), data = h)\nmod2 &lt;- lm(scale(Social.support) ~ scale(Ladder.score), data = h)\nmod3 &lt;- lm(scale(Social.support) ~ scale(Log.GDP.per.capita) + scale(Ladder.score), data = h)\nmod4 &lt;- lm(scale(Social.support) ~ scale(Ladder.score) * scale(Log.GDP.per.capita), data = h)\nexport_summs(mod1, mod2, mod3, mod4)\n\n\n\nModel 1Model 2Model 3Model 4\n\n(Intercept)0.00¬†¬†¬†¬†-0.00¬†¬†¬†¬†-0.00¬†¬†¬†¬†0.08¬†¬†¬†¬†\n\n(0.06)¬†¬†¬†(0.05)¬†¬†¬†(0.05)¬†¬†¬†(0.06)¬†¬†¬†\n\nscale(Log.GDP.per.capita)0.73 ***¬†¬†¬†¬†¬†¬†¬†0.25 **¬†0.25 **¬†\n\n(0.06)¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†(0.07)¬†¬†¬†(0.07)¬†¬†¬†\n\nscale(Ladder.score)¬†¬†¬†¬†¬†¬†¬†0.81 ***0.62 ***0.60 ***\n\n¬†¬†¬†¬†¬†¬†¬†(0.05)¬†¬†¬†(0.07)¬†¬†¬†(0.07)¬†¬†¬†\n\nscale(Ladder.score):scale(Log.GDP.per.capita)¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†-0.10 *¬†¬†\n\n¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†(0.05)¬†¬†¬†\n\nN140¬†¬†¬†¬†¬†¬†¬†140¬†¬†¬†¬†¬†¬†¬†140¬†¬†¬†¬†¬†¬†¬†140¬†¬†¬†¬†¬†¬†¬†\n\nR20.53¬†¬†¬†¬†0.66¬†¬†¬†¬†0.69¬†¬†¬†¬†0.70¬†¬†¬†¬†\n\n *** p &lt; 0.001;  ** p &lt; 0.01;  * p &lt; 0.05."
  },
  {
    "objectID": "calstats/Lectures/9_NHST.html",
    "href": "calstats/Lectures/9_NHST.html",
    "title": "Lecture 9 - Null Hypothesis Significance Testing (NHST)",
    "section": "",
    "text": "Use the ‚Äúcal_mini‚Äù dataset to test the following hypothesis : people with larger shoe sizes (IV = shoesize) are more likely to love oski (DV = oskilove).\nMake sure to do the following :\n\nLoad the dataset and check to make sure it loaded correctly.\nGraph your variables, and make sure the variables look good.\nDefine, graph, and interpret your linear model.\nReport inferential statistics (with the summary() function)."
  },
  {
    "objectID": "calstats/Lectures/9_NHST.html#check-in-r-exam-practice",
    "href": "calstats/Lectures/9_NHST.html#check-in-r-exam-practice",
    "title": "Lecture 9 - Null Hypothesis Significance Testing (NHST)",
    "section": "",
    "text": "Use the ‚Äúcal_mini‚Äù dataset to test the following hypothesis : people with larger shoe sizes (IV = shoesize) are more likely to love oski (DV = oskilove).\nMake sure to do the following :\n\nLoad the dataset and check to make sure it loaded correctly.\nGraph your variables, and make sure the variables look good.\nDefine, graph, and interpret your linear model.\nReport inferential statistics (with the summary() function)."
  },
  {
    "objectID": "calstats/Lectures/9_NHST.html#agenda-announcements",
    "href": "calstats/Lectures/9_NHST.html#agenda-announcements",
    "title": "Lecture 9 - Null Hypothesis Significance Testing (NHST)",
    "section": "Agenda & Announcements",
    "text": "Agenda & Announcements\n\nThe Final Project\n\nMilestone #3 Due Sunday. Many grumbles about this!? Think it‚Äôd be good to prioritize this, but y‚Äôall adults and technically there‚Äôs no stated late penalty for Milestone assignments, tho your GSI may not give you feedback and then you‚Äôll get behind and future you will be sad.\nMilestone #4 Due Next Friday. Define linear models to analyze your final project data! itshappening.gif\nQuestions / Comments / Concerns. How‚Äôs data collection going? Data export issues?\n\n‚ÄúMega‚Äù R Exam is In Two Weeks (4/25)\n\n85 minutes. No botched deadlines this time! DSP students get extra time.\nStudy Guide Posted (or soon).\nWill be shorter than the ‚ÄúMini Exam‚Äù.\n\nLoad data.\nDefine and interpret linear models.\nNo likert scales; focus on Chapters 5-10\n\n\nChapter 10. On Multiple Regression. More regression! It is chill.\nTHE END IS NEAR.\n\n4/11 : Inferential Stats\n4/18 : Multiple Regression + Review\n4/25 : MEGA EXAM\n5/2 : Conclusion to Introduction\n5/9 : RRR Week Final Project Workshop (VIRTUAL)"
  },
  {
    "objectID": "calstats/Lectures/9_NHST.html#chapter-9-recap-nhst-is-confusing",
    "href": "calstats/Lectures/9_NHST.html#chapter-9-recap-nhst-is-confusing",
    "title": "Lecture 9 - Null Hypothesis Significance Testing (NHST)",
    "section": "Chapter 9 Recap : NHST is Confusing!",
    "text": "Chapter 9 Recap : NHST is Confusing!\n\n\n\n\n\n\n\nChapter 9 Check-In Results\nProf.¬†Comments"
  },
  {
    "objectID": "calstats/Lectures/9_NHST.html#some-more-nhst-interpretation-examples",
    "href": "calstats/Lectures/9_NHST.html#some-more-nhst-interpretation-examples",
    "title": "Lecture 9 - Null Hypothesis Significance Testing (NHST)",
    "section": "Some More NHST Interpretation Examples",
    "text": "Some More NHST Interpretation Examples\n\nIs there a relationship between narcissism (DV = NPI) and testosterone?\n\n\nh &lt;- read.csv(\"~/Dropbox/!WHY STATS/Class Datasets/hormone_data.csv\", stringsAsFactors = T)\nmod1 &lt;- lm(NPI ~ test, data = h)\nplot(NPI ~ test, data = h)\nabline(mod1, lwd = 5)\n\n\n\n\n\n\n\nsummary(mod1)\n\n\nCall:\nlm(formula = NPI ~ test, data = h)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.29855 -0.36531 -0.02762  0.27570  1.36340 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 2.885533   0.126962  22.727   &lt;2e-16 ***\ntest        0.003887   0.001502   2.588   0.0114 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5375 on 84 degrees of freedom\n  (36 observations deleted due to missingness)\nMultiple R-squared:  0.07386,   Adjusted R-squared:  0.06284 \nF-statistic: 6.699 on 1 and 84 DF,  p-value: 0.01136\n\n\n\nIs there a relationship between narcissism (DV = NPI) and sex?\n\n\nlibrary(gplots)\n\n\nAttaching package: 'gplots'\n\n\nThe following object is masked from 'package:stats':\n\n    lowess\n\nmod2 &lt;- lm(NPI ~ sex, data = h)\nplotmeans(NPI ~ sex, data = h, connect = F)\n\n\n\n\n\n\n\nsummary(mod2)\n\n\nCall:\nlm(formula = NPI ~ sex, data = h)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.47662 -0.34342 -0.03022  0.36298  1.36618 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.01382    0.08998  33.494   &lt;2e-16 ***\nsexmale      0.26280    0.10741   2.447    0.016 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5247 on 112 degrees of freedom\n  (8 observations deleted due to missingness)\nMultiple R-squared:  0.05074,   Adjusted R-squared:  0.04226 \nF-statistic: 5.986 on 1 and 112 DF,  p-value: 0.01597\n\n\n\nIs there a relationship between testosterone (DV = test) and sex?\n\n\nmod3 &lt;- lm(test ~ sex, data = h)\nplotmeans(test ~ sex, data = h, connect = F)\n\n\n\n\n\n\n\nsummary(mod3)\n\n\nCall:\nlm(formula = test ~ sex, data = h)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-60.144 -17.211  -3.365  12.111 137.486 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   41.396      6.031   6.864 9.00e-10 ***\nsexmale       49.288      7.208   6.838 1.01e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 31.34 on 88 degrees of freedom\n  (32 observations deleted due to missingness)\nMultiple R-squared:  0.347, Adjusted R-squared:  0.3396 \nF-statistic: 46.76 on 1 and 88 DF,  p-value: 1.014e-09"
  },
  {
    "objectID": "calstats/Lectures/9_NHST.html#chapter-10-multiple-regression",
    "href": "calstats/Lectures/9_NHST.html#chapter-10-multiple-regression",
    "title": "Lecture 9 - Null Hypothesis Significance Testing (NHST)",
    "section": "Chapter 10 : Multiple Regression",
    "text": "Chapter 10 : Multiple Regression\nSo, in the models above, we see‚Ä¶.\n\nTestosterone is related to narcissism.\nSex is related to testosterone.\nSex and testosterone are related to each other‚Ä¶‚Ä¶\n\nCOULD IT BE‚Ä¶.THAT‚Ä¶..NO‚Ä¶.IT‚ÄôS TOO MUCH‚Ä¶..BUT‚Ä¶.COULD IT BE THAT‚Ä¶..\n‚Ä¶.THE REASON TESTOSTERONE SEEMS RELATED TO NARCISSISM IS REALLY BECAUSE SEX IS RELATED TO NARCISSISM??\n‚Ä¶OR MAYBE‚Ä¶.\n‚Ä¶.THE REASON SEX SEEMS RELATED TO NARCISSISM IS REALLY BECAUSE TESTOSTERONE IS RELATED TO NARCISSISM???\nMORE REGRESSION IS NEEDED!!!!\nSTAY TUNED."
  },
  {
    "objectID": "calstats/Lectures/9_NHST.html#milestone-4",
    "href": "calstats/Lectures/9_NHST.html#milestone-4",
    "title": "Lecture 9 - Null Hypothesis Significance Testing (NHST)",
    "section": "Milestone #4",
    "text": "Milestone #4"
  },
  {
    "objectID": "gradstats/gradlabs/10Lab_TeachRegression.html",
    "href": "gradstats/gradlabs/10Lab_TeachRegression.html",
    "title": "Lab 10 : Teaching is Learning",
    "section": "",
    "text": "Goal : The goal of this lab assignment is to write a tutorial that walks a hypothetical student through interpreting the results of a linear regression with an interaction effect. What this lab looks like is open-ended, but it should be a) engaging, b) transparent (so a student can follow along), and c) contains a link to the dataset that you use, so future generations of students can follow along :)\nMore specifically, your tutorial should include the following.\n\nA clearly stated research question and theory you can test with the variables in the dataset.\nGraphing the variables needed to for your model, and doing any data cleaning or transformations needed.\nDefining the needed linear model(s) to test your theory, and interpreting the slope(s), intercept, and \\(R^2\\) for your model. Include a graph for at least one model, and connect the statistics to the graph.\nChoose one of the following :\n\nExplaining how a linear model changes after some transformation (e.g., z-scoring, log the DV, adding a quadratic term to the model.)\nDefining, testing, and interpreting an interaction effect (along with a graph!) and interpreting the slope(s) and \\(R^2\\) value from this model.\n\nEstimating and interpreting sampling error of the linear models (using either bootstrapping or NHST.)\nDoing regression diagnostics (e.g., assumptions; cross-validation; etc.)\n\nFeel free to keep this as short and streamlined as possible, but the idea is that someone could read through this and learn a little bit about the theory and practice of linear regression.\nWhen you are done, please submit this tutorial to the bCourses assignment, and then upload a link to the dataset and your tutorial as a .PDF to the 205 Vision Board."
  },
  {
    "objectID": "calstats/Lectures/10_MultipleRegressionReview.html",
    "href": "calstats/Lectures/10_MultipleRegressionReview.html",
    "title": "Lecture 10 - Multiple Regression & MEGA Review",
    "section": "",
    "text": "Work with a buddy, or go lone wolf, to test the following theories using the ‚Äúcal_mini‚Äù dataset. Name the dataset d to follow along with the professor.\n\nThere is a relationship between how tired people are (DV = tired) and how bored people are (IV = bored).\nThere is a relationship between how tired people are (DV = tired) and how many hours of sleep people got (IV = hrs.sleep).\n\nYou and your buddy should each define and interpret ONE of the linear models above. Use the models to answer the questions below for the check-in?\n\nWhat is the relationship between bored and tired (in ‚Äúraw‚Äù / ‚Äúunstandardized‚Äù units)?\nIs this result statistically significant? Why / why not?\nWhat is the relationship between hrs.sleep and tired (in ‚Äúraw‚Äù / ‚Äúunstandardized‚Äù units)?\nIs this result statistically significant? Why / why not?\nWhich variable - bored or hrs.sleep - is a BETTER predictor of tired? Why / how do you know?\n\nProfessor Code\n\nd &lt;- read.csv(\"~/Dropbox/!WHY STATS/Class Datasets/cal_mini_SP25.csv\", stringsAsFactors = T)\nhead(d)\n\n  pace engaging fb.friends insta.followers insta.follows bored thirsty tired\n1    3        4          0             993           996     4      10     6\n2    3        4       2450            2800          2678     8      10    10\n3    2        5        836             918          1222     5       5     7\n4    3        4         28             716           521     4       6     8\n5    3        4         NA              NA            NA    NA      NA    NA\n6    3        3          0             172            57     7       1     9\n  satlife oskilove rlove socialmed.use class.attention hrs.sleep selfpow.data\n1       4        0     5             6               8       7.0            5\n2       7       10    10             5               5       6.0            5\n3       6        7     4             8               5       8.0            3\n4       9        0     6             6               7       7.5            7\n5      NA       NA    NA            NA              NA        NA           NA\n6       9        5     2             8               8       7.0            9\n  corppow.data success.work success.priv selfesteem catdog tuhoburat calgame\n1            7            6            9          8   dogs   turtles     Yes\n2           10            7            7          7   dogs    horses     Yes\n3            8            6            3          7   dogs   turtles     Yes\n4            6           10            0         10   cats    horses     Yes\n5           NA           NA           NA         NA                         \n6           10            9            9          8   cats    horses     Yes\n   caffeine breakfast is.female long.hair has.water shoesize height happy\n1    Rarely        No        No        No       Yes     10.0     68     8\n2 Sometimes       Yes       Yes       Yes        No     10.0     72     8\n3 Sometimes        No        No        No        No      9.5     68     6\n4     Never       Yes        No        No       Yes     10.5     68     8\n5                                                         NA     NA    NA\n6 Sometimes       Yes       Yes       Yes       Yes      7.0     63     8\n                  drink stoned72 multilingual waitlist\n1 sometimes, professor!       no          yes       no\n2     never, professor!       no          yes       no\n3 sometimes, professor!       no          yes       no\n4     never, professor!       no          yes      yes\n5                                                     \n6     never, professor!       no           no       no\n\nnrow(d)\n\n[1] 156\n\n## describing variables\nhist(d$bored)\n\n\n\n\n\n\n\nhist(d$tired)\n\n\n\n\n\n\n\nhist(d$hrs.sleep)\n\n\n\n\n\n\n\n## linear model 1 : predicting tired from bored.\nmod &lt;- lm(tired ~ bored,data=d)\nplot(jitter(tired) ~ jitter(bored),data=d)\nabline(mod, lwd = 5)\n\n\n\n\n\n\n\nplot(mod) # NOT YET!!!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncoef(mod) # intercept = 6.1351767   slope = 0.1568112 \n\n(Intercept)       bored \n  6.1351767   0.1568112 \n\n## linear model 2 : predicting tired from hrs.sleep.\n\n\nsummary(mod)\n\n\nCall:\nlm(formula = tired ~ bored, data = d)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.2920 -1.0760  0.2376  1.5512  3.7080 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  6.13518    0.39553   15.51   &lt;2e-16 ***\nbored        0.15681    0.07881    1.99   0.0484 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.297 on 151 degrees of freedom\n  (3 observations deleted due to missingness)\nMultiple R-squared:  0.02555,   Adjusted R-squared:  0.0191 \nF-statistic: 3.959 on 1 and 151 DF,  p-value: 0.04842\n\n\n\nmod2 &lt;- lm(tired ~ hrs.sleep, data = d)\nplot(tired ~ hrs.sleep, data = d)\nabline(mod2, lwd = 5)\n\n\n\n\n\n\n\n\n\nsummary(mod2)\n\n\nCall:\nlm(formula = tired ~ hrs.sleep, data = d)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.2259 -1.2039  0.2757  1.4980  3.4554 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   9.2700     0.9021  10.276   &lt;2e-16 ***\nhrs.sleep    -0.3407     0.1246  -2.734    0.007 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.275 on 150 degrees of freedom\n  (4 observations deleted due to missingness)\nMultiple R-squared:  0.04748,   Adjusted R-squared:  0.04113 \nF-statistic: 7.477 on 1 and 150 DF,  p-value: 0.007001\n\n0.04748^.5 # the square root of R2 = r = the correlation coefficient.\n\n[1] 0.2178991\n\n\n\n\n\n\n\nProfessor Code Goes Here."
  },
  {
    "objectID": "calstats/Lectures/10_MultipleRegressionReview.html#check-in-testing-theories",
    "href": "calstats/Lectures/10_MultipleRegressionReview.html#check-in-testing-theories",
    "title": "Lecture 10 - Multiple Regression & MEGA Review",
    "section": "",
    "text": "Work with a buddy, or go lone wolf, to test the following theories using the ‚Äúcal_mini‚Äù dataset. Name the dataset d to follow along with the professor.\n\nThere is a relationship between how tired people are (DV = tired) and how bored people are (IV = bored).\nThere is a relationship between how tired people are (DV = tired) and how many hours of sleep people got (IV = hrs.sleep).\n\nYou and your buddy should each define and interpret ONE of the linear models above. Use the models to answer the questions below for the check-in?\n\nWhat is the relationship between bored and tired (in ‚Äúraw‚Äù / ‚Äúunstandardized‚Äù units)?\nIs this result statistically significant? Why / why not?\nWhat is the relationship between hrs.sleep and tired (in ‚Äúraw‚Äù / ‚Äúunstandardized‚Äù units)?\nIs this result statistically significant? Why / why not?\nWhich variable - bored or hrs.sleep - is a BETTER predictor of tired? Why / how do you know?\n\nProfessor Code\n\nd &lt;- read.csv(\"~/Dropbox/!WHY STATS/Class Datasets/cal_mini_SP25.csv\", stringsAsFactors = T)\nhead(d)\n\n  pace engaging fb.friends insta.followers insta.follows bored thirsty tired\n1    3        4          0             993           996     4      10     6\n2    3        4       2450            2800          2678     8      10    10\n3    2        5        836             918          1222     5       5     7\n4    3        4         28             716           521     4       6     8\n5    3        4         NA              NA            NA    NA      NA    NA\n6    3        3          0             172            57     7       1     9\n  satlife oskilove rlove socialmed.use class.attention hrs.sleep selfpow.data\n1       4        0     5             6               8       7.0            5\n2       7       10    10             5               5       6.0            5\n3       6        7     4             8               5       8.0            3\n4       9        0     6             6               7       7.5            7\n5      NA       NA    NA            NA              NA        NA           NA\n6       9        5     2             8               8       7.0            9\n  corppow.data success.work success.priv selfesteem catdog tuhoburat calgame\n1            7            6            9          8   dogs   turtles     Yes\n2           10            7            7          7   dogs    horses     Yes\n3            8            6            3          7   dogs   turtles     Yes\n4            6           10            0         10   cats    horses     Yes\n5           NA           NA           NA         NA                         \n6           10            9            9          8   cats    horses     Yes\n   caffeine breakfast is.female long.hair has.water shoesize height happy\n1    Rarely        No        No        No       Yes     10.0     68     8\n2 Sometimes       Yes       Yes       Yes        No     10.0     72     8\n3 Sometimes        No        No        No        No      9.5     68     6\n4     Never       Yes        No        No       Yes     10.5     68     8\n5                                                         NA     NA    NA\n6 Sometimes       Yes       Yes       Yes       Yes      7.0     63     8\n                  drink stoned72 multilingual waitlist\n1 sometimes, professor!       no          yes       no\n2     never, professor!       no          yes       no\n3 sometimes, professor!       no          yes       no\n4     never, professor!       no          yes      yes\n5                                                     \n6     never, professor!       no           no       no\n\nnrow(d)\n\n[1] 156\n\n## describing variables\nhist(d$bored)\n\n\n\n\n\n\n\nhist(d$tired)\n\n\n\n\n\n\n\nhist(d$hrs.sleep)\n\n\n\n\n\n\n\n## linear model 1 : predicting tired from bored.\nmod &lt;- lm(tired ~ bored,data=d)\nplot(jitter(tired) ~ jitter(bored),data=d)\nabline(mod, lwd = 5)\n\n\n\n\n\n\n\nplot(mod) # NOT YET!!!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncoef(mod) # intercept = 6.1351767   slope = 0.1568112 \n\n(Intercept)       bored \n  6.1351767   0.1568112 \n\n## linear model 2 : predicting tired from hrs.sleep.\n\n\nsummary(mod)\n\n\nCall:\nlm(formula = tired ~ bored, data = d)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.2920 -1.0760  0.2376  1.5512  3.7080 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  6.13518    0.39553   15.51   &lt;2e-16 ***\nbored        0.15681    0.07881    1.99   0.0484 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.297 on 151 degrees of freedom\n  (3 observations deleted due to missingness)\nMultiple R-squared:  0.02555,   Adjusted R-squared:  0.0191 \nF-statistic: 3.959 on 1 and 151 DF,  p-value: 0.04842\n\n\n\nmod2 &lt;- lm(tired ~ hrs.sleep, data = d)\nplot(tired ~ hrs.sleep, data = d)\nabline(mod2, lwd = 5)\n\n\n\n\n\n\n\n\n\nsummary(mod2)\n\n\nCall:\nlm(formula = tired ~ hrs.sleep, data = d)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.2259 -1.2039  0.2757  1.4980  3.4554 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   9.2700     0.9021  10.276   &lt;2e-16 ***\nhrs.sleep    -0.3407     0.1246  -2.734    0.007 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.275 on 150 degrees of freedom\n  (4 observations deleted due to missingness)\nMultiple R-squared:  0.04748,   Adjusted R-squared:  0.04113 \nF-statistic: 7.477 on 1 and 150 DF,  p-value: 0.007001\n\n0.04748^.5 # the square root of R2 = r = the correlation coefficient.\n\n[1] 0.2178991\n\n\n\n\n\n\n\nProfessor Code Goes Here."
  },
  {
    "objectID": "calstats/Lectures/10_MultipleRegressionReview.html#agenda-announcements",
    "href": "calstats/Lectures/10_MultipleRegressionReview.html#agenda-announcements",
    "title": "Lecture 10 - Multiple Regression & MEGA Review",
    "section": "Agenda & Announcements",
    "text": "Agenda & Announcements\n\nMEGA R Exam is NEXT WEEK.\n\nSame format as Mini Exam. Take home, open note, on your own, make sure you adhere to ChatGPT policy (clearly stated that you used with complete documentation of your searches & the output in an appendix.)\n85 Minutes (start at 2:10 PM ‚Äì&gt; submit by 3:25 PM)\n\n128 Minutes for 150% DSP (will be marked as LATE so reminder reader you have DSP!)\n170 Minutes of 200% DSP (will be marked as LATE so reminder reader you have DSP!)\n\nWorth 24% of Your Grade; 15 Points on the Exam\n\nData Cleaning & Descriptive Stats [3 Points]\nDefining and Interpreting Linear Models [7 Points]\nInterpreting NHST Output [3 Points]\nInterpreting Multiple Regression [2 Points]\n\n\nLab 8 : Take a practice exam & submit.\nMilestone #4 : Due Sunday.\nMilestone #5 : Draft!\n\nDue Friday, May 2nd at 2:00 PM\nBring it all together!!!!\n\nTHE END IS NEAR\n\n5/2 : Last Class : The learning has stopped. So what did we learn again?\n5/9 : RRR Week (Virtual) : Project Workshop"
  },
  {
    "objectID": "calstats/Lectures/10_MultipleRegressionReview.html#review-multiple-regression-hair-length-predicts-height",
    "href": "calstats/Lectures/10_MultipleRegressionReview.html#review-multiple-regression-hair-length-predicts-height",
    "title": "Lecture 10 - Multiple Regression & MEGA Review",
    "section": "REVIEW : Multiple Regression (Hair Length Predicts Height?!?!?????)",
    "text": "REVIEW : Multiple Regression (Hair Length Predicts Height?!?!?????)\n\nData Cleaning and Descriptive Statistics\n\n## Data Cleaning\nlibrary(gplots)\n\n\nAttaching package: 'gplots'\n\n\nThe following object is masked from 'package:stats':\n\n    lowess\n\nd &lt;- read.csv(\"~/Dropbox/!WHY STATS/Class Datasets/cal_mini_SP25.csv\", stringsAsFactors = T)\n\nd$height[d$height &lt; 10 | d$height &gt; 100] &lt;- NA\nlevels(d$is.female)[1] &lt;- NA\nlevels(d$long.hair)[1] &lt;- NA\n\npar(mfrow = c(1,3))\nhist(d$height)\nplot(d$is.female, xlab = \"Is Female?\")\nplot(d$long.hair, xlab = \"Has Long Hair?\")\n\n\n\n\n\n\n\n\n\n\nActivity and Discussion : Comparing Models\n\nICE-BREAKER :\n\nlet‚Äôs keep it light mode : if you HAD to get a tattoo, what would you get? where would you get it? would it face toward you or other people?\nbring on the heavy mode : if you could change one thing about your childhood, what would you change?\n\nMODEL INTERPRETATION : What Do You Observe in Model 1? Model 2?\nMODEL 3 : What Do You Observe Changing About the Slopes from the Bivariate Model (Models 1 and Model 2) to the Multivariate Model (Model 3)?\nOther Questions That You, the Students, Have?\n\n\nHeight ~ long.hairHeight ~ is.femaleheight ~ long.hair + is.female\n\n\n\nplot(d$long.hair)\n\n\n\n\n\n\n\nmoda &lt;- lm(height ~ long.hair, data = d)\nsummary(moda)\n\n\nCall:\nlm(formula = height ~ long.hair, data = d)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-14.2143  -2.4129  -0.2143   2.5871  11.7857 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   66.2143     0.5501 120.361   &lt;2e-16 ***\nlong.hairYes  -1.8013     0.7085  -2.542   0.0121 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.117 on 139 degrees of freedom\n  (15 observations deleted due to missingness)\nMultiple R-squared:  0.04443,   Adjusted R-squared:  0.03756 \nF-statistic: 6.463 on 1 and 139 DF,  p-value: 0.01211\n\nplotmeans(height ~ long.hair, data = d, connect = F)\n\n\n\n\n\n\n\n\n\n\n\nmodb &lt;- lm(height ~ is.female, data = d)\nsummary(modb)\n\n\nCall:\nlm(formula = height ~ is.female, data = d)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-11.833  -1.833  -0.425   2.167   9.575 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   68.4250     0.5799 117.993  &lt; 2e-16 ***\nis.femaleYes  -4.5923     0.6852  -6.702 4.72e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.668 on 139 degrees of freedom\n  (15 observations deleted due to missingness)\nMultiple R-squared:  0.2442,    Adjusted R-squared:  0.2388 \nF-statistic: 44.92 on 1 and 139 DF,  p-value: 4.724e-10\n\nplotmeans(height ~ is.female, data = d, connect = F)\n\n\n\n\n\n\n\n\n\n\n\nmodc &lt;- lm(height ~ long.hair + is.female, data = d)\nsummary(modc)\n\n\nCall:\nlm(formula = height ~ long.hair + is.female, data = d)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.2631  -2.0006  -0.2671   1.9994   9.7288 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   68.2712     0.6010 113.599  &lt; 2e-16 ***\nlong.hairYes   0.7375     0.7699   0.958     0.34    \nis.femaleYes  -5.0080     0.8414  -5.952  2.1e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.68 on 137 degrees of freedom\n  (16 observations deleted due to missingness)\nMultiple R-squared:  0.243, Adjusted R-squared:  0.2319 \nF-statistic: 21.99 on 2 and 137 DF,  p-value: 5.226e-09\n\n\n\n\n\n\n\nIV1 and IV2 are related to each other, and each related to the DV)\n\nplot(d$long.hair ~ d$is.female)\n\n\n\n\n\n\n\n\n\n\nMultiple Regression : Visualized in Multi-Dimensional Space!\nThe code below may not work on your computer; see lecture recording for an interpretation / explanation!\n\n#install.packages('rgl')\n#install.packages('car')\nlibrary(car)\nlibrary(rgl)\n\nscatter3d(as.numeric(d$is.female), # IV1 - must be numeric (if not already)\n          d$height, # DV\n          as.numeric(d$long.hair)) # IV2 - must be numeric (if not already)\n\n\nReporting Effects in a Regression Table.\nTable 1. Unstandardized Regression Coefficients; Predicting Height from Long.Hair and Is.Female.\n\n\n\n\nModel 1\nModel 2\nModel 3\n\n\n\n\nIntercept\n\n\n\n\n\nLong.Hair (0 = No; 1 = Yes)\n\n\n\n\n\nIs.Female (0 = No; 1 = Yes)\n\n\n\n\n\n\\(R^2\\)\n\n\n\n\n\n\nThere‚Äôs a Package in R For This!\n\n# install.packages(\"jtools\") # a new package!!!\nlibrary(jtools) # make sure you installed the new package first.\n\nWarning: package 'jtools' was built under R version 4.3.3\n\nexport_summs(moda, modb, modc,\n             coefs = c(\"Long Hair (0 = No, 1 = Yes)\" = \"long.hairYes\",\n                       \"Is Female (0 = No, 1 = Yes)\" = \"is.femaleYes\"))\n\n\n\nModel 1Model 2Model 3\n\nLong Hair (0 = No, 1 = Yes)-1.80 *¬†¬†¬†¬†¬†¬†¬†0.74¬†¬†¬†¬†\n\n(0.71)¬†¬†¬†¬†¬†¬†¬†¬†(0.77)¬†¬†¬†\n\nIs Female (0 = No, 1 = Yes)¬†¬†¬†¬†¬†-4.59 ***-5.01 ***\n\n¬†¬†¬†¬†¬†(0.69)¬†¬†¬†(0.84)¬†¬†¬†\n\nN141¬†¬†¬†¬†¬†141¬†¬†¬†¬†¬†¬†¬†140¬†¬†¬†¬†¬†¬†¬†\n\nR20.04¬†¬†0.24¬†¬†¬†¬†0.24¬†¬†¬†¬†\n\n *** p &lt; 0.001;  ** p &lt; 0.01;  * p &lt; 0.05."
  },
  {
    "objectID": "calstats/Lectures/10_MultipleRegressionReview.html#break-time-meet-back-at-345",
    "href": "calstats/Lectures/10_MultipleRegressionReview.html#break-time-meet-back-at-345",
    "title": "Lecture 10 - Multiple Regression & MEGA Review",
    "section": "BREAK TIME : MEET BACK AT 3:45",
    "text": "BREAK TIME : MEET BACK AT 3:45"
  },
  {
    "objectID": "calstats/Lectures/10_MultipleRegressionReview.html#multiple-regression-tables",
    "href": "calstats/Lectures/10_MultipleRegressionReview.html#multiple-regression-tables",
    "title": "Lecture 10 - Multiple Regression & MEGA Review",
    "section": "Multiple Regression Tables",
    "text": "Multiple Regression Tables\n\nICE BREAKER : If you could learn one skill all at once (like Neo in the Matrix; have y‚Äôall seen that movie???), what would it be??\nEvaluate the table above and think about multiple regression.\n\nhow would you write out Model 1, Model 2, Model 3, etc.\n\nModel 1 :\n\nwhat do we learn from this table / what changes in slope seem important (and why)?¬†\nwhat are some other real-life (and psychological) examples of situations where a 3rd variable might be important to study??\nwhat is the multivariate regression you will include in your final project? why might this be interesting to test?"
  },
  {
    "objectID": "calstats/Lectures/10_MultipleRegressionReview.html#milestone-4-anyone-want-to-share-their-example",
    "href": "calstats/Lectures/10_MultipleRegressionReview.html#milestone-4-anyone-want-to-share-their-example",
    "title": "Lecture 10 - Multiple Regression & MEGA Review",
    "section": "Milestone #4 : Anyone want to share their example?",
    "text": "Milestone #4 : Anyone want to share their example?\n\nStudent Work Go Here."
  },
  {
    "objectID": "calstats/Lectures/10_MultipleRegressionReview.html#other-exam-questions-practice",
    "href": "calstats/Lectures/10_MultipleRegressionReview.html#other-exam-questions-practice",
    "title": "Lecture 10 - Multiple Regression & MEGA Review",
    "section": "Other Exam Questions / Practice",
    "text": "Other Exam Questions / Practice\n\nMiscelaneous Student Questions Go Here\n\nnames(d)\n\n [1] \"pace\"            \"engaging\"        \"fb.friends\"      \"insta.followers\"\n [5] \"insta.follows\"   \"bored\"           \"thirsty\"         \"tired\"          \n [9] \"satlife\"         \"oskilove\"        \"rlove\"           \"socialmed.use\"  \n[13] \"class.attention\" \"hrs.sleep\"       \"selfpow.data\"    \"corppow.data\"   \n[17] \"success.work\"    \"success.priv\"    \"selfesteem\"      \"catdog\"         \n[21] \"tuhoburat\"       \"calgame\"         \"caffeine\"        \"breakfast\"      \n[25] \"is.female\"       \"long.hair\"       \"has.water\"       \"shoesize\"       \n[29] \"height\"          \"happy\"           \"drink\"           \"stoned72\"       \n[33] \"multilingual\"    \"waitlist\"       \n\nlibrary(gplots)\nplot(d$tuhoburat)\n\n\n\n\n\n\n\nlevels(d$tuhoburat)[1] &lt;- NA\nmod1 &lt;- lm(hrs.sleep ~ tuhoburat, data = d)\nplotmeans(hrs.sleep ~ tuhoburat, data = d, connect = F)"
  },
  {
    "objectID": "gradstats/gradlabs/11_MLM.html#announcements",
    "href": "gradstats/gradlabs/11_MLM.html#announcements",
    "title": "Lecture 11 - Multilevel Models (MLM)",
    "section": "Announcements",
    "text": "Announcements\n\nFinal Project\n\nDefine a dataset [Milestone 1!]\nPre-register your analyses\n\nForgot to instruct y‚Äôall to do this!\nGoals of Pre-registration :\n\nThink carefully and clearly about your dataset (outliers, measures, models)\nBe consistent in your analyses; keep things simple.\nWhen you deviate from your analysis plan, that‚Äôs okay! You are learning and exploring. Just good to recognize and state this.\n\n\n\nThis is Not an Exam\n\nDiscussed at the End of Class!\nWill be chill? I hope. There‚Äôs a rubric that emphasizes trying.\n\nThe End Is Near.\n\n4/18 : Multilevel Models (MORE LINES)\n4/25 : Exam Recap + More on Multilevel Models\n5/2 : Conclusion (The Learning Has Stopped; Would You Like to Know More?)"
  },
  {
    "objectID": "gradstats/gradlabs/11_MLM.html#multilevel-models-mlm-conceptual-understanding",
    "href": "gradstats/gradlabs/11_MLM.html#multilevel-models-mlm-conceptual-understanding",
    "title": "Lecture 11 - Multilevel Models (MLM)",
    "section": "Multilevel Models (MLM) : Conceptual Understanding",
    "text": "Multilevel Models (MLM) : Conceptual Understanding\n\nDefinition\n\nMultilevel Models (MLM)\n\nHierarchical linear models, mixed effects models, random effects models, etc.\nMODELS MODELS MODELS : It‚Äôs just a line! Lots of lines. (Lots of lines models?)\n\n\n\n\nSome Pictures\n\n\n\n\n‚ÄúFixed Effects‚Äù\n‚ÄúRandom Effects‚Äù\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Formulas\n\nThe Linear Model Equation\n\n\\[\\huge y = \\beta_0 + \\beta_1x_1 + ... + \\beta_kx_k + \\epsilon\\]\n\nThe MLM Equation (Level Notation)\n\n\n\n\n\nWhy Are We Doing This?\n\nThe Assumption of Independence Has Been Violated! (MLM increases our power and reliability as scientists.)\n\nMultiple measures of an individual gives you a more reliable estimate of what and who they are.\nA person serves as their own control; examining how an individual changes over time (as a result of some other variable or an experimental manipulation)\n\nBetter than a purely ‚Äúfixed effects‚Äù approach.\n\nWhile we could just account for group variation by adding this to our model as dummy-coded group identifiers‚Ä¶\n‚Ä¶the MLM results in a simpler model (less coefficients; we just allow the intercepts and slopes to vary)\n\nModel more complex phenomenon.\n\nHow people change over time (within-person variation).\nSimpson‚Äôs Paradox\nOther Examples?"
  },
  {
    "objectID": "gradstats/gradlabs/11_MLM.html#how-do-we-do-this-in-r",
    "href": "gradstats/gradlabs/11_MLM.html#how-do-we-do-this-in-r",
    "title": "Lecture 11 - Multilevel Models (MLM)",
    "section": "How Do We Do This in R?",
    "text": "How Do We Do This in R?\n\nExample 1 : The Sleep Dataset\nFrom the ?sleep dataset; ‚ÄúData which show the effect of two soporific drugs (increase in hours of sleep compared to control).‚Äù\n\nExtra : increase in hours of sleep\nGroup : drug given (1 = control; 2 = drug)\nID : patient ID\n\n\nThe Linear Model (a ‚ÄúBetween Person‚Äù Study)\n\nlibrary(ggplot2)\nggplot(sleep, aes(y = extra, x = group)) + \n  geom_point(size=2) + \n  stat_summary(fun.data=mean_se, color = 'red', size = 1.25, linewidth = 2)\n\n\n\n\n\n\n\nlmod &lt;- lm(extra ~ as.factor(group), data = sleep)\nsummary(lmod)\n\n\nCall:\nlm(formula = extra ~ as.factor(group), data = sleep)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-2.430 -1.305 -0.580  1.455  3.170 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)         0.7500     0.6004   1.249   0.2276  \nas.factor(group)2   1.5800     0.8491   1.861   0.0792 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.899 on 18 degrees of freedom\nMultiple R-squared:  0.1613,    Adjusted R-squared:  0.1147 \nF-statistic: 3.463 on 1 and 18 DF,  p-value: 0.07919\n\n\n\n\nThe Linear Model, with ID as a grouping factor (a ‚ÄúWithin-Person‚Äù Study)\nMany linear models! Look at the graph below. What‚Äôs going on? What do you observe? How might this help us understand the relationship between these two variables?\n\nggplot(sleep, aes(y = extra, x = group, color = ID)) + \n  geom_point(size=2) + \n  geom_line(aes(group = ID), linewidth = 0.75)\n\n\n\n\n\n\n\n\nRandom Intercepts : Still just one equation. But a lot more lines!\n\n#install.packages(\"lme4\")\nlibrary(lme4)\n\nLoading required package: Matrix\n\nlibrary(lmerTest)\n\n\nAttaching package: 'lmerTest'\n\n\nThe following object is masked from 'package:lme4':\n\n    lmer\n\n\nThe following object is masked from 'package:stats':\n\n    step\n\nlibrary(Matrix)\nmlmod &lt;- lmer(extra ~ as.factor(group) + (1 | ID), data = sleep)\nsummary(mlmod)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: extra ~ as.factor(group) + (1 | ID)\n   Data: sleep\n\nREML criterion at convergence: 70\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-1.63372 -0.34157  0.03346  0.31511  1.83859 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n ID       (Intercept) 2.8483   1.6877  \n Residual             0.7564   0.8697  \nNumber of obs: 20, groups:  ID, 10\n\nFixed effects:\n                  Estimate Std. Error      df t value Pr(&gt;|t|)   \n(Intercept)         0.7500     0.6004 11.0814   1.249  0.23735   \nas.factor(group)2   1.5800     0.3890  9.0000   4.062  0.00283 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\nas.fctr(g)2 -0.324\n\n\n\nFixed Effects : The ‚ÄúAverage‚Äù across all the grouping variables. Our friend the linear model!\n\nIntercept : ???? (discussed in lecture!)\nSlope : ???? (discussed in lecture!)\nCorrelation of Fixed Effects : How our intercept and slope are related to each other. ????? (discussed in lecture!)\n\nRandom Effects :\n\nICC = Intraclass Correlation Coefficient = how much the variation in our grouping variable (here : subject) explains total variation.\nTo calculate : take variance of intercept / total variance\n\n2.8483 / (2.8483 + 0.7564)\n\n[1] 0.7901628\n\n\nOR :\n\nlibrary(performance)\nicc(mlmod)\n\n# Intraclass Correlation Coefficient\n\n    Adjusted ICC: 0.790\n  Unadjusted ICC: 0.668\n\n\n\n\nMore about those random effects. We can examine them for the individuals in the study. They are adjustments to the intercept (people start with different baselines of sleep.)\n\nfixef(mlmod)\n\n      (Intercept) as.factor(group)2 \n             0.75              1.58 \n\nranef(mlmod)\n\n$ID\n   (Intercept)\n1   -0.2118668\n2   -1.7125900\n3   -0.9622284\n4   -1.8450067\n5   -1.4477565\n6    2.0833569\n7    2.7013017\n8   -0.3001446\n9    0.6709115\n10   1.0240229\n\nwith conditional variances for \"ID\" \n\nvar(ranef(mlmod)$ID)\n\n            (Intercept)\n(Intercept)    2.514447\n\n\n\n\nBut Professor, Where are the S***s???\nBy default, lmer does not run statistical tests. I heard this was because the author of the package was philosophically opposed to them, but I think it‚Äôs also because there are continued debates about how best to calculate and interpret p-values for statistics that, by definition, can vary.\nYou can report confidence intervals from the results of the lmer model.\n\nconfint(mlmod)\n\nComputing profile confidence intervals ...\n\n\n                       2.5 %   97.5 %\n.sig01             0.9660521 2.786017\n.sigma             0.5627986 1.377856\n(Intercept)       -0.4630090 1.963009\nas.factor(group)2  0.7814283 2.378572\n\n\nHowever, if you really want the stars, there‚Äôs another package that adds the stars, and gives some other useful features.\n\n#install.packages(\"lmerTest\")\nlibrary(lmerTest) # note that the function lmer from package lme4 has been masked.\nmlmod &lt;- lmer(extra ~ as.factor(group) + (1 | ID), data = sleep) # the same model; same equation\nsummary(mlmod) # new output!\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: extra ~ as.factor(group) + (1 | ID)\n   Data: sleep\n\nREML criterion at convergence: 70\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-1.63372 -0.34157  0.03346  0.31511  1.83859 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n ID       (Intercept) 2.8483   1.6877  \n Residual             0.7564   0.8697  \nNumber of obs: 20, groups:  ID, 10\n\nFixed effects:\n                  Estimate Std. Error      df t value Pr(&gt;|t|)   \n(Intercept)         0.7500     0.6004 11.0814   1.249  0.23735   \nas.factor(group)2   1.5800     0.3890  9.0000   4.062  0.00283 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\nas.fctr(g)2 -0.324\n\nranova(mlmod) # a way to test whether inclusion of random effect improves the model fit or not.\n\nANOVA-like table for random-effects: Single term deletions\n\nModel:\nextra ~ as.factor(group) + (1 | ID)\n         npar  logLik    AIC    LRT Df Pr(&gt;Chisq)   \n&lt;none&gt;      4 -34.978 77.956                        \n(1 | ID)    3 -39.384 84.768 8.8118  1   0.002993 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ndetach(\"package:lmerTest\") # won't be using this?\n\n\n\nA Neat Thing : The ‚ÄúPaired T-Test‚Äù is Just a Narrow Form of the MLM\n\nd &lt;- sleep # copying the data\nsleepwide &lt;- data.frame(d[1:10,1], d[11:20,1]) # moving into wide format\nnames(sleepwide) &lt;- c(\"Extra1\", \"Extra2\") # renaming variables\nsleepwide # new data; in the \"wide\" format.\n\n   Extra1 Extra2\n1     0.7    1.9\n2    -1.6    0.8\n3    -0.2    1.1\n4    -1.2    0.1\n5    -0.1   -0.1\n6     3.4    4.4\n7     3.7    5.5\n8     0.8    1.6\n9     0.0    4.6\n10    2.0    3.4\n\nt.test(sleepwide$Extra1, sleepwide$Extra2, paired = T) # comparing mean of T1 to mean of T2, assuming a paired distribution....\n\n\n    Paired t-test\n\ndata:  sleepwide$Extra1 and sleepwide$Extra2\nt = -4.0621, df = 9, p-value = 0.002833\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -2.4598858 -0.7001142\nsample estimates:\nmean difference \n          -1.58 \n\n\n\n\nWhat About Random Slopes?\nFor random intercepts and random slopes : Still just one equation‚Ä¶.but‚Ä¶.too many lines for the model to converge.\n\n#mlmod2 &lt;- lmer(extra ~ as.factor(group) + (group | ID), data = sleep)\n\n\n\n\nExample 2 : Another Sleep Dataset\nA classic teaching dataset from lmer. Hooray!\n\n?sleepstudy\ns &lt;- sleepstudy\n\nA Question : Is there a relationship between number of days of sleep deprivation and reaction time?\n\n\nThe Graph\nHow might we graph this in ggplot?\n\nlibrary(ggplot2)\nggplot(sleepstudy, aes(y = Reaction, x = Days, color = Subject)) + \n  geom_point(size=2) + \n  # facet_wrap(~Subject) + \n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n  # geom_line(aes(group = Subject), linewidth = 0.75)\n\n\n\nInterpreting the Model (Random Intercepts)\nWhat model would we define?\n\nReaction is the DV\nI‚Äôm adding Days as a Fixed IV (so I‚Äôll get the average effect of # of days of sleep deprivation on reaction time)\nI‚Äôm also adding a random intercept : (1 | Subject) that will estimate how much the intercept (the 1 term) of individual raction times (the level 2 variable) varies by Subject (the level 1 grouping variable).\n\n\nlibrary(lme4)\nl2 &lt;- lmer(Reaction ~ Days + (1 | Subject), data = sleepstudy)\nsummary(l2)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: Reaction ~ Days + (1 | Subject)\n   Data: sleepstudy\n\nREML criterion at convergence: 1786.5\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.2257 -0.5529  0.0109  0.5188  4.2506 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n Subject  (Intercept) 1378.2   37.12   \n Residual              960.5   30.99   \nNumber of obs: 180, groups:  Subject, 18\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept) 251.4051     9.7467   25.79\nDays         10.4673     0.8042   13.02\n\nCorrelation of Fixed Effects:\n     (Intr)\nDays -0.371\n\n\nHow do we interpret the results of this model?\n\nFixed Effects : these deal with the ‚Äúaverage‚Äù effects - ignoring all those important individual differences (which are accounted for in the random effects.)\n\nIntercept = 251.41 = the average person‚Äôs reaction time at 0 days of sleep deprivation is 251.4 milliseconds.\nDays = 10.47 = for every day of sleep deprivation, the average person‚Äôs reaction time increases by 10.47 MS; the standard error is an estimate of how much variation we‚Äôd expect in this average slope due\n\nRandom Effects : these describe those individual differences; specifcally the\n\nSubject (Intercept) = 37.12\nResidual = 30.99\n\n\n\n\nInterpreting the Model (Random Intercepts and Slopes)\nWhat model would we define?\n\nlmer(Reaction ~ Days + (Days | Subject), data = sleepstudy)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: Reaction ~ Days + (Days | Subject)\n   Data: sleepstudy\nREML criterion at convergence: 1743.628\nRandom effects:\n Groups   Name        Std.Dev. Corr\n Subject  (Intercept) 24.741       \n          Days         5.922   0.07\n Residual             25.592       \nNumber of obs: 180, groups:  Subject, 18\nFixed Effects:\n(Intercept)         Days  \n     251.41        10.47  \n\n\nHow do we interpret the results of this model?"
  },
  {
    "objectID": "gradstats/gradlabs/11_MLM.html#would-you-like-to-learn-more",
    "href": "gradstats/gradlabs/11_MLM.html#would-you-like-to-learn-more",
    "title": "Lecture 11 - Multilevel Models (MLM)",
    "section": "Would You Like to Learn More?",
    "text": "Would You Like to Learn More?\n\nATTEND NEXT WEEK : more on mixed effects (centering; cross-level interactions, and more!)\nA Nice Overview, For and By Psychologists\nA More In-Depth Mini-Textbook\nA Nice Overview of the sleepstudy\nTAKE A CLASS :\n\nSophia Rabe-Hasketh in the Education Department [Stata]\nAaron Fisher is offering a class maybe?"
  },
  {
    "objectID": "gradstats/gradlabs/11_MLM.html#presentation",
    "href": "gradstats/gradlabs/11_MLM.html#presentation",
    "title": "Lecture 11 - Multilevel Models (MLM)",
    "section": "Presentation",
    "text": "Presentation"
  },
  {
    "objectID": "gradstats/gradlabs/11_MLM.html#this-is-not-an-exam",
    "href": "gradstats/gradlabs/11_MLM.html#this-is-not-an-exam",
    "title": "Lecture 11 - Multilevel Models (MLM)",
    "section": "This is Not an Exam",
    "text": "This is Not an Exam\n\nLook over the assignment.\n\nQuestions about the task?\nQuestions about the grading?\nQuestions about the dataset?\nQuestions about the research question?\n\nData will be posted during the break :)"
  },
  {
    "objectID": "calstats/Lectures/10_MultipleRegressionReview.html#break-time-meet-back-at-330",
    "href": "calstats/Lectures/10_MultipleRegressionReview.html#break-time-meet-back-at-330",
    "title": "Lecture 10 - Multiple Regression & MEGA Review",
    "section": "BREAK TIME : MEET BACK AT 3:30",
    "text": "BREAK TIME : MEET BACK AT 3:30"
  },
  {
    "objectID": "calstats/Lectures/10_MultipleRegressionReview.html#more-multiple-regression",
    "href": "calstats/Lectures/10_MultipleRegressionReview.html#more-multiple-regression",
    "title": "Lecture 10 - Multiple Regression & MEGA Review",
    "section": "MORE MULTIPLE REGRESSION",
    "text": "MORE MULTIPLE REGRESSION\n\nlibrary(jtools)\nmod1 &lt;- lm(tired ~ bored,data=d)\nmod2 &lt;- lm(tired ~ hrs.sleep,data=d)\nmod3 &lt;- lm(tired ~ bored + hrs.sleep,data=d)\nexport_summs(mod1, mod2, mod3)\n\n\n\nModel 1Model 2Model 3\n\n(Intercept)6.14 ***9.27 ***8.65 ***\n\n(0.40)¬†¬†¬†(0.90)¬†¬†¬†(0.93)¬†¬†¬†\n\nbored0.16 *¬†¬†¬†¬†¬†¬†¬†¬†¬†0.18 *¬†¬†\n\n(0.08)¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†(0.08)¬†¬†¬†\n\nhrs.sleep¬†¬†¬†¬†¬†¬†¬†-0.34 **¬†-0.37 **¬†\n\n¬†¬†¬†¬†¬†¬†¬†(0.12)¬†¬†¬†(0.12)¬†¬†¬†\n\nN153¬†¬†¬†¬†¬†¬†¬†152¬†¬†¬†¬†¬†¬†¬†151¬†¬†¬†¬†¬†¬†¬†\n\nR20.03¬†¬†¬†¬†0.05¬†¬†¬†¬†0.08¬†¬†¬†¬†\n\n *** p &lt; 0.001;  ** p &lt; 0.01;  * p &lt; 0.05.\n\n\n\nsummary(mod1)\n\n\nCall:\nlm(formula = tired ~ bored, data = d)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.2920 -1.0760  0.2376  1.5512  3.7080 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  6.13518    0.39553   15.51   &lt;2e-16 ***\nbored        0.15681    0.07881    1.99   0.0484 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.297 on 151 degrees of freedom\n  (3 observations deleted due to missingness)\nMultiple R-squared:  0.02555,   Adjusted R-squared:  0.0191 \nF-statistic: 3.959 on 1 and 151 DF,  p-value: 0.04842"
  },
  {
    "objectID": "calstats/Lectures/10_MultipleRegressionReview.html#anohter-example-hahaaha.-a-bust-ignore",
    "href": "calstats/Lectures/10_MultipleRegressionReview.html#anohter-example-hahaaha.-a-bust-ignore",
    "title": "Lecture 10 - Multiple Regression & MEGA Review",
    "section": "ANOHTER EXAMPLE HAHAAHA. A BUST! IGNORE!!!",
    "text": "ANOHTER EXAMPLE HAHAAHA. A BUST! IGNORE!!!\n\nnames(d)\n\n [1] \"pace\"            \"engaging\"        \"fb.friends\"      \"insta.followers\"\n [5] \"insta.follows\"   \"bored\"           \"thirsty\"         \"tired\"          \n [9] \"satlife\"         \"oskilove\"        \"rlove\"           \"socialmed.use\"  \n[13] \"class.attention\" \"hrs.sleep\"       \"selfpow.data\"    \"corppow.data\"   \n[17] \"success.work\"    \"success.priv\"    \"selfesteem\"      \"catdog\"         \n[21] \"tuhoburat\"       \"calgame\"         \"caffeine\"        \"breakfast\"      \n[25] \"is.female\"       \"long.hair\"       \"has.water\"       \"shoesize\"       \n[29] \"height\"          \"happy\"           \"drink\"           \"stoned72\"       \n[33] \"multilingual\"    \"waitlist\"       \n\nm1 &lt;- lm(rlove ~ engaging, data = d)\nm2 &lt;- lm(rlove ~ bored, data = d)\nm3 &lt;- lm(rlove ~ bored + engaging, data = d)\n\nexport_summs(m1, m2, m3)\n\n\n\nModel 1Model 2Model 3\n\n(Intercept)3.59 ***5.81 ***4.42 ***\n\n(0.97)¬†¬†¬†(0.39)¬†¬†¬†(1.29)¬†¬†¬†\n\nengaging0.42¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†0.30¬†¬†¬†¬†\n\n(0.24)¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†(0.27)¬†¬†¬†\n\nbored¬†¬†¬†¬†¬†¬†¬†-0.13¬†¬†¬†¬†-0.08¬†¬†¬†¬†\n\n¬†¬†¬†¬†¬†¬†¬†(0.08)¬†¬†¬†(0.09)¬†¬†¬†\n\nN150¬†¬†¬†¬†¬†¬†¬†150¬†¬†¬†¬†¬†¬†¬†150¬†¬†¬†¬†¬†¬†¬†\n\nR20.02¬†¬†¬†¬†0.02¬†¬†¬†¬†0.03¬†¬†¬†¬†\n\n *** p &lt; 0.001;  ** p &lt; 0.01;  * p &lt; 0.05.\n\n\n\n\n\nSome Pre-Recorded NHST Review Videos\n\nNote : I used last semester‚Äôs dataset for these examples, so you will likely get different results if you try and replicate in this semester‚Äôs class; a good example of how NHST doesn‚Äôt really tell us whether the results are ‚Äútruth‚Äù or not, or whether they will replicate, etc.\nExample 1 : LOVEWATER ~ smoke.pot\n\n\n\nExamples 2 - 4 : faster explanations!"
  },
  {
    "objectID": "gradstats/gradlabs/12_MLM2.html",
    "href": "gradstats/gradlabs/12_MLM2.html",
    "title": "Lecture 12 | More Multilevel Models (MLM) and Not An Exam Debrief",
    "section": "",
    "text": "Some code is below that I will run, but we won‚Äôt see the output of.\n\n\nlibrary(tidyverse)\nlibrary(lme4)\nlibrary(ggplot2)\nlibrary(ggthemes)\n\n## Dataset : Wide Format\ndw &lt;- read.csv(\"~/Desktop/grad_class_wide.csv\", stringsAsFactors = T)\n\n## Converting Wide to Long Format\ndl &lt;- dw %&gt;%\n  pivot_longer(\n    cols = c(starts_with(\"HowGoing\"), starts_with(\"LoveR\")),\n    names_to = c(\".value\", \"Timepoint\"),\n    names_pattern = \"(HowGoing|LoveR)(\\\\d+)\"\n  )\ndl &lt;- dl[,c(1, 12:14)]\nd &lt;- dl\n\n## Some Data Cleaning\nd$Username &lt;- as.factor(d$Username)\nd$Timepoint &lt;- as.numeric(d$Timepoint)\n\n\nOkay, it‚Äôs modeling time.\n\n\n\n\n\n\n\n\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: HowGoing ~ Timepoint + (Timepoint | Username)\n   Data: d\n\nREML criterion at convergence: 839.5\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.0948 -0.4597  0.1015  0.4900  3.8208 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n Username (Intercept) 0.58001  0.7616        \n          Timepoint   0.01546  0.1243   -0.16\n Residual             1.75949  1.3265        \nNumber of obs: 230, groups:  Username, 28\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  6.88277    0.23215  29.648\nTimepoint   -0.15593    0.03972  -3.926\n\nCorrelation of Fixed Effects:\n          (Intr)\nTimepoint -0.597\n\n\n\nCheck-In Stuff stops here.\n\n\n\n\nConverting Data from Wide to Long\n\nWide Format : Each repeated measure is its own column; each row is an individual.\n\nLong Format : Each row is an individual observation; each column is a variable.\n\nfixed effects : repeat across rows (e.g., username; demographic & personality and other between-person variables)\nrandom effects : change across rows (e.g., mood; emotion; reaction times; other within-person variables)\n\n\nOften need to convert wide to long; it‚Äôs a pain. Professor Used ChatGPT for This :( and it was easier :)\n\n\nReporting the Results of MLM : It Gets Complicated\n\nWhat fixed vs.¬†random effects to include?\nRandom effects can be averaged into fixed effects, and both can be included in the model.\n\na person‚Äôs mood varies over the day (random effect)\nthe person‚Äôs mood tends to be higher than another person‚Äôs (fixed effect)\n\n\n\n\n# the modelsummary function : https://francish.net/mlmusingr/MLM_Appendix_A.pdf\n# install.packages(\"modelsummary\")\nlibrary(modelsummary)\nmodelsummary(list(\"Random Intercept Model\" = mlm, \"Random Intercept\\nFixed Slope Model\" = mlm2, \"Random Intercept\\nRandom Slope Model\" = mlm3), \n             stars = TRUE,\n             title = \"Multilevel Model Regression Example\")\n\n\n\n    \n\n    \n    \n      \n        \n        Multilevel Model Regression Example\n              \n                 \n                Random Intercept Model\n                Random Intercept\nFixed Slope Model\n                Random Intercept\nRandom Slope Model\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n        \n                \n                  (Intercept)\n                  6.107***\n                  6.862***\n                  6.883***\n                \n                \n                  \n                  (0.197)\n                  (0.254)\n                  (0.232)\n                \n                \n                  Timepoint\n                  \n                  -0.149***\n                  -0.156***\n                \n                \n                  \n                  \n                  (0.032)\n                  (0.040)\n                \n                \n                  SD (Intercept Username)\n                  0.897\n                  0.908\n                  0.762\n                \n                \n                  SD (Timepoint Username)\n                  \n                  \n                  0.124\n                \n                \n                  Cor (Intercept~Timepoint Username)\n                  \n                  \n                  -0.161\n                \n                \n                  SD (Observations)\n                  1.443\n                  1.374\n                  1.326\n                \n                \n                  Num.Obs.\n                  230\n                  230\n                  230\n                \n                \n                  R2 Marg.\n                  0.000\n                  0.064\n                  0.069\n                \n                \n                  R2 Cond.\n                  0.279\n                  0.349\n                  0.400\n                \n                \n                  AIC\n                  866.7\n                  852.7\n                  851.5\n                \n                \n                  BIC\n                  877.1\n                  866.5\n                  872.2\n                \n                \n                  ICC\n                  0.3\n                  0.3\n                  0.4\n                \n                \n                  RMSE\n                  1.38\n                  1.30\n                  1.24"
  },
  {
    "objectID": "gradstats/gradlabs/12_MLM2.html#check-in-multilevel-models-mlm",
    "href": "gradstats/gradlabs/12_MLM2.html#check-in-multilevel-models-mlm",
    "title": "Lecture 12 | More Multilevel Models (MLM) and Not An Exam Debrief",
    "section": "",
    "text": "Some code is below that I will run, but we won‚Äôt see the output of.\n\n\nlibrary(tidyverse)\nlibrary(lme4)\nlibrary(ggplot2)\nlibrary(ggthemes)\n\n## Dataset : Wide Format\ndw &lt;- read.csv(\"~/Desktop/grad_class_wide.csv\", stringsAsFactors = T)\n\n## Converting Wide to Long Format\ndl &lt;- dw %&gt;%\n  pivot_longer(\n    cols = c(starts_with(\"HowGoing\"), starts_with(\"LoveR\")),\n    names_to = c(\".value\", \"Timepoint\"),\n    names_pattern = \"(HowGoing|LoveR)(\\\\d+)\"\n  )\ndl &lt;- dl[,c(1, 12:14)]\nd &lt;- dl\n\n## Some Data Cleaning\nd$Username &lt;- as.factor(d$Username)\nd$Timepoint &lt;- as.numeric(d$Timepoint)\n\n\nOkay, it‚Äôs modeling time.\n\n\n\n\n\n\n\n\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: HowGoing ~ Timepoint + (Timepoint | Username)\n   Data: d\n\nREML criterion at convergence: 839.5\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.0948 -0.4597  0.1015  0.4900  3.8208 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n Username (Intercept) 0.58001  0.7616        \n          Timepoint   0.01546  0.1243   -0.16\n Residual             1.75949  1.3265        \nNumber of obs: 230, groups:  Username, 28\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  6.88277    0.23215  29.648\nTimepoint   -0.15593    0.03972  -3.926\n\nCorrelation of Fixed Effects:\n          (Intr)\nTimepoint -0.597\n\n\n\nCheck-In Stuff stops here.\n\n\n\n\nConverting Data from Wide to Long\n\nWide Format : Each repeated measure is its own column; each row is an individual.\n\nLong Format : Each row is an individual observation; each column is a variable.\n\nfixed effects : repeat across rows (e.g., username; demographic & personality and other between-person variables)\nrandom effects : change across rows (e.g., mood; emotion; reaction times; other within-person variables)\n\n\nOften need to convert wide to long; it‚Äôs a pain. Professor Used ChatGPT for This :( and it was easier :)\n\n\nReporting the Results of MLM : It Gets Complicated\n\nWhat fixed vs.¬†random effects to include?\nRandom effects can be averaged into fixed effects, and both can be included in the model.\n\na person‚Äôs mood varies over the day (random effect)\nthe person‚Äôs mood tends to be higher than another person‚Äôs (fixed effect)\n\n\n\n\n# the modelsummary function : https://francish.net/mlmusingr/MLM_Appendix_A.pdf\n# install.packages(\"modelsummary\")\nlibrary(modelsummary)\nmodelsummary(list(\"Random Intercept Model\" = mlm, \"Random Intercept\\nFixed Slope Model\" = mlm2, \"Random Intercept\\nRandom Slope Model\" = mlm3), \n             stars = TRUE,\n             title = \"Multilevel Model Regression Example\")\n\n\n\n    \n\n    \n    \n      \n        \n        Multilevel Model Regression Example\n              \n                 \n                Random Intercept Model\n                Random Intercept\nFixed Slope Model\n                Random Intercept\nRandom Slope Model\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n        \n                \n                  (Intercept)\n                  6.107***\n                  6.862***\n                  6.883***\n                \n                \n                  \n                  (0.197)\n                  (0.254)\n                  (0.232)\n                \n                \n                  Timepoint\n                  \n                  -0.149***\n                  -0.156***\n                \n                \n                  \n                  \n                  (0.032)\n                  (0.040)\n                \n                \n                  SD (Intercept Username)\n                  0.897\n                  0.908\n                  0.762\n                \n                \n                  SD (Timepoint Username)\n                  \n                  \n                  0.124\n                \n                \n                  Cor (Intercept~Timepoint Username)\n                  \n                  \n                  -0.161\n                \n                \n                  SD (Observations)\n                  1.443\n                  1.374\n                  1.326\n                \n                \n                  Num.Obs.\n                  230\n                  230\n                  230\n                \n                \n                  R2 Marg.\n                  0.000\n                  0.064\n                  0.069\n                \n                \n                  R2 Cond.\n                  0.279\n                  0.349\n                  0.400\n                \n                \n                  AIC\n                  866.7\n                  852.7\n                  851.5\n                \n                \n                  BIC\n                  877.1\n                  866.5\n                  872.2\n                \n                \n                  ICC\n                  0.3\n                  0.3\n                  0.4\n                \n                \n                  RMSE\n                  1.38\n                  1.30\n                  1.24"
  },
  {
    "objectID": "gradstats/gradlabs/12_MLM2.html#not-an-exam-debrief",
    "href": "gradstats/gradlabs/12_MLM2.html#not-an-exam-debrief",
    "title": "Lecture 12 | More Multilevel Models (MLM) and Not An Exam Debrief",
    "section": "Not an Exam Debrief",
    "text": "Not an Exam Debrief\n\nActivity and Discussion : Debrief of Team Work\n\nOne Dataset (and Psych 205 Assignment)\n\nOur Class Analyses\n29 Teams; 61 Analysts : Each team did what y‚Äôall did.\n\nActivity : Round Robin Peer Review\n\nStep 1 : Visit this Google Sheet\nStep 2 : Find your name; share your early ATHLETIC EXPERIENCES (growth mindset etc. etc.); then use the OSF LINK HERE to find your Assigned Team‚Äôs report.\nStep 3 : Skim / review the report; fill out the columns for the below questions.\n\nData Cleaning This Team Did for Red Card Variable.\n\nWas this what you did? Something similar? Radically different? Something you‚Äôve never heard of and might want to look into later?\n\nData Cleaning This Team Did for Race Variable.\nType of Model They Used.\nWhat was the effect (was darker skin related to more or less red cards? Was this significant?)\nQuestions or Comments You Had About The Team‚Äôs Approach (e.g., what would you want / need to learn more about? Was there something cool they did that we should know about?)\n\nStep 4 : Work with the members of your table to share your knowledge.\n\nWhat did the assigned team you read about do and find?\nWhat did you do and find?\nWhat changes might you make to your own analyses based on what others have done?\n\n\n\n\n\n\n\n\n\n\nOriginal Article :\nFull Article : Silberzahn R, Uhlmann EL, Martin DP, et al.¬†Many Analysts, One Data Set: Making Transparent How Variations in Analytic Choices Affect Results. Advances in Methods and Practices in Psychological Science. 2018;1(3):337-356. doi:10.1177/2515245917747646\n\nMiscellaneous Materials :\n\nOSF Files\nOriginal Google Doc I adapted for your exam\n\nAuthors & Agenda : Part of the ‚ÄúOpen Science‚Äù movement. Goal to develop better and more transparent systems of science.\n\n\n\nKey Findings :\n\n\n\nDiscussion Questions :\n\nWhat practices can we carry into our own research?\nWhat else seems relevant or important about the paper / this project?"
  },
  {
    "objectID": "gradstats/gradlabs/12_MLM2.html#presentation",
    "href": "gradstats/gradlabs/12_MLM2.html#presentation",
    "title": "Lecture 12 | More Multilevel Models (MLM) and Not An Exam Debrief",
    "section": "Presentation",
    "text": "Presentation"
  },
  {
    "objectID": "gradstats/gradlabs/12_MLM2.html#more-mlm",
    "href": "gradstats/gradlabs/12_MLM2.html#more-mlm",
    "title": "Lecture 12 | More Multilevel Models (MLM) and Not An Exam Debrief",
    "section": "More MLM",
    "text": "More MLM"
  },
  {
    "objectID": "gradstats/gradlabs/12_MLM2.html#next-week-final-project-draft-and-pizza",
    "href": "gradstats/gradlabs/12_MLM2.html#next-week-final-project-draft-and-pizza",
    "title": "Lecture 12 | More Multilevel Models (MLM) and Not An Exam Debrief",
    "section": "Next Week : Final Project Draft (and PIZZA)",
    "text": "Next Week : Final Project Draft (and PIZZA)\n\nPut together a summary graph (or table) that illustrates some of the main ideas. If there was ONE PAGE to show people about your project, what would you put on that page?\n\nMake it something people WANT to look at and can LEARN from.\nNo tiny font and giant margins!\nPeople will remember 1-2 things about your project (¬± 2).\n\nBring whatever you have :) but be prepared to chat about your data and project and get feedback.\nPsych department is paying (says¬†they¬†will¬†reimburse¬†me) for a PIZZA PARTY."
  },
  {
    "objectID": "gradstats/gradlabs/13_AllDone.html#final-project-show-and-tell",
    "href": "gradstats/gradlabs/13_AllDone.html#final-project-show-and-tell",
    "title": "",
    "section": "Final Project Show and Tell",
    "text": "Final Project Show and Tell\n\nFind a Buddy.\n\nWalk them through your study!\n\nWhat is your question (and why should we care)?\nHow did you measure / manipulate these variables?\n\nWalk them through your graph / results!\n\nSome volunteers to share with the class? Low-stakes practice for more stressful situations where you will be more prepared :)"
  },
  {
    "objectID": "gradstats/gradlabs/13_AllDone.html#things-to-remember-with-regression-and-all-statistics",
    "href": "gradstats/gradlabs/13_AllDone.html#things-to-remember-with-regression-and-all-statistics",
    "title": "13_AllDone",
    "section": "Things to Remember with Regression (and all Statistics)",
    "text": "Things to Remember with Regression (and all Statistics)\n\nRECAP : An Example Interaction Effect\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nTable : Linear Models for the Interaction Effect from Lecture 10\n\n\n\n\nModel 1Model 2Model 3Model 4\n\n(Intercept)0.00¬†¬†¬†¬†-0.00¬†¬†¬†¬†-0.00¬†¬†¬†¬†0.08¬†¬†¬†¬†\n\n(0.06)¬†¬†¬†(0.05)¬†¬†¬†(0.05)¬†¬†¬†(0.06)¬†¬†¬†\n\nscale(Log.GDP.per.capita)0.73 ***¬†¬†¬†¬†¬†¬†¬†0.25 **¬†0.25 **¬†\n\n(0.06)¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†(0.07)¬†¬†¬†(0.07)¬†¬†¬†\n\nscale(Ladder.score)¬†¬†¬†¬†¬†¬†¬†0.81 ***0.62 ***0.60 ***\n\n¬†¬†¬†¬†¬†¬†¬†(0.05)¬†¬†¬†(0.07)¬†¬†¬†(0.07)¬†¬†¬†\n\nscale(Ladder.score):scale(Log.GDP.per.capita)¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†-0.10 *¬†¬†\n\n¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†(0.05)¬†¬†¬†\n\nN140¬†¬†¬†¬†¬†¬†¬†140¬†¬†¬†¬†¬†¬†¬†140¬†¬†¬†¬†¬†¬†¬†140¬†¬†¬†¬†¬†¬†¬†\n\nR20.53¬†¬†¬†¬†0.66¬†¬†¬†¬†0.69¬†¬†¬†¬†0.70¬†¬†¬†¬†\n\n *** p &lt; 0.001;  ** p &lt; 0.01;  * p &lt; 0.05.\n\n\n\n\n\n\nWatch Out For Overfitting.\nWhen your model is too complex, each variable in the model (parameter) increases the model complexity.\n\ncomplex models that perfectly fit the data are problematic: you essentially describing your sample, and not the underlying population (which is usually the goal of multiple regression.)\nWe don‚Äôt expect over-fit models to generalize to other samples. [Image source]\n\n\nCross Validation. To ensure your model generalizes to other samples, you can a) replicate, or b) cross-validate your data. Cross validation involves dividing your sample into sub-samples; define a model on one sample, then test the model in the other(s).\n\nHere‚Äôs the most simple example of cross-validation (‚Äútrain-test split‚Äù; ‚Äúholdout cross validation‚Äù)\n\nsample(0:1, nrow(h), replace = T, prob = c(.7, .3)) # using the sample function\n\n  [1] 0 1 1 1 1 0 0 1 1 1 1 0 1 1 1 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 0 0 1\n [38] 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 1 0 1 0 1 1 0 0 1 0 0\n [75] 1 0 0 0 0 1 0 0 0 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 0 1 1 1 0 0 0 1 1 1\n[112] 1 0 1 0 0 0 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 1 1 1 0 0 0 0 1 1 0\n\nset.seed(424242)\nrandom.selection &lt;- sample(0:1, nrow(h), replace = T, prob = c(.7, .3))\nhtrain &lt;- h[random.selection == 0,]\nhtest &lt;- h[random.selection == 1,]\n\n## Model in training Data\ntrain.mod &lt;- lm(Social.support ~ Ladder.score * Log.GDP.per.capita, data = htrain)\nsummary(train.mod)\n\n\nCall:\nlm(formula = Social.support ~ Ladder.score * Log.GDP.per.capita, \n    data = htrain)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.70478 -0.08978 -0.00238  0.09815  0.68575 \n\nCoefficients:\n                                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                     -0.53619    0.32701  -1.640    0.105    \nLadder.score                     0.27113    0.06472   4.189 7.39e-05 ***\nLog.GDP.per.capita               0.45651    0.27441   1.664    0.100    \nLadder.score:Log.GDP.per.capita -0.05874    0.04648  -1.264    0.210    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2043 on 77 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.6821,    Adjusted R-squared:  0.6697 \nF-statistic: 55.07 on 3 and 77 DF,  p-value: &lt; 2.2e-16\n\npredict(train.mod) # the predicted values of the DV, based on our model.\n\n        1         2         4         6         7         8         9        10 \n1.5658691 1.5408531 1.5020893 1.4986799 1.4973743 1.4764175 1.4602632 1.4549315 \n       12        13        17        18        19        20        21        22 \n1.4243403 1.4373031 1.4344788 1.4128527 1.4112355 1.4030480 1.3998894 1.4102398 \n       24        29        31        34        35        37        38        39 \n1.4011533 1.3396428 1.3641704 1.3482150 1.3461128 1.3248942 1.3221398 1.3289260 \n       41        42        44        46        51        53        54        55 \n1.3314375 1.2782423 1.2902556 1.3075235 1.2863680 1.2282990 1.2373548 1.2754498 \n       58        60        61        63        65        67        68        70 \n1.2405613 1.2413826 1.1974643 1.2585895 1.2202362 1.1927948 1.2029163 1.2210060 \n       74        75        79        84        85        86        87        89 \n1.1740772 1.1404057 0.9840313 1.1276332 1.1053595 1.1804468 1.1102798 1.0129970 \n       91        93        94        97        98        99       101       102 \n1.0924736 1.0104227 1.0439202 0.9598416 1.0922332 0.9636387 1.0327200 0.9640955 \n      104       105       108       110       111       112       114       116 \n0.9457778 1.0148509 0.9220134 0.8400436 0.8920740 0.8246022 0.8768503 0.8327770 \n      117       119       120       121       122       123       127       128 \n0.8033361 0.8444944 0.8469843 0.7486051 0.7665298 0.7408530 0.8474353 0.8303265 \n      129       130       132       133       134       140       141       142 \n0.7734868 0.6925578 0.6519904 0.5952518 0.6387592 0.5175113 0.5352949 0.6073950 \n      143 \n0.1536218 \n\n## Applying the model to our testing dataset.\npredict(train.mod, newdata = htest) # produces predicted values from our training model, using the testing data.\n\n        3         5        11        14        15        16        23        25 \n1.5312384 1.4997124 1.4484716 1.4318518 1.4287474 1.4292117 1.4062990 1.3720706 \n       26        27        28        30        32        33        36        40 \n1.3650023 1.3798016 1.3789961 1.3876634 1.3514627 1.3144987 1.3447837 1.3373362 \n       43        45        47        48        49        50        52        56 \n1.2634123 1.3119962 1.2556643 1.2868124 1.2923920 1.2884980 1.2889539 1.2726265 \n       57        59        64        66        69        71        72        73 \n1.2316819 1.2574693 1.2544183 1.2250269 1.2161932 1.1997594 1.2238602 1.1740769 \n       76        77        78        80        81        82        83        90 \n1.2016273 1.1730867 1.1831393 1.1495913 1.1658541 1.1392808 1.1255433 0.9620724 \n       92        95        96       100       103       106       107       109 \n1.0555989 1.0678431 1.0118759 1.0386631        NA 0.9924835 0.9759377 0.8072950 \n      113       115       118       124       125       126       131       135 \n0.7929260 0.9196825 0.8406228 0.7647379 0.8645359 0.8175708 0.6811526 0.7280377 \n      136       137       138       139 \n0.5490131 0.7535235 0.5643139 0.4975936 \n\npredval.test &lt;- predict(train.mod, newdata = htest)  # saves these predicted values from the testing dataset.\n\n## Calculating R^2\ntest.mod.resid &lt;- htest$Social.support - predval.test\nSSE &lt;- sum(test.mod.resid^2, na.rm = T)\nSSE\n\n[1] 1.541793\n\ntest.resid &lt;- htest$Social.support - mean(htest$Social.support, na.rm = T)\nsum(test.resid^2)\n\n[1] NA\n\nSST &lt;- sum(test.resid^2, na.rm = T)\n\n(SST - SSE)/SST\n\n[1] 0.7040524\n\n\nYou‚Äôll often see a few different methods of evaluating model fit.\n\n\\(R^2\\). Our good friend. The proportion of variance explained by the model (vs.¬†the mean)\nRooted Mean Squared Error (RMSE). The average amount of residual error (actual - predicted values).\nMean Absoulte Error. The average of the absolute value of the residuals; less sensitive to outliers than RMSE or \\(R^2\\).\n\nAnd there are different methods of defining the test and training datasets. And different packages and tutorials to do this. Here‚Äôs one, called ‚ÄúLeave one out cross validation - LOOCV‚Äù; gif below via Wikipedia.\n\n\n\n\n\n\n# install.packages(\"caret\")\nlibrary(caret)\n\nLoading required package: lattice\n\ntrain.control &lt;- trainControl(method = \"LOOCV\")\nloocvmod &lt;- train(Social.support ~ Ladder.score * Log.GDP.per.capita, data = h, method = \"lm\",\n                  trControl = train.control, na.action = \"na.omit\")\n\nprint(loocvmod)\n\nLinear Regression \n\n143 samples\n  2 predictor\n\nNo pre-processing\nResampling: Leave-One-Out Cross-Validation \nSummary of sample sizes: 139, 139, 139, 139, 139, 139, ... \nResampling results:\n\n  RMSE       Rsquared   MAE      \n  0.1902603  0.6720989  0.1361974\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n\n\n\nWatch Out For Multicollinearity.\nIf your independent variables are highly related, then your multivariate regression slope estimates are not uniquely determined. weird things happen to your coefficients, and this makes it hard to interpret your effects.\n\nIN R : check the ‚Äúvariance inflation factor‚Äù (VIF); a measure of how much one IV is related to all the other IVs in the model. ‚ÄúTradition‚Äù is that if VIF is &gt; 5 (or I‚Äôve also seen VIF &gt; 10) there‚Äôs a problem in the regression.\n\\(\\huge VIF_j=\\frac{1}{1-R_{j}^{2}}\\)\n\n\nlibrary(car)\n\nLoading required package: carData\n\nvif(mod4) # doesn't seem like multicollinearity is a problem.\n\nthere are higher-order terms (interactions) in this model\nconsider setting type = 'predictor'; see ?vif\n\n\n                          scale(Ladder.score) \n                                     2.490104 \n                    scale(Log.GDP.per.capita) \n                                     2.443312 \nscale(Ladder.score):scale(Log.GDP.per.capita) \n                                     1.057269 \n\n## creating a highly correlated second IV.\njitter(h$Healthy.life.expectancy, 300)\n\n  [1]  0.63649887  0.68930612  0.71828952  0.66823278  0.78147940  0.64639281\n  [7]  0.66783200  0.65547462  0.71468602  0.72225337  0.71773673  0.64999642\n [13]  0.62952106  0.64185465  0.72092831  0.69836634  0.64067483  0.60323449\n [19]  0.61564585  0.70589567  0.74527554  0.54082440  0.57966955  0.74032124\n [25]  0.52293196  0.54462176  0.75101008  0.55563685  0.60367356  0.74810486\n [31]  0.69789473  0.54448689  0.56294229  0.69718620  0.62746987  0.69055692\n [37]  0.62267421  0.70380172  0.62007029  0.68723153  0.74690172  0.49826800\n [43]  0.58734907  0.50056514  0.69238462  0.52731539  0.57692015  0.64389105\n [49]  0.57280871  0.69425428  0.82612563  0.77274534  0.38400182  0.57725457\n [55]  0.70352020  0.61930331  0.55762718  0.57337490  0.53553203  0.65959492\n [61]  0.45026458          NA  0.66845212  0.69307250  0.59281784  0.52861070\n [67]  0.54621673  0.69875356  0.45710762  0.43500797  0.56482900  0.54559839\n [73]  0.50964668  0.60188374  0.54674710  0.60113762  0.41675213  0.67774238\n [79]  0.45482879  0.51207586  0.53754193  0.65166685  0.26579890  0.58060330\n [85]  0.53234986  0.85807969  0.66611649          NA  0.25647428  0.12829974\n [91]  0.51774245  0.49561179  0.47517286  0.47895859  0.35411168  0.25447142\n [97]  0.21655523  0.58171885  0.35686944  0.51373138  0.48182973  0.20760587\n[103]          NA  0.31835400  0.46658725  0.34832926  0.52310604  0.28958395\n[109]  0.33832352  0.33133514  0.42088085  0.27791840  0.16285129  0.31895572\n[115]  0.62433954  0.30866350  0.37751970  0.43665557  0.41469131  0.38656597\n[121]  0.32196352  0.25386893  0.35348747  0.26579842  0.62022055  0.43845057\n[127]  0.49687114  0.62927456  0.48772773  0.38520825  0.37719937  0.40156511\n[133]  0.33585422  0.29027768  0.12197319  0.39450247  0.18769012  0.21595348\n[139]  0.24276263  0.24909294 -0.03799866  0.57172747  0.27471988\n\nh$health2 &lt;- jitter(h$Healthy.life.expectancy, 300)\n\nplot(h$health2, h$Healthy.life.expectancy) # yup.\n\n\n\n\n\n\n\nmultimod &lt;- lm(Ladder.score ~ Healthy.life.expectancy + health2, data = h) # both in the model.\nsummary(multimod) # results! Things look good....\n\n\nCall:\nlm(formula = Ladder.score ~ Healthy.life.expectancy + health2, \n    data = h)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.0155 -0.4313  0.1701  0.5191  1.5809 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               2.6514     0.2202  12.041  &lt; 2e-16 ***\nHealthy.life.expectancy   7.5885     1.9725   3.847 0.000182 ***\nhealth2                  -2.0676     1.8604  -1.111 0.268360    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7703 on 137 degrees of freedom\n  (3 observations deleted due to missingness)\nMultiple R-squared:  0.5809,    Adjusted R-squared:  0.5747 \nF-statistic: 94.93 on 2 and 137 DF,  p-value: &lt; 2.2e-16\n\nvif(multimod) # ...but wait!\n\nHealthy.life.expectancy                 health2 \n               24.78939                24.78939 \n\n\n\n\n4. Don‚Äôt Forget to Evaluate Those Regression Assumptions\n\npar(mfrow = c(2,2))\nplot(mod4)\n\n\n\n\n\n\n\n\n\n\nWould You Like to Learn More??\n\nAn overview of ML methods (including the partitioning approach) for evaluating models\nAnother overview of cross-validation methods.\nSome more notes on multicollinearity and VIFs."
  },
  {
    "objectID": "calstats/Lectures/11_Conclusion2Intro.html#announcements-and-agenda",
    "href": "calstats/Lectures/11_Conclusion2Intro.html#announcements-and-agenda",
    "title": "Lecture 12 | Conclusion to Introduction to Statstics",
    "section": "Announcements and Agenda",
    "text": "Announcements and Agenda\n\n2:10 - 2:25. Check-In and Announcements\n2:25 - 3:30. Final Project Stuff\n3:30 - 3:45. Break Time\n3:45 - ????. The Learning is Over; Would You Like to Learn More???"
  },
  {
    "objectID": "calstats/Lectures/11_Conclusion2Intro.html#final-project-stuff",
    "href": "calstats/Lectures/11_Conclusion2Intro.html#final-project-stuff",
    "title": "Lecture 12 | Conclusion to Introduction to Statstics",
    "section": "Final Project Stuff",
    "text": "Final Project Stuff\n\nProfessor Advice\n\nUse the rubric as a guide.\n\nSet reasonable expectations : ‚Äúdone is better than perfect‚Äù.\nIf it‚Äôs not there, your TA cannot give you credit for it.\nNo ‚Äúright‚Äù way to do the study; just want to see evidence that you tried and are learning. (But see the rubric.)\n\nKeep it simple.\n\nSimple measures :\n\nokay to use one measure of your DV even thought you had two.\nokay to simplify your categorical variable (e.g., ‚Äúwhich of these 15 music types do you love the most‚Äù ‚Äì&gt; ‚Äúdo you like experimental jazz (0 = no; 1 = yes)‚Äù)\n\nSimple models : okay to just\nDiscussion Section : summary of ways that you could have (or should have) been more complicated in the Discussion Section\nAppendix : List of all your measures. Good practice to be fully transparent about what you measured in the study.\n\nUse paragraphs and headers to guide the reader.\n\nEach new idea is a new paragraph. No solid wall of text please!\nHeaders help orient your reader to the different contents.\n\n\n\n\nMore Writing\n\nThe Abstract : the TLDR of your paper\nComponents of an Abstract :\n\n1-2 sentences : what is your topic and why should we care?\n1-2 sentences : what did you measure / do in your study?\n1-2 sentences : what did you find?\n1-2 sentences : why does this matter?\n\nActivity : Write an Abstract for Each Other\n\nProfessor and Student Example Goes Here :\nYOUR TURN!\n\n\n\nThe Discussion Section : Two Paragraphs Minimum!\n\nSummary of Results (and Future Studies) :\n\nWhat you found (NO STATS)\nWhy this matters / how this knowledge could be used / what other questions you might ask in the future?\n\nLimitations and Future Directions\n\nWhat would you do differently in your study?\nHow could (or should) future research address these limitations, and why would that be important to do?\n\n\n\n\n\nThe Rich Lyons is¬†our¬†chancellor¬†and¬†the¬†professor¬†of¬†this¬†class¬†has Invited me¬†to¬†give¬†a Distinguished GoBears Guest Lecture\n\nStudent Milestone 5 Example Goes Here\nStudent Milestone 5 Example Goes Here\nStudent Milestone 5 Example Goes Here"
  },
  {
    "objectID": "calstats/Lectures/11_Conclusion2Intro.html#break-time-meet-back-at-345",
    "href": "calstats/Lectures/11_Conclusion2Intro.html#break-time-meet-back-at-345",
    "title": "Lecture 12 | Conclusion to Introduction to Statstics",
    "section": "BREAK TIME : MEET BACK AT 3:45",
    "text": "BREAK TIME : MEET BACK AT 3:45"
  },
  {
    "objectID": "calstats/Lectures/11_Conclusion2Intro.html#the-learning-is-over-would-you-like-to-learn-more",
    "href": "calstats/Lectures/11_Conclusion2Intro.html#the-learning-is-over-would-you-like-to-learn-more",
    "title": "Lecture 12 | Conclusion to Introduction to Statstics",
    "section": "The Learning is Over; Would You Like to Learn More?",
    "text": "The Learning is Over; Would You Like to Learn More?\n\nThings to Remember in 50 Years\n\nYou Can Learn Statistics and Computers : It is a process :)\n\ncheck-in graph goes here.\n\n\n\nLife is Complex : Psychology Tries to Use Statistics to Study Complexity\n\nLinear Models Make Predictions (with Errors)\n\nHard to quantify; but best estimates that the average correlation between two variables is r &lt; .30\n\n\n\n\n\n\n\n\nRichard, F. D., Bond Jr, C. F., & Stokes-Zoota, J. J. (2003). One hundred years of social psychology quantitatively described. Review of general psychology, 7(4), 331-363.\n\n\n\nWhat a r = .3 looks like. [Source]\n\n\n\nBut also, remember that a ‚Äúcorrelation‚Äù / slope could look like any of the following graphs [Anscombe‚Äôs Quartet]\n\n\n\nNot Just Error‚Ä¶Researchers are biased.\n\n\n\n\n\n\n\nBias\nYour Final Project Experiences?\n\n\n\n\nprevious beliefs bias. we are more likely to believe data that supports our results.\nDid you study something that was personally relevant to you? (‚ÄúMe-search‚Äù)\n\nClap : Yes / No\n\n\n\npositive evidence. we seek out information in ways that supports our beliefs.\nWas your theory (the ‚Äúalternative‚Äù) something that you personally believe? Did you look for, or include, research in your introduction that did not support your theory?\n\nclap : yes / no\n\n\n\navailability bias. we are more likely to believe memorable or recent data.\nHow did you feel when your results were significant? When your results were not significant?\n\nclap : happy / neutral / sad\n\n\n\nsocial influence. we believe things others (w/ status) do.\nWere you influenced by the topics or methods your friends / classmates were doing?\nDid you trust research articles that were more recent and / or highly cited (higher citation counts)?\n\nClap : Yes / No\n\n\n\n\n\n\nOur Methods and Measures Are Biased‚Ä¶.\n\n\nMaybe All of Science is Biased?\n\np-hacking : making changes to your model or data in order to ‚Äúget‚Äù your p-values.\n\nvisit : tinyurl.com/phackingfun\nmodel : economic performance ~ political power + error\n\ndecide how to operationalize political power\ndecide how to operationalize economic performance\ndecide how to adjust your model\n\n\n\n\n\nremember : there‚Äôs a replication crisis in psychology\n\n\n\n\nOriginal study effect size versus replication effect size (correlation coefficients).\n\n\n\n\nGood to identify that bias and change it.\n\nPre-Registration : Deciding what you are going to do\n\n\n\nOpen-Science : Making parts of the scientific method more open and transparent.\n\nCenter for Open Science\nOpen Science Foundation (OSF). Hosting\n\n\n\n\nIs there even a ‚ÄúTRUTH‚Äù about what people are like?\n\n\n\nBeginning of Semester\nEnd of Semester\n\n\n\n\n\n\n\n\n\n\n\n\n\nThings to Immediately Forget (or Learn More About)\n\nQuadratic Effects : A Regression Line Can Bend\n\n\nBut careful about overfitting‚Ä¶.\n\n\n\n\nInteraction Effects : The Regression Line Changes Depending on Some Other Variable\n\nLink to (OPTIONAL) Videos. No chapter (yet!)\nData : Bertrand, M., & Mullainathan, S. (2004). Are Emily and Greg more employable than Lakisha and Jamal? A field experiment on labor market discrimination. American economic review, 94(4), 991-1013.\n\nWhat is the relationship between resume quality and callback for white sounding names?\nWhat is the relationship between resume quality and callback for black sounding names?\nWhat is the difference in these slopes (is there an interaction effect?)\n\n\n\n\nFrantz Fanon, Black Skin White Masks (1967) : ‚ÄúTo speak means to be in a position to use a certain syntax, to grasp the morphology of this or that language, but it means above all to assume a culture, to support the weight of civilization‚Ä¶Every colonized people‚Äìin other words, every people in whose soul an inferiority complex has been created by the death and burial of its local cultural originality‚Äìfinds itself face to face with the language of the civilizing nation; that is, with the culture of the mother country. The colonized is elevated above his jungle status in proportion to his adoption of the mother country‚Äôs cultural standards.‚Äù\n\n\n\nGeneralized Linear Models (e.g., ‚ÄúLogistic Regression‚Äù)\n\nDiscussion : What do you observe about the relationship between testosterone and sex (as a DV)? Why is this model ‚Äúwrong‚Äù??\n\n\n\nThe Generalized Linear Model : Same Ideas (slope; intercept; error), but a ‚ÄúLink‚Äù Function that Transforms the Line to Fit Different Distributions.\n\n\n\nExample : Defining a logistic regression (binomial distribution)\n\n\n\n\n\nMultilevel Linear Models (e.g., HLM; MLM; dependent t-test)\n\nExample : The Sleep Dataset (?sleep) : ‚ÄúData which show the effect of two soporific drugs (increase in hours of sleep compared to control).‚Äù\n\nExtra : increase in hours of sleep\nGroup : drug given (1 = control; 2 = drug)\nID : patient ID\n\nSleep Data as a ‚ÄúGeneral‚Äù Linear Model (a ‚ÄúBetween Person‚Äù Study)\n\n\nlibrary(ggplot2)\nggplot(sleep, aes(y = extra, x = group)) + \n  geom_point(size=2) + \n  stat_summary(fun.data=mean_se, color = 'red', size = 1.25, linewidth = 2)\n\n\n\n\n\n\n\nlmod &lt;- lm(extra ~ as.factor(group), data = sleep)\nsummary(lmod)\n\n\nCall:\nlm(formula = extra ~ as.factor(group), data = sleep)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-2.430 -1.305 -0.580  1.455  3.170 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)         0.7500     0.6004   1.249   0.2276  \nas.factor(group)2   1.5800     0.8491   1.861   0.0792 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.899 on 18 degrees of freedom\nMultiple R-squared:  0.1613,    Adjusted R-squared:  0.1147 \nF-statistic: 3.463 on 1 and 18 DF,  p-value: 0.07919\n\n\n\nSleep Data as a ‚ÄúMultilevel‚Äù Linear Model (a ‚ÄúWithin-Person‚Äù Study). Many linear models! Look at the graph below. What‚Äôs going on? What do you observe? How might this help us understand the relationship between these two variables?\n\n\nggplot(sleep, aes(y = extra, x = group, color = ID)) + \n  geom_point(size=2) + \n  geom_line(aes(group = ID), linewidth = 0.75)\n\n\n\n\n\n\n\n\n\nWhat‚Äôs going on in the (random intercept) model. Still just one equation.\n\n\n#install.packages(\"lme4\")\nlibrary(lme4)\n\nLoading required package: Matrix\n\nlibrary(lmerTest)\n\n\nAttaching package: 'lmerTest'\n\n\nThe following object is masked from 'package:lme4':\n\n    lmer\n\n\nThe following object is masked from 'package:stats':\n\n    step\n\nlibrary(Matrix)\nmlmod &lt;- lmer(extra ~ as.factor(group) + (1 | ID), data = sleep)\nsummary(mlmod)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: extra ~ as.factor(group) + (1 | ID)\n   Data: sleep\n\nREML criterion at convergence: 70\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-1.63372 -0.34157  0.03346  0.31511  1.83859 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n ID       (Intercept) 2.8483   1.6877  \n Residual             0.7564   0.8697  \nNumber of obs: 20, groups:  ID, 10\n\nFixed effects:\n                  Estimate Std. Error      df t value Pr(&gt;|t|)   \n(Intercept)         0.7500     0.6004 11.0814   1.249  0.23735   \nas.factor(group)2   1.5800     0.3890  9.0000   4.062  0.00283 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\nas.fctr(g)2 -0.324\n\n\n\n\n\nStatistics Exists Outside of Psych 101\n\nBooks and Blogs and YouTube!\n\nBooks :\n\nR for Data Science (Hadley Wickham) : http://r4ds.had.co.nz/\nModels Demystified : https://m-clark.github.io/book.html\n\nBlogs :\n\nAndrew Gelman blog : http://andrewgelman.com/¬†\nNelson, Simons, Simonsohn : datacolada.org/\n\nTwitter / BlueSky Follows :\n\n@siminevazire (personality & open science)\n@djnavarro (cognitive sci and r art)\n@NeilLewisJr (psych and education)\n\n\n\n\nCourses at Cal!\n\nPsych 102 (Honors Students Priority). Taught be me in the Fall :)\nStat 133. Hear good things?\nSit in / Register for Psych 205 (Graduate Statistics). I taught this this semester; maybe again? IDK.\n\n\n\nPractice The Thing You Want to Do!\n\nRA ‚Äî&gt; Honors Thesis Pipeline. Psych 199 positions are posted on the psych website, but also do direct reach out to grad students who are involved in research that interests you.\nOther Supported Research Opportunities at Cal. Go bears.\nBerkeley Undergraduate Research Journal.Maybe your final project goes here?"
  },
  {
    "objectID": "calstats/Lectures/11_Conclusion2Intro.html#farewell",
    "href": "calstats/Lectures/11_Conclusion2Intro.html#farewell",
    "title": "Lecture 12 | Conclusion to Introduction to Statstics",
    "section": "FAREWELL!!",
    "text": "FAREWELL!!"
  },
  {
    "objectID": "gradstats/gradlabs/13_AllDone.html#would-you-like-to-learn-more",
    "href": "gradstats/gradlabs/13_AllDone.html#would-you-like-to-learn-more",
    "title": "",
    "section": "Would You Like to Learn More?",
    "text": "Would You Like to Learn More?\n\nWatch Out for Overfitting in Your Models\nWhen your model is too complex, each variable in the model (parameter) increases the model complexity.\n\nACTIVITY : Where would you draw the line of best fit here?\n\n\nlibrary(ggplot2)\n# Fakin' some data.\nset.seed(42)\nn &lt;- 100\nx &lt;- seq(-5, 5, length.out = n)\ny &lt;- sin(x) + rnorm(n, sd = 2)\nd &lt;- data.frame(x, y)\n\n# Graphin the fake data.\nggplot(d, aes(x, y)) +\n  geom_point(size = 2) +\n  # stat_smooth(method = \"lm\", formula = y ~ poly(x, 25), se = FALSE, color = \"red\", size = 2) +\n  # labs(title = \"Overfit Model (25-Degree Polynomial IV\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nKEY IDEA : complex models that perfectly fit the data are problematic\n\nYou essentially describing your sample, and not the underlying population (which is usually the goal of multiple regression.)\n\nWe don‚Äôt expect over-fit models to generalize to other samples. [Image source]\n\n\n\n\n\n\n\nCross Validation : Conceptual Understanding\nCross Validation. To ensure your model generalizes to other samples, you can a) replicate, or b) cross-validate your data. Cross validation involves dividing your sample into sub-samples; define a model on one sample, then test the model in the other(s).\n\n\n\n\n\n\n\nApplication : Cross-Validating Our Data (Lecture 10 Interaction Effect)\n\n\nWarning: package 'jtools' was built under R version 4.3.3\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nTable : Linear Models for the Interaction Effect from Lecture 10\n\n\n\n\nModel 1Model 2Model 3Model 4\n\n(Intercept)0.00¬†¬†¬†¬†-0.00¬†¬†¬†¬†-0.00¬†¬†¬†¬†0.08¬†¬†¬†¬†\n\n(0.06)¬†¬†¬†(0.05)¬†¬†¬†(0.05)¬†¬†¬†(0.06)¬†¬†¬†\n\nscale(Log.GDP.per.capita)0.73 ***¬†¬†¬†¬†¬†¬†¬†0.25 **¬†0.25 **¬†\n\n(0.06)¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†(0.07)¬†¬†¬†(0.07)¬†¬†¬†\n\nscale(Ladder.score)¬†¬†¬†¬†¬†¬†¬†0.81 ***0.62 ***0.60 ***\n\n¬†¬†¬†¬†¬†¬†¬†(0.05)¬†¬†¬†(0.07)¬†¬†¬†(0.07)¬†¬†¬†\n\nscale(Ladder.score):scale(Log.GDP.per.capita)¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†-0.10 *¬†¬†\n\n¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†(0.05)¬†¬†¬†\n\nN140¬†¬†¬†¬†¬†¬†¬†140¬†¬†¬†¬†¬†¬†¬†140¬†¬†¬†¬†¬†¬†¬†140¬†¬†¬†¬†¬†¬†¬†\n\nR20.53¬†¬†¬†¬†0.66¬†¬†¬†¬†0.69¬†¬†¬†¬†0.70¬†¬†¬†¬†\n\n *** p &lt; 0.001;  ** p &lt; 0.01;  * p &lt; 0.05.\n\n\n\n\nHere‚Äôs the most simple example of cross-validation (‚Äútrain-test split‚Äù; ‚Äúholdout cross validation‚Äù)\n\nsample(0:1, nrow(h), replace = T, prob = c(.7, .3)) # using the sample function\n\n  [1] 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 0 1 0\n [38] 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0\n [75] 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0\n[112] 0 0 1 0 0 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0\n\nset.seed(424242)\nrandom.selection &lt;- sample(0:1, nrow(h), replace = T, prob = c(.7, .3))\nhtrain &lt;- h[random.selection == 0,]\nhtest &lt;- h[random.selection == 1,]\n\n## Model in training Data\ntrain.mod &lt;- lm(Social.support ~ Ladder.score * Log.GDP.per.capita, data = htrain)\nsummary(train.mod)\n\n\nCall:\nlm(formula = Social.support ~ Ladder.score * Log.GDP.per.capita, \n    data = htrain)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.70478 -0.08978 -0.00238  0.09815  0.68575 \n\nCoefficients:\n                                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                     -0.53619    0.32701  -1.640    0.105    \nLadder.score                     0.27113    0.06472   4.189 7.39e-05 ***\nLog.GDP.per.capita               0.45651    0.27441   1.664    0.100    \nLadder.score:Log.GDP.per.capita -0.05874    0.04648  -1.264    0.210    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2043 on 77 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.6821,    Adjusted R-squared:  0.6697 \nF-statistic: 55.07 on 3 and 77 DF,  p-value: &lt; 2.2e-16\n\npredict(train.mod) # the predicted values of the DV, based on our model.\n\n        1         2         4         6         7         8         9        10 \n1.5658691 1.5408531 1.5020893 1.4986799 1.4973743 1.4764175 1.4602632 1.4549315 \n       12        13        17        18        19        20        21        22 \n1.4243403 1.4373031 1.4344788 1.4128527 1.4112355 1.4030480 1.3998894 1.4102398 \n       24        29        31        34        35        37        38        39 \n1.4011533 1.3396428 1.3641704 1.3482150 1.3461128 1.3248942 1.3221398 1.3289260 \n       41        42        44        46        51        53        54        55 \n1.3314375 1.2782423 1.2902556 1.3075235 1.2863680 1.2282990 1.2373548 1.2754498 \n       58        60        61        63        65        67        68        70 \n1.2405613 1.2413826 1.1974643 1.2585895 1.2202362 1.1927948 1.2029163 1.2210060 \n       74        75        79        84        85        86        87        89 \n1.1740772 1.1404057 0.9840313 1.1276332 1.1053595 1.1804468 1.1102798 1.0129970 \n       91        93        94        97        98        99       101       102 \n1.0924736 1.0104227 1.0439202 0.9598416 1.0922332 0.9636387 1.0327200 0.9640955 \n      104       105       108       110       111       112       114       116 \n0.9457778 1.0148509 0.9220134 0.8400436 0.8920740 0.8246022 0.8768503 0.8327770 \n      117       119       120       121       122       123       127       128 \n0.8033361 0.8444944 0.8469843 0.7486051 0.7665298 0.7408530 0.8474353 0.8303265 \n      129       130       132       133       134       140       141       142 \n0.7734868 0.6925578 0.6519904 0.5952518 0.6387592 0.5175113 0.5352949 0.6073950 \n      143 \n0.1536218 \n\n## Applying the model to our testing dataset.\npredict(train.mod, newdata = htest) # produces predicted values from our training model, using the testing data.\n\n        3         5        11        14        15        16        23        25 \n1.5312384 1.4997124 1.4484716 1.4318518 1.4287474 1.4292117 1.4062990 1.3720706 \n       26        27        28        30        32        33        36        40 \n1.3650023 1.3798016 1.3789961 1.3876634 1.3514627 1.3144987 1.3447837 1.3373362 \n       43        45        47        48        49        50        52        56 \n1.2634123 1.3119962 1.2556643 1.2868124 1.2923920 1.2884980 1.2889539 1.2726265 \n       57        59        64        66        69        71        72        73 \n1.2316819 1.2574693 1.2544183 1.2250269 1.2161932 1.1997594 1.2238602 1.1740769 \n       76        77        78        80        81        82        83        90 \n1.2016273 1.1730867 1.1831393 1.1495913 1.1658541 1.1392808 1.1255433 0.9620724 \n       92        95        96       100       103       106       107       109 \n1.0555989 1.0678431 1.0118759 1.0386631        NA 0.9924835 0.9759377 0.8072950 \n      113       115       118       124       125       126       131       135 \n0.7929260 0.9196825 0.8406228 0.7647379 0.8645359 0.8175708 0.6811526 0.7280377 \n      136       137       138       139 \n0.5490131 0.7535235 0.5643139 0.4975936 \n\npredval.test &lt;- predict(train.mod, newdata = htest)  # saves these predicted values from the testing dataset.\n\n## Calculating R^2\ntest.mod.resid &lt;- htest$Social.support - predval.test\nSSE &lt;- sum(test.mod.resid^2, na.rm = T)\nSSE\n\n[1] 1.541793\n\ntest.resid &lt;- htest$Social.support - mean(htest$Social.support, na.rm = T)\nSST &lt;- sum(test.resid^2, na.rm = T)\n(SST - SSE)/SST\n\n[1] 0.7040524\n\n\n\n\nInterpreting Cross Validation\nYou‚Äôll often see a few different methods of evaluating model fit.\n\n\\(R^2\\). Our good friend. The proportion of variance explained by the model (vs.¬†the mean)\nRooted Mean Squared Error (RMSE). The average amount of residual error (actual - predicted values).\nMean Absoulte Error. The average of the absolute value of the residuals; less sensitive to outliers than RMSE or \\(R^2\\).\n\nAnd there are different methods of defining the test and training datasets. And different packages and tutorials to do this. Here‚Äôs one, called ‚ÄúLeave one out cross validation - LOOCV‚Äù; gif below via Wikipedia.\n\n\n\n\n\n\n# install.packages(\"caret\")\nlibrary(caret)\n\nWarning: package 'caret' was built under R version 4.3.3\n\n\nLoading required package: lattice\n\ntrain.control &lt;- trainControl(method = \"LOOCV\")\nloocvmod &lt;- train(Social.support ~ Ladder.score * Log.GDP.per.capita, data = h, method = \"lm\",\n                  trControl = train.control, na.action = \"na.omit\")\nprint(loocvmod)\n\nLinear Regression \n\n143 samples\n  2 predictor\n\nNo pre-processing\nResampling: Leave-One-Out Cross-Validation \nSummary of sample sizes: 139, 139, 139, 139, 139, 139, ... \nResampling results:\n\n  RMSE       Rsquared   MAE      \n  0.1902603  0.6720989  0.1361974\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n\n\n\n\nWatch Out for Multicollinearity.\nIf your independent variables are highly related, then your multivariate regression slope estimates are not uniquely determined. weird things happen to your coefficients, and this makes it hard to interpret your effects.\n\nIN R : check the ‚Äúvariance inflation factor‚Äù (VIF); a measure of how much one IV is related to all the other IVs in the model. ‚ÄúTradition‚Äù is that if VIF is &gt; 5 (or I‚Äôve also seen VIF &gt; 10) there‚Äôs a problem in the regression.\n\\(\\huge VIF_j=\\frac{1}{1-R_{j}^{2}}\\)\n\n\nlibrary(car)\n\nWarning: package 'car' was built under R version 4.3.3\n\n\nLoading required package: carData\n\nvif(mod4) # doesn't seem like multicollinearity is a problem.\n\nthere are higher-order terms (interactions) in this model\nconsider setting type = 'predictor'; see ?vif\n\n\n                          scale(Ladder.score) \n                                     2.490104 \n                    scale(Log.GDP.per.capita) \n                                     2.443312 \nscale(Ladder.score):scale(Log.GDP.per.capita) \n                                     1.057269 \n\n## creating a highly correlated second IV for the sake of this example.\njitter(h$Healthy.life.expectancy, 300)\n\n  [1]  0.63649887  0.68930612  0.71828952  0.66823278  0.78147940  0.64639281\n  [7]  0.66783200  0.65547462  0.71468602  0.72225337  0.71773673  0.64999642\n [13]  0.62952106  0.64185465  0.72092831  0.69836634  0.64067483  0.60323449\n [19]  0.61564585  0.70589567  0.74527554  0.54082440  0.57966955  0.74032124\n [25]  0.52293196  0.54462176  0.75101008  0.55563685  0.60367356  0.74810486\n [31]  0.69789473  0.54448689  0.56294229  0.69718620  0.62746987  0.69055692\n [37]  0.62267421  0.70380172  0.62007029  0.68723153  0.74690172  0.49826800\n [43]  0.58734907  0.50056514  0.69238462  0.52731539  0.57692015  0.64389105\n [49]  0.57280871  0.69425428  0.82612563  0.77274534  0.38400182  0.57725457\n [55]  0.70352020  0.61930331  0.55762718  0.57337490  0.53553203  0.65959492\n [61]  0.45026458          NA  0.66845212  0.69307250  0.59281784  0.52861070\n [67]  0.54621673  0.69875356  0.45710762  0.43500797  0.56482900  0.54559839\n [73]  0.50964668  0.60188374  0.54674710  0.60113762  0.41675213  0.67774238\n [79]  0.45482879  0.51207586  0.53754193  0.65166685  0.26579890  0.58060330\n [85]  0.53234986  0.85807969  0.66611649          NA  0.25647428  0.12829974\n [91]  0.51774245  0.49561179  0.47517286  0.47895859  0.35411168  0.25447142\n [97]  0.21655523  0.58171885  0.35686944  0.51373138  0.48182973  0.20760587\n[103]          NA  0.31835400  0.46658725  0.34832926  0.52310604  0.28958395\n[109]  0.33832352  0.33133514  0.42088085  0.27791840  0.16285129  0.31895572\n[115]  0.62433954  0.30866350  0.37751970  0.43665557  0.41469131  0.38656597\n[121]  0.32196352  0.25386893  0.35348747  0.26579842  0.62022055  0.43845057\n[127]  0.49687114  0.62927456  0.48772773  0.38520825  0.37719937  0.40156511\n[133]  0.33585422  0.29027768  0.12197319  0.39450247  0.18769012  0.21595348\n[139]  0.24276263  0.24909294 -0.03799866  0.57172747  0.27471988\n\nh$health2 &lt;- jitter(h$Healthy.life.expectancy, 300)\n\nplot(h$health2, h$Healthy.life.expectancy) # yup.\n\n\n\n\n\n\n\nmultimod &lt;- lm(Ladder.score ~ Healthy.life.expectancy + health2, data = h) # both in the model.\nsummary(multimod) # results! Things look good....\n\n\nCall:\nlm(formula = Ladder.score ~ Healthy.life.expectancy + health2, \n    data = h)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.0155 -0.4313  0.1701  0.5191  1.5809 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               2.6514     0.2202  12.041  &lt; 2e-16 ***\nHealthy.life.expectancy   7.5885     1.9725   3.847 0.000182 ***\nhealth2                  -2.0676     1.8604  -1.111 0.268360    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7703 on 137 degrees of freedom\n  (3 observations deleted due to missingness)\nMultiple R-squared:  0.5809,    Adjusted R-squared:  0.5747 \nF-statistic: 94.93 on 2 and 137 DF,  p-value: &lt; 2.2e-16\n\nvif(multimod) # ...but wait!\n\nHealthy.life.expectancy                 health2 \n               24.78939                24.78939 \n\n\n\n\nThings to Read!\n\nAn overview of ML methods (including the partitioning approach) for evaluating models\nAnother overview of cross-validation methods.\nSome more notes on multicollinearity and VIFs.\n\n\n\nA Discussion on Effect Size and the Course and Psychology\n\nThere‚Äôs a LOT of Error in Our Predictions of People.\n\n\n\nRichard, F. D., Bond Jr, C. F., & Stokes-Zoota, J. J. (2003). One hundred years of social psychology quantitatively described. Review of general psychology, 7(4), 331-363.\n\n\n\n\nBut Life, In General, is Complex\n\nARTICLE : Meyer, G. J., Finn, S. E., Eyde, L. D., Kay, G. G., Moreland, K. L., Dies, R. R., Eisman, E.J., Kubiszyn, & Reed, G. M. (2001). Psychological testing and psychological assessment: A review of evidence and issues. American psychologist, 56(2), 128.\nACTIVITY :\n\nFind a ‚ÄúHard Science‚Äù effect from Table 1. Does the size of this correlation surprise you? Why / why not?\nFind a ‚ÄúPsych Science‚Äù effect from Table 2. Does the size of this correlation surprise you? Why / why not?\n\n\n\n\nSmall Effects Matter\n\nFunder DC, Ozer DJ. Evaluating Effect Size in Psychological Research: Sense and Nonsense. Advances in Methods and Practices in Psychological Science. 2019;2(2):156-168. doi:10.1177/2515245919847202\n\n\n\n\nDoing Good Science is Hard But Important to Be ‚ÄúValid‚Äù\n\nAllen C, Mehler DMA (2019) Correction: Open science challenges, benefits and tips in early career and beyond. PLOS Biology 17(12): e3000587. https://doi.org/10.1371/journal.pbio.3000587\n\n\n\n\nPercentages of null findings among RRs and traditional (non-RR) literature [46,47], with their respective 95% confidence intervals.\n\n\n\n\nOther Methods Exist for Describing the World\n\nQualitative studies.\nPhilosophy.\nCritical Theory.\nHistory.\nArt.\nReligion / faith.\n\n\n\nWhat are the Benefits of Using the Scientific Approach? What are the Limitations (or Dangers)?\n\n\nIs a ‚ÄúValid‚Äù Psychological Science Possible?\n\n\n\n\n\n\n\nWeek 1\nWeek 13\n\n\n\n\n\ngoes here from check-in.\n\n\n\ngoes here from check-in.\n\n\n\nFarewell!¬†Feel¬†free¬†to¬†stay¬†in¬†touch¬†:)¬†it¬†has¬†been¬†a¬†pleasure¬†and¬†privilege¬†to¬†work¬†with¬†y‚Äôall¬†this¬†semester¬†&lt;3"
  },
  {
    "objectID": "gradstats/gradlabs/13_AllDone.html#watch-out-for-multicollinearity.",
    "href": "gradstats/gradlabs/13_AllDone.html#watch-out-for-multicollinearity.",
    "title": "13_AllDone",
    "section": "Watch Out for Multicollinearity.",
    "text": "Watch Out for Multicollinearity.\nIf your independent variables are highly related, then your multivariate regression slope estimates are not uniquely determined. weird things happen to your coefficients, and this makes it hard to interpret your effects.\n\nIN R : check the ‚Äúvariance inflation factor‚Äù (VIF); a measure of how much one IV is related to all the other IVs in the model. ‚ÄúTradition‚Äù is that if VIF is &gt; 5 (or I‚Äôve also seen VIF &gt; 10) there‚Äôs a problem in the regression.\n\\(\\huge VIF_j=\\frac{1}{1-R_{j}^{2}}\\)\n\n\nlibrary(car)\n\nLoading required package: carData\n\nvif(mod4) # doesn't seem like multicollinearity is a problem.\n\nthere are higher-order terms (interactions) in this model\nconsider setting type = 'predictor'; see ?vif\n\n\n                          scale(Ladder.score) \n                                     2.490104 \n                    scale(Log.GDP.per.capita) \n                                     2.443312 \nscale(Ladder.score):scale(Log.GDP.per.capita) \n                                     1.057269 \n\n## creating a highly correlated second IV for the sake of this example.\njitter(h$Healthy.life.expectancy, 300)\n\n  [1]  0.63649887  0.68930612  0.71828952  0.66823278  0.78147940  0.64639281\n  [7]  0.66783200  0.65547462  0.71468602  0.72225337  0.71773673  0.64999642\n [13]  0.62952106  0.64185465  0.72092831  0.69836634  0.64067483  0.60323449\n [19]  0.61564585  0.70589567  0.74527554  0.54082440  0.57966955  0.74032124\n [25]  0.52293196  0.54462176  0.75101008  0.55563685  0.60367356  0.74810486\n [31]  0.69789473  0.54448689  0.56294229  0.69718620  0.62746987  0.69055692\n [37]  0.62267421  0.70380172  0.62007029  0.68723153  0.74690172  0.49826800\n [43]  0.58734907  0.50056514  0.69238462  0.52731539  0.57692015  0.64389105\n [49]  0.57280871  0.69425428  0.82612563  0.77274534  0.38400182  0.57725457\n [55]  0.70352020  0.61930331  0.55762718  0.57337490  0.53553203  0.65959492\n [61]  0.45026458          NA  0.66845212  0.69307250  0.59281784  0.52861070\n [67]  0.54621673  0.69875356  0.45710762  0.43500797  0.56482900  0.54559839\n [73]  0.50964668  0.60188374  0.54674710  0.60113762  0.41675213  0.67774238\n [79]  0.45482879  0.51207586  0.53754193  0.65166685  0.26579890  0.58060330\n [85]  0.53234986  0.85807969  0.66611649          NA  0.25647428  0.12829974\n [91]  0.51774245  0.49561179  0.47517286  0.47895859  0.35411168  0.25447142\n [97]  0.21655523  0.58171885  0.35686944  0.51373138  0.48182973  0.20760587\n[103]          NA  0.31835400  0.46658725  0.34832926  0.52310604  0.28958395\n[109]  0.33832352  0.33133514  0.42088085  0.27791840  0.16285129  0.31895572\n[115]  0.62433954  0.30866350  0.37751970  0.43665557  0.41469131  0.38656597\n[121]  0.32196352  0.25386893  0.35348747  0.26579842  0.62022055  0.43845057\n[127]  0.49687114  0.62927456  0.48772773  0.38520825  0.37719937  0.40156511\n[133]  0.33585422  0.29027768  0.12197319  0.39450247  0.18769012  0.21595348\n[139]  0.24276263  0.24909294 -0.03799866  0.57172747  0.27471988\n\nh$health2 &lt;- jitter(h$Healthy.life.expectancy, 300)\n\nplot(h$health2, h$Healthy.life.expectancy) # yup.\n\n\n\n\n\n\n\nmultimod &lt;- lm(Ladder.score ~ Healthy.life.expectancy + health2, data = h) # both in the model.\nsummary(multimod) # results! Things look good....\n\n\nCall:\nlm(formula = Ladder.score ~ Healthy.life.expectancy + health2, \n    data = h)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.0155 -0.4313  0.1701  0.5191  1.5809 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               2.6514     0.2202  12.041  &lt; 2e-16 ***\nHealthy.life.expectancy   7.5885     1.9725   3.847 0.000182 ***\nhealth2                  -2.0676     1.8604  -1.111 0.268360    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7703 on 137 degrees of freedom\n  (3 observations deleted due to missingness)\nMultiple R-squared:  0.5809,    Adjusted R-squared:  0.5747 \nF-statistic: 94.93 on 2 and 137 DF,  p-value: &lt; 2.2e-16\n\nvif(multimod) # ...but wait!\n\nHealthy.life.expectancy                 health2 \n               24.78939                24.78939 \n\n\n\nWould You Like to Learn More??\n\nAn overview of ML methods (including the partitioning approach) for evaluating models\nAnother overview of cross-validation methods.\nSome more notes on multicollinearity and VIFs."
  },
  {
    "objectID": "gradstats/gradlabs/13_AllDone.html#more-mlm-or-in-this-case-linear-mixed-model",
    "href": "gradstats/gradlabs/13_AllDone.html#more-mlm-or-in-this-case-linear-mixed-model",
    "title": "",
    "section": "More MLM (or, in this case, ‚ÄúLinear Mixed Model‚Äù)",
    "text": "More MLM (or, in this case, ‚ÄúLinear Mixed Model‚Äù)\n\nICE BREAKER : what‚Äôs your favorite place in the bay area to see nature?\nTHE STUDY : participants were shocked while looking at either virtual nature, urban, or indoor images (while getting their brains scanned).\nTHE TABLE :\n\nWhat is going on with this models?\nWhat seems important?\nWhat seems irrelevant?\nWhat questions do you have?\n\n\n\n\n\nAbstract [full article]\nLinear Model Table [link to SI materials]"
  },
  {
    "objectID": "gradstats/gradlabs/13_AllDone.html",
    "href": "gradstats/gradlabs/13_AllDone.html",
    "title": "",
    "section": "",
    "text": "d‚Äî title: ‚ÄúLecture 13 : All Done‚Äù format: html"
  },
  {
    "objectID": "gradstats/gradlabs/13_AllDone.html#check-in-the-learning-has-stopped-long-live-the-learning",
    "href": "gradstats/gradlabs/13_AllDone.html#check-in-the-learning-has-stopped-long-live-the-learning",
    "title": "",
    "section": "Check-In : The Learning Has Stopped (Long Live The Learning)",
    "text": "Check-In : The Learning Has Stopped (Long Live The Learning)"
  }
]