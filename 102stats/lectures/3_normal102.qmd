---
title: "Lecture 3 : Normal Distributions"
format: html
---

## [Check-In Here](https://docs.google.com/forms/d/e/1FAIpQLSeVfYrb3ADGQqNv2V4FRomGcgryvdTog9KkeybpcuPa71ErWQ/viewform?usp=header)

**Look at the distribution below.**

-   What's the shape of this distribution? What does this shape tell you?

-   What would you consider to be an outlier in the distribution? Why?

```{r}
#| echo: true
d <- read.csv("../datasets/Protestant Work Ethic/data.csv", sep = "\t")

hist(d$Q1E, breaks = 100, 
     main = "RT for Answering Q1",
     xlab = "Response Time (RT) in ms",
     col = 'black', bor = 'white')
# abline(v = mean(d$Q1E, na.rm = T), col = 'red', lwd = 5)
```

**Question:** What other information would you want to know to determine who is an outlier on this question? (And why would this information be relevant?)

-   student response go here.
-   student response go here.
-   student response go here.
-   student response go here.

**IN R : finding and removing outliers**

```{r}


```

## Announcements & Agenda

**Goal : understand why and how the normal distribution is "normal"; focus more on understanding the mean as a prediction of this distribution vs. a prediction of the population, and learn one method of estimating how well the mean describes the population.**

[**R Script For Today**](https://www.dropbox.com/scl/fi/u1j2aukhlbwprns8s9vco/3_BootstrappingSelfEsteem.R?rlkey=xjhafvr9c49283h0mcrhfj9ec&dl=0)

-   9:10 - 9:30 \| Check-In and Removing Outliers
-   9:30 - 10:00 \| Lab 2 Review
-   10:00 - 10:30 \| Mean and Normal Distributions
-   10:30 - 10:40 \| BREAK TIME
-   10:40 - 11:00 \| Presentation
-   11:00 - 12:00 \| Sampling Error

## RECAP : Lab 2

### Loading a Different Kind of Dataset

-   Professor did not give good instructions on...

    -   ...how to turn the 0s into NAs

    -   ...how to load data that are not comma separated

-   Professor Method

```{r}
## Loading Dataset
selfes <- read.csv("../datasets/Self-Esteem Dataset/data.csv",
                   stringsAsFactors = T,
                   na.strings = "0", sep = "\t")
```

-   Other Methods (or Processes to figuring things out?)

    -   student examples go here maybe

### Creating a Scale \[skipping for time...see key?\]

1.  **Organize your items; reverse-score; evaluate reliability.**

```{r}
## Creating the Scale
poskey.df <- selfes[,c(1:2,4,6,7)] # pos-keyed items (from the codebook)
negkey.df <- selfes[,c(3,5,8:10)] # neg-keyed items (from the codebook)
negkeyR.df <- 5-negkey.df # reverse scoring the neg-keyed items
SELFES.DF <- data.frame(poskey.df, negkeyR.df) # bringing it all 2gether.

library(psych) # loading the library
alpha(SELFES.DF) # alpha reliability.
```

2.  **Average the items into one variable; graph & describe.**

```{r}
selfes$SELFES <- rowMeans(SELFES.DF, na.rm = T) # creating the scale
hist(selfes$SELFES, col = 'black', bor = 'white', # the graph
     main = "Histogram of Self-Esteem", 
     xlab = "Self-Esteem Score", breaks = 15)
```

### Mean is a Prediction (of the Sample)

::: panel-tabset
#### Where is the Mean?

```{r}
lils <- selfes[sample(1:nrow(selfes), 100),]# 100 random data points
plot(lils$SELFES, 
     ylab = "Self-Esteem (100 Points)",
     xlab = "Index") 
```

#### There is the Mean!

```{r}
plot(lils$SELFES,
     ylab = "Self-Esteem (100 Points)",
     xlab = "Index") 
abline(h = mean(lils$SELFES, na.rm = T), lwd = 5)
```

#### Quantifying Errors (Residuals)

```{r}
## quantifying errors (residuals)
residuals <- selfes$SELFES - mean(selfes$SELFES, na.rm = T)
SST <- sum(residuals^2, na.rm = T)
SST

SST/length(residuals) # average of squared residuals (variance)
sqrt(SST/length(residuals)) # average of residuals, unsquared (standard deviation)

sd(selfes$SELFES, na.rm = T) # 
```
:::

## The "Normal" Distribution

### When do we see a "normal" distribution?

1.  When "life is complex" (multiple influences on an outcome.)
2.  That complexity is independent.

### Discussion : Why is this variable *almost* Normal?

1.  What are the multiple & independent influences on people's self-esteem?
2.  What are some of the non-independent influences on people's self-esteem (that might make this not perfectly normal?)

```{r}
hist(selfes$SELFES, col = 'black', bor = 'white')
```

### Activity : Let's simulate a normal distribution in R

**Goal :** define an object called "life" that simulates what 1000 people's lives look like, if each life is the summation of 10 random coin flips (heads = 0, tails = 1).

**Code you will need :**

-   `coinflip <- c(0,1) # defining a coin-flip.`

-   `sample(x, n) # randomly sample from x n times`

-   `replicate(n, expr) # to repeat an expression n times`

-   `for(){} # our good friend the foor loop.`

**Professor Code Goes Here.**

```{r}


```

**What if the Coin is Biased?**

Modify the code to simulate 1000 coin flips where there's a 80% chance of flipping one option (i.e., increase the probability of flipping either heads = 0 or tails = 1).

What type of distribution do you expect to see? Why??

*Note : the sample() function can take another argument (prob) that can adjust the probability.*

```{r}

```

::: callout-warning
### !!! Critical Race Theory DEI Alert!!!

Francis Galton was a super racist and inventor of eugenics, and influenced (or invented) many statistics that we use today. For example, he defined the "central limit theorem" with the Galton Board (see image on the right). Whereas before, scholars considered the "average" to be the ideal state of humanity (it is closest to all the people; the *Platonic Ideal!*), Galton considered the goal of humanity to achieve to be better than average - something we have internalized today.

![](images/clipboard-2535747291.png){width="667"}

Indeed, Galton had a motivated agenda to use statistics to demonstrate there was a hierarchy to individual "eminence." In his own words:

*To conclude, the range of mental power between—I will not say the highest Caucasian and the lowest savage—but between the greatest and least of English intellects, is enormous. ... I propose in this chapter to range men according to their natural abilities, putting them into classes separated by equal degrees of merit, and to show the relative number of individuals included in the several classes.....The method I shall employ for discovering all this, is an application of the very curious theoretical law of “deviation from an average.” First, I will explain the law, and then I will show that the production of natural intellectual gifts comes justly within its scope. -* Galton, Hereditary Genius (1869). [Linked here](https://galton.org/books/hereditary-genius/text/v5/galton-1869-hereditary-genius-v5.htm).

**Why does it matter that a super racist invented statistics? I have a few ideas, but would like to hear your thoughts first :)**

-   reasons relevant :
-   reasons not reelvant:
:::

::: column-margin
![](images/clipboard-4189238383.png){width="372"}
:::

## BREAK TIME MEET BACK AT 10:30 & PRESENTATIONS

-   link to [article presentation](https://docs.google.com/presentation/d/1YZQ45_oj6TgiSIUpU7N6Ek5iTk2nn4T5RIpCz6Xi1K4/edit?usp=sharing)

## Sampling Error (Conceptual)

### Scientific Method Stuff

-   Sample v. Population
    -   Population : All the people relevant to your research question.
    -   Sample : The people in your study.
    -   KEY IDEA : Our sample will never equal the population!
        -   Sampling Bias : our sample differs from the population in **predictable** ways.
        -   Sampling Error : our sample differs from the population in **random** ways.
        -   Random : Each individual in the population has an equal probability of being in our sample.
-   For Lab 3 : Find an article; is the sample representative (probably not)? How might bias influence the results?!

### Sampling Error in R

1.  Be an omnipotent higher power who can create an entire world of individuals.

```{r}
# rnorm(10000000, mean = 100, sd = 20)
fakey <- rnorm(10000000, mean = 100, sd = 10)
length(fakey)
head(fakey)
```

2.  Run some stats on these data as we do.

```{r}
mean(fakey, na.rm = T)
hist(fakey)
abline(v = mean(fakey), lwd = 5)
```

3.  Take a random sample from this population.

```{r}
?sample # our friend, the sample function
sample(1:10, 1) # are you vibing with R?
sample(1:10000000, 1) # are you REALLY vibing?
sample(1:length(fakey), 1) # the numbers to sample from, another way. why better?
sample(1:length(fakey), 10) # a small sample
fakey[sample(1:length(fakey), 10)] # ten random individuals from fakey.
fakey[10000001]
lilfakey <- fakey[sample(1:length(fakey), 10)] # ten random individuals from fakey.
lilfakey
```

4.  Run some stats on this sample.

```{r}
mean(lilfakey)
hist(lilfakey)
abline(v = mean(lilfakey), lwd = 5)
```

5.  Repeat These Steps Until You Get "THE TRUTH"

```{r}
lilfakey <- fakey[sample(1:length(fakey), 10)] # ten random individuals from fakey.
hist(lilfakey, xlim = c(0,200), ylim = c(0,10),
     breaks = 5,
     main = paste(c("mean=", round(mean(lilfakey), 4)), sep = ""))
abline(v = mean(lilfakey), lwd = 5)
```

6.  Doing this 1000 times

```{r}
    truthbucket <- array()
    for(i in c(1:1000)){
      lilfakey <- fakey[sample(1:length(fakey), 10)] # ten random individuals from fakey.
      truthbucket[i] <- mean(lilfakey)
    }
    length(truthbucket)
    hist(truthbucket)
    mean(truthbucket)
```

**What's the Point, Professor??? (Sampling Error Edition)**

-   

## Bootstrapping to Estimate Sampling Error

Okay, let's work through a real example of using bootstrapping to estimate sampling error, and why this might be useful.

Remember that in the onboarding survey, we saw people rated their own skills as lower than their classmates' skills.

```{r}
d <- read.csv("../datasets/Onboarding Data/honor_onboard_FA25.csv", stringsAsFactors = T, na.strings = "")
par(mfrow = c(1,2))

hist(d$self.skills, breaks = c(0:5), 
     col = 'black', bor = 'white', main = "Computer Skills\n(Self-Perceptions)")

hist(d$class.skills, breaks = c(0:5),
     col = 'black', bor = 'white', main = "Computer Skills\n(Perceptions of Classmates)")
```

But would we expect to observe this same difference in a different sample of students???

Let's use bootstrapping to test this.

```{r}


```

### For Lab 3 :

1.  Keep getting practice working with datasets and interpreting variables in R.
2.  Try using Quarto.
3.  Use (and adapt) the bootstrapping for-loop to estimate how much sampling error we can expect in variables, and in the difference between variables.
