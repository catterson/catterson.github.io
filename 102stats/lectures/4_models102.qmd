---
title: "Lecture 4 | Sampling Error and Linear Models"
format: 
  html:
    code-overflow: 'wrap'
---

![](images/clipboard-3051119200.png){fig-align="center" width="519"}

## [Check-In Here](https://docs.google.com/forms/d/e/1FAIpQLSe-5mOk5GyxloW5syXGjPQ4zayc4ZaeYHmzkroLxVz7uOP_Jg/viewform?usp=header)

**Load the onboarding dataset (name this d to follow along with professor code in lecture).**

The variable `can.forloop` asked students whether they could write a for-loop or not.

What is the difference in the number of students who said that YES they could for-loop, compared to the number who said either NO, MAYBE, or had NO IDEA? 

Great to find the answer, but try to find a way to get R to calculate this difference using code so you don't have to manually type in any numbers for the calculation. (Hint : use indexing and the summary function.)

```{r}
d <- read.csv("~/Dropbox/!GRADSTATS/Datasets/CLASS DATASETS - 102 - FA25/Onboarding Data/honor_onboard_FA25.csv", stringsAsFactors = T)

head(d)
summary(d$can.forloop)

# asking R to find the # of yeses
length(d$can.forloop[d$can.forloop == "yes"]) # says Zero because it's Yes not yes
length(d$can.forloop[d$can.forloop == "Yes"]) # says Zero because it's Yes not yes

d$can.forloop == "Yes"
sum(d$can.forloop == "Yes") # adds up all the True values.

summary(d$can.forloop) # shows me the frequency for each level
summary(d$can.forloop)[4] # the number of Yes

# asking R to find the other groups.
sum(!d$can.forloop == "Yes") # adds up all the True values for things that ARE NOT ! can.forloop = "Yes"

# finding the difference.
sum(d$can.forloop == "Yes") - sum(!d$can.forloop == "Yes")

## OR; define quick objects to make the code a little more readable.
notYes <- sum(!d$can.forloop == "Yes")
Yes <- sum(d$can.forloop == "Yes")

Yes - notYes

```

### Announcements & Agenda

**Agenda**

-   11:10 - 12:00 : Check-In and Bootstrapping Review
-   12:00 - 12:40 : Break + Presentation
-   12:40 - 1:00 : Linear Models (the basics)
-   1:00 - 2:00 : Linear Models (there's more)

**Announcements**

-   **Lab 3.** Just giving everyone credit. Four rooms was wild I don't like thinking about it.

-   **No Brian Exam.** Professor did not know what he was doing but thinks two exams in two weeks is a BAD IDEA.

-   **Mini Exam is in TWO Weeks.** Yikes!!

    -   I give you data and a question, you generate a report (in Quarto? Or just screenshot copy/paste for now is fine too.)

        -   Data loading and cleaning.

        -   Scale creating & descriptive statistics.

        -   Linear Models

        -   Bootstrapping

        -   A fun challenge problem worth 1 point.

    -   Ask questions if / when you have them. Don't struggle on your own. Plenty of time to do that in other spaces!

    -   We will practice / review next week (Lab 5 is a practice exam.)

    -   Think it will be chill, and if not then professor takes the blame, alright?

## RECAP : Estimating Sampling Error

### the for-loop stuff we ended on.

We defined a fake dataset, called fakey with mean = 100 and sd = 30.

```{r}
fakey <- rnorm(10000000, mean = 100, sd = 30)
hist(fakey)
abline(v = mean(fakey), lwd = 5)
mean(fakey)
```

We then took 1000 random samples (n = 10) from this fake dataset, calculated a statistic for each of these samples, and then

```{r}
truthbucket <- array()
for(i in c(1:1000)){
  lilfakey <- fakey[sample(1:length(fakey), 10)] # ten random individuals from fakey.
  truthbucket[i] <- mean(lilfakey)
}

hist(truthbucket)
abline(v = mean(truthbucket), lwd = 5, col = 'red')
mean(truthbucket) # the mean
sd(truthbucket) # the standard deviation 
```

#### DISCUSSION :

-   ICE-BREAKER : least favorite candy?

-   why are we doing this (taking random samples from this fake dataset?)

    -   TLDR : trying to see how valid the samples we take out of the dataset are to the population; one sample is not necessarily valid because of a) sampling bias (people may differ in systematic ways) and b) **sampling error** (random people in our sample are different from the population and might bias our results in non-systematic ways).

    -   TLDR4R : estimating effects of sampling error

-   what does the distribution of fakey describe?

    -   a NORMAL distributon of the entire population of individual scores.

    -   the variation in our distribution illustrates the fact that people in the population differ.

-   what does the distribution of truthbucket describe?

    -   a NORMAL distibution of a test statistic (the mean, in this case) from a sample of (n = 10) individuals.

    -   the variation in our distribution illustrates sampling error (the fact that each sample will yeild a slightly different mean from our "true" population mean.

-   which graph (TRUTHBUCKET or FAKEY) would you want to minimize variation in?

    -   minimize variation in TRUTHBUCKET = each sample will be more representative of the "true" population.

    -   minimize variation in FAKEY = making people be more similar to each other...which can be bad from a celebrating individual differences perspective (yay bay area; boo fascism) but...increased variation —\> "more error"

-   what are TWO things you could do in order to decrease the standard deviation of truthbucket?

    -   increase the sample size

    -   increase the number of samples

    -   decrease the standard deviation of our original population

#### ACTIVITY : the TWO things you could do in order to decrease the standard deviation of lilfakey.

-   Thing One : increase the sample size?

```{r}
truthbucket100 <- array()
for(i in c(1:1000)){
  lilfakey <- fakey[sample(1:length(fakey), 100)] # 100 random individuals from fakey.
  truthbucket100[i] <- mean(lilfakey)
}

hist(truthbucket100)
abline(v = mean(truthbucket100), lwd = 5, col = 'red')
mean(truthbucket100) # the mean
sd(truthbucket100) # the standard deviation 
```

-   Thing Two : Increase the number of samples.

```{r}
megatruthbucket <- array()
for(i in c(1:10000)){
  lilfakey <- fakey[sample(1:length(fakey), 10)] # ten random individuals from fakey.
  megatruthbucket[i] <- mean(lilfakey)
}

hist(megatruthbucket)
abline(v = mean(megatruthbucket), lwd = 5, col = 'red')
mean(megatruthbucket) # the mean
sd(megatruthbucket) # the standard deviation 
```

-   Why does increasing the number of samples not influence the standard deviation of TRUTHBUCKET???

    -   too long, zoned out : each sample is still taking from the same population and the same number of people...so increasing the number of samples doesn't really influence how far away each individual statistic will be.

    -   the standard deviation of truth bucket = an estimate of sampling error = equivalent to "standard error".

### bootstrapping.

#### the logic.

Okay, let's work through a real example of using a for-loop to estimate sampling error with real data.

The idea is somewhat impossible (just like raising yourself up by your bootstraps) : we are going to :

1.  use our ONE dataset....to generate NEW datasets
2.  from each NEW dataset, we will :
    1.  calculate the test statistic we are interested in (i.e., the mean, but could be anything.)
    2.  save this test statistics.
    3.  examine the variation in test statistics.

**Discussion :** what's a problem with this method? why do people do this??

#### an example.

Remember that in the onboarding survey, we saw people rated their own skills as lower than their classmates' skills.

```{r}
d <- read.csv("../datasets/Onboarding Data/honor_onboard_FA25.csv", stringsAsFactors = T, na.strings = "")
par(mfrow = c(1,2))

hist(d$self.skills, breaks = c(0:5), 
     col = 'black', bor = 'white', main = "Computer Skills\n(Self-Perceptions)")

hist(d$class.skills, breaks = c(0:5),
     col = 'black', bor = 'white', main = "Computer Skills\n(Perceptions of Classmates)")
mean(d$self.skills)
```

But would we expect to observe this same difference in a different sample of students???

Let's use a for-loop to do this. Here's the logic bootstrapping to test this.

```{r}
d[sample(1:nrow(d), # taking our original dataset (d) and indexing new rows from 1-nrow(d)
         nrow(d), # tells R to resample the same size as our original dataset
         replace = T), ] # randomly sampling with replacement. so one person in the original data can be in the new dataset multiple times.
d[sample(1:nrow(d), nrow(d), replace = T), ] # same code, all as one line.

newclass <- d[sample(1:nrow(d), nrow(d), replace = T), ]
mean(newclass$self.skills)

allmyclassesaregreat <- array()
for(i in c(1:1000)){
  newclass <- d[sample(1:nrow(d), nrow(d), replace = T), ]
  allmyclassesaregreat[i] <- mean(newclass$self.skills)
}

hist(allmyclassesaregreat)

```

### defining the 95% confidence interval.

```{r}

```

![](images/clipboard-2205385268.png)

## BREAK TIME : Meet Back at 12:50

## [Presentations](https://docs.google.com/presentation/d/1YZQ45_oj6TgiSIUpU7N6Ek5iTk2nn4T5RIpCz6Xi1K4/edit?usp=sharing)

## RECAP : The Mean as Prediction

```{r}
#| include: false
d <- read.csv("~/Dropbox/!GRADSTATS/Datasets/CLASS DATASETS - 102 - FA25/Onboarding Data/honor_onboard_FA25.csv",
                           stringsAsFactors = T)
```

### The Mean is a Prediction (of the Sample)

::: panel-tabset
## Where is the Mean?

```{r}
plot(d$self.skills, 
     ylab = "Self-Perception of Skills",
     xlab = "Index") 
abline(h = mean(d$self.skills, na.rm = T), lwd = 0)
```

## There is the Mean!

```{r}
plot(d$self.skills, 
     ylab = "Self-Perception of Skills",
     xlab = "Index") 
abline(h = mean(d$self.skills, na.rm = T), lwd = 5)
```

## This is How We Really Visualize The Mean!

```{r}
hist(d$self.skills, 
     ylab = "Frequency",
     xlab = "Self-Perception of Skills") 
abline(v = mean(d$self.skills, na.rm = T), lwd = 5)
```
:::

## There is Error in Our Prediction of the Sample (Residual Error)

This prediction of the sample has some error (residual error). We can (and will need to) quantify this error.

```{r}
## quantifying errors (residuals)
residuals <- d$self.skills - mean(d$self.skills, na.rm = T)
SST <- sum(residuals^2)
SST

SST/length(residuals) # average of squared residuals (variance)
sqrt(SST/length(residuals)) # average of residuals, unsquared (standard deviation)

sd(d$self.skills) # slightly higher
sqrt(SST/(length(residuals)-1)) # the 'real' equation; n-1 to inflate our estimate / adjust for small samples.
```

## The Mean is a Prediction of our Population (with Sampling Error)

```{r}
m <- array()
for(i in c(1:1000)){
 nd <- d[sample(1:nrow(d), nrow(d), replace = T),] # a new sample
 m[i] <- mean(nd$self.skills, na.rm = T)
}
mean(d$self.skills, na.rm = T)
mean(m) # similar!

sum(m > 2.5) # all of them (100% greater than the midpoint of the scale.)

sd(m) # sampling error!

hist(m, xlim = c(1,5)) # our distribution of sampling estimates 
abline(v = c(mean(d$self.skills),
             mean(d$self.skills) + 1.96 * sd(m),
             mean(d$self.skills) - 1.96 * sd(m)),
       lwd = c(5,2,2), # two line widths
       lty = c(1,2,2)) # two line types
```

## Linear Models : Improving our Predictions (Numeric IV)

### The Mean as a Linear Model

We can define a linear model to predict a variable from some constant value (1), and the result of that will be the mean, since the mean is our best prediction (minimizes the residual errors) when we don't have any other information about the variable.

```{r}
lm(self.skills ~ 1, data = d) # predicting self.skills from a constant (1), using the datset = d
mod0 <- lm(self.skills ~ 1, data = d) # saving this as a model object
coef(mod0) # looking at the coefficients
mod0$residuals # finding the residuals
```

### Use information in the IV to predict the DV

Let's try the same activity, but now we will graph each individual's self-skill (still on the y-axis) in **relationship** to their perception of their classmates' skill (on the x-axis).

::: panel-tabset
## Where is the Line?

```{r}
plot(jitter(self.skills) ~ learn.r, data = d, 
     ylab = "Self-Perception of Skills", ylim = c(1,5),
     xlab = "Confidence in Learning R", xlim = c(1,5)) 
abline(lm(self.skills ~ learn.r, data = d), lwd = 0)
```

## There is the Line!

```{r}
plot(jitter(self.skills) ~ learn.r, data = d, 
     ylab = "Self-Perception of Skills", ylim = c(1,5),
     xlab = "Confidence in Learning R", xlim = c(1,5)) 
abline(lm(self.skills ~ learn.r, data = d), lwd = 5)
```
:::

**The Linear Model** :

![](images/clipboard-1514248515.png){fig-align="center" width="279"}

To define a linear model, we will first use the `lm()` function to predict some DV from an IV.

Then, we will graph the relationship between these two variables using the `plot()` function. I'm using `jitter()` on the DV in order to shift the points a little, since they are overlapping.

Then, I draw a line (defined by the linear model) using the `abline()` function. I've made the line width = 5 and color red to make it POP.

I can look at the coefficients of the model with the `coef()` function. These coefficients are described by the starting place of the line when the x value is zero (the intercept), and the adjustment we make to Y as the X values increase.

```{r}
mod1 <- lm(self.skills ~ learn.r, data = d)

plot(jitter(self.skills) ~ learn.r, # dv is jittered
     data = d, 
     main = "Jittered Data",
     xlim = c(1,5), ylim = c(1,5))

abline(mod1, lwd = 5, col = 'red')

coef(mod1)
# intercept = 1.46 = the predicted value of Y when ALL X values are ZERO.
# slope = .46 = relationship between learn.r and our DV (self.skills)
### as learn.r increase by ONE, then self.skills will increase by .46
### these units are in the original unit of measurement (1-5 likert scale.)
```

### There is Error in Our Prediction (residual error --\> R\^2)

In the graph above, I can see that the dots are not all exactly on the line. My predictions are wrong; this is residual error!

For example, a person who said their R knowledge is a 2 rated their skills as a 1. But this is different from our prediction (the line.)

To calculate this specific prediction, we can plug the person's R knowledge rating of 1 into our linear model :

-   self.skill \~ 1.46 + .46 \* learn.r

-   self.skill \~ 1.46 + .46 \* 2

-   self.skill \~ 2.38

So we predict a person who's learnR score is 2 would have a self-skill of 2.38. But this person is different - we call this a residual score = the difference between their actual score and our prediction = 2 - 2.38 = -.38. The fact this value is negative means the person is below the line.

Fortunately, R does the residual calculations for us, from the linear model object.

```{r}
mod1$residuals # R does the residual calculation for us. what will happen if we add this up?
sum(mod1$residuals) # they add to....
SSE <- sum(mod1$residuals^2) # so I square them
SSE # the total squared error when I use my model to make predictions.

## Visualizing Our Errors. (distance between actual scores and the line).
par(mfrow = c(1,2))
plot(d$self.skills, 
     ylab = "Self-Perception of Skills",
     xlab = "Index", main = "Mean as Model \n(SST = Total Sum of Squared Errors)") 
abline(h = mean(d$self.skills, na.rm = T), lwd = 5)
plot(jitter(self.skills) ~ learn.r, data = d, main = "Linear Model \n(SSE = Sum of Squared Errors When Model Making Predictions)", 
     xlim = c(1,5)) # jittered
abline(mod1, lwd = 5, col = 'red')


SST <- sum((d$self.skills - mean(d$self.skills))^2) # defining the total error

SST # the total squared error when usign the mean to make predictions.
SST - SSE # a difference in errors when using the mean vs. our model
(SST - SSE)/SST # the relative difference in errors = R^2 (R-squared.)

summary(mod1)$r.squared # R does this for us. But good to do "by hand" to understand.

```

### There is Error in Our Prediction of the Population (sampling error)

Yes, it's bootstrapping time!

```{r}
#| eval: false
#| include: true
bucket <- array()
for(i in c(1:1000)){
  nd <- d[sample(1:nrow(d), nrow(d), replace = T), ]
  modx <- lm(self.skills ~ learn.r, data = nd)
  bucket[i] <- coef(modx)[2]
}
hist(bucket) # what do we expect to see?
abline(v = mean(bucket), lwd = 5)
abline(v = mean(bucket) + 1.96*sd(bucket), lwd = 2, lty = 'dashed')
abline(v = mean(bucket) - 1.96*sd(bucket), lwd = 2, lty = 'dashed')
mean(bucket)
sd(bucket)



```

### Time for Another Example?

Probably not! But maybe!!!?

```{r}
names(d) # what other (numeric, for now) variable might predict self.skills?

```

## FOR LAB 4.

1.  Define linear models to predict a numeric DV from a numeric IV.
2.  Interpret the intercept, slope, and R\^2 value.
3.  Do some bootstrapping.
4.  Repeat w/ a different dataset.
5.  Yeah!
