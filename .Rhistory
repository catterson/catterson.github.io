head(extra.df) # checking to make sure I did this correctly
alpha(extra.df) # r is warning me that some are negatively keyed...
library(psych)
names(d)
d[,22:31]
extra.df <- d[,22:31] # using the codebook to index the 10 extraversion items
head(extra.df) # checking to make sure I did this correctly
alpha(extra.df)
alpha(extra.df, check.keys = T) # seeing that some are negatively keyed...
## look at the codebook and confirm these are negatively keyed
extraPOS <- extra.df[,c(1,3,5,7,9)] # defining the positively keyed items
extraNEG <- extra.df[,c(2,4,6,8,10)] # defining the negatively keyed items
extraNEG <- extra.df[,c(2,4,6,8,10)] # defining the negatively keyed items
extraNEG
6-extraNEG
extraNEG <- 6-extraNEG # reverse scoring my negatively keyed items
extraCLEAN <- cbind(extraPOS, extraNEG)
head(extraCLEAN)
alpha(extraCLEAN) # high alpha; hooray.
d$EXTRA <- rowMeans(extraCLEAN,   # 10 items into one variable.
na.rm = TRUE) # still calculate if there’s missing data
hist(d$EXTRA)                     # What do you learn / observe?
names(d)
library(psych)
names(d)
open.df <- d[,61:71] # using the codebook to index the 10 openversion items
head(open.df) # checking to make sure I did this correctly
alpha(open.df) # r is warning me that some are negatively keyed...
alpha(open.df, check.keys = T) # seeing that some are negatively keyed...
## look at the codebook and confirm these are negatively keyed
openPOS <- open.df[,c(1,3,5,7,9)] # defining the positively keyed items
openNEG <- open.df[,c(2,4,6,8,10)] # defining the negatively keyed items
openNEG <- 6-openNEG # reverse scoring my negatively keyed items
openCLEAN <- cbind(openPOS, openNEG)
head(openCLEAN)
alpha(openCLEAN) # high alpha; hooray.
alpha(openCLEAN) # high alpha; hooray.
openPOS <- open.df[,c(1,3,5,7,9, 10)] # defining the positively keyed items
openNEG <- open.df[,c(2,4,6,8)] # defining the negatively keyed items
openNEG <- 6-openNEG # reverse scoring my negatively keyed items
openCLEAN <- cbind(openPOS, openNEG)
head(openCLEAN)
alpha(openCLEAN) # high alpha; hooray.
open.df
open.df <- d[,62:71] # using the codebook to index the 10 openversion items
head(open.df) # checking to make sure I did this correctly
open.df <- d[,62:71] # using the codebook to index the 10 openversion items
head(open.df) # checking to make sure I did this correctly
alpha(open.df) # r is warning me that some are negatively keyed...
alpha(open.df, check.keys = T) # seeing that some are negatively keyed...
## look at the codebook and confirm these are negatively keyed
openPOS <- open.df[,c(1,3,5,7,9,10)] # defining the positively keyed items
openNEG <- open.df[,c(2,4,6,8)] # defining the negatively keyed items
openNEG <- 6-openNEG # reverse scoring my negatively keyed items
openCLEAN <- cbind(openPOS, openNEG)
head(openCLEAN)
alpha(openCLEAN) # high alpha; hooray.
openPOS <- open.df[,c(1,3,5,7,9)] # defining the positively keyed items
openNEG <- open.df[,c(2,4,6,8,10)] # defining the negatively keyed items
openNEG <- 6-openNEG # reverse scoring my negatively keyed items
openCLEAN <- cbind(openPOS, openNEG)
head(openCLEAN)
alpha(openCLEAN) # high alpha; hooray.
openPOS <- open.df[,c(1,3,5,7,9, 10)] # defining the positively keyed items
openNEG <- open.df[,c(2,4,6,8)] # defining the negatively keyed items
openNEG <- 6-openNEG # reverse scoring my negatively keyed items
openCLEAN <- cbind(openPOS, openNEG)
head(openCLEAN)
alpha(openCLEAN) # high alpha; hooray.
d$OPEN <- rowMeans(openCLEAN,   # 10 items into one variable.
na.rm = TRUE) # still calculate if there’s missing data
hist(d$OPEN)
## Adapt this Code to Define Another Personality Scale (BORD Activity).
# STEP 1: ORGANIZING THE ITEMS AND CHECKING THE ALPHA RELIABILITY
names(d)
neg.df <- d[,32:41]
head(neg.df) # checking to make sure I did this correctly
alpha(neg.df) # r is warning me that some are negatively keyed...
negPOS <- neg.df[,c(1,3,5,7,9,6,8,10)] # defining the positively keyed items
negNEG <- neg.df[,c(2,4)] # defining the negatively keyed items
negNEG <- 6-negNEG # reverse scoring my negatively keyed items
negCLEAN <- cbind(negPOS, negNEG)
head(negCLEAN)
alpha(negCLEAN) # high alpha; hooray.
d$NEG <- rowMeans(negCLEAN,   # 10 items into one variable.
na.rm = TRUE) # still calculate if there’s missing data
hist(d$NEG)
### loading datasets and libraries
install.packages("psych") # only ONE time!!!!
library(psych)
d <- read.csv("~/Dropbox/!WHY STATS/Class Datasets/214 - Class Datasets - FA25/Mini Dataset/mini_dvc_data.csv")
d <- read.csv("~/Dropbox/!WHY STATS/Class Datasets/214 - Class Datasets - FA25/Mini Dataset/mini_dvc_data.csv", stringsAsFactors = T)
head(d)
### removing outliers from a variable.
hist(hrs.sleep)
d <- read.csv("~/Dropbox/!WHY STATS/Class Datasets/214 - Class Datasets - FA25/Mini Dataset/mini_dvc_data.csv", stringsAsFactors = T)
### removing outliers from a variable.
hist(hrs.sleep)
### removing outliers from a variable.
hist(d$hrs.sleep)
### removing outliers from a variable.
hist(d$hrs.sleep, breaks = 15)
hist(d$hrs.sleep, breaks = 15, col = "lavender")
mean(d$hrs.sleep)
sd(d$hrs.sleep)
abline(v = mean(d$hrs.sleep), lwd = 5)
sd(d$hrs.sleep)
abline(v = mean(d$hrs.sleep) + sd(d$hrs.sleep), lwd = 5, lty = "dashed")
abline(v = mean(d$hrs.sleep) - sd(d$hrs.sleep), lwd = 5, lty = "dashed")
names(d)
plot(hrs.sleep ~ tired, data = d)
d$hrs.sleep[d$hrs.sleep < 24]
d$hrs.sleep[d$hrs.sleep > 20]
d$hrs.sleep[d$hrs.sleep > 20] <- NA
mean(d$hrs.sleep)
sd(d$hrs.sleep)
abline(v = mean(d$hrs.sleep), lwd = 5)
abline(v = mean(d$hrs.sleep) + sd(d$hrs.sleep), lwd = 5, lty = "dashed")
abline(v = mean(d$hrs.sleep) - sd(d$hrs.sleep), lwd = 5, lty = "dashed")
names(d)
## LINEAR MODEL PREVIEW.
plot(hrs.sleep ~ tired, data = d)
mod <- lm(hrs.sleep ~ tired, data = d)
mod <- lm(hrs.sleep ~ tired, data = d)
abline(mod, lwd = 5, col = 'red')
plot(hrs.sleep ~ bored, data = d)
mod <- lm(hrs.sleep ~ bored, data = d)
abline(mod, lwd = 5, col = 'red')
plot(tired ~ bored, data = d)
mod <- lm(tired ~ bored, data = d)
abline(mod, lwd = 5, col = 'red')
d$hrs.sleep[d$hrs.sleep > 20]
d <- read.csv("~/Dropbox/!WHY STATS/Class Datasets/214 - Class Datasets - FA25/Mini Dataset/mini_dvc_data.csv", stringsAsFactors = T)
head(d)
### removing outliers from a variable.
hist(d$hrs.sleep, breaks = 15, col = "lavender")
d <- read.csv("~/Dropbox/!WHY STATS/Class Datasets/214 - Class Datasets - FA25/Mini Dataset/mini_dvc_data.csv", stringsAsFactors = T)
head(d)
### graphing a variable.
hist(d$hrs.sleep, breaks = 15, col = "lavender")
mean(d$hrs.sleep)
sd(d$hrs.sleep)
abline(v = mean(d$hrs.sleep), lwd = 5)
abline(v = mean(d$hrs.sleep) + sd(d$hrs.sleep), lwd = 5, lty = "dashed")
abline(v = mean(d$hrs.sleep) - sd(d$hrs.sleep), lwd = 5, lty = "dashed")
d$hrs.sleep[d$hrs.sleep > 20]
##  d$hrs.sleep = in the dataset d look for the variable hrs.sleep
##  [d$hrs.sleep > 20] = indexing = asking R to point to values where d$hrs.sleep is greater than 20
d$hrs.sleep[d$hrs.sleep > 20] <- NA
## <- NA # assign NA to this value.
hist(d$hrs.sleep, breaks = 15, col = "lavender")
abline(v = mean(d$hrs.sleep, na.rm = T), lwd = 5)
abline(v = mean(d$hrs.sleep, na.rm = T) + sd(d$hrs.sleep, na.rm = T), lwd = 5, lty = "dashed")
abline(v = mean(d$hrs.sleep, na.rm = T) - sd(d$hrs.sleep, na.rm = T), lwd = 5, lty = "dashed")
m <- mean(d$hrs.sleep, na.rm = T)
s <- sd(d$hrs.sleep, na.rm = T)
m
## making this code ^^ look less junky!!!
mean(d$hrs.sleep)
selfes <- read.csv("~/Dropbox/!GRADSTATS/Datasets/Self-Esteem Dataset/data.csv",
stringsAsFactors = T,
na.strings = "0", sep = "\t")
## Q. What is the standard deviation of the variable age?
sd(d$age) # 15.02
d <- read.csv("~/Dropbox/!WHY STATS/Chapter Datasets/narcissism_data.csv", stringsAsFactors = T)
names(d)
library(psych)
## Q. How many individuals are in the dataset?
nrow(d) # 11243
## DO NOT REMOVE OUTLIERS YET!
## Q. What is the mean of the variable age.
describe(d$age)
mean(d$age) # 34.01
## Q. What is the median of the variable age?
median(d$age)
## Q. What is the standard deviation of the variable age?
sd(d$age) # 15.02
d <- read.csv("~/Dropbox/!WHY STATS/Class Datasets/214 - Class Datasets - FA25/Mini Dataset/mini_dvc_data.csv", stringsAsFactors = T)
## nrow(d) #
nrow(d)
plot(d$stoned72)
summary(d$stoned72)
names(d)
plot(selfes ~ height, data = d)
d$selfes
d$height
d$height[d$height > 80]
d$height[d$height > 80] <- NA
plot(selfes ~ height, data = d)
d$height[d$height < 20] <- NA
plot(selfes ~ height, data = d)
abline(lm(selfes ~ height, data = d))
plot(selfes ~ insta.follows, data = d)
abline(lm(selfes ~ insta.follows, data = d))
max(d$insta.follows)
max(d$insta.follows, na.rm = T)
max(scale(d$insta.follows), na.rm = T)
d <- read.csv("~/Dropbox/!GRADSTATS/Datasets/World Happiness Report - 2024/World-happiness-report-2024.csv", stringsAsFactors = T)
head(d)
hist(d$Ladder.score)
hist(d$Ladder.score, xlim = c(0,10))
d$Ladder.score
mean(d$Ladder.score)
d$Ladder.score[62] # country 62
d$Ladder.score[62] - mean(d$Ladder.score)
sd(d$Ladder.score)
(d$Ladder.score[62] - mean(d$Ladder.score))/sd(d$Ladder.score)
scale(d$Ladder.score)
cbind("Raw Score" == d$Ladder.score, "ZScore" == scale(d$Ladder.score))
data.frame("Raw Score" == d$Ladder.score, "ZScore" == scale(d$Ladder.score))
data.frame(cbind("Raw Score" == d$Ladder.score, "ZScore" == scale(d$Ladder.score)))
data.frame(cbind("Raw Score" = d$Ladder.score, "ZScore" = scale(d$Ladder.score)))
hist(d$Social.support)
mod <- lm(Ladder.score ~ Social.support, data = d)
plot(Ladder.score ~ Social.support, data = d)
abline(mod, lwd = 5)
abline(h = mean(d$Ladder.score))
abline(v = mean(d$Social.support))
mean(d$Social.support)
abline(v = mean(d$Social.support, na.rm = T))
mod <- lm(Ladder.score ~ Social.support, data = d) # define the model.
plot(Ladder.score ~ Social.support, data = d)
abline(mod, lwd = 5)
coef(mod)
coef(mod)
abline(h = mean(d$Ladder.score))
abline(v = mean(d$Social.support, na.rm = T))
## our prediction of happiness = 2.26 + 2.88 * Social.support
2.26 + 2.88 * 1
2.26 + 2.88 * .25
2.26 + 2.88 * 5
d$upperwhisker
cbind(d$lowerwhisker, d$Ladder.score, d$upperwhisker)
## THE BOOTSTRAPPING STUFF TO ESTIMATE SAMPLING ERROR.
mod <- lm(Ladder.score ~ Social.support, data = d) # define the model.
plot(Ladder.score ~ Social.support, data = d)
abline(mod, lwd = 5)
coef(mod)
d[sample(1:nrow(d), nrow(d), replace = T), ]
coef(mod)
coef(mod)[2]
nw <- d[sample(1:nrow(d), nrow(d), replace = T), ]
head(nw)
blah <- lm(Ladder.score ~ Social.support, data = nw) # same model, new data
blah
coef(blah)[2]
bucket[i] <- coef(blah)[2]
bucket <- array() # define a place to save our slopes.
bucket[i] <- coef(blah)[2] # save slope to bucket
i <- 1
bucket[i] <- coef(blah)[2] # save slope to bucket
bucket
bucket <- array() # define a place to save our slopes.
for(i in c(1:1000)){
nw <- d[sample(1:nrow(d), nrow(d), replace = T), ] # new data
blah <- lm(Ladder.score ~ Social.support, data = nw) # same model, new data
coef(blah)[2] # find slope
bucket[i] <- coef(blah)[2] # save slope to bucket
}
hist(bucket)
abline(v = mean(bucket), lwd = 5)
sd(bucket)
coef(mod)[2]
hist(bucket)
abline(v = coef(mod)[2], lwd = 5)
abline(v = coef(mod)[2] + sd(bucket), lwd = 1)
abline(v = coef(mod)[2] - sd(bucket), lwd = 1)
abline(v = coef(mod)[2], lwd = 5)
abline(v = coef(mod)[2] + 1.96 * sd(bucket), lwd = 2, lty = "dashed")
abline(v = coef(mod)[2] - 1.96 * sd(bucket), lwd = 2, lty = "dashed")
hist(bucket)
abline(v = coef(mod)[2], lwd = 5)
abline(v = coef(mod)[2] + 1.96 * sd(bucket), lwd = 2, lty = "dashed")
abline(v = coef(mod)[2] - 1.96 * sd(bucket), lwd = 2, lty = "dashed")
coef(mod)[2] + 1.96 * sd(bucket) # upper limit of slope, estimating sampling error
coef(mod)[2] + 1.96 * sd(bucket) # lower limit
coef(mod)[2] + 1.96 * sd(bucket) # upper limit of slope, estimating sampling error
coef(mod)[2] - 1.96 * sd(bucket) # lower limit
d <- read.csv("~/Library/CloudStorage/Dropbox/!WHY STATS/Class Datasets/101 - Class Datasets - FA25/mini_cal_data.csv", na.strings="", stringsAsFactors=TRUE)
View(d)
names(d)
nrow(d)
hist(d$stoned72)
d$stoned72 <- as.numeric(as.character(d$stoned72))
hist(d$stoned72)
as.numeric(as.character(d$stoned72))
d$stoned72 <- as.character(d$stoned72)
hist(d$stoned72)
d$stoned72 <- as.numeric(d$stoned72)
hist(d$stoned72)
as.numeric(d$stoned72)
d$stoned72
d <- read.csv("~/Library/CloudStorage/Dropbox/!WHY STATS/Class Datasets/101 - Class Datasets - FA25/mini_cal_data.csv", na.strings="", stringsAsFactors=TRUE)
plot(d$stoned72)
levels(d$stoned72)
d$stoned72
summary(d$stoned72)
summary(d)
d <- read.csv("~/Library/CloudStorage/Dropbox/!WHY STATS/Class Datasets/101 - Class Datasets - FA25/mini_cal_data.csv", stringsAsFactors=TRUE)
names(d)
nrow(d)
summary(d)
plot(d$stoned72)
levels(d$stoned72)
levels(d$stoned72)[1]
levels(d$stoned72)[1] <- NA
plot(d$stoned72)
summary(d$stoned72)
data.frame(d$selfes, d$is.happy, d$satlife, d$bored)
HAP.df <- data.frame(d$selfes, d$is.happy, d$satlife, d$bored)
head(HAP.df)
HAP.df <- cbind(d$selfes, d$is.happy, d$satlife, d$bored)
head(HAP.df)
HAP.df <- with(d, data.frame(selfes, is.happy, satlife, bored))
head(HAP.df)
HAP.df <- d[,c(7, 10, 20, 31)]
head(HAP.df)
alpha(HAP.df)
library(psych)
alpha(HAP.df)
d$bored
range(d$bored, na.rm = T)
10-d$bored
cbind("Regular" = d$bored, "Reverse Scored" = 10-d$bored)
HAP.df <- cbind(d$selfes, d$is.happy, d$satlife, 10-d$bored)
alpha(HAP.df)
HAP.df <- cbind(d$selfes, d$is.happy, d$satlife, 10-d$bored)
alpha(HAP.df)
alpha(HAP.df)
HAP.df
rowMeans(HAP.df)
rowMeans(HAP.df, na.rm = T)
d$HAPPY <-rowMeans(HAP.df, na.rm = T)
hist(d$HAPPY)
hist(d$HAPPY, xlim = c(0,10))
describe(d$HAPPY)
hist(d$HAPPY, xlim = c(0,50))
hist(d$HAPPY)
hist(d$HAPPY, xlim = c(0,10))
d$HAPPY[42]
d$HAPPY[42]
d$HAPPY[42] - mean(d$HAPPY, na.rm = T)
mean(d$HAPPY, na.rm = T)
d$HAPPY == mean(d$HAPPY, na.rm = T)
sd(d$HAPPY, na.rm = T)
(d$HAPPY[42] - mean(d$HAPPY, na.rm = T)) / sd(d$HAPPY, na.rm = T)
scale(d$HAPPY)
scale(d$HAPPY)[42]
(d$HAPPY[42] - mean(d$HAPPY, na.rm = T)) / sd(d$HAPPY, na.rm = T)
happyitisalmostover <- rowMeans(HAP.df, na.rm = T)
d$happyitisalmostover
d$happyitisalmostover
names(d)
max(d$HAPPY, na.rm = T)
scale(max(d$HAPPY, na.rm = T))
(9.5 - mean(d$HAPPY, na.rm = T))
(9.5 - mean(d$HAPPY, na.rm = T))/sd(d$HAPPY, na.rm = T)
d$stoned72 == "yes"
d$stoned72
d$stoned72 == "yes"
d[d$stoned72 == "yes",]
dstonedY <- d[d$stoned72 == "yes",]
dY <- d[d$stoned72 == "yes",]
dN <- d[d$stoned72 == "no",]
dY$HAPPY
dN$HAPPY
mean(dY$HAPPY, na.rm = T)
mean(dN$HAPPY, na.rm = T)
par(mfrow = c(1,2))
hist(dY$HAPPY, main = "Happiness (Was Stoned in Last 72 Hrs)")
hist(dN$HAPPY, main = "Happiness (Was Not Stoned)")
d[d$HAPPY > 3,]
####################################################
## Chapter 6 : Linear Models
####################################################
## CHECK-IN.
d <- read.csv("~/Dropbox/!WHY STATS/Class Datasets/214 - Class Datasets - FA25/Mini Dataset/mini_dvc_data.csv", stringsAsFactors = T)
####################################################
## Chapter 6 : Linear Models
####################################################
## CHECK-IN.
d <- read.csv("~/Dropbox/!WHY STATS/Chapter Datasets/World-happiness-report-2024.csv", stringsAsFactors = T)
names(d)
head(d)
hist(d$Ladder.score)
## calculating the mean
hist(d$Ladder.score)
## calculating the mean
mean(d$Ladder.score)
summary(d$Ladder.score)
describe(d$Ladder.score)
####################################################
## Chapter 6 : Linear Models
####################################################
## CHECK-IN.
library(psych)
describe(d$Ladder.score)
summary(d$Ladder.score)
summary(d)
## finding the maximum and minimum values.
range(d$Ladder.score)
summary(d$Ladder.score)
max(d$Ladder.score)
## z-scores review. (why the f*!@& do we care about this???)
d$Ladder.score[42] # has a happiness of 6.287
## IT IS DIFFERENT FROM THE MEAN.
d$Ladder.score[42] - mean(d$Ladder.score) # difference from mean
sd(d$Ladder.score)
## ILLUSTRATING THIS.
hist(d$Ladder.score)
abline(v = mean(d$Ladder.score), lwd = 5)
abline(v = d$Ladder.score[42], col = 'blue', lwd = 2)
abline(v = mean(d$Ladder.score) + sd(d$Ladder.score), lty = 2) # the mean
abline(v = mean(d$Ladder.score) - sd(d$Ladder.score), lty = 2) # the mean
abline(v = mean(d$Ladder.score) - sd(d$Ladder.score), lty = 2, lwd = 2) # the mean
abline(v = mean(d$Ladder.score) + sd(d$Ladder.score), lty = 2, lwd = 2) # the mean
hist(d$Ladder.score)
abline(v = mean(d$Ladder.score), lwd = 5) # the mean
abline(v = d$Ladder.score[42], col = 'blue', lwd = 3) # the mean
abline(v = mean(d$Ladder.score) + sd(d$Ladder.score), lty = 2, lwd = 3) # the mean + sd
abline(v = mean(d$Ladder.score) - sd(d$Ladder.score), lty = 2, lwd = 3)
(d$Ladder.score[42] - mean(d$Ladder.score))/sd(d$Ladder.score)
scale(d$Ladder.score)[42]
cbind(d$Ladder.score, scale(d$Ladder.score))
max(scale(d$Ladder.score))
min(scale(d$Ladder.score))
round(mean(scale(d$Ladder.score)), 5)
mean(scale(d$Ladder.score))
plot(Ladder.score ~ Social.support, data = d)
plot(d$Ladder.score ~ d$Social.support)
plot(Ladder.score ~ Social.support, data = d)
plot(d$Ladder.score ~ d$Social.support)
mod <- lm(d$Ladder.score ~ d$Social.support)
abline(mod, lwd = 5, col = "gold")
coef(mod)
## use the model --> specific predictions.
## Ladder = 2.26 + 2.88 * social support
2.26 + 2.88 * .5
2.26 + 2.88 * 1.5
names(d)
names(d)[6:11]
## EXAMPLE : BALPRIT
plot(d$Ladder.score ~ d$Healthy.life.expectancy)
mod <- lm(d$Ladder.score ~ d$Healthy.life.expectancy)
abline(mod, lwd = 5, color = "gold")
abline(mod, lwd = 5, color = "gold")
abline(mod, lwd = 5, color = "gold")
abline(mod, lwd = 5, color = "gold")
abline(mod, lwd = 5, color = "gold")
abline(mod, lwd = 5, color = "gold")
abline(mod, lwd = 5, color = "gold")
abline(mod, lwd = 5, color = "gold")
abline(mod, lwd = 5, color = "gold")
abline(mod, lwd = 5, color = "gold")
abline(mod, lwd = 5, color = "gold")
abline(mod, lwd = 5, color = "gold")
abline(mod, lwd = 5, color = "gold")
abline(mod, lwd = 5, color = "gold")
abline(mod, lwd = 5, color = "gold")
abline(mod, lwd = 5, color = "gold")
abline(mod, lwd = 5, color = "gold")
abline(mod, lwd = 5, color = "gold")
abline(mod, lwd = 5, color = "gold")
abline(mod, lwd = 5, color = "gold")
abline(mod, lwd = 5, color = "gold")
abline(mod, lwd = 5, color = "gold")
abline(mod, lwd = 5, color = "gold")
abline(mod, lwd = 5, color = "gold")
abline(mod, lwd = 5, color = "gold")
abline(mod, lwd = 5, color = "gold")
abline(mod, lwd = 5, color = "gold")
abline(mod, lwd = 5, color = "gold")
abline(mod, lwd = 5, color = "gold")
abline(mod, lwd = 5, color = "gold")
abline(mod, lwd = 5, color = "gold")
abline(mod, lwd = 5, color = "gold")
abline(mod, lwd = 5, color = "gold")
abline(mod, lwd = 5, color = "gold")
abline(mod, lwd = 5, color = "gold")
abline(mod, lwd = 5, color = "gold")
abline(mod, lwd = 5, color = "gold")
abline(mod, lwd = 5, color = "gold")
abline(mod, lwd = 5, col = "gold")
plot(d$Ladder.score ~ d$Healthy.life.expectancy)
mod <- lm(d$Ladder.score ~ d$Healthy.life.expectancy)
abline(mod, lwd = 5, col = "gold")
coef(mod)
mod
hist(d$Healthy.life.expectancy)
range(d$Healthy.life.expectancy)
range(d$Healthy.life.expectancy, na.rm = T)
plot(d$Ladder.score ~ d$Healthy.life.expectancy)
mod <- lm(d$Ladder.score ~ d$Healthy.life.expectancy)
abline(mod, lwd = 5, col = "gold")
coef(mod)
plot(d$Ladder.score ~ d$Healthy.life.expectancy, xlim = c(0, 1))
mod <- lm(d$Ladder.score ~ d$Healthy.life.expectancy)
abline(mod, lwd = 5, col = "gold")
coef(mod)
## EXAMPLE : BALPRIT
plot(d$Ladder.score ~ d$Healthy.life.expectancy, xlim = c(0, 1), ylim = c(0,10))
mod <- lm(d$Ladder.score ~ d$Healthy.life.expectancy)
abline(mod, lwd = 5, col = "gold")
coef(mod)
## ladder = 2.70 + 5.44 * HLE
2.70 + 5.44 * 1
## OFFICE HOURS. 10/8/2025
## TOPICS
### functions happen to datasets
d <- read.csv("~/Dropbox/!WHY STATS/Chapter Datasets/World-happiness-report-2024.csv", stringsAsFactors = T)
nrow
nrow(d$Ladder.score)
