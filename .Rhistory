hist(mod$residuals)
bucket <- array()
for(i in c(1:1000)){
n2 <- n[sample(1:nrow(n), nrow(n), replace = T), ] # new sample; based on the old sample
boot.mod <- lm(score ~ age, data = n2) # new model, based on the new sample.
bucket[i] <- coef(boot.mod)[2] # my slope, saved to bucket
}
bucket <- array()
for(i in c(1:1000)){
n2 <- n[sample(1:nrow(n), nrow(n), replace = T), ] # new sample; based on the old sample
boot.mod <- lm(score ~ age, data = n2) # new model, based on the new sample.
bucket[i] <- coef(boot.mod)[2] # my slope, saved to bucket
}
```
bucket <- array()
for(i in c(1:1000)){
n2 <- n[sample(1:nrow(n), nrow(n), replace = T), ] # new sample; based on the old sample
boot.mod <- lm(score ~ age, data = n2) # new model, based on the new sample.
bucket[i] <- coef(boot.mod)[2] # my slope, saved to bucket
}
sd(bucket)
coef(mod)[2] + 1.96*sd(bucket) # upper limit of 95% CI for slope
coef(mod)[2] - 1.96*sd(bucket) # lower limit
coef(mod)[2] # slope from my original sample
coef(mod)[2] # slope from my original sample
coef(mod)[2] + 1.96*sd(bucket) # upper limit of 95% CI for slope
coef(mod)[2] - 1.96*sd(bucket) # lower limit
sd(bucket)
n[c(n$elapse < 45 | n$elapse > 3600) & !is.na(n$elapse), ]
p <- read.csv("~/Dropbox/!GRADSTATS/gradlab/Datasets/Protestant Work Ethic/data.csv",
stringsAsFactors = T,
sep = "\t", na.strings = "0")
names(p)
?seq
keys <- seq(1,57, by = 3)
p <- p[,c(keys, 58:ncol(p))]
names(p)
write.csv(p, "~/Dropbox/!WHY STATS/101Exams/protestant_work_ethic_cleaned.csv", row.names = F)
summary(p$age)
hist(p$age)
p <- read.csv("~/Downloads/protestant_work_ethic_cleaned.csv", stringsAsFactors=TRUE)
View(p)
head(p)
nrow(p)
names(p)
p[,1:19]
PWE.df <- data.frame(p[,1:19])
head(PWE.df)
p[,c(1:8, 10-12, 14, 16-19)]
p[,c(1:8, 10:12, 14, 16:19)]
posPWE.df <- p[,c(1:8, 10:12, 14, 16:19)]
negPWE.df <- p[,c(9, 13, 15)]
negPWE.df
range(PWE.df, na.rm = T)
RnegPWE.df <- 6-negPWE.df
PWEALL.df <- cbind(posPWE.df, RnegPWE.df)
PWEALL.df <- cbind(posPWE.df, RnegPWE.df)
head(PWEALL.df)
### Libraries Used
library(psych)
psych::alpha(PWEALL.df)
p$PWE
p$PWE <- rowMeans(PWEALL.df, na.rm = T)
hist(p$PWE)
hist(p$PWE, main = "", xlab = "Protestant Work Ethic Scale (1-5)",
col = 'black', bor = 'white')
describe(p$PWE)
## Problem 3. Remove outliers and graph a numeric variable [4 points]. The variable testelapse measures (in seconds) how long people took to answer the survey questions. There are outliers in this variable that you will need to remove.
hist(p$testelapse)
50000/60
50000/60/60
hist(p$testelapse[p$testelapse < 60 * 60])
1000/60
hist(p$testelapse[p$testelapse < 60 * 30])
60 * 30
sd(p$testelapse)
p$testelapse[p$testelapse < 60 * 30]
p$testelapse[p$testelapse > 60 * 30]
length(p$testelapse[p$testelapse > 60 * 30])
p[p$testelapse > 60 * 30] <- NA
p[p$testelapse > 60 * 30,]
p[p$testelapse > 60 * 30,] <- NA
p[p$testelapse > 60 * 30,]
hist(p$testelapse)
range(p$testelapse, na.rm = T)
ncol(p)
p[p$testelapse < 65,]
p[p$testelapse < 65,] <- NA
p[p$testelapse < 65 & !is.na(p$testelapse),]
nrow(p[p$testelapse < 65 & !is.na(p$testelapse),]) # more outliers!!
### Libraries Used
library(psych)
### Loading the Data
p <- read.csv("~/Downloads/protestant_work_ethic_cleaned.csv", stringsAsFactors=TRUE)
head(p)
p[,1:19] # indexing the first 19 variables (my items)
PWE.df <- data.frame(p[,1:19]) # creating an object that refers to these 19 items.
head(PWE.df) # checking my work.
posPWE.df <- p[,c(1:8, 10:12, 14, 16:19)]
negPWE.df <- p[,c(9, 13, 15)]
range(PWE.df, na.rm = T) # how to reverse score; a 1-5 scale, so subtract from 1+5 = 6.
RnegPWE.df <- 6-negPWE.df # my reverse-scored items
PWEALL.df <- cbind(posPWE.df, RnegPWE.df)
## whatstudentsprobablydid.df <- with(p, data.frame(Q1A, Q2A, Q3A...., 6-Q9A))
head(PWEALL.df)
psych::alpha(PWEALL.df) # raw alpha = .88
# Create a new variable in the dataset that is the average of the 19 items. Graph a histogram of this new variable, and change the default settings so the graph looks “nice”.
p$PWE <- rowMeans(PWEALL.df, na.rm = T)
hist(p$PWE, main = "", xlab = "Protestant Work Ethic Scale (1-5)",
col = 'black', bor = 'white')
# Report the mean, standard deviation, and range of this scale, and describe the shape of the distribution that you see in terms of skewness. Then, describe what each of these statistics (mean, sd, range) tells you about people’s beliefs about the protestant work ethic.
describe(p$PWE)
# Does the scale seem like a valid way to measure this variable? Why / why not?
## Problem 3. Remove outliers and graph a numeric variable [4 points]. The variable testelapse measures (in seconds) how long people took to answer the survey questions. There are outliers in this variable that you will need to remove.
hist(p$testelapse)
hist(p$testelapse[p$testelapse < 60 * 30])
sd(p$testelapse)
60 * 30
p$testelapse[p$testelapse > 60 * 30] # there are 13 outliers
length(p$testelapse[p$testelapse > 60 * 30]) # there are 13 outliers who were too slow!!
p[p$testelapse < 65 & !is.na(p$testelapse),] # more outliers!!
nrow(p[p$testelapse < 65 & !is.na(p$testelapse),]) # 33 more outliers who were too fast!
p[p$testelapse < 65,] # more outliers!!
p[p$testelapse > 60 * 30,] <- NA
p[p$testelapse < 65 & !is.na(p$testelapse),] <- NA
hist(p$testelapse)
summary(p$testelapse)
60 * 30
sd(p$testelapse)
sd(p$testelapse, na.rm = T)
max(p$testelapse
## the max value as a z-score
max(p$testelapse)
## the max value as a z-score
max(p$testelapse, na.rm = T)
max(p$testelapse, na.rm = T) - mean(p$testelapse, na.rm = T)
(max(p$testelapse, na.rm = T) - mean(p$testelapse, na.rm = T))/sd(p$testelapse, na.rm = T)
summary(p$testelapse)
sd(p$testelapse, na.rm = T)
168/60
## Mini Exam - Professor Key - SP25
### Libraries Used
library(psych)
### Loading the Data
p <- read.csv("~/Downloads/protestant_work_ethic_cleaned.csv", stringsAsFactors=TRUE)
head(p)
nrow(p)
names(p)
### P2. Create a scale. Create a scale for the 19-Protestant Work Ethic Variables. Note that questions 9A, 13A, and 15A are negatively-keyed, and will need to be reverse scored.
# Combine the items into a data.frame and report the alpha reliability of the items. Does the scale seem reliable?
p[,1:19] # indexing the first 19 variables (my items)
PWE.df <- data.frame(p[,1:19]) # creating an object that refers to these 19 items.
head(PWE.df) # checking my work.
posPWE.df <- p[,c(1:8, 10:12, 14, 16:19)]
negPWE.df <- p[,c(9, 13, 15)]
range(PWE.df, na.rm = T) # how to reverse score; a 1-5 scale, so subtract from 1+5 = 6.
RnegPWE.df <- 6-negPWE.df # my reverse-scored items
PWEALL.df <- cbind(posPWE.df, RnegPWE.df)
## whatstudentsprobablydid.df <- with(p, data.frame(Q1A, Q2A, Q3A...., 6-Q9A))
head(PWEALL.df)
psych::alpha(PWEALL.df) # raw alpha = .88
# Create a new variable in the dataset that is the average of the 19 items. Graph a histogram of this new variable, and change the default settings so the graph looks “nice”.
p$PWE <- rowMeans(PWEALL.df, na.rm = T)
hist(p$PWE, main = "", xlab = "Protestant Work Ethic Scale (1-5)",
col = 'black', bor = 'white')
# Report the mean, standard deviation, and range of this scale, and describe the shape of the distribution that you see in terms of skewness. Then, describe what each of these statistics (mean, sd, range) tells you about people’s beliefs about the protestant work ethic.
describe(p$PWE)
hist(p$testelapse)
200000/60
200000/60/60
range(p$testelapse)
p$testelapse[p$testelapse > 30*60 | p$testelapse < 30]
length(p$testelapse[p$testelapse > 30*60 | p$testelapse < 30])
p[p$testelapse > 30*60 | p$testelapse < 30,]
# Remove these outliers from the dataset. Graph the variable testelapse after the outliers have been removed.
p[p$testelapse > 30*60 | p$testelapse < 30,] # finding my outliers.
p[p$testelapse > 30*60 | p$testelapse < 30,] <- NA
hist(p$testelapse)
range(p$testelapse)
range(p$testelapse, na.rm = T)
range(p$testelapse, na.rm = T)
range(p$testelapse, na.rm = T)
range(p$testelapse, na.rm = T) - mean(p$testelapse, na.rm = T)
sd(p$testelapse, na.rm = T)
(range(p$testelapse, na.rm = T) - mean(p$testelapse, na.rm = T))/sd(p$testelapse, na.rm = T)
p$urban
as.factor(p$urban)
p$urban <- as.factor(p$urban)
p$urban
levels(p$urban)
levels(p$urban) <- c("Rural", "Suburban", "Urban")
p$urban
plot(p$urban)
plot(p$urban, xlab = "What type of area did you live when you were a child?",
col = 'black', bor = 'white')
summary(p$urban)
plot(WC ~ pubdate, data = d) # a quick graph to help me understand the model.
mod <- lm(WC ~ pubdate, data = d) # my model
library(haven) # the tool I found searching online. others might exist! Post on discord if you found another one you think is helpful.
d <- read_sav("objectivityposted.sav")
head(d) # looks good
nrow(d) # seems right
mod <- lm(WC ~ pubdate, data = d) # my model
coef(mod) # the coefficients from my model
plot(WC ~ pubdate, data = d) # a quick graph to help me understand the model.
abline(mod, lwd = 5) # the line.
bucket <- array()
for(i in c(1:1000)){
dx <- d[sample(1:nrow(d), nrow(d), replace = T),]
mx <- lm(WC ~ pubdate, data = dx)
bucket[i] <- coef(mx)[2]
}
par(mfrow = c(1,2))
hist(d$WC)
hist(d$pubdate)
scale(range(d$WC, na.rm = T))
zWC <- scale(d$WC) # z-scoring word count.
range(zWC, na.rm = T) #
range(WC, na.rm = T) # yeah, so the abstract with
range(d$WC, na.rm = T) # yeah, so the abstract with
range(zWC, na.rm = T) # yeah, so the abstract with
d$WC[d$WC > 500]
d$WC[d$WC > 500] <- NA
zWC <- scale(d$WC) # z-scoring word count again; w. the outlier removed.
range(zWC, na.rm = T) # yeah, so the abstract with over 500 words is very different from the others. Let's remove it.
par(mfrow = c(1,2))
hist(d$WC)
hist(d$pubdate)
mod <- lm(WC ~ pubdate, data = d) # my model
coef(mod) # the coefficients from my model
plot(WC ~ pubdate, data = d) # a quick graph to help me understand the model.
abline(mod, lwd = 5) # the line.
bucket <- array()
for(i in c(1:1000)){
dx <- d[sample(1:nrow(d), nrow(d), replace = T),]
mx <- lm(WC ~ pubdate, data = dx)
bucket[i] <- coef(mx)[2]
}
sd(bucket)
hist(bucket)
hist(bucket, main = "Illustration of Sampling Error")
abline(v = coef(mod)[2], lwd = 5)
hist(bucket, main = "Illustration of Sampling Error\nBlack Line = Original Slope\nRed Lines = 95% CI")
abline(v = c(coef(mod)[2] + 1.96*sd(bucket),
coef(mod)[2] - 1.96*sd(bucket)), lwd = 5)
hist(bucket, main = "Illustration of Sampling Error\nBlack Line = Original Slope\nRed Lines = 95% CI")
abline(v = coef(mod)[2], lwd = 5)
abline(v = c(coef(mod)[2] + 1.96*sd(bucket),
coef(mod)[2] - 1.96*sd(bucket)), lwd = 5)
hist(bucket, main = "Illustration of Sampling Error\nBlack Line = Original Slope\nRed Lines = 95% CI")
abline(v = coef(mod)[2], lwd = 5)
abline(v = c(coef(mod)[2] + 1.96*sd(bucket),
coef(mod)[2] - 1.96*sd(bucket)), lwd = 2, col = 'red')
slope95 <- c(coef(mod)[2] + 1.96*sd(bucket),
coef(mod)[2] - 1.96*sd(bucket))
slope95
hist(bucket, main = "Illustration of Sampling Error\nBlack Line = Original Slope\nRed Lines = 95% CI")
abline(v = coef(mod)[2], lwd = 5)
abline(v = slope95, lwd = 2, col = 'red')
ggplot(d, aes(y = WC, x = pubdate)) +
geom_point() +
geom_smooth(method = "lm")
`{r} round(coef(mod)[1], 2)`
round(coef(mod)[1], 2)
round(coef(mod)[2], 2)
round(summary(mod)$r.squared * 100, 2)
summary(mod)$r.squared
round(summary(mod)$r.squared^.5, 2)
par(mfrow = c(2,2))
plot(mod)
hist(mod$residuals)
hist(mod$residuals, breaks = 20)
?I
## model diagnostics
par(mfrow = c(2,2))
mod2 <- lm(WC ~ pubdate + I(pubdate^2), data = d) # adding a quadratic term; I inhibits the ^ from being used as part of the formula, so we can use it to manipulate the variable pubdate.
plot(mod)
plot(mod2)
## model diagnostics
summary(mod)$r.squared
summary(mod2)$r.squared
par(mfrow = c(1,2))
plot(dW$pocu)
dPOC <- d[d$pocu == 1, ]
dW <- d[d$pocu == -1, ]
par(mfrow = c(1,2))
plot(dPOC$pocu)
plot(dW$pocu)
par(mfrow = c(1,2))
plot(as.factor(dPOC$pocu))
plot(as.factor(dW$pocu))
par(mfrow = c(1,2))
plot(as.factor(dPOC$pocu), main = "Scholars of Color")
plot(as.factor(dW$pocu), main = "White Scholars")
par(mfrow = c(1,2))
plot(as.factor(dPOC$pocu), main = "Scholars of Color", ylim = c(1,1000))
plot(as.factor(dW$pocu), main = "White Scholars", ylim = c(1,1000))
par(mfrow = c(1,2))
plot(as.factor(dPOC$pocu), main = "Scholars of Color", ylim = c(1,1500))
plot(as.factor(dW$pocu), main = "White Scholars", ylim = c(1,1500))
par(mfrow = c(1,2)) # the baseR way
hist(dW$power)
hist(dPOC$power)
par(mfrow = c(1,2)) # the baseR way
hist(dW$power, xlim = c(1,20))
hist(dPOC$power, xlim = c(1,20)
)
par(mfrow = c(1,2)) # the baseR way
hist(dW$power, xlim = c(1,20))
hist(dPOC$power, xlim = c(1,20))
par(mfrow = c(1,2)) # the baseR way
hist(dW$power, xlim = c(0,20))
hist(dPOC$power, xlim = c(0,20))
par(mfrow = c(2,2))
plot(mod)
par(mfrow = c(1,2))
plot(as.factor(dPOC$pocu), main = "Scholars of Color", ylim = c(0,1250))
plot(as.factor(dW$pocu), main = "White Scholars", ylim = c(0,1250))
par(mfrow = c(1,2)) # the baseR way
hist(dW$power, xlim = c(0,20))
hist(dPOC$power, xlim = c(0,20))
par(mfrow = c(1,2)) # the baseR way
hist(dW$power, xlim = c(0,20))
hist(dPOC$power, xlim = c(0,20))
par(mfrow = c(1,2))
plot(as.factor(dPOC$pocu), main = "Scholars of Color", ylim = c(0,1250))
plot(as.factor(dW$pocu), main = "White Scholars", ylim = c(0,1250))
length(dW$pocu)
length(dPOC$pocu)
summary(as.factor(d$pocu)) # showing this worked with numbers
summary(as.factor(d$pocu)) # showing this worked with numbers
length(dW$pocu) # same as above.
length(dPOC$pocu) # same as above.
par(mfrow = c(1,2)) # the baseR way
hist(dW$power, xlim = c(0,20))
hist(dPOC$power, xlim = c(0,20))
powerbucket <- matrix(nrow = 1000, ncol = 3)
for(i in c(1:1000)){
dx <- d[sample(1:nrow(d), nrow(d), replace = T),]
dxPOC <- dx[dx$pocu == 1, ]
dxW <- dx[dx$pocu == -1, ]
powerPOC <- mean(dxPOC$power, na.rm = T)
powerW <- mean(dxW$power, na.rm = T)
powerbucket[i,1] <- powerPOC
powerbucket[i,2] <- powerW
powerbucket[i,3] <- powerPOC - powerW
}
names(powerbucket)
names(powerbucket) <- c("SE.POC", "SE.W", "SE.DIFF")
summary(powerbucket)
colnames(powerbucket)
colnames(powerbucket) <- c("SE.POC", "SE.W", "SE.DIFF")
summary(powerbucket)
sd(powerbucket)
lapply(powerbucket, FUN = sd())
sapply(powerbucket, FUN = sd())
sapply(powerbucket, FUN = sd
sapply(powerbucket, FUN = sd)
lapply(powerbucket, FUN = sd)
tapply(powerbucket, FUN = sd)
colmeans(powerbucket)
colMeans(powerbucket)
?sapply
sapply(powerbucket, st)
sapply(powerbucket, sd)
table(powerbucket)
powerbucket
lapply(table(powerbucket), sd)
lapply(powerbucket, 2, sd)
apply(powerbucket, 2, sd)
apply(powerbucket, 2, mean) # takes the matrix, focuses on the columns [2], calculates the mean
apply(powerbucket, 2, sd)
rbind(apply(powerbucket, 2, mean),
apply(powerbucket, 2, sd))
rbind(mean = apply(powerbucket, 2, mean),
sd = apply(powerbucket, 2, sd))# takes the matrix, focuses on the columns [2], calculates the mean
round(rbind(mean = apply(powerbucket, 2, mean),
sd = apply(powerbucket, 2, sd)), 2)# takes the matrix, focuses on the columns [2], calculates the mean
sd(powerbucket[,2]
)
cbind(diffPower - 1.96*sd(powerbucket[,3]),
diffPower + 1.96*sd(powerbucket[,3]))
diffPower <- mean(dPOC$power, na.rm = T) - mean(dW$power, na.rm = T)
cbind(diffPower - 1.96*sd(powerbucket[,3]),
diffPower + 1.96*sd(powerbucket[,3]))
d$POC <- d$pocu
d$POC[d$POC == -1] <- 0
d$POC <- d$pocu
d$POC[d$POC == -1] <- 0
mod2 <- lm(power ~ POC, data = d)
coef(mod2)[2] # the difference
emod <- lm(posemo ~ negemo, data = d)
plot(posemo ~ negemo, data = d)
abline(emod, lwd = 5)
coef(emod)
summary(emod)$r.squared
coef(emod)
cor(d$posemo, d$negemo)
summary(emod)$r.squared
summary(emod)$r.squared^.5
library(psych)
emo.df <- data.frame(d$posemo, d$negemo)
psych:alpha(emo.df)
library(psych)
emo.df <- data.frame(d$posemo, d$negemo)
psych::alpha(emo.df)
f <- rnorm(100, mean = 100, sd = 15)
sample(1:f, 10)
f[sample(1:f, 10), ]
f[sample(1:f, 10)]
f[sample(1:nrow(f), 10)]
f[sample(1:nrow(f), 10)]
sample(1:nrow(f), 10)
1:nrow(f)
f <- rnorm(100, mean = 100, sd = 15)
f[sample(1:nrow(f), 10)]
f <- rnorm(100, mean = 100, sd = 15)
f
f[sample(1:length(f), 10)]
hist(f)
f <- rnorm(100, mean = 100, sd = 15)
o <- rnorm(10, mean = 1000, sd = 150) # outliers appeared
f[sample(1:length(f), 10)] <- o # outliers infiltrated our fake data
hist(f)
mean(f)
median(f)
median(f) - 100 # difference between mean and true mean
emean <- mean(f) - 100 # error = difference between mean and true mean
emedian <- median(f) - 100 # error = difference between mean and true mean
emean > emedian
emean^2 > emedian^2
meany <- array()
meany <- array()
for(i in c(1:1000)){
f <- rnorm(100, mean = 100, sd = 15)
o <- rnorm(10, mean = 1000, sd = 150) # outliers appeared
f[sample(1:length(f), 10)] <- o # outliers infiltrated our fake data
mean(f) # the mean of our fake data w/ outliers
median(f) # the median of our fake data w/ outliers.
emean <- mean(f) - 100 # error = difference between mean and true mean
emedian <- median(f) - 100 # error = difference between mean and true mean
meany[i] <- emean^2 > emedian^2
}
meany
meanmedmatrix <- matrix(nrow = 1000, ncol = 2)
errortest <- array() #
errortest <- array() #
meanmedmatrix <- matrix(nrow = 1000, ncol = 2)
for(i in c(1:1000)){
f <- rnorm(100, mean = 100, sd = 15)
o <- rnorm(10, mean = 1000, sd = 150) # outliers appeared
f[sample(1:length(f), 10)] <- o # outliers infiltrated our fake data
mean(f) # the mean of our fake data w/ outliers
median(f) # the median of our fake data w/ outliers.
emean <- mean(f) - 100 # error = difference between mean and true mean
emedian <- median(f) - 100 # error = difference between mean and true mean
errortest[i] <- emean^2 > emedian^2
meanmedmatrix[i,1] <- emean
meanmedmatrix[i,2] <- emedian
}
sum(errortest)
sum(errortest)/length(errortest) #
meanmedmatrix
colMeans(meanmedmatrix)
colnames(meanmedmatrix)
colnames(meanmedmatrix) <- c("MeanError", "MedianError")
colMeans(meanmedmatrix)
head(d) # looks good
nrow(d) # seems right
cohen.d(d$power, group = as.factor(pocu))
cohen.d(d$power, group = as.factor(d$pocu))
title: "Grad Stats -
title: "Grad Stats -
title: "Grad Stats -
diffPow / sd(diffPow)
## Problem 3. Power Problems (5 points)
In the paper, the researchers predicted that scholars of color would be less likely to use power words (variable = `power`) in their abstracts than White scholars. Test this theory. *(Note: there’s a way to do this using a linear model, but we haven’t yet learned this method, so follow the steps below (or, if you know of another way, feel free to use that method too as long as you get the correct answer! Will chat about the linear model way after the exam.)*
diffPower / sd(diffPower)
diffPower / sd(diffPower, na.rm = T)
diffPower / sd(diffPower, na.rm = T)
sd(diffPower, na.rm = T)
diffPower
diffPower / sd(d$power, na.rm = T)
sd(dPOC$power)
sd(dW$power)
nrow(dPOC)
nrow(dW)
nP <- nrow(dPOC)
nW <- nrow(dW)
var(dPOC$power)
varP <- var(dPOC$power)
varW <- var(dW$power)
nP <- nrow(dPOC)
nW <- nrow(dW)
sdpool <- (((nP - 1)*varP) + ((nW-1)*varW))/(nP + nW - 2)
varpool <- (((nP - 1)*varP) + ((nW-1)*varW))/(nP + nW - 2)
varpool^.5
diffPower / (varpool^.5)
sd(bucket)
sd(powerbucket)
diffPower / sd(powerbucket)
t.test(dW$power, dPOC$power)
diffPower / sd(powerbucket)
sd(powerbucket)
powerbucket[,3]
sd(powerbucket[,3])
seDIFF <- sd(powerbucket[,3]) # sampling error of my difference scores
diffPower / seDIFF
t.test(dW$power, dPOC$power)
t.test(dW$power, dPOC$power, var.equal = TRUE)
t.test(dW$power, dPOC$power, var.equal = FALSE)
diffPower / seDIFF
