---
title: "Class 13 - All Done."
format: html
---

## [Check-In : The Learning Has Stopped (Long Live The Learning)](https://docs.google.com/forms/d/e/1FAIpQLSch8I8aVxQEYssCt7ZUbRcQ9zINsy9yzvqiS4YQca1xJeHh_w/viewform?usp=dialog)

![](images/11_ponyobye.png)

## Part 1 : More MLM (or, in this case, "Linear Mixed Model")

### Example 2 : Another Sleep Dataset

A classic teaching dataset from lmer. Hooray!

```{r}
library(lme4)
?sleepstudy
s <- sleepstudy
```

**A Question :** Is there a relationship between number of days of sleep deprivation and reaction time?

#### The Graph

How might we graph this in ggplot?

::: {.panel-tabset}

##### graph a
```{r}
library(ggplot2)
ggplot(sleepstudy, aes(y = Reaction, x = Days, color = Subject)) + 
  geom_point(size=2) + 
  # facet_wrap(~Subject) + 
  geom_smooth(method = "lm")
```

##### graph b
```{r}
library(ggplot2)
ggplot(sleepstudy, aes(y = Reaction, x = Days, color = Subject)) + 
  geom_point(size=2) + 
  facet_wrap(~Subject) + 
  geom_smooth(method = "lm")
```

:::

#### Interpreting the Model (Random Intercepts)

What model would we define?

-   Reaction is the DV

-   I'm adding Days as a Fixed IV (so I'll get the average effect of \# of days of sleep deprivation on reaction time)

-   I'm also adding a random intercept : `(1 | Subject)` that will estimate how much the intercept (the 1 term) of individual raction times (the level 2 variable) varies by Subject (the level 1 grouping variable).

```{r}
l2 <- lmer(Reaction ~ Days + (1 | Subject), data = sleepstudy)
summary(l2)
```

How do we interpret the results of this model?

-   **Fixed Effects :** these deal with the "average" effects - ignoring all those important individual differences (which are accounted for in the random effects.)

    -   Intercept = 251.41 = the average person's reaction time at 0 days of sleep deprivation is 251.4 milliseconds.

    -   Days = 10.47 = for every day of sleep deprivation, the average person's reaction time increases by 10.47 MS; the standard error is an estimate of how much variation we'd expect in this average slope due

-   **Random Effects :** these describe those individual differences in people's starting responses to the DV (random intercepts) and individual differences in the relationship between the IV and the DV (random slopes).

    -   Subject (Intercept) = 37.12

    -   Residual = 30.99

#### Interpreting the Model (Random Intercepts and Slopes)

**What model would we define?**

```{r}
l3 <- lmer(Reaction ~ Days + (Days | Subject), data = sleepstudy)
summary(l3)
```

**How do we interpret the results of this model?**

-   Fixed Effects :

    -   Intercept :

    -   Slope (Days)

-   Random Effects :

    -   Intercept :

    -   Slope :

-   **Correlation of Fixed Effects :** How are higher levels of one fixed (averaged) coefficient related to the others?

### Example : Nature and Brains and Pain

-   ICE BREAKER : what's your favorite place in the bay area to see nature?

-   THE STUDY : participants were shocked while looking at either virtual nature, urban, or indoor images (while getting their brains scanned).

-   THE TABLE :

    -   What is going on with this models?

    -   What seems important?

    -   What seems irrelevant?

    -   What questions do you have?

| Abstract \[[full article](https://www.nature.com/articles/s41467-025-56870-2#Abs1)\] | Linear Model Table \[[link to SI materials](https://static-content.springer.com/esm/art%3A10.1038%2Fs41467-025-56870-2/MediaObjects/41467_2025_56870_MOESM1_ESM.pdf)\] |
|-------------------------|----------------------------------------------|
| ![](images/clipboard-3811689465.png) | ![](images/clipboard-3359860780.png) |

## Part 2 : Final Project Time

### Show and Tell

-   **Find a Buddy.**

    -   Walk them through your study (Milestone 3)

        -   What is your question (and why should we care)?

        -   How did you measure / manipulate these variables?

    -   Walk them through your graph / results!

-   **Some volunteers to share with the class?** Low-stakes practice for more stressful situations where you will be more prepared :)

### Are We Doing This Correctly?

#### Article : 10 Common Mistakes

1. Absence of an adequate control condition/group
2. Interpreting comparisons between two effects without directly comparing them
3. Spurious correlations
4. Inflating the units of analysis
5. Correlation and causation
6. Use of small samples
7. Circular analysis
8. Flexibility of analysis
9. Failing to correct for multiple comparisons
10. Over-interpreting non-significant results

## Part 3 : Would You Like to Learn More?

### Watch Out for Overfitting in Your Models

When your model is too complex, each variable in the model (parameter) increases the model complexity.

-   ACTIVITY : Where would you draw the line of best fit here?

```{r}
library(ggplot2)
# Fakin' some data.
set.seed(42)
n <- 100
x <- seq(-5, 5, length.out = n)
y <- sin(x) + rnorm(n, sd = 2)
d <- data.frame(x, y)

# Graphin the fake data.
ggplot(d, aes(x, y)) +
  geom_point(size = 2) +
  stat_smooth(method = "lm", formula = y ~ poly(x, 25), se = FALSE, color = "red", size = 2) +
  labs(title = "Overfit Model (25-Degree Polynomial IV") +
  theme_minimal()
```

-   **KEY IDEA :** **complex models that perfectly fit the data are problematic**

    -   You essentially describing your sample, and not the underlying population (which is usually the goal of multiple regression.)

-   We don’t expect over-fit models to generalize to other samples. \[[Image source](https://m-clark.github.io/book-of-models/machine_learning.html#fig-over-under)\]

![](images/clipboard-1581050331.png){fig-align="center" width="516"}

#### Cross Validation : Conceptual Understanding

**Cross Validation.** To ensure your model generalizes to other samples, you can a) replicate, or b) cross-validate your data. Cross validation involves dividing your sample into sub-samples; define a model on one sample, then test the model in the other(s).

![](images/clipboard-2069964306.png){fig-align="center" width="881"}

#### Application : Cross-Validating Our Data (Lecture 10 Interaction Effect)

```{r}
#| echo: true
h <- read.csv("~/Dropbox/!GRADSTATS/Datasets/World Happiness Report - 2024/World-happiness-report-2024.csv", stringsAsFactors = T)
library(ggplot2)
library(jtools)

## Some data cleaning.
h$GDPcat <- ifelse(scale(h$Log.GDP.per.capita) > sd(h$Log.GDP.per.capita, na.rm = T), "High GDP", "Low GDP")
h$GDPcat <- as.factor(h$GDPcat)
# plot(h$GDPcat) # making sure this re-leveling worked.
ggplot(data = subset(h, !is.na(h$GDPcat)), aes(x = scale(Ladder.score), y = scale(Social.support), color = GDPcat)) + 
  geom_point(alpha = .5, position = "jitter") +
  geom_smooth(method = "lm") + labs(title = "Graph of an Interaction Effect from Lecture 10") + ylab("Social Support") + xlab("Happiness (Ladder Score)") +
  theme_apa()
```

**Table : Linear Models for the Interaction Effect from Lecture 10**

```{r}
#| echo: true
mod1 <- lm(scale(Social.support) ~ scale(Log.GDP.per.capita), data = h)
mod2 <- lm(scale(Social.support) ~ scale(Ladder.score), data = h)
mod3 <- lm(scale(Social.support) ~ scale(Log.GDP.per.capita) + scale(Ladder.score), data = h)
mod4 <- lm(scale(Social.support) ~ scale(Ladder.score) * scale(Log.GDP.per.capita), data = h)
export_summs(mod1, mod2, mod3, mod4)
```

**Here's the most simple example of cross-validation ("train-test split"; "holdout cross validation")**

```{r}
sample(0:1, nrow(h), replace = T, prob = c(.7, .3)) # using the sample function

set.seed(424242)
random.selection <- sample(0:1, nrow(h), replace = T, prob = c(.7, .3))
htrain <- h[random.selection == 0,]
htest <- h[random.selection == 1,]

## Model in training Data
train.mod <- lm(Social.support ~ Ladder.score * Log.GDP.per.capita, data = htrain)
summary(train.mod)
predict(train.mod) # the predicted values of the DV, based on our model.

## Applying the model to our testing dataset.
predict(train.mod, newdata = htest) # produces predicted values from our training model, using the testing data.
predval.test <- predict(train.mod, newdata = htest)  # saves these predicted values from the testing dataset.

## Calculating R^2
test.mod.resid <- htest$Social.support - predval.test
SSE <- sum(test.mod.resid^2, na.rm = T)
SSE

test.resid <- htest$Social.support - mean(htest$Social.support, na.rm = T)
SST <- sum(test.resid^2, na.rm = T)
(SST - SSE)/SST
```

#### Interpreting Cross Validation

**You'll often see a few different methods of evaluating model fit.**

-   $R^2$. Our good friend. The proportion of variance explained by the model (vs. the mean)
-   **Rooted Mean Squared Error (RMSE)**. The average amount of residual error (actual - predicted values).
-   **Mean Absoulte Error.** The average of the absolute value of the residuals; less sensitive to outliers than RMSE or $R^2$.

And there are different methods of defining the test and training datasets. And different packages and tutorials to do this. [Here's one, called "Leave one out cross validation - LOOCV"](https://www.sthda.com/english/articles/38-regression-model-validation/157-cross-validation-essentials-in-r/); gif below via Wikipedia.

![](images/10_960px-LOOCV.gif){fig-align="center"}

```{r}
# install.packages("caret")
library(caret)
train.control <- trainControl(method = "LOOCV")
loocvmod <- train(Social.support ~ Ladder.score * Log.GDP.per.capita, data = h, method = "lm",
                  trControl = train.control, na.action = "na.omit")
print(loocvmod)
```

### Watch Out for Multicollinearity.

If your independent variables are highly related, then your multivariate regression slope estimates are not uniquely determined. weird things happen to your coefficients, and this makes it hard to interpret your effects.

-   **IN R :** check the "variance inflation factor" (VIF); a measure of how much one IV is related to all the other IVs in the model. "Tradition" is that if VIF is \> 5 (or I've also seen VIF \> 10) there's a problem in the regression.

-   $\huge VIF_j=\frac{1}{1-R_{j}^{2}}$

```{r}
library(car)

vif(mod4) # doesn't seem like multicollinearity is a problem.

## creating a highly correlated second IV for the sake of this example.
jitter(h$Healthy.life.expectancy, 300)
h$health2 <- jitter(h$Healthy.life.expectancy, 300)

plot(h$health2, h$Healthy.life.expectancy) # yup.

multimod <- lm(Ladder.score ~ Healthy.life.expectancy + health2, data = h) # both in the model.
summary(multimod) # results! Things look good....
vif(multimod) # ...but wait!
```

### Things to Read!

-   [An overview of ML methods (including the partitioning approach) for evaluating models](https://m-clark.github.io/book-of-models/machine_learning.html#sec-ml-generalization)
-   [Another overview of cross-validation methods.](https://www.sthda.com/english/articles/38-regression-model-validation/157-cross-validation-essentials-in-r/)
-   [Some more notes on multicollinearity and VIFs.](https://online.stat.psu.edu/stat462/node/180/)

### A Discussion on Effect Size and the Course and Psychology

#### There's a LOT of Error in Our Predictions of People.

[![Richard, F. D., Bond Jr, C. F., & Stokes-Zoota, J. J. (2003). One hundred years of social psychology quantitatively described. Review of general psychology, 7(4), 331-363.](images/clipboard-512392152.png){width="478"}](https://journals.sagepub.com/doi/pdf/10.1037/1089-2680.7.4.331?casa_token=sPnTYYnQZGkAAAAA:_3gOMjwp8AeQzdcBG2yd3_2FQ3Yuh9Cie_f8GxIPN1n4p9fcjrniLIHbQtUTJZI20WaFNdmUgyPD)

#### But Life, In General, is Complex

-   ARTICLE : [Meyer, G. J., Finn, S. E., Eyde, L. D., Kay, G. G., Moreland, K. L., Dies, R. R., Eisman, E.J., Kubiszyn, & Reed, G. M. (2001). Psychological testing and psychological assessment: A review of evidence and issues. American psychologist, 56(2), 128.](https://www.researchgate.net/profile/Stephen-Finn-4/publication/289963910_Psychological_testing_and_psychological_assessment_A_review_of_evidence_and_issues/links/5a7f3f2baca272a73768210d/Psychological-testing-and-psychological-assessment-A-review-of-evidence-and-issues.pdf)

-   ACTIVITY :

    -   Find a "Hard Science" effect from Table 1. Does the size of this correlation surprise you? Why / why not?

    -   Find a "Psych Science" effect from Table 2. Does the size of this correlation surprise you? Why / why not?

#### Small Effects Matter

-   Funder DC, Ozer DJ. Evaluating Effect Size in Psychological Research: Sense and Nonsense. Advances in Methods and Practices in Psychological Science. 2019;2(2):156-168. [doi:10.1177/2515245919847202](https://journals.sagepub.com/doi/full/10.1177/2515245919847202#bibr28-2515245919847202)

![](images/clipboard-1259562376.png)

#### Doing Good Science is Hard But Important to Be "Valid"

-   Allen C, Mehler DMA (2019) Correction: Open science challenges, benefits and tips in early career and beyond. PLOS Biology 17(12): e3000587. [https://doi.org/10.1371/journal.pbio.3000587](https://doi.org/10.1371/journal.pbio.3000587Allen%20C,%20Mehler%20DMA%20(2019)%20Correction:%20Open%20science%20challenges,%20benefits%20and%20tips%20in%20early%20career%20and%20beyond.%20PLOS%20Biology%2017(12):%20e3000587.%20https://doi.org/10.1371/journal.pbio.3000587)

![Percentages of null findings among RRs and traditional (non-RR) literature \[46,47\], with their respective 95% confidence intervals.](images/clipboard-1668702103.png){width="486"}

#### Other Methods Exist for Describing the World

1.  Qualitative studies.
2.  Philosophy.
3.  Critical Theory.
4.  History.
5.  Art.
6.  Religion / faith.

#### What are the Benefits of Using the Scientific Approach? What are the Limitations (or Dangers)?

##### Is a “Valid” Psychological Science Possible?

^~Farewell! Feel free to stay in touch :) it has been a pleasure and privilege to work with y'all this semester \<3~^

![](images/clipboard-1645322211.png){alt="Farewell! Feel free to stay in touch :) it has been a pleasure and privilege to work with y'all this semester <3"}
