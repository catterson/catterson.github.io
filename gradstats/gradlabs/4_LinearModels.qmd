---
title: "Lecture 4 | Linear Models"
format: 
  html:
    code-overflow: 'wrap'
---

![](images/clipboard-3051119200.png){fig-align="center" width="420"}

## [Check-In Here](https://docs.google.com/forms/d/e/1FAIpQLSe-5mOk5GyxloW5syXGjPQ4zayc4ZaeYHmzkroLxVz7uOP_Jg/viewform?usp=header)

1.  **Load the grad onboarding dataset (name this d to follow along with professor code in lecture).** The variable `can.forloop` asked students whether they could write a for-loop or not. What is the difference in the number of students who said that YES they could for-loop, compared to the number who said either NO or MAYBE? Find a way to get R to calculate this difference using code (hint : use indexing and the summary function.)

    ```{r}
    d <- read.csv("../datasets/Grad Onboard 2025/grad_onboard_SP25.csv",
                               stringsAsFactors = T)
    plot(d$can.forloop)
    summary(d$can.forloop) # finding the values for each group.
    (15 + 10) - 8  # answering question with numbers

    (summary(d$can.forloop)[1] + summary(d$can.forloop)[2]) - summary(d$can.forloop)[3] # answering question with R code. a lot of code!
    s <- summary(d$can.forloop) # saving this output, since using it multiple times
    (s[1] + s[2]) - s[3] # same answer; much easier to look at!
    ```

2.  **Now, write a for-loop to estimate how much sampling error might influence this number.** Generate 1000 new samples, and re-calculate the difference between YES and NO + MAYBEs. What percentage of re-sampled groups would show that there are more people who CAN write a for-loop than NO or MAYBE?

    ```{r}
    bucket <- array()
    set.seed(424242) # "locks" in the randomization so everyone gets the same "random" result; must run immediately before the for-loop.
    for(i in c(1:1000)){
      nd <- d[sample(1:nrow(d), nrow(d), replace = T), ]
      s <- summary(nd$can.forloop)
      bucket[i] <- (s[1] + s[2]) - s[3]
    }
    hist(bucket) # a normal distribution!
    mean(bucket) # very similar to my original answer
    sd(bucket) # the estimate of sampling error
    sum(bucket > 0) # all of the re-sampled slopes show the same positive difference (more people less familiar w/ for-loops than are familiar.)
    ```

3.  **A bonus question.** R wrote you a secret message. Run the code below to see it.

```{r}
#| eval: false
#| include: true
greyScale <- colorRampPalette(c("pink","red"))
secretmessage <- function(r, col){
  t <- seq(0,2*pi,length.out=100)
  x <- r*sin(t)^3
  y <- (13*r/16)*cos(t) - (5*r/16)*cos(2*t) - (2*r/16)*cos(3*t) - (r/16)*cos(4*t)
  polygon(x,y,col=col,border=NA)
}

# create new plot canvas
plot.new()
# limits are approximate here
plot.window(xlim=c(-16,16),ylim=c(-16,13))

# use mapply to loop; invisible to turn off an annoying output.
invisible(mapply(secretmessage,seq(16,0,length.out=100),greyScale(100)))

## source : https://stackoverflow.com/questions/6542825/equation-driven-smoothly-shaded-concentric-shapes
## source : https://stackoverflow.com/questions/12984991/stop-lapply-from-printing-to-console
```

### Announcements & Agenda

**Agenda**

-   9:10 - 9:30 : Check-In and Review
-   9:30 - 10:30 : Linear Models (Basics)
-   10:30 - 11:00 : Break & Presentation
-   11:00 - 12:00 : Linear Models (Continued)

**Announcements**

-   **Discord!?!?**

-   **Lab Keys and Late Assignments.**

    -   would like to post key ASAP.

    -   but late labs + key = TROUBLE. Ideas?

-   **Mini Exam in TWO Weeks.**

    -   I give you data and a question, you generate a report in Quarto.

        -   Data loading and cleaning.

        -   Scale creating & descriptive statistics.

        -   Linear Models (TODAY!)

        -   Bootstrapping

        -   A fun challenge problem worth 1 point.

    -   Take home (9AM-12PM).

    -   Ask questions on ZOOM if / when you have them. Okay? Don't struggle on your own. Plenty of time to do that in other spaces!

    -   We will practice / review next week (Lab 5 a practice exam.)

    -   Think it will be chill, and if not then professor takes the blame, alright?

## RECAP : The Mean as Prediction

```{r}
#| include: false
d <- read.csv("../datasets/Grad Onboard 2025/grad_onboard_SP25.csv",
                           stringsAsFactors = T)
```

### The Mean is a Prediction (of the Sample)

::: panel-tabset
## Where is the Mean?

```{r}
plot(d$self.skills, 
     ylab = "Self-Perception of Skills",
     xlab = "Index") 
abline(h = mean(d$self.skills, na.rm = T), lwd = 0)
```

## There is the Mean!

```{r}
plot(d$self.skills, 
     ylab = "Self-Perception of Skills",
     xlab = "Index") 
abline(h = mean(d$self.skills, na.rm = T), lwd = 5)
```
:::

## There is Error in Our Prediction of the Sample (Residual Error)

This prediction of the sample has some error (residual error). We can (and will need to) quantify this error.

```{r}
## quantifying errors (residuals)
residuals <- d$self.skills - mean(d$self.skills, na.rm = T)
SST <- sum(residuals^2)
SST

SST/length(residuals) # average of squared residuals (variance)
sqrt(SST/length(residuals)) # average of residuals, unsquared (standard deviation)

sd(d$self.skills) # slightly higher
sqrt(SST/(length(residuals)-1)) # the 'real' equation; n-1 to inflate our estimate / adjust for small samples.
```

## The Mean is a Prediction of our Population (with Sampling Error)

```{r}
m <- array()
for(i in c(1:1000)){
 nd <- d[sample(1:nrow(d), nrow(d), replace = T),] # a new sample
 m[i] <- mean(nd$self.skills, na.rm = T)
}
mean(d$self.skills, na.rm = T)
mean(m) # similar!

sum(m > 2.5) # all of them (100% greater than the midpoint of the scale.)

sd(m) # sampling error!

hist(m, xlim = c(1,5)) # our distribution of sampling estimates 
abline(v = c(mean(d$self.skills),
             mean(d$self.skills) + 1.96 * sd(m),
             mean(d$self.skills) - 1.96 * sd(m)),
       lwd = c(5,2,2), # two line widths
       lty = c(1,2,2)) # two line types
```

## Linear Models : Improving our Predictions (Numeric IV)

### The Mean as a Model

Note : I skipped over this in class today; the basic idea is that we can define a linear model to predict a variable from some constant value (1), and the result of that will be the mean, since the mean is our best prediction (minimizes the residual errors) when we don't have any other information about the variable.

```{r}
lm(self.skills ~ 1, data = d) # predicting self.skills from a constant (1), using the datset = d
mod0 <- lm(self.skills ~ 1, data = d) # saving this as a model object
coef(mod0) # looking at the coefficients
mod0$residuals # finding the residuals
```

### Use information in the IV to predict the DV

Let's try the same activity, but now we will graph each individual's self-skill (still on the y-axis) in **relationship** to their perception of their classmates' skill (on the x-axis).

::: panel-tabset
## Where is the Line?

```{r}
plot(jitter(self.skills) ~ class.skills, data = d, 
     ylab = "Self-Perception of Skills",
     xlab = "Perception of Classmates' Skills") 
abline(lm(self.skills ~ class.skills, data = d), lwd = 0)
```

## There is the Line!

```{r}
plot(jitter(self.skills) ~ class.skills, data = d, 
     ylab = "Self-Perception of Skills",
     xlab = "Perception of Classmates' Skills") 
abline(lm(self.skills ~ class.skills, data = d), lwd = 5)
```
:::

**The Linear Model** :

![](images/clipboard-1514248515.png){fig-align="center" width="279"}

To define a linear model, we will first use the `lm()` function to predict some DV from an IV.

Then, we will graph the relationship between these two variables using the `plot()` function. I'm using `jitter()` on the DV in order to shift the points a little, since they are overlapping.

Then, I draw a line (defined by the linear model) using the `abline()` function. I've made the line width = 5 and color red to make it POP.

I can look at the coefficients of the model with the `coef()` function. These coefficients are described by the starting place of the line when the x value is zero (the intercept), and the adjustment we make to Y as the X values increase.

```{r}
mod1 <- lm(self.skills ~ class.skills, data = d)

plot(jitter(self.skills) ~ class.skills, data = d, main = "Jittered Data") # jittered
abline(mod1, lwd = 5, col = 'red')

coef(mod1)
# intercept = 1.95 = the predicted value of Y when ALL X values are ZERO.
# slope = .38 = relationship between class.skills and our DV (self.skills)
### as class.skills increase by ONE, then self.skills will increase by .38
### these units are in the original unit of measurement (1-5 likert scale.)
```

### There is Error in Our Prediction (residual error --\> R\^2)

In the graph above, I can see that the dots are not all exactly on the line. My predictions are wrong; this is residual error! For example, a person with an actual self-skill score of 2 has as a class-skill rating of 1. Our prediction of this person's self-skill score, based on their class-skill rating of 1 is the the result of the linear model :

-   self.skill \~ 1.95 + .38 \* class.skill

-   self.skill \~ 1.95 + .38 \* 1

-   self.skill \~ 2.33

So the person's residual score = the difference between their actual score and our prediction = 2 - 2.33 = -.33. Fortunately, R does the residual calcualtions for us, from the linear model object.

```{r}
mod1$residuals # R does the residual calculation for us. what will happen if we add this up?
sum(mod1$residuals) # they add to....
SSE <- sum(mod1$residuals^2) # so I square them
SSE # the total squared error when I use my model to make predictions.

## Visualizing Our Errors. (distance between actual scores and the line).
par(mfrow = c(1,2))
plot(d$self.skills, 
     ylab = "Self-Perception of Skills",
     xlab = "Index", main = "Mean as Model \n(SST = Total Sum of Squared Errors)") 
abline(h = mean(d$self.skills, na.rm = T), lwd = 5)
plot(jitter(self.skills) ~ class.skills, data = d, main = "Linear Model \n(SSE = Sum of Squared Errors When Model Making Predictions)", 
     xlim = c(1,5)) # jittered
abline(mod1, lwd = 5, col = 'red')


SST <- sum((d$self.skills - mean(d$self.skills))^2) # defining the total error
SST - SSE # a difference in errors when using the mean vs. our model
(SST - SSE)/SST # the relative difference in errors = R^2 (R-squared.)

summary(mod1)$r.squared # R does this for us. But good to do "by hand" to understand.

```

### There is Error in Our Prediction of the Population (sampling error)

Note : I ran out of time to do this in class, but recorded a video at the VERY END of lecture recording that works through this code.

```{r}
#| eval: false
#| include: true
bucket <- array()
for(i in c(1:1000)){
  nd <- d[sample(1:nrow(d), nrow(d), replace = T), ]
  modx <- lm(self.skills ~ class.skills, data = nd)
  bucket[i] <- coef(modx)[2]
}
hist(bucket) # what do we expect to see?
abline(v = mean(bucket), lwd = 5)
abline(v = mean(bucket) + 1.96*sd(bucket), lwd = 2, lty = 'dashed')
abline(v = mean(bucket) - 1.96*sd(bucket), lwd = 2, lty = 'dashed')
mean(bucket)
sd(bucket)



```

### Time for Another Example?

Hah! Next time :)

```{r}
names(d) # what other (numeric, for now) variable might predict self.skills?

```

## BREAK TIME : Meet Back at 10:40

## [Presentations](https://docs.google.com/presentation/d/1YZQ45_oj6TgiSIUpU7N6Ek5iTk2nn4T5RIpCz6Xi1K4/edit?usp=sharing)

## FOR LAB 4.

1.  Define linear models to predict a numeric DV from a numeric IV.
2.  Interpret the intercept, slope, and R\^2 value.
3.  Do some bootstrapping.
4.  Repeat w/ a different dataset.
5.  Play around with ggplot2()
6.  Find and evaluate a graph.
