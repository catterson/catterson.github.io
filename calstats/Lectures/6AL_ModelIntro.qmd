---
title: "Introduction to Linear Models"
format: 
  html:
      code-overflow: wrap
---

## CANVAS DOWN: ACCESS COURSE MATERIALS HERE : https://tinyurl.com/calstatFA25

## [CHECK-IN : Post Mini-Exam Survey!](https://docs.google.com/forms/d/e/1FAIpQLSeQnoTd_QK7Lp91-T98IyCR0p0gqWNJiAPUAcEFmpbE7Qv0qg/viewform?usp=header)

![](images/clipboard-3082930128.png){width="498"}

### Announcements

-   **R Exam Is Over!**
    -   The one reader working hard to grade!
    -   Key + "Learning from the R Exam" will post after all the exams are graded.
-   **Final Project Milestone #3**
    -   See Lecture 5 Notes for a guide.
    -   Goal :
        -   Start collecting data (in Google Forms); take other people's surveys.
        -   **Try to have your final project data collected in the next week :** will analyze project data as part of lab assignments.
        -   DONE IS BETTER THAN PERFECT. (But make sure your DV is measured with NUMBERS. This is critical.)

### Agenda

-   3:10 - 3:30 \| Check-In and Announcements / Exam Debrief.

-   3:30 - 4:15 \| The Linear Model (PREDICTION IS A LINE)

-   4:15 - 4:30 \| Break Time

-   4:30 - 5:00 \| The Linear Model (ERROR IN PREDICTIONS)

-   5:00 - 5:30 \| Reliability and Validity \[RECAP\]

-   5:30 - 6:00 \| Final Project Workshop (Milestone #3)

## The Linear Model : Prediction is a Line

### [Link to Professor R Script](https://www.dropbox.com/scl/fi/acc9v4cvhpgxfwyq3fwnl/6A_IntroductionToLinearModels.R?rlkey=r1ux1fcu8qdnmd7kuftpp1qdi&dl=0)

### RECAP : The Mean as Prediction

Previously, we discussed how the mean could be used to make predictions of individuals.

$\huge y_i = \hat{Y} + \epsilon_i$

::: column-margin
$\Large y_i$ = the DV = the individual’s actual score we are trying to predict (remember $_i$ = index; a specific individual.)

-   **on the graph:** each individual dot (on the y-axis; the x-axis just describes when people submitted the survey.

$\Large \hat{Y}$ = our prediction (the mean).

-   **on the graph:** the solid red line

$\Large \epsilon$ = residual error = distance between the predicted values of y and the individual's actual value of y

-   **on the graph:** the distance between each dot and the line.
:::

```{r}
d <- read.csv("~/Dropbox/!WHY STATS/Class Datasets/101 - Class Datasets/Mini Data/mini_DATA.csv", stringsAsFactors = T)
plot(d$insta.follows, main = "Mean as a Model (Red Line)",
     xlab = "Index (Row in Dataset)",
     ylab = "# Of Accounts a Person Follows")
abline(h = mean(d$insta.follows, na.rm = T), lwd = 5, col = 'red')
```

We also talked about how we could quantify the total error in these predictions, by adding up the squared residual errors (the sum of squared errors).

```{r}
residual <- d$insta.follows - mean(d$insta.follows, na.rm = T)
SST <- sum(residual^2, na.rm = T)
SST
```

This number made no sense, but it is a critical statistic, since it quantifies how valid our predictions of individuals were when using the mean to make predictions.

To give the statistics some context, we divided the sum of squared errors by the sample size (this is the variance) and then un-squared this number (by taking the square root). This new statistic - the standard deviation - served as an average of residual error that describes how far the average person differs from the mean.

```{r}
n <- length(na.omit(d$insta.follows)) # total number of individuals; omitting missing data.
sqrt(SST/(n-1)) # the equation for the standard deviation
sd(d$insta.follows, na.rm = T) # the function to get the same answer.
```

As scientists, our goal is to make accurate predictions of individuals. So we would want to find a way to make the sum of squared errors equal zero - have no error in our predictions. The mean is a good starting place, but it's one number. And people are complex.

|  |  |
|------------------------------------------|------------------------------|
| **the mean** | **the linear model ™ ©** |
| ![](lecture_images/6L_egg1.gif){width="324"} | ![](lecture_images/6L_egg2.gif) |

**The mean is an okay starting place for our predictions, but we can try to do better!**

**DISCUSS :**

-   **ICE BREAKER :** if you had to live inside one social media platform, what would it be and why???

-   **THINK ABOUT A LINEAR MODEL :** how do you think the variables (above) would help (or not help) us predict the number of accounts someone follows on instagram (insta.follows)? Why / why not???

```{r}
names(d[,sapply(d, is.numeric)])
```

|  |  |
|---------------------------------|---------------------------------------|
| **Variables we think would help us make predictions** | **Variables we think would not help us make predictions** |
|  |  |

### The Linear Model in FOUR EASY STEPS.

The model is a line that updates our predictions of one variable based on knowledge of another.

1.  Define your model : what is your DV? What are your IVs? How do you think they will be related???

2.  Graph your DV and IV(s) : make sure the data look good.

```{r}
par(mfrow = c(1,2))
hist(d$insta.follows)
hist(d$socmeduse)
```

2.  Plot the relationship between the two variables.

```{r}
plot(insta.follows ~ socmeduse, data = d)
```

3.  Define the linear model and interpret the intercept and slope of the model.

```{r}
mod <- lm(insta.follows ~ socmeduse, data = d) # defines the model; saves as mod
plot(insta.follows ~ socmeduse, data = d) # graphs the relationship.
abline(mod, lwd = 5, col = 'red') # draws a red line of width five based on mod
coef(mod) # shows us the terms inside mod.
```

**equation for a line : y = a + bX**

::: column-margin
$\Large y_i$ = the DV = each individual’s actual score on the dependent variable.

-   **on the graph:** the value of each dot on the y-axis

$\Large a$ = the intercept = the starting place for our prediction. You can think of the intercept as "the predicted value of y when all x values are zero”.)

-   **on the graph:** the value of the line at X = 0

$\Large X_i$ = the IV = the individual's actual score on the independent variable (a different variable than the DV).

-   **on the graph:** the value of each dot on the x-axis

$\Large b_1$ = the slope = an adjustment we make in our prediction of y, based on the individual's x value.

-   **on the graph:** how much the line increases in y value when x-values increase by 1 unit.

$\Large \epsilon_i$ = residual error = the distance between our prediction and the individual's actual y value.

-   **on the graph:** the distance between each individual data point and the line.
:::

## Activity : Define another model to predict insta.follows from another numeric IV!!!

```{r}
## Student Examples Go Here?
```

## BREAK TIME : MEET BACK AT 4:30

![](images/clipboard-2931715095.png)

## The Linear Model : Error in Our Predictions

```{r}
par(mfrow = c(1,2))
plot(d$insta.follows, main = "Using the Mean To Make Predictions (Black Line)",
     xlab = "Index (Row in Dataset)",
     ylab = "# Of Accounts a Person Follows")
abline(h = mean(d$insta.follows, na.rm = T), lwd = 5, col = 'black')

plot(insta.follows ~ socmeduse, data = d,
     main = "Using Linear Model to Make Predictions") # graphs the relationship 
abline(mod, lwd = 5, col = 'red') # draws a red line of width five based on mod
```

```{r}
residual <- d$insta.follows - mean(d$insta.follows, na.rm = T)
SST <- sum(residual^2, na.rm = T)
SST

head(mod$residuals) # the residuals from my model
sum(mod$residuals) # add up to zero

SSM <- sum(mod$residuals^2) 
SSM
SST - SSM
(SST - SSM)/SST
```


